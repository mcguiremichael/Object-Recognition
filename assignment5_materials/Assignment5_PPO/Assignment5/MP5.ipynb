{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:255: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:256: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:257: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -1.446995. Value loss: 7.394697. Entropy: 1.790212.\n",
      "Iteration 2: Policy loss: -1.446225. Value loss: 6.261855. Entropy: 1.785242.\n",
      "Iteration 3: Policy loss: -1.420722. Value loss: 5.935813. Entropy: 1.780717.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -5.571462. Value loss: 66.334053. Entropy: 1.766625.\n",
      "Iteration 5: Policy loss: -5.743255. Value loss: 58.227165. Entropy: 1.776056.\n",
      "Iteration 6: Policy loss: -5.517460. Value loss: 54.872612. Entropy: 1.779818.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -2.662891. Value loss: 43.241081. Entropy: 1.764398.\n",
      "Iteration 8: Policy loss: -2.797832. Value loss: 45.214840. Entropy: 1.773946.\n",
      "Iteration 9: Policy loss: -2.675509. Value loss: 39.119541. Entropy: 1.762149.\n",
      "now time :  2019-02-25 18:39:54.087786\n",
      "episode: 1   score: 160.0  epsilon: 1.0    steps: 596  evaluation reward: 160.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -1.171639. Value loss: 41.231827. Entropy: 1.752817.\n",
      "Iteration 11: Policy loss: -1.252181. Value loss: 39.284859. Entropy: 1.755325.\n",
      "Iteration 12: Policy loss: -1.423982. Value loss: 40.109615. Entropy: 1.746622.\n",
      "episode: 2   score: 130.0  epsilon: 1.0    steps: 239  evaluation reward: 145.0\n",
      "episode: 3   score: 180.0  epsilon: 1.0    steps: 843  evaluation reward: 156.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -2.246829. Value loss: 60.771400. Entropy: 1.761159.\n",
      "Iteration 14: Policy loss: -2.141927. Value loss: 54.244881. Entropy: 1.755545.\n",
      "Iteration 15: Policy loss: -2.055633. Value loss: 45.924706. Entropy: 1.743846.\n",
      "episode: 4   score: 180.0  epsilon: 1.0    steps: 462  evaluation reward: 162.5\n",
      "episode: 5   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 166.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: 0.035408. Value loss: 25.273125. Entropy: 1.746534.\n",
      "Iteration 17: Policy loss: -0.015540. Value loss: 21.869335. Entropy: 1.732552.\n",
      "Iteration 18: Policy loss: -0.137436. Value loss: 21.788021. Entropy: 1.733069.\n",
      "episode: 6   score: 170.0  epsilon: 1.0    steps: 34  evaluation reward: 166.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: 0.958232. Value loss: 11.837079. Entropy: 1.711352.\n",
      "Iteration 20: Policy loss: 0.942267. Value loss: 9.430126. Entropy: 1.711605.\n",
      "Iteration 21: Policy loss: 0.861770. Value loss: 10.002069. Entropy: 1.718169.\n",
      "episode: 7   score: 105.0  epsilon: 1.0    steps: 622  evaluation reward: 157.85714285714286\n",
      "episode: 8   score: 230.0  epsilon: 1.0    steps: 928  evaluation reward: 166.875\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: -3.440964. Value loss: 294.776550. Entropy: 1.696634.\n",
      "Iteration 23: Policy loss: -3.688741. Value loss: 252.079834. Entropy: 1.707084.\n",
      "Iteration 24: Policy loss: -3.228233. Value loss: 192.541946. Entropy: 1.662772.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.995619. Value loss: 43.219414. Entropy: 1.562394.\n",
      "Iteration 26: Policy loss: 1.123569. Value loss: 34.318806. Entropy: 1.575412.\n",
      "Iteration 27: Policy loss: 0.985438. Value loss: 26.049007. Entropy: 1.587204.\n",
      "episode: 9   score: 210.0  epsilon: 1.0    steps: 890  evaluation reward: 171.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: -0.276490. Value loss: 30.200066. Entropy: 1.652518.\n",
      "Iteration 29: Policy loss: -0.229061. Value loss: 23.824738. Entropy: 1.653607.\n",
      "Iteration 30: Policy loss: -0.199830. Value loss: 22.085642. Entropy: 1.657205.\n",
      "episode: 10   score: 465.0  epsilon: 1.0    steps: 297  evaluation reward: 201.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: 0.644865. Value loss: 32.285946. Entropy: 1.658492.\n",
      "Iteration 32: Policy loss: 0.675133. Value loss: 21.702734. Entropy: 1.655264.\n",
      "Iteration 33: Policy loss: 0.578832. Value loss: 18.058910. Entropy: 1.657555.\n",
      "episode: 11   score: 225.0  epsilon: 1.0    steps: 391  evaluation reward: 203.1818181818182\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: -0.425891. Value loss: 38.242188. Entropy: 1.667881.\n",
      "Iteration 35: Policy loss: -0.300188. Value loss: 28.726089. Entropy: 1.668301.\n",
      "Iteration 36: Policy loss: -0.177911. Value loss: 27.088165. Entropy: 1.666305.\n",
      "episode: 12   score: 150.0  epsilon: 1.0    steps: 121  evaluation reward: 198.75\n",
      "episode: 13   score: 20.0  epsilon: 1.0    steps: 608  evaluation reward: 185.0\n",
      "episode: 14   score: 145.0  epsilon: 1.0    steps: 670  evaluation reward: 182.14285714285714\n",
      "episode: 15   score: 185.0  epsilon: 1.0    steps: 951  evaluation reward: 182.33333333333334\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.991565. Value loss: 58.491386. Entropy: 1.680510.\n",
      "Iteration 38: Policy loss: 0.532041. Value loss: 40.156742. Entropy: 1.674566.\n",
      "Iteration 39: Policy loss: 0.527068. Value loss: 40.772968. Entropy: 1.673373.\n",
      "episode: 16   score: 55.0  epsilon: 1.0    steps: 773  evaluation reward: 174.375\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: -1.128504. Value loss: 44.989510. Entropy: 1.665036.\n",
      "Iteration 41: Policy loss: -1.437954. Value loss: 33.417694. Entropy: 1.670369.\n",
      "Iteration 42: Policy loss: -1.251955. Value loss: 28.867140. Entropy: 1.675068.\n",
      "episode: 17   score: 345.0  epsilon: 1.0    steps: 213  evaluation reward: 184.41176470588235\n",
      "episode: 18   score: 50.0  epsilon: 1.0    steps: 287  evaluation reward: 176.94444444444446\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.579843. Value loss: 52.735516. Entropy: 1.675708.\n",
      "Iteration 44: Policy loss: 0.824690. Value loss: 36.588913. Entropy: 1.677392.\n",
      "Iteration 45: Policy loss: 0.754573. Value loss: 29.858822. Entropy: 1.663478.\n",
      "episode: 19   score: 40.0  epsilon: 1.0    steps: 713  evaluation reward: 169.73684210526315\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: -0.302019. Value loss: 43.452095. Entropy: 1.678288.\n",
      "Iteration 47: Policy loss: -0.091004. Value loss: 34.854595. Entropy: 1.683138.\n",
      "Iteration 48: Policy loss: -0.219476. Value loss: 27.201712. Entropy: 1.678797.\n",
      "episode: 20   score: 70.0  epsilon: 1.0    steps: 852  evaluation reward: 164.75\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: -1.907104. Value loss: 48.858833. Entropy: 1.693816.\n",
      "Iteration 50: Policy loss: -1.722910. Value loss: 34.000202. Entropy: 1.676336.\n",
      "Iteration 51: Policy loss: -1.698303. Value loss: 31.542427. Entropy: 1.690667.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.499227. Value loss: 50.417519. Entropy: 1.683939.\n",
      "Iteration 53: Policy loss: 0.366839. Value loss: 35.988960. Entropy: 1.678948.\n",
      "Iteration 54: Policy loss: 0.610924. Value loss: 27.101316. Entropy: 1.690167.\n",
      "episode: 21   score: 345.0  epsilon: 1.0    steps: 406  evaluation reward: 173.33333333333334\n",
      "episode: 22   score: 240.0  epsilon: 1.0    steps: 984  evaluation reward: 176.36363636363637\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 1.798833. Value loss: 45.717964. Entropy: 1.698603.\n",
      "Iteration 56: Policy loss: 2.030869. Value loss: 29.991014. Entropy: 1.673492.\n",
      "Iteration 57: Policy loss: 1.687965. Value loss: 25.084625. Entropy: 1.678155.\n",
      "episode: 23   score: 245.0  epsilon: 1.0    steps: 601  evaluation reward: 179.34782608695653\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.673278. Value loss: 54.660183. Entropy: 1.684825.\n",
      "Iteration 59: Policy loss: 0.604435. Value loss: 43.241238. Entropy: 1.699870.\n",
      "Iteration 60: Policy loss: 0.556153. Value loss: 37.506248. Entropy: 1.688204.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 1.637470. Value loss: 27.933174. Entropy: 1.668561.\n",
      "Iteration 62: Policy loss: 1.413434. Value loss: 17.671940. Entropy: 1.674493.\n",
      "Iteration 63: Policy loss: 1.682651. Value loss: 14.753959. Entropy: 1.660259.\n",
      "episode: 24   score: 270.0  epsilon: 1.0    steps: 16  evaluation reward: 183.125\n",
      "episode: 25   score: 345.0  epsilon: 1.0    steps: 202  evaluation reward: 189.6\n",
      "episode: 26   score: 340.0  epsilon: 1.0    steps: 318  evaluation reward: 195.3846153846154\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: -3.782067. Value loss: 274.596100. Entropy: 1.582310.\n",
      "Iteration 65: Policy loss: -3.017450. Value loss: 278.639557. Entropy: 1.522673.\n",
      "Iteration 66: Policy loss: -4.017591. Value loss: 208.521103. Entropy: 1.494853.\n",
      "episode: 27   score: 285.0  epsilon: 1.0    steps: 689  evaluation reward: 198.7037037037037\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 2.340363. Value loss: 78.998169. Entropy: 1.596805.\n",
      "Iteration 68: Policy loss: 2.178692. Value loss: 51.851456. Entropy: 1.594365.\n",
      "Iteration 69: Policy loss: 2.174268. Value loss: 47.836163. Entropy: 1.588418.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 3.177228. Value loss: 63.958893. Entropy: 1.593751.\n",
      "Iteration 71: Policy loss: 3.413423. Value loss: 48.222702. Entropy: 1.610063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72: Policy loss: 3.240810. Value loss: 47.459927. Entropy: 1.634043.\n",
      "episode: 28   score: 30.0  epsilon: 1.0    steps: 173  evaluation reward: 192.67857142857142\n",
      "episode: 29   score: 150.0  epsilon: 1.0    steps: 598  evaluation reward: 191.20689655172413\n",
      "episode: 30   score: 180.0  epsilon: 1.0    steps: 781  evaluation reward: 190.83333333333334\n",
      "episode: 31   score: 275.0  epsilon: 1.0    steps: 1005  evaluation reward: 193.5483870967742\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 3.627331. Value loss: 48.241913. Entropy: 1.699124.\n",
      "Iteration 74: Policy loss: 3.352917. Value loss: 32.753712. Entropy: 1.667349.\n",
      "Iteration 75: Policy loss: 3.690335. Value loss: 27.435455. Entropy: 1.681661.\n",
      "episode: 32   score: 55.0  epsilon: 1.0    steps: 311  evaluation reward: 189.21875\n",
      "episode: 33   score: 305.0  epsilon: 1.0    steps: 494  evaluation reward: 192.72727272727272\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.006630. Value loss: 39.162506. Entropy: 1.692830.\n",
      "Iteration 77: Policy loss: 0.462171. Value loss: 34.250576. Entropy: 1.690605.\n",
      "Iteration 78: Policy loss: -0.001862. Value loss: 32.161518. Entropy: 1.698197.\n",
      "episode: 34   score: 180.0  epsilon: 1.0    steps: 96  evaluation reward: 192.35294117647058\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: 0.229834. Value loss: 33.389721. Entropy: 1.695337.\n",
      "Iteration 80: Policy loss: 0.020383. Value loss: 28.950918. Entropy: 1.694462.\n",
      "Iteration 81: Policy loss: -0.246597. Value loss: 26.849745. Entropy: 1.680301.\n",
      "episode: 35   score: 65.0  epsilon: 1.0    steps: 252  evaluation reward: 188.71428571428572\n",
      "episode: 36   score: 135.0  epsilon: 1.0    steps: 702  evaluation reward: 187.22222222222223\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 1.540910. Value loss: 43.158100. Entropy: 1.673335.\n",
      "Iteration 83: Policy loss: 1.726195. Value loss: 35.506107. Entropy: 1.671436.\n",
      "Iteration 84: Policy loss: 1.642049. Value loss: 26.987736. Entropy: 1.655934.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.925224. Value loss: 32.333214. Entropy: 1.691281.\n",
      "Iteration 86: Policy loss: -0.582384. Value loss: 21.960552. Entropy: 1.681947.\n",
      "Iteration 87: Policy loss: -0.459578. Value loss: 22.777189. Entropy: 1.679479.\n",
      "episode: 37   score: 185.0  epsilon: 1.0    steps: 573  evaluation reward: 187.16216216216216\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 1.413806. Value loss: 27.752081. Entropy: 1.687181.\n",
      "Iteration 89: Policy loss: 1.414245. Value loss: 20.710123. Entropy: 1.690823.\n",
      "Iteration 90: Policy loss: 1.334301. Value loss: 18.739441. Entropy: 1.693133.\n",
      "episode: 38   score: 85.0  epsilon: 1.0    steps: 308  evaluation reward: 184.47368421052633\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: -0.224994. Value loss: 37.966396. Entropy: 1.686584.\n",
      "Iteration 92: Policy loss: -0.411850. Value loss: 31.183945. Entropy: 1.680820.\n",
      "Iteration 93: Policy loss: -0.425004. Value loss: 26.806940. Entropy: 1.685445.\n",
      "episode: 39   score: 105.0  epsilon: 1.0    steps: 31  evaluation reward: 182.43589743589743\n",
      "episode: 40   score: 45.0  epsilon: 1.0    steps: 135  evaluation reward: 179.0\n",
      "episode: 41   score: 170.0  epsilon: 1.0    steps: 435  evaluation reward: 178.78048780487805\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: -0.458486. Value loss: 55.393730. Entropy: 1.692408.\n",
      "Iteration 95: Policy loss: -0.610270. Value loss: 44.972561. Entropy: 1.705125.\n",
      "Iteration 96: Policy loss: -0.479335. Value loss: 37.516693. Entropy: 1.706530.\n",
      "episode: 42   score: 215.0  epsilon: 1.0    steps: 884  evaluation reward: 179.64285714285714\n",
      "episode: 43   score: 260.0  epsilon: 1.0    steps: 958  evaluation reward: 181.51162790697674\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: 0.460537. Value loss: 36.013645. Entropy: 1.723159.\n",
      "Iteration 98: Policy loss: 0.497678. Value loss: 29.024031. Entropy: 1.719831.\n",
      "Iteration 99: Policy loss: 0.408539. Value loss: 23.435452. Entropy: 1.722728.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.314281. Value loss: 33.403172. Entropy: 1.695774.\n",
      "Iteration 101: Policy loss: 0.590650. Value loss: 29.482677. Entropy: 1.693150.\n",
      "Iteration 102: Policy loss: 0.428774. Value loss: 21.888617. Entropy: 1.709129.\n",
      "episode: 44   score: 30.0  epsilon: 1.0    steps: 176  evaluation reward: 178.0681818181818\n",
      "episode: 45   score: 195.0  epsilon: 1.0    steps: 538  evaluation reward: 178.44444444444446\n",
      "episode: 46   score: 325.0  epsilon: 1.0    steps: 710  evaluation reward: 181.6304347826087\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: -0.808273. Value loss: 57.277004. Entropy: 1.705740.\n",
      "Iteration 104: Policy loss: -1.069498. Value loss: 46.932983. Entropy: 1.705136.\n",
      "Iteration 105: Policy loss: -0.867716. Value loss: 40.245125. Entropy: 1.704530.\n",
      "episode: 47   score: 140.0  epsilon: 1.0    steps: 52  evaluation reward: 180.74468085106383\n",
      "episode: 48   score: 45.0  epsilon: 1.0    steps: 992  evaluation reward: 177.91666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: -0.303424. Value loss: 35.340393. Entropy: 1.729056.\n",
      "Iteration 107: Policy loss: -0.265897. Value loss: 25.453333. Entropy: 1.721033.\n",
      "Iteration 108: Policy loss: -0.313661. Value loss: 23.245186. Entropy: 1.723005.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: -1.070686. Value loss: 50.648014. Entropy: 1.717765.\n",
      "Iteration 110: Policy loss: -1.263357. Value loss: 39.687149. Entropy: 1.706888.\n",
      "Iteration 111: Policy loss: -1.456929. Value loss: 34.911526. Entropy: 1.711811.\n",
      "episode: 49   score: 280.0  epsilon: 1.0    steps: 287  evaluation reward: 180.0\n",
      "episode: 50   score: 135.0  epsilon: 1.0    steps: 706  evaluation reward: 179.1\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: 1.014139. Value loss: 35.170082. Entropy: 1.675880.\n",
      "Iteration 113: Policy loss: 1.327310. Value loss: 17.717648. Entropy: 1.688061.\n",
      "Iteration 114: Policy loss: 1.377611. Value loss: 15.637615. Entropy: 1.690664.\n",
      "now time :  2019-02-25 18:41:51.707343\n",
      "episode: 51   score: 170.0  epsilon: 1.0    steps: 410  evaluation reward: 178.92156862745097\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: -2.298936. Value loss: 61.685974. Entropy: 1.733534.\n",
      "Iteration 116: Policy loss: -2.258438. Value loss: 40.115982. Entropy: 1.722196.\n",
      "Iteration 117: Policy loss: -2.206883. Value loss: 33.914318. Entropy: 1.719165.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 1.112201. Value loss: 31.742687. Entropy: 1.715650.\n",
      "Iteration 119: Policy loss: 1.117084. Value loss: 21.193136. Entropy: 1.713368.\n",
      "Iteration 120: Policy loss: 1.009973. Value loss: 18.218517. Entropy: 1.707261.\n",
      "episode: 52   score: 165.0  epsilon: 1.0    steps: 55  evaluation reward: 178.65384615384616\n",
      "episode: 53   score: 225.0  epsilon: 1.0    steps: 135  evaluation reward: 179.52830188679246\n",
      "episode: 54   score: 230.0  epsilon: 1.0    steps: 815  evaluation reward: 180.46296296296296\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: 0.880536. Value loss: 51.811008. Entropy: 1.693493.\n",
      "Iteration 122: Policy loss: 0.802849. Value loss: 37.444580. Entropy: 1.689590.\n",
      "Iteration 123: Policy loss: 0.765367. Value loss: 32.413498. Entropy: 1.683557.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: -1.922021. Value loss: 64.890572. Entropy: 1.697756.\n",
      "Iteration 125: Policy loss: -2.157342. Value loss: 50.053356. Entropy: 1.696347.\n",
      "Iteration 126: Policy loss: -1.826155. Value loss: 41.708687. Entropy: 1.690176.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 1.681300. Value loss: 38.420181. Entropy: 1.716430.\n",
      "Iteration 128: Policy loss: 1.549893. Value loss: 29.375671. Entropy: 1.724085.\n",
      "Iteration 129: Policy loss: 1.494140. Value loss: 23.355625. Entropy: 1.731541.\n",
      "episode: 55   score: 300.0  epsilon: 1.0    steps: 549  evaluation reward: 182.63636363636363\n",
      "episode: 56   score: 155.0  epsilon: 1.0    steps: 654  evaluation reward: 182.14285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 1.661342. Value loss: 33.874485. Entropy: 1.699284.\n",
      "Iteration 131: Policy loss: 1.598543. Value loss: 21.786894. Entropy: 1.699288.\n",
      "Iteration 132: Policy loss: 1.735163. Value loss: 17.794155. Entropy: 1.694766.\n",
      "episode: 57   score: 335.0  epsilon: 1.0    steps: 371  evaluation reward: 184.82456140350877\n",
      "episode: 58   score: 190.0  epsilon: 1.0    steps: 983  evaluation reward: 184.91379310344828\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.525161. Value loss: 51.568081. Entropy: 1.689142.\n",
      "Iteration 134: Policy loss: 0.472924. Value loss: 34.846043. Entropy: 1.676441.\n",
      "Iteration 135: Policy loss: 0.790344. Value loss: 27.416672. Entropy: 1.678120.\n",
      "episode: 59   score: 135.0  epsilon: 1.0    steps: 184  evaluation reward: 184.0677966101695\n",
      "episode: 60   score: 110.0  epsilon: 1.0    steps: 882  evaluation reward: 182.83333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.700322. Value loss: 30.559502. Entropy: 1.699408.\n",
      "Iteration 137: Policy loss: 0.785025. Value loss: 13.814157. Entropy: 1.713538.\n",
      "Iteration 138: Policy loss: 0.628580. Value loss: 13.518687. Entropy: 1.705960.\n",
      "episode: 61   score: 390.0  epsilon: 1.0    steps: 397  evaluation reward: 186.2295081967213\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 3.586385. Value loss: 19.816984. Entropy: 1.691988.\n",
      "Iteration 140: Policy loss: 3.399038. Value loss: 13.670723. Entropy: 1.685280.\n",
      "Iteration 141: Policy loss: 3.514662. Value loss: 12.947233. Entropy: 1.687480.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 0.961873. Value loss: 29.570498. Entropy: 1.676178.\n",
      "Iteration 143: Policy loss: 1.036892. Value loss: 23.187536. Entropy: 1.679691.\n",
      "Iteration 144: Policy loss: 1.120178. Value loss: 20.589661. Entropy: 1.672894.\n",
      "episode: 62   score: 260.0  epsilon: 1.0    steps: 6  evaluation reward: 187.41935483870967\n",
      "episode: 63   score: 105.0  epsilon: 1.0    steps: 379  evaluation reward: 186.11111111111111\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: -1.411177. Value loss: 37.458057. Entropy: 1.684188.\n",
      "Iteration 146: Policy loss: -1.330418. Value loss: 26.643661. Entropy: 1.685297.\n",
      "Iteration 147: Policy loss: -1.209197. Value loss: 24.228819. Entropy: 1.677265.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: -1.487658. Value loss: 37.388145. Entropy: 1.672288.\n",
      "Iteration 149: Policy loss: -1.530768. Value loss: 29.797777. Entropy: 1.666796.\n",
      "Iteration 150: Policy loss: -1.396371. Value loss: 22.432371. Entropy: 1.672211.\n",
      "episode: 64   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 186.484375\n",
      "episode: 65   score: 295.0  epsilon: 1.0    steps: 737  evaluation reward: 188.15384615384616\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: -1.772294. Value loss: 320.608093. Entropy: 1.621269.\n",
      "Iteration 152: Policy loss: -1.683404. Value loss: 303.300964. Entropy: 1.570128.\n",
      "Iteration 153: Policy loss: -1.341015. Value loss: 228.298920. Entropy: 1.592802.\n",
      "episode: 66   score: 380.0  epsilon: 1.0    steps: 221  evaluation reward: 191.06060606060606\n",
      "episode: 67   score: 155.0  epsilon: 1.0    steps: 777  evaluation reward: 190.52238805970148\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 1.730412. Value loss: 22.975815. Entropy: 1.533395.\n",
      "Iteration 155: Policy loss: 1.582055. Value loss: 17.744448. Entropy: 1.534244.\n",
      "Iteration 156: Policy loss: 1.925433. Value loss: 13.839067. Entropy: 1.542234.\n",
      "episode: 68   score: 50.0  epsilon: 1.0    steps: 259  evaluation reward: 188.4558823529412\n",
      "episode: 69   score: 180.0  epsilon: 1.0    steps: 473  evaluation reward: 188.33333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: -0.242956. Value loss: 41.370571. Entropy: 1.622165.\n",
      "Iteration 158: Policy loss: -0.380866. Value loss: 33.794716. Entropy: 1.624151.\n",
      "Iteration 159: Policy loss: 0.006609. Value loss: 28.855694. Entropy: 1.596394.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: -0.022186. Value loss: 13.705695. Entropy: 1.648526.\n",
      "Iteration 161: Policy loss: 0.189708. Value loss: 12.673933. Entropy: 1.652248.\n",
      "Iteration 162: Policy loss: -0.143116. Value loss: 11.420367. Entropy: 1.641744.\n",
      "episode: 70   score: 160.0  epsilon: 1.0    steps: 33  evaluation reward: 187.92857142857142\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 1.083781. Value loss: 36.718102. Entropy: 1.637663.\n",
      "Iteration 164: Policy loss: 1.163736. Value loss: 22.619854. Entropy: 1.659753.\n",
      "Iteration 165: Policy loss: 1.172279. Value loss: 21.406542. Entropy: 1.660566.\n",
      "episode: 71   score: 110.0  epsilon: 1.0    steps: 617  evaluation reward: 186.83098591549296\n",
      "episode: 72   score: 335.0  epsilon: 1.0    steps: 981  evaluation reward: 188.88888888888889\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: -3.078532. Value loss: 261.296326. Entropy: 1.624906.\n",
      "Iteration 167: Policy loss: -3.179986. Value loss: 232.888519. Entropy: 1.586266.\n",
      "Iteration 168: Policy loss: -2.484630. Value loss: 169.096741. Entropy: 1.563166.\n",
      "episode: 73   score: 105.0  epsilon: 1.0    steps: 788  evaluation reward: 187.73972602739727\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: -0.255377. Value loss: 36.925213. Entropy: 1.511604.\n",
      "Iteration 170: Policy loss: -0.171447. Value loss: 23.666435. Entropy: 1.509378.\n",
      "Iteration 171: Policy loss: -0.222404. Value loss: 20.772663. Entropy: 1.502056.\n",
      "episode: 74   score: 110.0  epsilon: 1.0    steps: 470  evaluation reward: 186.6891891891892\n",
      "episode: 75   score: 210.0  epsilon: 1.0    steps: 646  evaluation reward: 187.0\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: -0.256479. Value loss: 37.472534. Entropy: 1.601462.\n",
      "Iteration 173: Policy loss: -0.220711. Value loss: 30.945314. Entropy: 1.596950.\n",
      "Iteration 174: Policy loss: -0.525625. Value loss: 27.314304. Entropy: 1.597846.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 0.847353. Value loss: 29.191267. Entropy: 1.582387.\n",
      "Iteration 176: Policy loss: 0.626525. Value loss: 22.760866. Entropy: 1.569059.\n",
      "Iteration 177: Policy loss: 0.849233. Value loss: 20.990437. Entropy: 1.574593.\n",
      "episode: 76   score: 120.0  epsilon: 1.0    steps: 60  evaluation reward: 186.1184210526316\n",
      "episode: 77   score: 455.0  epsilon: 1.0    steps: 235  evaluation reward: 189.6103896103896\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: -1.170979. Value loss: 278.215637. Entropy: 1.618618.\n",
      "Iteration 179: Policy loss: -0.593120. Value loss: 170.560593. Entropy: 1.622731.\n",
      "Iteration 180: Policy loss: -0.001474. Value loss: 157.429413. Entropy: 1.579462.\n",
      "episode: 78   score: 285.0  epsilon: 1.0    steps: 352  evaluation reward: 190.83333333333334\n",
      "episode: 79   score: 35.0  epsilon: 1.0    steps: 644  evaluation reward: 188.86075949367088\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 2.233694. Value loss: 23.636614. Entropy: 1.571337.\n",
      "Iteration 182: Policy loss: 2.019627. Value loss: 15.472469. Entropy: 1.554744.\n",
      "Iteration 183: Policy loss: 2.065273. Value loss: 13.029124. Entropy: 1.559560.\n",
      "episode: 80   score: 110.0  epsilon: 1.0    steps: 551  evaluation reward: 187.875\n",
      "episode: 81   score: 120.0  epsilon: 1.0    steps: 897  evaluation reward: 187.03703703703704\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 2.975114. Value loss: 57.846500. Entropy: 1.549414.\n",
      "Iteration 185: Policy loss: 3.016943. Value loss: 45.692467. Entropy: 1.558745.\n",
      "Iteration 186: Policy loss: 2.873785. Value loss: 40.821724. Entropy: 1.557113.\n",
      "episode: 82   score: 55.0  epsilon: 1.0    steps: 57  evaluation reward: 185.4268292682927\n",
      "episode: 83   score: 55.0  epsilon: 1.0    steps: 249  evaluation reward: 183.85542168674698\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: 1.913904. Value loss: 20.963579. Entropy: 1.531169.\n",
      "Iteration 188: Policy loss: 1.751137. Value loss: 15.952103. Entropy: 1.523521.\n",
      "Iteration 189: Policy loss: 1.923497. Value loss: 14.381743. Entropy: 1.520343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 84   score: 465.0  epsilon: 1.0    steps: 875  evaluation reward: 187.20238095238096\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 0.602559. Value loss: 22.674053. Entropy: 1.473097.\n",
      "Iteration 191: Policy loss: 0.406330. Value loss: 20.361053. Entropy: 1.476636.\n",
      "Iteration 192: Policy loss: 0.556847. Value loss: 14.580667. Entropy: 1.463428.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: -1.455430. Value loss: 26.055922. Entropy: 1.348237.\n",
      "Iteration 194: Policy loss: -1.363055. Value loss: 18.550213. Entropy: 1.343106.\n",
      "Iteration 195: Policy loss: -1.398001. Value loss: 18.513401. Entropy: 1.327157.\n",
      "episode: 85   score: 260.0  epsilon: 1.0    steps: 454  evaluation reward: 188.05882352941177\n",
      "episode: 86   score: 105.0  epsilon: 1.0    steps: 745  evaluation reward: 187.09302325581396\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 3.339564. Value loss: 65.835533. Entropy: 1.469316.\n",
      "Iteration 197: Policy loss: 3.395505. Value loss: 39.632793. Entropy: 1.461219.\n",
      "Iteration 198: Policy loss: 3.188062. Value loss: 33.607101. Entropy: 1.455151.\n",
      "episode: 87   score: 110.0  epsilon: 1.0    steps: 261  evaluation reward: 186.20689655172413\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: 1.768144. Value loss: 18.922228. Entropy: 1.346839.\n",
      "Iteration 200: Policy loss: 1.585428. Value loss: 11.023710. Entropy: 1.355755.\n",
      "Iteration 201: Policy loss: 1.939071. Value loss: 8.736117. Entropy: 1.392571.\n",
      "episode: 88   score: 155.0  epsilon: 1.0    steps: 593  evaluation reward: 185.85227272727272\n",
      "episode: 89   score: 180.0  epsilon: 1.0    steps: 958  evaluation reward: 185.7865168539326\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 1.724898. Value loss: 36.526897. Entropy: 1.399946.\n",
      "Iteration 203: Policy loss: 2.101903. Value loss: 31.045385. Entropy: 1.440710.\n",
      "Iteration 204: Policy loss: 1.938856. Value loss: 27.907103. Entropy: 1.417555.\n",
      "episode: 90   score: 180.0  epsilon: 1.0    steps: 98  evaluation reward: 185.72222222222223\n",
      "episode: 91   score: 110.0  epsilon: 1.0    steps: 888  evaluation reward: 184.8901098901099\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: -0.952742. Value loss: 25.294134. Entropy: 1.449070.\n",
      "Iteration 206: Policy loss: -0.863404. Value loss: 19.509102. Entropy: 1.457082.\n",
      "Iteration 207: Policy loss: -0.843585. Value loss: 17.234552. Entropy: 1.438156.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: -1.130400. Value loss: 17.602652. Entropy: 1.351654.\n",
      "Iteration 209: Policy loss: -1.140302. Value loss: 15.152593. Entropy: 1.330886.\n",
      "Iteration 210: Policy loss: -1.085827. Value loss: 12.936924. Entropy: 1.340498.\n",
      "episode: 92   score: 260.0  epsilon: 1.0    steps: 212  evaluation reward: 185.70652173913044\n",
      "episode: 93   score: 110.0  epsilon: 1.0    steps: 468  evaluation reward: 184.89247311827958\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: 0.039246. Value loss: 21.856554. Entropy: 1.466614.\n",
      "Iteration 212: Policy loss: 0.144734. Value loss: 18.137045. Entropy: 1.466083.\n",
      "Iteration 213: Policy loss: 0.046558. Value loss: 16.007893. Entropy: 1.456427.\n",
      "episode: 94   score: 135.0  epsilon: 1.0    steps: 760  evaluation reward: 184.36170212765958\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: 0.527357. Value loss: 18.744333. Entropy: 1.447836.\n",
      "Iteration 215: Policy loss: 0.539697. Value loss: 11.522676. Entropy: 1.423604.\n",
      "Iteration 216: Policy loss: 0.648569. Value loss: 8.329413. Entropy: 1.399449.\n",
      "episode: 95   score: 185.0  epsilon: 1.0    steps: 311  evaluation reward: 184.3684210526316\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.605962. Value loss: 14.745074. Entropy: 1.465832.\n",
      "Iteration 218: Policy loss: 0.895744. Value loss: 12.584246. Entropy: 1.451324.\n",
      "Iteration 219: Policy loss: 0.705518. Value loss: 11.076720. Entropy: 1.435667.\n",
      "episode: 96   score: 155.0  epsilon: 1.0    steps: 625  evaluation reward: 184.0625\n",
      "episode: 97   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 184.02061855670104\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: 0.672961. Value loss: 19.550993. Entropy: 1.419596.\n",
      "Iteration 221: Policy loss: 0.472223. Value loss: 17.460905. Entropy: 1.431103.\n",
      "Iteration 222: Policy loss: 0.398306. Value loss: 16.553751. Entropy: 1.424144.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: 1.382348. Value loss: 9.939915. Entropy: 1.318132.\n",
      "Iteration 224: Policy loss: 1.364959. Value loss: 7.274640. Entropy: 1.309651.\n",
      "Iteration 225: Policy loss: 1.335361. Value loss: 6.538198. Entropy: 1.302976.\n",
      "episode: 98   score: 155.0  epsilon: 1.0    steps: 7  evaluation reward: 183.72448979591837\n",
      "episode: 99   score: 110.0  epsilon: 1.0    steps: 472  evaluation reward: 182.97979797979798\n",
      "episode: 100   score: 155.0  epsilon: 1.0    steps: 797  evaluation reward: 182.7\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: 1.056659. Value loss: 18.166842. Entropy: 1.302058.\n",
      "Iteration 227: Policy loss: 1.103562. Value loss: 13.612440. Entropy: 1.283304.\n",
      "Iteration 228: Policy loss: 1.061156. Value loss: 12.934946. Entropy: 1.309770.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: 0.734120. Value loss: 12.910003. Entropy: 1.305106.\n",
      "Iteration 230: Policy loss: 0.738535. Value loss: 11.847612. Entropy: 1.275251.\n",
      "Iteration 231: Policy loss: 0.809783. Value loss: 11.347490. Entropy: 1.279149.\n",
      "now time :  2019-02-25 18:44:03.487981\n",
      "episode: 101   score: 180.0  epsilon: 1.0    steps: 131  evaluation reward: 182.9\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: 1.514147. Value loss: 15.723431. Entropy: 1.239311.\n",
      "Iteration 233: Policy loss: 1.583744. Value loss: 13.016333. Entropy: 1.225871.\n",
      "Iteration 234: Policy loss: 1.559742. Value loss: 11.829502. Entropy: 1.250277.\n",
      "episode: 102   score: 180.0  epsilon: 1.0    steps: 683  evaluation reward: 183.4\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: -0.167797. Value loss: 20.587397. Entropy: 1.178771.\n",
      "Iteration 236: Policy loss: 0.018740. Value loss: 17.612610. Entropy: 1.192064.\n",
      "Iteration 237: Policy loss: -0.248483. Value loss: 15.357055. Entropy: 1.186398.\n",
      "episode: 103   score: 210.0  epsilon: 1.0    steps: 275  evaluation reward: 183.7\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: -0.455704. Value loss: 16.770517. Entropy: 1.110052.\n",
      "Iteration 239: Policy loss: -0.220507. Value loss: 13.600771. Entropy: 1.147213.\n",
      "Iteration 240: Policy loss: -0.237652. Value loss: 13.120544. Entropy: 1.094798.\n",
      "episode: 104   score: 105.0  epsilon: 1.0    steps: 491  evaluation reward: 182.95\n",
      "episode: 105   score: 185.0  epsilon: 1.0    steps: 542  evaluation reward: 183.0\n",
      "episode: 106   score: 155.0  epsilon: 1.0    steps: 903  evaluation reward: 182.85\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 0.498569. Value loss: 13.145011. Entropy: 1.198074.\n",
      "Iteration 242: Policy loss: 0.474163. Value loss: 11.321578. Entropy: 1.188581.\n",
      "Iteration 243: Policy loss: 0.665820. Value loss: 10.395206. Entropy: 1.202545.\n",
      "episode: 107   score: 165.0  epsilon: 1.0    steps: 38  evaluation reward: 183.45\n",
      "episode: 108   score: 155.0  epsilon: 1.0    steps: 820  evaluation reward: 182.7\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: 1.157687. Value loss: 11.601713. Entropy: 1.131269.\n",
      "Iteration 245: Policy loss: 1.181088. Value loss: 9.162602. Entropy: 1.146241.\n",
      "Iteration 246: Policy loss: 1.197992. Value loss: 9.415701. Entropy: 1.132503.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 1.516920. Value loss: 14.922440. Entropy: 1.184358.\n",
      "Iteration 248: Policy loss: 1.335255. Value loss: 13.148233. Entropy: 1.207358.\n",
      "Iteration 249: Policy loss: 1.292256. Value loss: 12.127954. Entropy: 1.184053.\n",
      "episode: 109   score: 155.0  epsilon: 1.0    steps: 174  evaluation reward: 182.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: 0.416231. Value loss: 9.970453. Entropy: 1.227948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251: Policy loss: 0.268657. Value loss: 7.993870. Entropy: 1.179927.\n",
      "Iteration 252: Policy loss: 0.416335. Value loss: 7.712707. Entropy: 1.190038.\n",
      "episode: 110   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 179.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.451152. Value loss: 7.807262. Entropy: 1.053278.\n",
      "Iteration 254: Policy loss: 0.366645. Value loss: 6.230274. Entropy: 1.059868.\n",
      "Iteration 255: Policy loss: 0.410222. Value loss: 5.345844. Entropy: 1.092538.\n",
      "episode: 111   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 179.15\n",
      "episode: 112   score: 105.0  epsilon: 1.0    steps: 562  evaluation reward: 178.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: -0.681511. Value loss: 20.260069. Entropy: 0.977629.\n",
      "Iteration 257: Policy loss: -0.836573. Value loss: 14.787963. Entropy: 0.998167.\n",
      "Iteration 258: Policy loss: -0.740807. Value loss: 14.938766. Entropy: 0.963496.\n",
      "episode: 113   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 180.6\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: -0.768578. Value loss: 18.334843. Entropy: 1.122341.\n",
      "Iteration 260: Policy loss: -0.622792. Value loss: 15.025940. Entropy: 1.153045.\n",
      "Iteration 261: Policy loss: -0.473262. Value loss: 12.276639. Entropy: 1.122970.\n",
      "episode: 114   score: 180.0  epsilon: 1.0    steps: 107  evaluation reward: 180.95\n",
      "episode: 115   score: 210.0  epsilon: 1.0    steps: 413  evaluation reward: 181.2\n",
      "episode: 116   score: 165.0  epsilon: 1.0    steps: 843  evaluation reward: 182.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: -0.143157. Value loss: 22.709988. Entropy: 1.110647.\n",
      "Iteration 263: Policy loss: 0.089082. Value loss: 15.481585. Entropy: 1.068762.\n",
      "Iteration 264: Policy loss: -0.018051. Value loss: 15.310943. Entropy: 1.054759.\n",
      "episode: 117   score: 105.0  epsilon: 1.0    steps: 193  evaluation reward: 179.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: -0.008707. Value loss: 13.609722. Entropy: 1.053816.\n",
      "Iteration 266: Policy loss: 0.275295. Value loss: 7.294294. Entropy: 1.061696.\n",
      "Iteration 267: Policy loss: 0.040379. Value loss: 7.783482. Entropy: 1.031735.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.075973. Value loss: 11.236634. Entropy: 1.027989.\n",
      "Iteration 269: Policy loss: 0.128053. Value loss: 9.835504. Entropy: 1.028622.\n",
      "Iteration 270: Policy loss: 0.032552. Value loss: 10.276187. Entropy: 1.016999.\n",
      "episode: 118   score: 155.0  epsilon: 1.0    steps: 766  evaluation reward: 180.95\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: 0.598033. Value loss: 6.949164. Entropy: 1.266544.\n",
      "Iteration 272: Policy loss: 0.561726. Value loss: 4.500401. Entropy: 1.266867.\n",
      "Iteration 273: Policy loss: 0.592043. Value loss: 4.340854. Entropy: 1.273283.\n",
      "episode: 119   score: 155.0  epsilon: 1.0    steps: 377  evaluation reward: 182.1\n",
      "episode: 120   score: 180.0  epsilon: 1.0    steps: 613  evaluation reward: 183.2\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.847030. Value loss: 11.445232. Entropy: 1.061112.\n",
      "Iteration 275: Policy loss: 0.690797. Value loss: 9.359549. Entropy: 1.075235.\n",
      "Iteration 276: Policy loss: 0.802810. Value loss: 8.460314. Entropy: 1.072747.\n",
      "episode: 121   score: 155.0  epsilon: 1.0    steps: 991  evaluation reward: 181.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: 0.064764. Value loss: 19.050386. Entropy: 0.757747.\n",
      "Iteration 278: Policy loss: 0.001977. Value loss: 11.368707. Entropy: 0.820603.\n",
      "Iteration 279: Policy loss: 0.140349. Value loss: 9.155101. Entropy: 0.786991.\n",
      "episode: 122   score: 180.0  epsilon: 1.0    steps: 894  evaluation reward: 180.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: -0.353923. Value loss: 21.436592. Entropy: 0.962142.\n",
      "Iteration 281: Policy loss: -0.346392. Value loss: 18.563126. Entropy: 1.003205.\n",
      "Iteration 282: Policy loss: -0.364675. Value loss: 16.600075. Entropy: 1.013320.\n",
      "episode: 123   score: 210.0  epsilon: 1.0    steps: 30  evaluation reward: 180.35\n",
      "episode: 124   score: 155.0  epsilon: 1.0    steps: 211  evaluation reward: 179.2\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.470336. Value loss: 13.826343. Entropy: 1.038142.\n",
      "Iteration 284: Policy loss: 0.521329. Value loss: 11.963143. Entropy: 1.047270.\n",
      "Iteration 285: Policy loss: 0.513043. Value loss: 10.803572. Entropy: 1.061725.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: 0.167515. Value loss: 12.400347. Entropy: 0.879635.\n",
      "Iteration 287: Policy loss: 0.168622. Value loss: 9.835610. Entropy: 0.892479.\n",
      "Iteration 288: Policy loss: 0.263033. Value loss: 9.019490. Entropy: 0.907787.\n",
      "episode: 125   score: 105.0  epsilon: 1.0    steps: 657  evaluation reward: 176.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: -2.644618. Value loss: 216.837570. Entropy: 0.959589.\n",
      "Iteration 290: Policy loss: -1.646298. Value loss: 82.125031. Entropy: 0.889589.\n",
      "Iteration 291: Policy loss: -1.381011. Value loss: 65.257553. Entropy: 0.871435.\n",
      "episode: 126   score: 105.0  epsilon: 1.0    steps: 268  evaluation reward: 174.45\n",
      "episode: 127   score: 105.0  epsilon: 1.0    steps: 1001  evaluation reward: 172.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: -0.101849. Value loss: 21.780296. Entropy: 0.688470.\n",
      "Iteration 293: Policy loss: -0.016299. Value loss: 13.234962. Entropy: 0.712836.\n",
      "Iteration 294: Policy loss: -0.039447. Value loss: 10.523357. Entropy: 0.658035.\n",
      "episode: 128   score: 565.0  epsilon: 1.0    steps: 458  evaluation reward: 178.0\n",
      "episode: 129   score: 180.0  epsilon: 1.0    steps: 548  evaluation reward: 178.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.560936. Value loss: 15.469451. Entropy: 0.646295.\n",
      "Iteration 296: Policy loss: 0.618348. Value loss: 10.714525. Entropy: 0.663377.\n",
      "Iteration 297: Policy loss: 0.741576. Value loss: 9.063768. Entropy: 0.655196.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: -0.922136. Value loss: 17.402164. Entropy: 0.667602.\n",
      "Iteration 299: Policy loss: -0.902657. Value loss: 15.278957. Entropy: 0.650003.\n",
      "Iteration 300: Policy loss: -0.875693. Value loss: 15.376633. Entropy: 0.678520.\n",
      "episode: 130   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 178.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: 0.129751. Value loss: 12.544621. Entropy: 0.915526.\n",
      "Iteration 302: Policy loss: 0.153950. Value loss: 11.241385. Entropy: 0.890132.\n",
      "Iteration 303: Policy loss: 0.289778. Value loss: 9.399752. Entropy: 0.931436.\n",
      "episode: 131   score: 180.0  epsilon: 1.0    steps: 21  evaluation reward: 177.65\n",
      "episode: 132   score: 180.0  epsilon: 1.0    steps: 134  evaluation reward: 178.9\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: 0.247631. Value loss: 6.278534. Entropy: 0.781965.\n",
      "Iteration 305: Policy loss: 0.453276. Value loss: 4.185918. Entropy: 0.787899.\n",
      "Iteration 306: Policy loss: 0.450871. Value loss: 4.631249. Entropy: 0.791259.\n",
      "episode: 133   score: 180.0  epsilon: 1.0    steps: 708  evaluation reward: 177.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: 0.272240. Value loss: 12.968226. Entropy: 0.622642.\n",
      "Iteration 308: Policy loss: 0.107971. Value loss: 12.006978. Entropy: 0.632491.\n",
      "Iteration 309: Policy loss: 0.125907. Value loss: 9.837626. Entropy: 0.614249.\n",
      "episode: 134   score: 165.0  epsilon: 1.0    steps: 287  evaluation reward: 177.5\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: -0.319483. Value loss: 13.082246. Entropy: 0.957561.\n",
      "Iteration 311: Policy loss: -0.093708. Value loss: 10.010675. Entropy: 0.982036.\n",
      "Iteration 312: Policy loss: -0.245963. Value loss: 9.152401. Entropy: 1.022415.\n",
      "episode: 135   score: 120.0  epsilon: 1.0    steps: 475  evaluation reward: 178.05\n",
      "episode: 136   score: 135.0  epsilon: 1.0    steps: 580  evaluation reward: 178.05\n",
      "episode: 137   score: 160.0  epsilon: 1.0    steps: 905  evaluation reward: 177.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: 2.044637. Value loss: 17.327808. Entropy: 0.829335.\n",
      "Iteration 314: Policy loss: 2.029523. Value loss: 9.869965. Entropy: 0.814965.\n",
      "Iteration 315: Policy loss: 1.908351. Value loss: 8.308845. Entropy: 0.853987.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 1.124578. Value loss: 12.760306. Entropy: 0.633052.\n",
      "Iteration 317: Policy loss: 1.095892. Value loss: 11.729929. Entropy: 0.626465.\n",
      "Iteration 318: Policy loss: 1.009474. Value loss: 11.874995. Entropy: 0.644320.\n",
      "episode: 138   score: 105.0  epsilon: 1.0    steps: 40  evaluation reward: 178.0\n",
      "episode: 139   score: 180.0  epsilon: 1.0    steps: 867  evaluation reward: 178.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.116538. Value loss: 14.367938. Entropy: 0.993298.\n",
      "Iteration 320: Policy loss: 0.235559. Value loss: 10.387097. Entropy: 0.999250.\n",
      "Iteration 321: Policy loss: 0.161660. Value loss: 10.673099. Entropy: 1.047394.\n",
      "episode: 140   score: 165.0  epsilon: 1.0    steps: 155  evaluation reward: 179.95\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: -0.020966. Value loss: 8.150765. Entropy: 0.826289.\n",
      "Iteration 323: Policy loss: -0.041050. Value loss: 8.342905. Entropy: 0.859633.\n",
      "Iteration 324: Policy loss: -0.064897. Value loss: 6.224071. Entropy: 0.839967.\n",
      "episode: 141   score: 105.0  epsilon: 1.0    steps: 306  evaluation reward: 179.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: -0.007094. Value loss: 16.682131. Entropy: 0.714428.\n",
      "Iteration 326: Policy loss: 0.157129. Value loss: 13.083959. Entropy: 0.679209.\n",
      "Iteration 327: Policy loss: -0.110594. Value loss: 12.798534. Entropy: 0.695236.\n",
      "episode: 142   score: 105.0  epsilon: 1.0    steps: 494  evaluation reward: 178.2\n",
      "episode: 143   score: 210.0  epsilon: 1.0    steps: 679  evaluation reward: 177.7\n",
      "episode: 144   score: 105.0  epsilon: 1.0    steps: 924  evaluation reward: 178.45\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.236726. Value loss: 6.569585. Entropy: 0.988570.\n",
      "Iteration 329: Policy loss: 0.266647. Value loss: 6.217039. Entropy: 0.984358.\n",
      "Iteration 330: Policy loss: 0.204626. Value loss: 5.297995. Entropy: 0.982474.\n",
      "episode: 145   score: 165.0  epsilon: 1.0    steps: 602  evaluation reward: 178.15\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: 0.628924. Value loss: 9.556656. Entropy: 0.703112.\n",
      "Iteration 332: Policy loss: 0.555899. Value loss: 8.217271. Entropy: 0.745138.\n",
      "Iteration 333: Policy loss: 0.582507. Value loss: 8.160317. Entropy: 0.762956.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: -0.098862. Value loss: 5.569752. Entropy: 0.945974.\n",
      "Iteration 335: Policy loss: -0.077231. Value loss: 4.617770. Entropy: 0.938299.\n",
      "Iteration 336: Policy loss: -0.076187. Value loss: 5.821564. Entropy: 0.930623.\n",
      "episode: 146   score: 210.0  epsilon: 1.0    steps: 89  evaluation reward: 177.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: 0.085069. Value loss: 13.062063. Entropy: 1.095448.\n",
      "Iteration 338: Policy loss: -0.117760. Value loss: 7.986624. Entropy: 1.063248.\n",
      "Iteration 339: Policy loss: 0.020097. Value loss: 8.711521. Entropy: 1.070545.\n",
      "episode: 147   score: 180.0  epsilon: 1.0    steps: 206  evaluation reward: 177.4\n",
      "episode: 148   score: 180.0  epsilon: 1.0    steps: 773  evaluation reward: 178.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: -0.073006. Value loss: 5.630816. Entropy: 0.881766.\n",
      "Iteration 341: Policy loss: -0.104145. Value loss: 4.291984. Entropy: 0.819873.\n",
      "Iteration 342: Policy loss: -0.093382. Value loss: 3.778870. Entropy: 0.864890.\n",
      "episode: 149   score: 180.0  epsilon: 1.0    steps: 357  evaluation reward: 177.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: -0.096952. Value loss: 10.815528. Entropy: 0.764923.\n",
      "Iteration 344: Policy loss: -0.123429. Value loss: 9.853230. Entropy: 0.758235.\n",
      "Iteration 345: Policy loss: -0.192635. Value loss: 8.885527. Entropy: 0.786642.\n",
      "episode: 150   score: 210.0  epsilon: 1.0    steps: 730  evaluation reward: 178.5\n",
      "now time :  2019-02-25 18:46:14.531000\n",
      "episode: 151   score: 165.0  epsilon: 1.0    steps: 943  evaluation reward: 178.45\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: -0.833844. Value loss: 14.079351. Entropy: 1.045426.\n",
      "Iteration 347: Policy loss: -0.832739. Value loss: 10.369461. Entropy: 1.062513.\n",
      "Iteration 348: Policy loss: -0.809848. Value loss: 9.445185. Entropy: 1.065840.\n",
      "episode: 152   score: 210.0  epsilon: 1.0    steps: 417  evaluation reward: 178.9\n",
      "episode: 153   score: 135.0  epsilon: 1.0    steps: 635  evaluation reward: 178.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: 0.818543. Value loss: 15.029222. Entropy: 0.787549.\n",
      "Iteration 350: Policy loss: 0.952471. Value loss: 11.684589. Entropy: 0.833517.\n",
      "Iteration 351: Policy loss: 0.657003. Value loss: 10.518796. Entropy: 0.872589.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: -1.178785. Value loss: 18.365894. Entropy: 0.946443.\n",
      "Iteration 353: Policy loss: -1.043569. Value loss: 18.047081. Entropy: 0.936078.\n",
      "Iteration 354: Policy loss: -0.849678. Value loss: 13.656510. Entropy: 0.999227.\n",
      "episode: 154   score: 185.0  epsilon: 1.0    steps: 120  evaluation reward: 177.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 0.079329. Value loss: 9.911694. Entropy: 1.253898.\n",
      "Iteration 356: Policy loss: 0.000820. Value loss: 9.216507. Entropy: 1.285060.\n",
      "Iteration 357: Policy loss: 0.254624. Value loss: 7.287690. Entropy: 1.282193.\n",
      "episode: 155   score: 180.0  epsilon: 1.0    steps: 817  evaluation reward: 176.35\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 0.352669. Value loss: 9.891526. Entropy: 1.177465.\n",
      "Iteration 359: Policy loss: 0.446395. Value loss: 6.840010. Entropy: 1.165368.\n",
      "Iteration 360: Policy loss: 0.356929. Value loss: 6.249375. Entropy: 1.154921.\n",
      "episode: 156   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 176.6\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 0.778533. Value loss: 10.111890. Entropy: 0.970740.\n",
      "Iteration 362: Policy loss: 0.823692. Value loss: 7.248378. Entropy: 0.930382.\n",
      "Iteration 363: Policy loss: 0.732573. Value loss: 6.808183. Entropy: 0.979388.\n",
      "episode: 157   score: 155.0  epsilon: 1.0    steps: 278  evaluation reward: 174.8\n",
      "episode: 158   score: 155.0  epsilon: 1.0    steps: 762  evaluation reward: 174.45\n",
      "episode: 159   score: 180.0  epsilon: 1.0    steps: 1003  evaluation reward: 174.9\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.275404. Value loss: 15.969618. Entropy: 1.023918.\n",
      "Iteration 365: Policy loss: 0.437246. Value loss: 10.590127. Entropy: 0.986350.\n",
      "Iteration 366: Policy loss: 0.308482. Value loss: 9.171556. Entropy: 1.002011.\n",
      "episode: 160   score: 210.0  epsilon: 1.0    steps: 468  evaluation reward: 175.9\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: -0.174830. Value loss: 14.830535. Entropy: 0.999612.\n",
      "Iteration 368: Policy loss: -0.039765. Value loss: 11.131163. Entropy: 1.018146.\n",
      "Iteration 369: Policy loss: 0.075995. Value loss: 10.672663. Entropy: 0.991962.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: -0.652132. Value loss: 18.549902. Entropy: 0.986602.\n",
      "Iteration 371: Policy loss: -0.649710. Value loss: 15.915616. Entropy: 0.929977.\n",
      "Iteration 372: Policy loss: -0.609736. Value loss: 12.508320. Entropy: 0.956857.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: -0.634561. Value loss: 13.529976. Entropy: 1.230283.\n",
      "Iteration 374: Policy loss: -0.649324. Value loss: 8.542240. Entropy: 1.219043.\n",
      "Iteration 375: Policy loss: -0.648344. Value loss: 7.209041. Entropy: 1.244534.\n",
      "episode: 161   score: 210.0  epsilon: 1.0    steps: 43  evaluation reward: 174.1\n",
      "episode: 162   score: 270.0  epsilon: 1.0    steps: 520  evaluation reward: 174.2\n",
      "episode: 163   score: 180.0  epsilon: 1.0    steps: 868  evaluation reward: 174.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.755725. Value loss: 6.381102. Entropy: 1.122677.\n",
      "Iteration 377: Policy loss: 0.755028. Value loss: 5.400436. Entropy: 1.106681.\n",
      "Iteration 378: Policy loss: 0.881160. Value loss: 4.898935. Entropy: 1.134820.\n",
      "episode: 164   score: 210.0  epsilon: 1.0    steps: 249  evaluation reward: 174.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: -1.078150. Value loss: 14.331660. Entropy: 0.862760.\n",
      "Iteration 380: Policy loss: -1.101031. Value loss: 11.723396. Entropy: 0.905521.\n",
      "Iteration 381: Policy loss: -1.088434. Value loss: 11.871847. Entropy: 0.921247.\n",
      "episode: 165   score: 165.0  epsilon: 1.0    steps: 311  evaluation reward: 173.65\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 1.336081. Value loss: 11.377154. Entropy: 1.050846.\n",
      "Iteration 383: Policy loss: 1.331780. Value loss: 8.071862. Entropy: 1.036355.\n",
      "Iteration 384: Policy loss: 1.368378. Value loss: 7.378043. Entropy: 1.037183.\n",
      "episode: 166   score: 155.0  epsilon: 1.0    steps: 502  evaluation reward: 171.4\n",
      "episode: 167   score: 155.0  epsilon: 1.0    steps: 671  evaluation reward: 171.4\n",
      "episode: 168   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 172.45\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 0.266189. Value loss: 9.719123. Entropy: 1.071171.\n",
      "Iteration 386: Policy loss: 0.255889. Value loss: 6.519396. Entropy: 1.063550.\n",
      "Iteration 387: Policy loss: 0.028437. Value loss: 6.068065. Entropy: 1.064870.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: -0.437873. Value loss: 12.267101. Entropy: 1.019530.\n",
      "Iteration 389: Policy loss: -0.539460. Value loss: 10.879629. Entropy: 1.030176.\n",
      "Iteration 390: Policy loss: -0.515521. Value loss: 10.419330. Entropy: 1.035710.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: -0.616735. Value loss: 5.344347. Entropy: 1.269566.\n",
      "Iteration 392: Policy loss: -0.601344. Value loss: 3.533838. Entropy: 1.287145.\n",
      "Iteration 393: Policy loss: -0.507898. Value loss: 3.731489. Entropy: 1.281756.\n",
      "episode: 169   score: 180.0  epsilon: 1.0    steps: 86  evaluation reward: 172.45\n",
      "episode: 170   score: 210.0  epsilon: 1.0    steps: 583  evaluation reward: 172.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: -0.646391. Value loss: 11.840386. Entropy: 1.254978.\n",
      "Iteration 395: Policy loss: -0.720022. Value loss: 10.434752. Entropy: 1.266234.\n",
      "Iteration 396: Policy loss: -0.638078. Value loss: 8.571286. Entropy: 1.251250.\n",
      "episode: 171   score: 210.0  epsilon: 1.0    steps: 801  evaluation reward: 173.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: -1.484258. Value loss: 234.192474. Entropy: 0.876311.\n",
      "Iteration 398: Policy loss: -0.980261. Value loss: 158.576828. Entropy: 0.978262.\n",
      "Iteration 399: Policy loss: -1.229549. Value loss: 231.883377. Entropy: 0.839250.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: -0.371831. Value loss: 31.658140. Entropy: 0.752451.\n",
      "Iteration 401: Policy loss: -0.120612. Value loss: 23.917065. Entropy: 0.733281.\n",
      "Iteration 402: Policy loss: -0.551958. Value loss: 20.618294. Entropy: 0.729496.\n",
      "episode: 172   score: 485.0  epsilon: 1.0    steps: 208  evaluation reward: 175.45\n",
      "episode: 173   score: 210.0  epsilon: 1.0    steps: 753  evaluation reward: 176.5\n",
      "episode: 174   score: 210.0  epsilon: 1.0    steps: 1004  evaluation reward: 177.5\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.068582. Value loss: 25.482399. Entropy: 1.019444.\n",
      "Iteration 404: Policy loss: 0.091396. Value loss: 18.209259. Entropy: 1.046214.\n",
      "Iteration 405: Policy loss: 0.036253. Value loss: 17.551817. Entropy: 1.042675.\n",
      "episode: 175   score: 260.0  epsilon: 1.0    steps: 308  evaluation reward: 178.0\n",
      "episode: 176   score: 180.0  epsilon: 1.0    steps: 407  evaluation reward: 178.6\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 1.056699. Value loss: 13.557047. Entropy: 0.894328.\n",
      "Iteration 407: Policy loss: 0.994569. Value loss: 9.363911. Entropy: 0.794194.\n",
      "Iteration 408: Policy loss: 1.025244. Value loss: 9.201818. Entropy: 0.800322.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.588885. Value loss: 11.128219. Entropy: 0.765882.\n",
      "Iteration 410: Policy loss: 0.740499. Value loss: 10.281226. Entropy: 0.763837.\n",
      "Iteration 411: Policy loss: 0.675633. Value loss: 8.784951. Entropy: 0.792233.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: -0.295519. Value loss: 13.396260. Entropy: 1.137583.\n",
      "Iteration 413: Policy loss: -0.437981. Value loss: 11.931601. Entropy: 1.132771.\n",
      "Iteration 414: Policy loss: -0.407538. Value loss: 11.370544. Entropy: 1.120131.\n",
      "episode: 177   score: 210.0  epsilon: 1.0    steps: 9  evaluation reward: 176.15\n",
      "episode: 178   score: 180.0  epsilon: 1.0    steps: 525  evaluation reward: 175.1\n",
      "episode: 179   score: 210.0  epsilon: 1.0    steps: 850  evaluation reward: 176.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: -0.382286. Value loss: 3.397653. Entropy: 0.964296.\n",
      "Iteration 416: Policy loss: -0.233673. Value loss: 2.653313. Entropy: 0.973957.\n",
      "Iteration 417: Policy loss: -0.339776. Value loss: 2.737740. Entropy: 0.955119.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: -0.792807. Value loss: 12.670674. Entropy: 0.626048.\n",
      "Iteration 419: Policy loss: -0.853505. Value loss: 9.033785. Entropy: 0.658004.\n",
      "Iteration 420: Policy loss: -0.792238. Value loss: 7.975086. Entropy: 0.632499.\n",
      "episode: 180   score: 105.0  epsilon: 1.0    steps: 327  evaluation reward: 176.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: -0.038712. Value loss: 14.923636. Entropy: 0.926604.\n",
      "Iteration 422: Policy loss: -0.013235. Value loss: 12.926050. Entropy: 0.923530.\n",
      "Iteration 423: Policy loss: -0.115704. Value loss: 12.142385. Entropy: 0.919341.\n",
      "episode: 181   score: 180.0  epsilon: 1.0    steps: 131  evaluation reward: 177.4\n",
      "episode: 182   score: 210.0  epsilon: 1.0    steps: 458  evaluation reward: 178.95\n",
      "episode: 183   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 180.5\n",
      "episode: 184   score: 210.0  epsilon: 1.0    steps: 927  evaluation reward: 177.95\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: -0.160748. Value loss: 8.817189. Entropy: 0.973985.\n",
      "Iteration 425: Policy loss: -0.060470. Value loss: 6.808855. Entropy: 0.981137.\n",
      "Iteration 426: Policy loss: -0.220685. Value loss: 6.707548. Entropy: 0.960360.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: -1.799563. Value loss: 178.227173. Entropy: 0.582760.\n",
      "Iteration 428: Policy loss: -2.914232. Value loss: 166.262390. Entropy: 0.550324.\n",
      "Iteration 429: Policy loss: -2.286754. Value loss: 137.677505. Entropy: 0.494504.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: 0.972889. Value loss: 22.604618. Entropy: 0.911594.\n",
      "Iteration 431: Policy loss: 1.462016. Value loss: 13.838224. Entropy: 0.851110.\n",
      "Iteration 432: Policy loss: 1.060307. Value loss: 13.991682. Entropy: 0.858767.\n",
      "episode: 185   score: 380.0  epsilon: 1.0    steps: 104  evaluation reward: 179.15\n",
      "episode: 186   score: 180.0  epsilon: 1.0    steps: 563  evaluation reward: 179.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: 2.074263. Value loss: 26.317207. Entropy: 0.795905.\n",
      "Iteration 434: Policy loss: 1.960092. Value loss: 16.447395. Entropy: 0.919944.\n",
      "Iteration 435: Policy loss: 1.812898. Value loss: 11.877901. Entropy: 0.891216.\n",
      "episode: 187   score: 210.0  epsilon: 1.0    steps: 801  evaluation reward: 180.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.185276. Value loss: 10.192568. Entropy: 0.618903.\n",
      "Iteration 437: Policy loss: 0.109918. Value loss: 8.028811. Entropy: 0.555390.\n",
      "Iteration 438: Policy loss: 0.254417. Value loss: 6.362631. Entropy: 0.616654.\n",
      "episode: 188   score: 165.0  epsilon: 1.0    steps: 346  evaluation reward: 181.0\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: -0.226548. Value loss: 283.450928. Entropy: 0.500822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 440: Policy loss: -1.012044. Value loss: 299.712921. Entropy: 0.571251.\n",
      "Iteration 441: Policy loss: -0.208278. Value loss: 195.712845. Entropy: 0.410842.\n",
      "episode: 189   score: 180.0  epsilon: 1.0    steps: 166  evaluation reward: 181.0\n",
      "episode: 190   score: 210.0  epsilon: 1.0    steps: 502  evaluation reward: 181.3\n",
      "episode: 191   score: 210.0  epsilon: 1.0    steps: 735  evaluation reward: 182.3\n",
      "episode: 192   score: 410.0  epsilon: 1.0    steps: 1005  evaluation reward: 183.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: 0.734335. Value loss: 17.207094. Entropy: 0.877990.\n",
      "Iteration 443: Policy loss: 0.736543. Value loss: 15.523857. Entropy: 0.892018.\n",
      "Iteration 444: Policy loss: 0.718938. Value loss: 13.771951. Entropy: 0.916106.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.381624. Value loss: 7.669224. Entropy: 0.633778.\n",
      "Iteration 446: Policy loss: 0.349319. Value loss: 8.339935. Entropy: 0.616543.\n",
      "Iteration 447: Policy loss: 0.487838. Value loss: 7.749843. Entropy: 0.622279.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: -0.258167. Value loss: 10.754154. Entropy: 0.698087.\n",
      "Iteration 449: Policy loss: -0.214963. Value loss: 10.155530. Entropy: 0.727049.\n",
      "Iteration 450: Policy loss: -0.198035. Value loss: 9.367791. Entropy: 0.679617.\n",
      "episode: 193   score: 180.0  epsilon: 1.0    steps: 614  evaluation reward: 184.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: 2.526544. Value loss: 11.518139. Entropy: 0.977925.\n",
      "Iteration 452: Policy loss: 2.191141. Value loss: 6.140093. Entropy: 1.223760.\n",
      "Iteration 453: Policy loss: 2.354697. Value loss: 4.841189. Entropy: 1.220831.\n",
      "episode: 194   score: 180.0  epsilon: 1.0    steps: 95  evaluation reward: 184.95\n",
      "episode: 195   score: 105.0  epsilon: 1.0    steps: 365  evaluation reward: 184.15\n",
      "episode: 196   score: 180.0  epsilon: 1.0    steps: 851  evaluation reward: 184.4\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 2.277853. Value loss: 16.409309. Entropy: 0.868916.\n",
      "Iteration 455: Policy loss: 2.367280. Value loss: 9.553826. Entropy: 0.906982.\n",
      "Iteration 456: Policy loss: 1.855107. Value loss: 7.712785. Entropy: 1.005717.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: -0.214268. Value loss: 16.626812. Entropy: 1.001900.\n",
      "Iteration 458: Policy loss: -0.188057. Value loss: 10.484272. Entropy: 0.996475.\n",
      "Iteration 459: Policy loss: -0.366746. Value loss: 10.536854. Entropy: 1.022078.\n",
      "episode: 197   score: 180.0  epsilon: 1.0    steps: 196  evaluation reward: 184.4\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: 1.441789. Value loss: 20.781153. Entropy: 1.239331.\n",
      "Iteration 461: Policy loss: 1.622704. Value loss: 17.721205. Entropy: 1.230904.\n",
      "Iteration 462: Policy loss: 1.588681. Value loss: 14.747423. Entropy: 1.224470.\n",
      "episode: 198   score: 155.0  epsilon: 1.0    steps: 398  evaluation reward: 184.4\n",
      "episode: 199   score: 210.0  epsilon: 1.0    steps: 658  evaluation reward: 185.4\n",
      "episode: 200   score: 135.0  epsilon: 1.0    steps: 907  evaluation reward: 185.2\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.283700. Value loss: 7.236567. Entropy: 1.219023.\n",
      "Iteration 464: Policy loss: 0.208846. Value loss: 5.919065. Entropy: 1.208904.\n",
      "Iteration 465: Policy loss: 0.206792. Value loss: 3.844527. Entropy: 1.210202.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: 1.363438. Value loss: 14.707198. Entropy: 0.895918.\n",
      "Iteration 467: Policy loss: 1.440282. Value loss: 10.767529. Entropy: 0.834142.\n",
      "Iteration 468: Policy loss: 1.337087. Value loss: 9.283752. Entropy: 0.840844.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: -0.051097. Value loss: 17.239712. Entropy: 1.102788.\n",
      "Iteration 470: Policy loss: -0.039810. Value loss: 16.314199. Entropy: 1.114696.\n",
      "Iteration 471: Policy loss: -0.098268. Value loss: 13.921755. Entropy: 1.087148.\n",
      "now time :  2019-02-25 18:48:35.300640\n",
      "episode: 201   score: 210.0  epsilon: 1.0    steps: 573  evaluation reward: 185.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 1.422903. Value loss: 15.392797. Entropy: 1.110297.\n",
      "Iteration 473: Policy loss: 1.496795. Value loss: 10.255434. Entropy: 1.107285.\n",
      "Iteration 474: Policy loss: 1.415060. Value loss: 10.137595. Entropy: 1.119991.\n",
      "episode: 202   score: 135.0  epsilon: 1.0    steps: 2  evaluation reward: 185.05\n",
      "episode: 203   score: 180.0  epsilon: 1.0    steps: 271  evaluation reward: 184.75\n",
      "episode: 204   score: 180.0  epsilon: 1.0    steps: 799  evaluation reward: 185.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: -1.424694. Value loss: 207.703522. Entropy: 0.950066.\n",
      "Iteration 476: Policy loss: -1.851283. Value loss: 173.154602. Entropy: 0.811176.\n",
      "Iteration 477: Policy loss: -1.846818. Value loss: 191.050629. Entropy: 0.717748.\n",
      "episode: 205   score: 180.0  epsilon: 1.0    steps: 230  evaluation reward: 185.45\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: -0.019988. Value loss: 23.766136. Entropy: 0.820121.\n",
      "Iteration 479: Policy loss: -0.213264. Value loss: 15.954340. Entropy: 0.812443.\n",
      "Iteration 480: Policy loss: -0.107343. Value loss: 16.125885. Entropy: 0.802662.\n",
      "episode: 206   score: 180.0  epsilon: 1.0    steps: 449  evaluation reward: 185.7\n",
      "episode: 207   score: 180.0  epsilon: 1.0    steps: 691  evaluation reward: 185.85\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.481680. Value loss: 12.766846. Entropy: 1.124910.\n",
      "Iteration 482: Policy loss: 0.538544. Value loss: 8.584985. Entropy: 1.144106.\n",
      "Iteration 483: Policy loss: 0.516382. Value loss: 8.017626. Entropy: 1.145627.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: -0.307564. Value loss: 15.317716. Entropy: 0.800825.\n",
      "Iteration 485: Policy loss: -0.550226. Value loss: 11.582010. Entropy: 0.801359.\n",
      "Iteration 486: Policy loss: -0.394467. Value loss: 9.434324. Entropy: 0.828081.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.903074. Value loss: 26.208302. Entropy: 0.916376.\n",
      "Iteration 488: Policy loss: 1.059573. Value loss: 13.685963. Entropy: 0.907904.\n",
      "Iteration 489: Policy loss: 1.034210. Value loss: 11.984444. Entropy: 0.882608.\n",
      "episode: 208   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 186.4\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: -1.258587. Value loss: 302.450165. Entropy: 0.867664.\n",
      "Iteration 491: Policy loss: -0.957874. Value loss: 237.192612. Entropy: 0.718143.\n",
      "Iteration 492: Policy loss: -0.828808. Value loss: 200.646332. Entropy: 0.755818.\n",
      "episode: 209   score: 380.0  epsilon: 1.0    steps: 65  evaluation reward: 188.65\n",
      "episode: 210   score: 210.0  epsilon: 1.0    steps: 341  evaluation reward: 188.95\n",
      "episode: 211   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 188.95\n",
      "episode: 212   score: 685.0  epsilon: 1.0    steps: 934  evaluation reward: 194.75\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: 0.686841. Value loss: 24.290928. Entropy: 0.725226.\n",
      "Iteration 494: Policy loss: 0.435513. Value loss: 18.582869. Entropy: 0.709873.\n",
      "Iteration 495: Policy loss: 0.564852. Value loss: 16.389038. Entropy: 0.741457.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 1.805782. Value loss: 29.288734. Entropy: 0.694761.\n",
      "Iteration 497: Policy loss: 1.577888. Value loss: 24.815113. Entropy: 0.697258.\n",
      "Iteration 498: Policy loss: 1.690692. Value loss: 19.157919. Entropy: 0.707342.\n",
      "episode: 213   score: 155.0  epsilon: 1.0    steps: 143  evaluation reward: 194.2\n",
      "episode: 214   score: 180.0  epsilon: 1.0    steps: 494  evaluation reward: 194.2\n",
      "episode: 215   score: 155.0  epsilon: 1.0    steps: 736  evaluation reward: 193.65\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: 3.065992. Value loss: 33.591694. Entropy: 0.915590.\n",
      "Iteration 500: Policy loss: 2.938062. Value loss: 26.637569. Entropy: 0.971884.\n",
      "Iteration 501: Policy loss: 3.103261. Value loss: 22.305330. Entropy: 0.924171.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 502: Policy loss: 1.376376. Value loss: 18.670271. Entropy: 0.839032.\n",
      "Iteration 503: Policy loss: 1.454679. Value loss: 15.891736. Entropy: 0.848653.\n",
      "Iteration 504: Policy loss: 1.070795. Value loss: 11.736226. Entropy: 0.839092.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: 0.316117. Value loss: 19.500633. Entropy: 0.674985.\n",
      "Iteration 506: Policy loss: 0.250573. Value loss: 18.376005. Entropy: 0.663267.\n",
      "Iteration 507: Policy loss: 0.215282. Value loss: 17.263618. Entropy: 0.664497.\n",
      "episode: 216   score: 95.0  epsilon: 1.0    steps: 174  evaluation reward: 192.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 1.637205. Value loss: 15.825288. Entropy: 0.874560.\n",
      "Iteration 509: Policy loss: 1.582072. Value loss: 9.866695. Entropy: 0.847636.\n",
      "Iteration 510: Policy loss: 1.567608. Value loss: 9.857630. Entropy: 0.855689.\n",
      "episode: 217   score: 180.0  epsilon: 1.0    steps: 98  evaluation reward: 193.7\n",
      "episode: 218   score: 185.0  epsilon: 1.0    steps: 531  evaluation reward: 194.0\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.532654. Value loss: 15.912787. Entropy: 0.762019.\n",
      "Iteration 512: Policy loss: 0.368490. Value loss: 12.309842. Entropy: 0.775845.\n",
      "Iteration 513: Policy loss: 0.497771. Value loss: 11.238376. Entropy: 0.778079.\n",
      "episode: 219   score: 155.0  epsilon: 1.0    steps: 781  evaluation reward: 194.0\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 1.201240. Value loss: 13.123144. Entropy: 0.761554.\n",
      "Iteration 515: Policy loss: 1.240178. Value loss: 9.705241. Entropy: 0.773800.\n",
      "Iteration 516: Policy loss: 1.431317. Value loss: 10.237076. Entropy: 0.792676.\n",
      "episode: 220   score: 260.0  epsilon: 1.0    steps: 307  evaluation reward: 194.8\n",
      "episode: 221   score: 260.0  epsilon: 1.0    steps: 904  evaluation reward: 195.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: 0.406864. Value loss: 25.144630. Entropy: 0.948165.\n",
      "Iteration 518: Policy loss: 0.185948. Value loss: 17.867599. Entropy: 0.946560.\n",
      "Iteration 519: Policy loss: 0.334986. Value loss: 16.674501. Entropy: 0.959341.\n",
      "episode: 222   score: 155.0  epsilon: 1.0    steps: 401  evaluation reward: 195.6\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: -2.526109. Value loss: 168.577423. Entropy: 0.882136.\n",
      "Iteration 521: Policy loss: -3.082438. Value loss: 141.356735. Entropy: 0.860907.\n",
      "Iteration 522: Policy loss: -2.782359. Value loss: 127.010284. Entropy: 0.900941.\n",
      "episode: 223   score: 265.0  epsilon: 1.0    steps: 697  evaluation reward: 196.15\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: 0.075939. Value loss: 17.471188. Entropy: 0.801513.\n",
      "Iteration 524: Policy loss: 0.339922. Value loss: 10.734927. Entropy: 0.834954.\n",
      "Iteration 525: Policy loss: 0.111107. Value loss: 9.448269. Entropy: 0.809498.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: 3.093130. Value loss: 25.132513. Entropy: 0.709727.\n",
      "Iteration 527: Policy loss: 3.016862. Value loss: 11.728258. Entropy: 0.697678.\n",
      "Iteration 528: Policy loss: 3.054878. Value loss: 10.552095. Entropy: 0.708014.\n",
      "episode: 224   score: 210.0  epsilon: 1.0    steps: 601  evaluation reward: 196.7\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: -0.355629. Value loss: 32.179153. Entropy: 0.807158.\n",
      "Iteration 530: Policy loss: -0.287073. Value loss: 18.746496. Entropy: 0.823146.\n",
      "Iteration 531: Policy loss: -0.376316. Value loss: 14.506224. Entropy: 0.817480.\n",
      "episode: 225   score: 210.0  epsilon: 1.0    steps: 15  evaluation reward: 197.75\n",
      "episode: 226   score: 495.0  epsilon: 1.0    steps: 160  evaluation reward: 201.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: 0.334957. Value loss: 12.467363. Entropy: 0.875545.\n",
      "Iteration 533: Policy loss: 0.513508. Value loss: 8.467095. Entropy: 0.878752.\n",
      "Iteration 534: Policy loss: 0.359814. Value loss: 8.625337. Entropy: 0.879529.\n",
      "episode: 227   score: 210.0  epsilon: 1.0    steps: 382  evaluation reward: 202.7\n",
      "episode: 228   score: 260.0  epsilon: 1.0    steps: 847  evaluation reward: 199.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: -0.394160. Value loss: 148.148285. Entropy: 0.701303.\n",
      "Iteration 536: Policy loss: -1.173478. Value loss: 191.253967. Entropy: 0.661569.\n",
      "Iteration 537: Policy loss: -1.089111. Value loss: 111.762909. Entropy: 0.694841.\n",
      "episode: 229   score: 355.0  epsilon: 1.0    steps: 445  evaluation reward: 201.4\n",
      "episode: 230   score: 180.0  epsilon: 1.0    steps: 899  evaluation reward: 201.1\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 0.750256. Value loss: 20.374506. Entropy: 1.070081.\n",
      "Iteration 539: Policy loss: 0.748171. Value loss: 15.330504. Entropy: 1.072527.\n",
      "Iteration 540: Policy loss: 0.775906. Value loss: 13.499002. Entropy: 1.078954.\n",
      "episode: 231   score: 75.0  epsilon: 1.0    steps: 188  evaluation reward: 200.05\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: -1.701690. Value loss: 27.460085. Entropy: 0.941314.\n",
      "Iteration 542: Policy loss: -1.554002. Value loss: 19.331593. Entropy: 0.899301.\n",
      "Iteration 543: Policy loss: -1.760205. Value loss: 18.528307. Entropy: 0.931657.\n",
      "episode: 232   score: 270.0  epsilon: 1.0    steps: 765  evaluation reward: 200.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: 0.745022. Value loss: 17.014606. Entropy: 0.903501.\n",
      "Iteration 545: Policy loss: 0.713018. Value loss: 11.530006. Entropy: 0.879501.\n",
      "Iteration 546: Policy loss: 0.693627. Value loss: 9.271192. Entropy: 0.880302.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: -1.077882. Value loss: 16.767450. Entropy: 0.782401.\n",
      "Iteration 548: Policy loss: -0.875663. Value loss: 12.264780. Entropy: 0.762596.\n",
      "Iteration 549: Policy loss: -1.162053. Value loss: 9.903398. Entropy: 0.804124.\n",
      "episode: 233   score: 180.0  epsilon: 1.0    steps: 523  evaluation reward: 200.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 1.887158. Value loss: 13.421920. Entropy: 0.643639.\n",
      "Iteration 551: Policy loss: 1.428734. Value loss: 7.351698. Entropy: 0.716456.\n",
      "Iteration 552: Policy loss: 1.647828. Value loss: 6.149099. Entropy: 0.841941.\n",
      "episode: 234   score: 110.0  epsilon: 1.0    steps: 501  evaluation reward: 200.4\n",
      "episode: 235   score: 110.0  epsilon: 1.0    steps: 836  evaluation reward: 200.3\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 2.937398. Value loss: 29.837107. Entropy: 0.892874.\n",
      "Iteration 554: Policy loss: 2.874682. Value loss: 18.159422. Entropy: 0.839869.\n",
      "Iteration 555: Policy loss: 2.829374. Value loss: 14.394662. Entropy: 0.877933.\n",
      "episode: 236   score: 310.0  epsilon: 1.0    steps: 87  evaluation reward: 202.05\n",
      "episode: 237   score: 125.0  epsilon: 1.0    steps: 958  evaluation reward: 201.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: -0.994380. Value loss: 256.854340. Entropy: 1.118924.\n",
      "Iteration 557: Policy loss: -1.117770. Value loss: 156.958847. Entropy: 1.078855.\n",
      "Iteration 558: Policy loss: -1.397383. Value loss: 139.145859. Entropy: 1.068118.\n",
      "episode: 238   score: 360.0  epsilon: 1.0    steps: 232  evaluation reward: 204.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: 1.249027. Value loss: 15.362932. Entropy: 0.883698.\n",
      "Iteration 560: Policy loss: 1.693950. Value loss: 9.892616. Entropy: 0.927705.\n",
      "Iteration 561: Policy loss: 1.207233. Value loss: 10.418689. Entropy: 1.002747.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: -0.828722. Value loss: 249.237137. Entropy: 1.065721.\n",
      "Iteration 563: Policy loss: -0.384011. Value loss: 134.593735. Entropy: 1.095057.\n",
      "Iteration 564: Policy loss: -0.316304. Value loss: 140.752975. Entropy: 1.060212.\n",
      "episode: 239   score: 105.0  epsilon: 1.0    steps: 630  evaluation reward: 203.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 3.051019. Value loss: 174.455414. Entropy: 0.960369.\n",
      "Iteration 566: Policy loss: 2.777721. Value loss: 73.299248. Entropy: 0.893781.\n",
      "Iteration 567: Policy loss: 2.436461. Value loss: 53.590107. Entropy: 0.962585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 240   score: 210.0  epsilon: 1.0    steps: 315  evaluation reward: 203.95\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: -0.636937. Value loss: 44.097435. Entropy: 1.064749.\n",
      "Iteration 569: Policy loss: -1.197904. Value loss: 29.090227. Entropy: 1.034243.\n",
      "Iteration 570: Policy loss: -0.895331. Value loss: 19.351875. Entropy: 1.061089.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: 2.608484. Value loss: 288.826233. Entropy: 0.955756.\n",
      "Iteration 572: Policy loss: 2.312109. Value loss: 257.155579. Entropy: 0.807778.\n",
      "Iteration 573: Policy loss: 2.746093. Value loss: 132.830917. Entropy: 0.851549.\n",
      "episode: 241   score: 735.0  epsilon: 1.0    steps: 767  evaluation reward: 210.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: -0.671981. Value loss: 49.235245. Entropy: 1.085204.\n",
      "Iteration 575: Policy loss: -0.223348. Value loss: 24.794386. Entropy: 1.055096.\n",
      "Iteration 576: Policy loss: -0.391772. Value loss: 20.666946. Entropy: 1.071571.\n",
      "episode: 242   score: 110.0  epsilon: 1.0    steps: 187  evaluation reward: 210.3\n",
      "episode: 243   score: 145.0  epsilon: 1.0    steps: 385  evaluation reward: 209.65\n",
      "episode: 244   score: 105.0  epsilon: 1.0    steps: 957  evaluation reward: 209.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 1.435471. Value loss: 23.345072. Entropy: 1.247576.\n",
      "Iteration 578: Policy loss: 1.631441. Value loss: 12.774565. Entropy: 1.251309.\n",
      "Iteration 579: Policy loss: 1.549129. Value loss: 11.271795. Entropy: 1.233467.\n",
      "episode: 245   score: 280.0  epsilon: 1.0    steps: 92  evaluation reward: 210.8\n",
      "episode: 246   score: 105.0  epsilon: 1.0    steps: 639  evaluation reward: 209.75\n",
      "episode: 247   score: 115.0  epsilon: 1.0    steps: 773  evaluation reward: 209.1\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: 1.407793. Value loss: 50.450729. Entropy: 1.133746.\n",
      "Iteration 581: Policy loss: 1.195779. Value loss: 36.155640. Entropy: 1.173568.\n",
      "Iteration 582: Policy loss: 1.206224. Value loss: 24.507889. Entropy: 1.161240.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: 2.207012. Value loss: 31.072365. Entropy: 1.242794.\n",
      "Iteration 584: Policy loss: 2.302174. Value loss: 21.984999. Entropy: 1.238494.\n",
      "Iteration 585: Policy loss: 2.186132. Value loss: 21.140575. Entropy: 1.229338.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.997041. Value loss: 15.840739. Entropy: 1.129589.\n",
      "Iteration 587: Policy loss: 1.021721. Value loss: 10.447080. Entropy: 1.149440.\n",
      "Iteration 588: Policy loss: 0.954544. Value loss: 11.245579. Entropy: 1.191907.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.611140. Value loss: 5.114297. Entropy: 0.980601.\n",
      "Iteration 590: Policy loss: 0.532136. Value loss: 3.739733. Entropy: 1.031987.\n",
      "Iteration 591: Policy loss: 0.496930. Value loss: 3.160108. Entropy: 0.986484.\n",
      "episode: 248   score: 115.0  epsilon: 1.0    steps: 335  evaluation reward: 208.45\n",
      "episode: 249   score: 110.0  epsilon: 1.0    steps: 442  evaluation reward: 207.75\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 3.608548. Value loss: 16.518744. Entropy: 1.034775.\n",
      "Iteration 593: Policy loss: 3.282449. Value loss: 9.374006. Entropy: 1.070215.\n",
      "Iteration 594: Policy loss: 3.583007. Value loss: 8.133503. Entropy: 1.088811.\n",
      "episode: 250   score: 105.0  epsilon: 1.0    steps: 942  evaluation reward: 206.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: -5.942111. Value loss: 598.278809. Entropy: 1.105851.\n",
      "Iteration 596: Policy loss: -3.965269. Value loss: 365.055725. Entropy: 1.076674.\n",
      "Iteration 597: Policy loss: -5.769147. Value loss: 437.579102. Entropy: 1.099367.\n",
      "now time :  2019-02-25 18:50:55.399262\n",
      "episode: 251   score: 335.0  epsilon: 1.0    steps: 130  evaluation reward: 208.4\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 1.376692. Value loss: 25.913271. Entropy: 1.218447.\n",
      "Iteration 599: Policy loss: 1.287495. Value loss: 16.448694. Entropy: 1.226841.\n",
      "Iteration 600: Policy loss: 1.239840. Value loss: 14.913754. Entropy: 1.253996.\n",
      "episode: 252   score: 210.0  epsilon: 1.0    steps: 583  evaluation reward: 208.4\n",
      "episode: 253   score: 125.0  epsilon: 1.0    steps: 706  evaluation reward: 208.3\n",
      "episode: 254   score: 345.0  epsilon: 1.0    steps: 830  evaluation reward: 209.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 1.424765. Value loss: 28.585609. Entropy: 1.152502.\n",
      "Iteration 602: Policy loss: 1.504073. Value loss: 19.722719. Entropy: 1.176958.\n",
      "Iteration 603: Policy loss: 1.449216. Value loss: 16.093168. Entropy: 1.147003.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: -0.692987. Value loss: 214.138351. Entropy: 1.052311.\n",
      "Iteration 605: Policy loss: -0.664957. Value loss: 154.581284. Entropy: 1.039384.\n",
      "Iteration 606: Policy loss: -1.278471. Value loss: 140.961166. Entropy: 1.027382.\n",
      "episode: 255   score: 185.0  epsilon: 1.0    steps: 7  evaluation reward: 209.95\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: 2.865304. Value loss: 47.482708. Entropy: 0.881524.\n",
      "Iteration 608: Policy loss: 2.786136. Value loss: 25.943523. Entropy: 0.898618.\n",
      "Iteration 609: Policy loss: 2.568710. Value loss: 20.706337. Entropy: 0.887427.\n",
      "episode: 256   score: 75.0  epsilon: 1.0    steps: 614  evaluation reward: 208.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 2.050606. Value loss: 28.394798. Entropy: 1.028423.\n",
      "Iteration 611: Policy loss: 1.743275. Value loss: 11.759300. Entropy: 1.094694.\n",
      "Iteration 612: Policy loss: 2.026763. Value loss: 8.717218. Entropy: 1.052386.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -1.433587. Value loss: 319.104248. Entropy: 1.087779.\n",
      "Iteration 614: Policy loss: -1.159556. Value loss: 230.474304. Entropy: 1.081877.\n",
      "Iteration 615: Policy loss: -1.421277. Value loss: 214.610596. Entropy: 1.041594.\n",
      "episode: 257   score: 485.0  epsilon: 1.0    steps: 315  evaluation reward: 212.2\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: 1.102736. Value loss: 33.730564. Entropy: 1.024076.\n",
      "Iteration 617: Policy loss: 1.164309. Value loss: 22.803728. Entropy: 1.020637.\n",
      "Iteration 618: Policy loss: 1.049584. Value loss: 18.803219. Entropy: 1.041008.\n",
      "episode: 258   score: 185.0  epsilon: 1.0    steps: 215  evaluation reward: 212.5\n",
      "episode: 259   score: 155.0  epsilon: 1.0    steps: 445  evaluation reward: 212.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 1.813182. Value loss: 36.776558. Entropy: 1.086916.\n",
      "Iteration 620: Policy loss: 1.757137. Value loss: 20.797125. Entropy: 1.117466.\n",
      "Iteration 621: Policy loss: 1.720307. Value loss: 13.214062. Entropy: 1.114504.\n",
      "episode: 260   score: 140.0  epsilon: 1.0    steps: 717  evaluation reward: 211.55\n",
      "episode: 261   score: 140.0  epsilon: 1.0    steps: 813  evaluation reward: 210.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.863601. Value loss: 24.393204. Entropy: 1.094746.\n",
      "Iteration 623: Policy loss: 0.752071. Value loss: 16.659338. Entropy: 1.142996.\n",
      "Iteration 624: Policy loss: 0.709466. Value loss: 13.971612. Entropy: 1.161761.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 1.902493. Value loss: 20.035238. Entropy: 1.074985.\n",
      "Iteration 626: Policy loss: 1.599581. Value loss: 16.090641. Entropy: 1.069276.\n",
      "Iteration 627: Policy loss: 1.675492. Value loss: 11.550643. Entropy: 1.077771.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.414392. Value loss: 19.356192. Entropy: 1.017995.\n",
      "Iteration 629: Policy loss: 0.465800. Value loss: 11.459042. Entropy: 0.964678.\n",
      "Iteration 630: Policy loss: 0.377700. Value loss: 9.680524. Entropy: 1.001509.\n",
      "episode: 262   score: 225.0  epsilon: 1.0    steps: 45  evaluation reward: 210.4\n",
      "episode: 263   score: 140.0  epsilon: 1.0    steps: 611  evaluation reward: 210.0\n",
      "episode: 264   score: 75.0  epsilon: 1.0    steps: 846  evaluation reward: 208.65\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: 3.744893. Value loss: 13.447838. Entropy: 1.129303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 632: Policy loss: 3.786635. Value loss: 5.819323. Entropy: 1.077302.\n",
      "Iteration 633: Policy loss: 3.601938. Value loss: 5.146475. Entropy: 1.135568.\n",
      "episode: 265   score: 105.0  epsilon: 1.0    steps: 287  evaluation reward: 208.05\n",
      "episode: 266   score: 365.0  epsilon: 1.0    steps: 1013  evaluation reward: 210.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 4.376854. Value loss: 17.391495. Entropy: 1.072004.\n",
      "Iteration 635: Policy loss: 4.326365. Value loss: 9.552444. Entropy: 1.029243.\n",
      "Iteration 636: Policy loss: 3.933841. Value loss: 10.146520. Entropy: 1.099871.\n",
      "episode: 267   score: 155.0  epsilon: 1.0    steps: 482  evaluation reward: 210.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 1.117403. Value loss: 21.288670. Entropy: 1.148572.\n",
      "Iteration 638: Policy loss: 1.096349. Value loss: 14.156104. Entropy: 1.175792.\n",
      "Iteration 639: Policy loss: 0.924472. Value loss: 14.846335. Entropy: 1.174303.\n",
      "episode: 268   score: 30.0  epsilon: 1.0    steps: 60  evaluation reward: 208.9\n",
      "episode: 269   score: 105.0  epsilon: 1.0    steps: 682  evaluation reward: 208.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.109952. Value loss: 10.426733. Entropy: 1.340408.\n",
      "Iteration 641: Policy loss: 0.151605. Value loss: 8.599092. Entropy: 1.343754.\n",
      "Iteration 642: Policy loss: 0.231941. Value loss: 9.232220. Entropy: 1.342977.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: -1.157907. Value loss: 12.365846. Entropy: 1.054088.\n",
      "Iteration 644: Policy loss: -1.151701. Value loss: 11.543032. Entropy: 0.981582.\n",
      "Iteration 645: Policy loss: -1.308491. Value loss: 12.664062. Entropy: 1.017418.\n",
      "episode: 270   score: 160.0  epsilon: 1.0    steps: 180  evaluation reward: 207.65\n",
      "episode: 271   score: 75.0  epsilon: 1.0    steps: 919  evaluation reward: 206.3\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 1.901557. Value loss: 8.614110. Entropy: 1.164010.\n",
      "Iteration 647: Policy loss: 1.973440. Value loss: 5.439155. Entropy: 1.170875.\n",
      "Iteration 648: Policy loss: 1.890327. Value loss: 4.445825. Entropy: 1.151424.\n",
      "episode: 272   score: 320.0  epsilon: 1.0    steps: 376  evaluation reward: 204.65\n",
      "episode: 273   score: 110.0  epsilon: 1.0    steps: 564  evaluation reward: 203.65\n",
      "episode: 274   score: 305.0  epsilon: 1.0    steps: 863  evaluation reward: 204.6\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: -4.826779. Value loss: 524.140320. Entropy: 1.145774.\n",
      "Iteration 650: Policy loss: -4.890873. Value loss: 362.953491. Entropy: 1.173154.\n",
      "Iteration 651: Policy loss: -5.873750. Value loss: 416.770386. Entropy: 1.011443.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: -0.654247. Value loss: 17.357792. Entropy: 1.020795.\n",
      "Iteration 653: Policy loss: -0.494633. Value loss: 12.798702. Entropy: 1.004227.\n",
      "Iteration 654: Policy loss: -0.469135. Value loss: 12.795105. Entropy: 1.006998.\n",
      "episode: 275   score: 110.0  epsilon: 1.0    steps: 759  evaluation reward: 203.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: 2.544201. Value loss: 49.789719. Entropy: 1.162614.\n",
      "Iteration 656: Policy loss: 2.244088. Value loss: 32.445660. Entropy: 1.174454.\n",
      "Iteration 657: Policy loss: 2.260360. Value loss: 26.529341. Entropy: 1.176655.\n",
      "episode: 276   score: 140.0  epsilon: 1.0    steps: 417  evaluation reward: 202.7\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 0.293534. Value loss: 13.286432. Entropy: 1.104126.\n",
      "Iteration 659: Policy loss: 0.291714. Value loss: 8.311396. Entropy: 1.125326.\n",
      "Iteration 660: Policy loss: 0.277503. Value loss: 5.995143. Entropy: 1.142042.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: -2.827030. Value loss: 266.780487. Entropy: 1.040302.\n",
      "Iteration 662: Policy loss: -2.369749. Value loss: 180.403961. Entropy: 1.019554.\n",
      "Iteration 663: Policy loss: -2.307057. Value loss: 121.420662. Entropy: 1.026170.\n",
      "episode: 277   score: 385.0  epsilon: 1.0    steps: 211  evaluation reward: 204.45\n",
      "episode: 278   score: 230.0  epsilon: 1.0    steps: 1012  evaluation reward: 204.95\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: 0.399272. Value loss: 46.629799. Entropy: 1.027058.\n",
      "Iteration 665: Policy loss: 0.895044. Value loss: 30.765402. Entropy: 0.966251.\n",
      "Iteration 666: Policy loss: 0.242096. Value loss: 25.306963. Entropy: 1.031322.\n",
      "episode: 279   score: 75.0  epsilon: 1.0    steps: 448  evaluation reward: 203.6\n",
      "episode: 280   score: 120.0  epsilon: 1.0    steps: 566  evaluation reward: 203.75\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: 1.039237. Value loss: 63.027687. Entropy: 1.030145.\n",
      "Iteration 668: Policy loss: 0.933617. Value loss: 33.488335. Entropy: 1.057594.\n",
      "Iteration 669: Policy loss: 0.722883. Value loss: 27.013512. Entropy: 1.021033.\n",
      "episode: 281   score: 180.0  epsilon: 1.0    steps: 300  evaluation reward: 203.75\n",
      "episode: 282   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 203.75\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: 0.404951. Value loss: 22.836863. Entropy: 0.986420.\n",
      "Iteration 671: Policy loss: 0.500370. Value loss: 16.110725. Entropy: 0.934263.\n",
      "Iteration 672: Policy loss: 0.565049. Value loss: 14.826789. Entropy: 0.985482.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: 0.707502. Value loss: 18.804268. Entropy: 0.928614.\n",
      "Iteration 674: Policy loss: 0.709667. Value loss: 15.787777. Entropy: 0.917835.\n",
      "Iteration 675: Policy loss: 0.647621. Value loss: 14.591881. Entropy: 0.934879.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: 1.277606. Value loss: 10.603004. Entropy: 0.758072.\n",
      "Iteration 677: Policy loss: 1.109091. Value loss: 5.807471. Entropy: 0.759733.\n",
      "Iteration 678: Policy loss: 1.226399. Value loss: 4.978527. Entropy: 0.753177.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: -1.007405. Value loss: 19.373859. Entropy: 0.721465.\n",
      "Iteration 680: Policy loss: -0.977280. Value loss: 10.324354. Entropy: 0.725271.\n",
      "Iteration 681: Policy loss: -0.997440. Value loss: 9.237033. Entropy: 0.760180.\n",
      "episode: 283   score: 265.0  epsilon: 1.0    steps: 35  evaluation reward: 204.3\n",
      "episode: 284   score: 335.0  epsilon: 1.0    steps: 690  evaluation reward: 205.55\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.365313. Value loss: 21.563437. Entropy: 1.014565.\n",
      "Iteration 683: Policy loss: 0.642342. Value loss: 16.047590. Entropy: 0.951838.\n",
      "Iteration 684: Policy loss: 0.590850. Value loss: 17.383472. Entropy: 0.931437.\n",
      "episode: 285   score: 180.0  epsilon: 1.0    steps: 143  evaluation reward: 203.55\n",
      "episode: 286   score: 210.0  epsilon: 1.0    steps: 609  evaluation reward: 203.85\n",
      "episode: 287   score: 210.0  epsilon: 1.0    steps: 957  evaluation reward: 203.85\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: -1.213531. Value loss: 212.312515. Entropy: 0.893891.\n",
      "Iteration 686: Policy loss: -1.373157. Value loss: 135.594193. Entropy: 0.562656.\n",
      "Iteration 687: Policy loss: -0.969726. Value loss: 76.999451. Entropy: 0.771185.\n",
      "episode: 288   score: 210.0  epsilon: 1.0    steps: 353  evaluation reward: 204.3\n",
      "episode: 289   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 204.6\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: -0.328016. Value loss: 18.748169. Entropy: 0.858635.\n",
      "Iteration 689: Policy loss: -0.408293. Value loss: 14.808881. Entropy: 0.844049.\n",
      "Iteration 690: Policy loss: -0.345273. Value loss: 14.840087. Entropy: 0.862815.\n",
      "episode: 290   score: 460.0  epsilon: 1.0    steps: 879  evaluation reward: 207.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: -0.213216. Value loss: 20.292877. Entropy: 0.814474.\n",
      "Iteration 692: Policy loss: -0.400409. Value loss: 15.376615. Entropy: 0.810396.\n",
      "Iteration 693: Policy loss: -0.202308. Value loss: 15.035598. Entropy: 0.799039.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 0.039749. Value loss: 8.487818. Entropy: 0.895559.\n",
      "Iteration 695: Policy loss: 0.178574. Value loss: 6.704656. Entropy: 0.914362.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 696: Policy loss: 0.259666. Value loss: 6.439619. Entropy: 0.894459.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: 1.233210. Value loss: 12.451632. Entropy: 0.584001.\n",
      "Iteration 698: Policy loss: 1.357694. Value loss: 6.719467. Entropy: 0.603833.\n",
      "Iteration 699: Policy loss: 1.335443. Value loss: 6.085748. Entropy: 0.590659.\n",
      "episode: 291   score: 180.0  epsilon: 1.0    steps: 83  evaluation reward: 206.8\n",
      "episode: 292   score: 165.0  epsilon: 1.0    steps: 707  evaluation reward: 204.35\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: 0.073618. Value loss: 34.765648. Entropy: 0.677720.\n",
      "Iteration 701: Policy loss: -0.302723. Value loss: 22.453690. Entropy: 0.725559.\n",
      "Iteration 702: Policy loss: 0.001130. Value loss: 15.503341. Entropy: 0.715273.\n",
      "episode: 293   score: 135.0  epsilon: 1.0    steps: 162  evaluation reward: 203.9\n",
      "episode: 294   score: 180.0  epsilon: 1.0    steps: 996  evaluation reward: 203.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: -1.450639. Value loss: 264.985840. Entropy: 0.737187.\n",
      "Iteration 704: Policy loss: -1.010433. Value loss: 180.922241. Entropy: 0.739724.\n",
      "Iteration 705: Policy loss: -1.590059. Value loss: 177.076889. Entropy: 0.735414.\n",
      "episode: 295   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 204.95\n",
      "episode: 296   score: 180.0  epsilon: 1.0    steps: 515  evaluation reward: 204.95\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: -0.506982. Value loss: 22.320780. Entropy: 0.725658.\n",
      "Iteration 707: Policy loss: -0.499716. Value loss: 19.731754. Entropy: 0.718895.\n",
      "Iteration 708: Policy loss: -0.382798. Value loss: 18.392895. Entropy: 0.746988.\n",
      "episode: 297   score: 410.0  epsilon: 1.0    steps: 305  evaluation reward: 207.25\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: 1.271202. Value loss: 18.603451. Entropy: 1.000121.\n",
      "Iteration 710: Policy loss: 1.187947. Value loss: 13.591244. Entropy: 0.980754.\n",
      "Iteration 711: Policy loss: 1.191081. Value loss: 11.681718. Entropy: 1.007171.\n",
      "episode: 298   score: 120.0  epsilon: 1.0    steps: 769  evaluation reward: 206.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.132339. Value loss: 6.485734. Entropy: 0.786470.\n",
      "Iteration 713: Policy loss: 0.078813. Value loss: 6.393651. Entropy: 0.786162.\n",
      "Iteration 714: Policy loss: 0.148074. Value loss: 4.669150. Entropy: 0.808173.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.432578. Value loss: 8.255375. Entropy: 0.720637.\n",
      "Iteration 716: Policy loss: 0.221593. Value loss: 5.723923. Entropy: 0.726490.\n",
      "Iteration 717: Policy loss: 0.418240. Value loss: 5.648428. Entropy: 0.724184.\n",
      "episode: 299   score: 180.0  epsilon: 1.0    steps: 115  evaluation reward: 206.6\n",
      "episode: 300   score: 135.0  epsilon: 1.0    steps: 728  evaluation reward: 206.6\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: -4.635457. Value loss: 430.039856. Entropy: 0.884918.\n",
      "Iteration 719: Policy loss: -4.413301. Value loss: 252.778671. Entropy: 0.857466.\n",
      "Iteration 720: Policy loss: -4.845509. Value loss: 206.598984. Entropy: 0.803483.\n",
      "now time :  2019-02-25 18:53:14.792537\n",
      "episode: 301   score: 165.0  epsilon: 1.0    steps: 176  evaluation reward: 206.15\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: -0.370688. Value loss: 18.790623. Entropy: 0.709061.\n",
      "Iteration 722: Policy loss: -0.443105. Value loss: 15.145959. Entropy: 0.745560.\n",
      "Iteration 723: Policy loss: -0.379309. Value loss: 12.991389. Entropy: 0.732836.\n",
      "episode: 302   score: 180.0  epsilon: 1.0    steps: 500  evaluation reward: 206.6\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: 2.592577. Value loss: 26.860165. Entropy: 0.615548.\n",
      "Iteration 725: Policy loss: 2.332175. Value loss: 19.865292. Entropy: 0.616154.\n",
      "Iteration 726: Policy loss: 2.378295. Value loss: 19.143553. Entropy: 0.649217.\n",
      "episode: 303   score: 110.0  epsilon: 1.0    steps: 302  evaluation reward: 205.9\n",
      "episode: 304   score: 120.0  epsilon: 1.0    steps: 880  evaluation reward: 205.3\n",
      "episode: 305   score: 440.0  epsilon: 1.0    steps: 939  evaluation reward: 207.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: 1.308353. Value loss: 22.843134. Entropy: 1.158331.\n",
      "Iteration 728: Policy loss: 1.406149. Value loss: 14.715344. Entropy: 1.131426.\n",
      "Iteration 729: Policy loss: 1.226284. Value loss: 12.246174. Entropy: 1.198956.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: -0.982536. Value loss: 17.498049. Entropy: 0.871923.\n",
      "Iteration 731: Policy loss: -0.814494. Value loss: 12.607882. Entropy: 0.914567.\n",
      "Iteration 732: Policy loss: -0.989588. Value loss: 13.036602. Entropy: 0.892147.\n",
      "episode: 306   score: 105.0  epsilon: 1.0    steps: 767  evaluation reward: 207.15\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: 1.261544. Value loss: 17.677689. Entropy: 0.835474.\n",
      "Iteration 734: Policy loss: 1.291699. Value loss: 13.261105. Entropy: 0.875585.\n",
      "Iteration 735: Policy loss: 1.259988. Value loss: 11.500885. Entropy: 0.870505.\n",
      "episode: 307   score: 105.0  epsilon: 1.0    steps: 23  evaluation reward: 206.4\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: 0.575793. Value loss: 9.773047. Entropy: 0.910141.\n",
      "Iteration 737: Policy loss: 0.501050. Value loss: 6.573981. Entropy: 0.907137.\n",
      "Iteration 738: Policy loss: 0.455483. Value loss: 6.404388. Entropy: 0.942068.\n",
      "episode: 308   score: 105.0  epsilon: 1.0    steps: 169  evaluation reward: 205.35\n",
      "episode: 309   score: 545.0  epsilon: 1.0    steps: 564  evaluation reward: 207.0\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: 2.167965. Value loss: 40.945969. Entropy: 0.775793.\n",
      "Iteration 740: Policy loss: 2.578843. Value loss: 21.920132. Entropy: 0.777428.\n",
      "Iteration 741: Policy loss: 2.304238. Value loss: 19.032297. Entropy: 0.743897.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: -4.875083. Value loss: 283.052399. Entropy: 0.616375.\n",
      "Iteration 743: Policy loss: -3.675829. Value loss: 221.571579. Entropy: 0.612825.\n",
      "Iteration 744: Policy loss: -3.975174. Value loss: 150.724747. Entropy: 0.394138.\n",
      "episode: 310   score: 75.0  epsilon: 1.0    steps: 45  evaluation reward: 205.65\n",
      "episode: 311   score: 380.0  epsilon: 1.0    steps: 972  evaluation reward: 207.35\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: -2.896576. Value loss: 217.149597. Entropy: 0.810591.\n",
      "Iteration 746: Policy loss: -2.825065. Value loss: 202.443878. Entropy: 0.815148.\n",
      "Iteration 747: Policy loss: -2.500126. Value loss: 162.227585. Entropy: 0.817698.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.296198. Value loss: 13.075259. Entropy: 0.596187.\n",
      "Iteration 749: Policy loss: 0.198721. Value loss: 7.756130. Entropy: 0.683598.\n",
      "Iteration 750: Policy loss: 0.320749. Value loss: 6.794121. Entropy: 0.659134.\n",
      "episode: 312   score: 260.0  epsilon: 1.0    steps: 861  evaluation reward: 203.1\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 1.584246. Value loss: 21.796926. Entropy: 0.754975.\n",
      "Iteration 752: Policy loss: 1.514038. Value loss: 12.260763. Entropy: 0.673259.\n",
      "Iteration 753: Policy loss: 1.559338. Value loss: 9.923366. Entropy: 0.713742.\n",
      "episode: 313   score: 335.0  epsilon: 1.0    steps: 469  evaluation reward: 204.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: 1.261246. Value loss: 53.351238. Entropy: 0.945553.\n",
      "Iteration 755: Policy loss: 1.962026. Value loss: 18.118948. Entropy: 0.908561.\n",
      "Iteration 756: Policy loss: 2.076374. Value loss: 11.935462. Entropy: 0.878528.\n",
      "episode: 314   score: 290.0  epsilon: 1.0    steps: 318  evaluation reward: 206.0\n",
      "episode: 315   score: 180.0  epsilon: 1.0    steps: 627  evaluation reward: 206.25\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: -0.273201. Value loss: 46.972828. Entropy: 0.798018.\n",
      "Iteration 758: Policy loss: -0.114243. Value loss: 26.794531. Entropy: 0.860088.\n",
      "Iteration 759: Policy loss: -0.378348. Value loss: 21.784847. Entropy: 0.825623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 316   score: 170.0  epsilon: 1.0    steps: 244  evaluation reward: 207.0\n",
      "episode: 317   score: 515.0  epsilon: 1.0    steps: 706  evaluation reward: 210.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: -4.505770. Value loss: 362.924103. Entropy: 0.887806.\n",
      "Iteration 761: Policy loss: -3.867243. Value loss: 246.897507. Entropy: 0.949970.\n",
      "Iteration 762: Policy loss: -4.728679. Value loss: 159.173431. Entropy: 0.956035.\n",
      "episode: 318   score: 380.0  epsilon: 1.0    steps: 72  evaluation reward: 212.3\n",
      "episode: 319   score: 160.0  epsilon: 1.0    steps: 999  evaluation reward: 212.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: -0.054388. Value loss: 18.754875. Entropy: 0.859732.\n",
      "Iteration 764: Policy loss: 0.031836. Value loss: 10.088614. Entropy: 0.870201.\n",
      "Iteration 765: Policy loss: -0.102389. Value loss: 9.090225. Entropy: 0.917261.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: 0.468463. Value loss: 22.701439. Entropy: 0.816836.\n",
      "Iteration 767: Policy loss: 0.546716. Value loss: 18.097824. Entropy: 0.820716.\n",
      "Iteration 768: Policy loss: 0.472472. Value loss: 17.041868. Entropy: 0.830615.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: 1.811369. Value loss: 28.306744. Entropy: 0.741193.\n",
      "Iteration 770: Policy loss: 1.585276. Value loss: 16.667114. Entropy: 0.749407.\n",
      "Iteration 771: Policy loss: 1.942428. Value loss: 11.608177. Entropy: 0.734900.\n",
      "episode: 320   score: 155.0  epsilon: 1.0    steps: 506  evaluation reward: 211.3\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: 2.032201. Value loss: 33.687729. Entropy: 0.566590.\n",
      "Iteration 773: Policy loss: 1.371287. Value loss: 16.425373. Entropy: 0.577865.\n",
      "Iteration 774: Policy loss: 1.994445. Value loss: 15.453671. Entropy: 0.554114.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: -2.416236. Value loss: 84.584023. Entropy: 0.555251.\n",
      "Iteration 776: Policy loss: -2.689778. Value loss: 87.631149. Entropy: 0.463780.\n",
      "Iteration 777: Policy loss: -2.593617. Value loss: 72.499344. Entropy: 0.548710.\n",
      "episode: 321   score: 120.0  epsilon: 1.0    steps: 253  evaluation reward: 209.9\n",
      "episode: 322   score: 185.0  epsilon: 1.0    steps: 262  evaluation reward: 210.2\n",
      "episode: 323   score: 110.0  epsilon: 1.0    steps: 566  evaluation reward: 208.65\n",
      "episode: 324   score: 380.0  epsilon: 1.0    steps: 737  evaluation reward: 210.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: 2.350625. Value loss: 30.341534. Entropy: 0.693943.\n",
      "Iteration 779: Policy loss: 2.315520. Value loss: 20.095911. Entropy: 0.684699.\n",
      "Iteration 780: Policy loss: 2.421723. Value loss: 17.938047. Entropy: 0.651452.\n",
      "episode: 325   score: 180.0  epsilon: 1.0    steps: 105  evaluation reward: 210.05\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: 1.163582. Value loss: 40.871067. Entropy: 0.733375.\n",
      "Iteration 782: Policy loss: 1.502276. Value loss: 17.228193. Entropy: 0.737432.\n",
      "Iteration 783: Policy loss: 1.441113. Value loss: 13.537210. Entropy: 0.730174.\n",
      "episode: 326   score: 240.0  epsilon: 1.0    steps: 966  evaluation reward: 207.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: 0.077686. Value loss: 25.426455. Entropy: 0.803818.\n",
      "Iteration 785: Policy loss: -0.133034. Value loss: 21.380339. Entropy: 0.866396.\n",
      "Iteration 786: Policy loss: -0.172959. Value loss: 16.027727. Entropy: 0.849361.\n",
      "episode: 327   score: 565.0  epsilon: 1.0    steps: 812  evaluation reward: 211.05\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: 0.734909. Value loss: 9.654158. Entropy: 0.928313.\n",
      "Iteration 788: Policy loss: 0.793640. Value loss: 8.458842. Entropy: 0.886995.\n",
      "Iteration 789: Policy loss: 0.674042. Value loss: 5.612224. Entropy: 0.922784.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: -0.152775. Value loss: 14.800857. Entropy: 0.636243.\n",
      "Iteration 791: Policy loss: -0.315737. Value loss: 10.714743. Entropy: 0.626571.\n",
      "Iteration 792: Policy loss: -0.210543. Value loss: 9.847696. Entropy: 0.597743.\n",
      "episode: 328   score: 110.0  epsilon: 1.0    steps: 271  evaluation reward: 209.55\n",
      "episode: 329   score: 185.0  epsilon: 1.0    steps: 408  evaluation reward: 207.85\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: 0.827393. Value loss: 20.149658. Entropy: 0.817060.\n",
      "Iteration 794: Policy loss: 0.769902. Value loss: 9.838785. Entropy: 0.784978.\n",
      "Iteration 795: Policy loss: 0.726178. Value loss: 9.100560. Entropy: 0.826102.\n",
      "episode: 330   score: 110.0  epsilon: 1.0    steps: 237  evaluation reward: 207.15\n",
      "episode: 331   score: 110.0  epsilon: 1.0    steps: 537  evaluation reward: 207.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: 2.425545. Value loss: 25.637897. Entropy: 0.929408.\n",
      "Iteration 797: Policy loss: 2.330812. Value loss: 15.778866. Entropy: 0.919802.\n",
      "Iteration 798: Policy loss: 2.293658. Value loss: 14.602380. Entropy: 0.891372.\n",
      "episode: 332   score: 115.0  epsilon: 1.0    steps: 670  evaluation reward: 205.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: -0.566634. Value loss: 23.263536. Entropy: 1.013138.\n",
      "Iteration 800: Policy loss: -0.354234. Value loss: 17.008564. Entropy: 1.019711.\n",
      "Iteration 801: Policy loss: -0.501956. Value loss: 14.295349. Entropy: 1.046570.\n",
      "episode: 333   score: 230.0  epsilon: 1.0    steps: 58  evaluation reward: 206.45\n",
      "episode: 334   score: 120.0  epsilon: 1.0    steps: 986  evaluation reward: 206.55\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: -0.489790. Value loss: 36.790417. Entropy: 1.064532.\n",
      "Iteration 803: Policy loss: -0.433573. Value loss: 27.079578. Entropy: 1.097203.\n",
      "Iteration 804: Policy loss: -0.482585. Value loss: 22.450092. Entropy: 1.086019.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: 0.371152. Value loss: 9.719935. Entropy: 0.888389.\n",
      "Iteration 806: Policy loss: 0.253357. Value loss: 7.418529. Entropy: 0.916924.\n",
      "Iteration 807: Policy loss: 0.274560. Value loss: 6.198661. Entropy: 0.948315.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: 1.107303. Value loss: 12.466595. Entropy: 1.018132.\n",
      "Iteration 809: Policy loss: 1.274182. Value loss: 9.981114. Entropy: 0.994149.\n",
      "Iteration 810: Policy loss: 1.213648. Value loss: 8.377921. Entropy: 0.964320.\n",
      "episode: 335   score: 105.0  epsilon: 1.0    steps: 302  evaluation reward: 206.5\n",
      "episode: 336   score: 320.0  epsilon: 1.0    steps: 463  evaluation reward: 206.6\n",
      "episode: 337   score: 190.0  epsilon: 1.0    steps: 799  evaluation reward: 207.25\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: -2.236819. Value loss: 405.393951. Entropy: 0.880513.\n",
      "Iteration 812: Policy loss: -2.055059. Value loss: 283.964081. Entropy: 1.010272.\n",
      "Iteration 813: Policy loss: -1.413157. Value loss: 164.402390. Entropy: 0.901851.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: 2.615795. Value loss: 40.582981. Entropy: 1.023901.\n",
      "Iteration 815: Policy loss: 2.670204. Value loss: 29.492561. Entropy: 1.027552.\n",
      "Iteration 816: Policy loss: 2.691078. Value loss: 25.341307. Entropy: 1.062439.\n",
      "episode: 338   score: 250.0  epsilon: 1.0    steps: 626  evaluation reward: 206.15\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: -3.120248. Value loss: 85.072227. Entropy: 1.088614.\n",
      "Iteration 818: Policy loss: -3.184518. Value loss: 43.305717. Entropy: 1.046617.\n",
      "Iteration 819: Policy loss: -3.569892. Value loss: 33.674694. Entropy: 1.039692.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: 2.984923. Value loss: 102.273338. Entropy: 1.112080.\n",
      "Iteration 821: Policy loss: 2.984528. Value loss: 42.906269. Entropy: 1.081137.\n",
      "Iteration 822: Policy loss: 3.259261. Value loss: 30.281488. Entropy: 1.037422.\n",
      "episode: 339   score: 145.0  epsilon: 1.0    steps: 14  evaluation reward: 206.55\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: -0.135022. Value loss: 241.504227. Entropy: 0.979895.\n",
      "Iteration 824: Policy loss: -0.373096. Value loss: 169.590988. Entropy: 0.987939.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 825: Policy loss: 0.285850. Value loss: 121.468132. Entropy: 0.986363.\n",
      "episode: 340   score: 130.0  epsilon: 1.0    steps: 482  evaluation reward: 205.75\n",
      "episode: 341   score: 460.0  epsilon: 1.0    steps: 961  evaluation reward: 203.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 3.504089. Value loss: 48.476681. Entropy: 0.874652.\n",
      "Iteration 827: Policy loss: 3.563432. Value loss: 23.815170. Entropy: 0.846059.\n",
      "Iteration 828: Policy loss: 3.713620. Value loss: 20.184858. Entropy: 0.863239.\n",
      "episode: 342   score: 250.0  epsilon: 1.0    steps: 658  evaluation reward: 204.4\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: -0.861620. Value loss: 208.167862. Entropy: 1.014819.\n",
      "Iteration 830: Policy loss: -1.469862. Value loss: 159.584930. Entropy: 1.012413.\n",
      "Iteration 831: Policy loss: -0.970864. Value loss: 153.397675. Entropy: 1.004148.\n",
      "episode: 343   score: 295.0  epsilon: 1.0    steps: 240  evaluation reward: 205.9\n",
      "episode: 344   score: 315.0  epsilon: 1.0    steps: 367  evaluation reward: 208.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: 1.264560. Value loss: 48.186291. Entropy: 0.923181.\n",
      "Iteration 833: Policy loss: 1.369697. Value loss: 34.710014. Entropy: 0.941603.\n",
      "Iteration 834: Policy loss: 1.441591. Value loss: 26.668333. Entropy: 0.961498.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: 0.523443. Value loss: 31.360023. Entropy: 1.007461.\n",
      "Iteration 836: Policy loss: 0.366413. Value loss: 25.681696. Entropy: 0.996957.\n",
      "Iteration 837: Policy loss: 0.433813. Value loss: 22.572084. Entropy: 0.995160.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: 0.897261. Value loss: 42.497997. Entropy: 0.783908.\n",
      "Iteration 839: Policy loss: 0.657374. Value loss: 32.260258. Entropy: 0.784779.\n",
      "Iteration 840: Policy loss: 0.892147. Value loss: 27.206314. Entropy: 0.765691.\n",
      "episode: 345   score: 110.0  epsilon: 1.0    steps: 3  evaluation reward: 206.3\n",
      "episode: 346   score: 475.0  epsilon: 1.0    steps: 581  evaluation reward: 210.0\n",
      "episode: 347   score: 490.0  epsilon: 1.0    steps: 842  evaluation reward: 213.75\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: 2.915080. Value loss: 75.349846. Entropy: 1.003209.\n",
      "Iteration 842: Policy loss: 3.408372. Value loss: 30.325190. Entropy: 1.018164.\n",
      "Iteration 843: Policy loss: 3.129541. Value loss: 24.271528. Entropy: 1.033700.\n",
      "episode: 348   score: 240.0  epsilon: 1.0    steps: 955  evaluation reward: 215.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: 1.983676. Value loss: 59.971939. Entropy: 1.041664.\n",
      "Iteration 845: Policy loss: 2.338478. Value loss: 32.888512. Entropy: 1.036884.\n",
      "Iteration 846: Policy loss: 2.143137. Value loss: 27.027285. Entropy: 1.057427.\n",
      "episode: 349   score: 225.0  epsilon: 1.0    steps: 424  evaluation reward: 216.15\n",
      "episode: 350   score: 210.0  epsilon: 1.0    steps: 715  evaluation reward: 217.2\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: 0.288920. Value loss: 40.640491. Entropy: 1.118549.\n",
      "Iteration 848: Policy loss: 0.424210. Value loss: 28.861698. Entropy: 1.121514.\n",
      "Iteration 849: Policy loss: 0.674456. Value loss: 26.173746. Entropy: 1.092837.\n",
      "now time :  2019-02-25 18:55:39.994631\n",
      "episode: 351   score: 135.0  epsilon: 1.0    steps: 348  evaluation reward: 215.2\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: 0.299090. Value loss: 43.406849. Entropy: 1.084868.\n",
      "Iteration 851: Policy loss: 0.592841. Value loss: 32.958305. Entropy: 1.110004.\n",
      "Iteration 852: Policy loss: 0.255854. Value loss: 26.708378. Entropy: 1.104409.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: 2.472506. Value loss: 30.279846. Entropy: 1.190456.\n",
      "Iteration 854: Policy loss: 2.641321. Value loss: 22.054407. Entropy: 1.263020.\n",
      "Iteration 855: Policy loss: 2.617960. Value loss: 17.827988. Entropy: 1.266864.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: -1.174672. Value loss: 221.922897. Entropy: 1.019382.\n",
      "Iteration 857: Policy loss: -1.224887. Value loss: 118.779861. Entropy: 1.015421.\n",
      "Iteration 858: Policy loss: -1.344744. Value loss: 103.494682. Entropy: 1.007143.\n",
      "episode: 352   score: 320.0  epsilon: 1.0    steps: 245  evaluation reward: 216.3\n",
      "episode: 353   score: 35.0  epsilon: 1.0    steps: 313  evaluation reward: 215.4\n",
      "episode: 354   score: 115.0  epsilon: 1.0    steps: 965  evaluation reward: 213.1\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: 0.908363. Value loss: 79.082802. Entropy: 1.134187.\n",
      "Iteration 860: Policy loss: 0.936583. Value loss: 51.840282. Entropy: 1.122699.\n",
      "Iteration 861: Policy loss: 0.972671. Value loss: 40.680134. Entropy: 1.154062.\n",
      "episode: 355   score: 265.0  epsilon: 1.0    steps: 54  evaluation reward: 213.9\n",
      "episode: 356   score: 425.0  epsilon: 1.0    steps: 548  evaluation reward: 217.4\n",
      "episode: 357   score: 225.0  epsilon: 1.0    steps: 843  evaluation reward: 214.8\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: 1.861976. Value loss: 38.656494. Entropy: 1.320055.\n",
      "Iteration 863: Policy loss: 1.657384. Value loss: 24.347767. Entropy: 1.341934.\n",
      "Iteration 864: Policy loss: 1.714348. Value loss: 20.875435. Entropy: 1.333906.\n",
      "episode: 358   score: 215.0  epsilon: 1.0    steps: 497  evaluation reward: 215.1\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: 1.606411. Value loss: 45.546936. Entropy: 1.314864.\n",
      "Iteration 866: Policy loss: 1.799051. Value loss: 30.699224. Entropy: 1.316145.\n",
      "Iteration 867: Policy loss: 1.827307. Value loss: 26.044699. Entropy: 1.330292.\n",
      "episode: 359   score: 235.0  epsilon: 1.0    steps: 673  evaluation reward: 215.9\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: -1.704475. Value loss: 35.168392. Entropy: 1.340853.\n",
      "Iteration 869: Policy loss: -1.423145. Value loss: 24.191095. Entropy: 1.337456.\n",
      "Iteration 870: Policy loss: -1.496192. Value loss: 22.474155. Entropy: 1.331918.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: 1.264060. Value loss: 41.851814. Entropy: 1.111099.\n",
      "Iteration 872: Policy loss: 1.114139. Value loss: 25.603722. Entropy: 1.116854.\n",
      "Iteration 873: Policy loss: 0.945571. Value loss: 25.721464. Entropy: 1.133169.\n",
      "episode: 360   score: 80.0  epsilon: 1.0    steps: 788  evaluation reward: 215.3\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: 2.058565. Value loss: 39.634472. Entropy: 1.162071.\n",
      "Iteration 875: Policy loss: 1.663913. Value loss: 28.862926. Entropy: 1.206933.\n",
      "Iteration 876: Policy loss: 1.918960. Value loss: 26.033634. Entropy: 1.191109.\n",
      "episode: 361   score: 125.0  epsilon: 1.0    steps: 620  evaluation reward: 215.15\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: 0.775508. Value loss: 31.361635. Entropy: 0.730483.\n",
      "Iteration 878: Policy loss: 1.217104. Value loss: 20.934757. Entropy: 0.724664.\n",
      "Iteration 879: Policy loss: 0.807588. Value loss: 18.178438. Entropy: 0.695218.\n",
      "episode: 362   score: 185.0  epsilon: 1.0    steps: 111  evaluation reward: 214.75\n",
      "episode: 363   score: 165.0  epsilon: 1.0    steps: 208  evaluation reward: 215.0\n",
      "episode: 364   score: 245.0  epsilon: 1.0    steps: 269  evaluation reward: 216.7\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: 0.794351. Value loss: 31.243233. Entropy: 1.132287.\n",
      "Iteration 881: Policy loss: 0.528715. Value loss: 25.314922. Entropy: 1.153477.\n",
      "Iteration 882: Policy loss: 0.660113. Value loss: 18.452295. Entropy: 1.135241.\n",
      "episode: 365   score: 260.0  epsilon: 1.0    steps: 901  evaluation reward: 218.25\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: 0.486445. Value loss: 27.958183. Entropy: 1.096894.\n",
      "Iteration 884: Policy loss: 0.462857. Value loss: 19.098013. Entropy: 1.062322.\n",
      "Iteration 885: Policy loss: 0.356903. Value loss: 16.132330. Entropy: 1.053255.\n",
      "episode: 366   score: 210.0  epsilon: 1.0    steps: 442  evaluation reward: 216.7\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: 0.956038. Value loss: 25.212267. Entropy: 1.073807.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 887: Policy loss: 0.959360. Value loss: 18.417784. Entropy: 1.083453.\n",
      "Iteration 888: Policy loss: 0.739437. Value loss: 15.917133. Entropy: 1.059605.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: -1.165611. Value loss: 248.782028. Entropy: 1.082668.\n",
      "Iteration 890: Policy loss: -0.228072. Value loss: 79.446518. Entropy: 1.060001.\n",
      "Iteration 891: Policy loss: -1.177929. Value loss: 86.419563. Entropy: 1.112710.\n",
      "episode: 367   score: 240.0  epsilon: 1.0    steps: 649  evaluation reward: 217.55\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: 1.527296. Value loss: 18.246973. Entropy: 0.806019.\n",
      "Iteration 893: Policy loss: 1.687675. Value loss: 13.927464. Entropy: 0.857328.\n",
      "Iteration 894: Policy loss: 1.586190. Value loss: 12.392501. Entropy: 0.810551.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: 0.204911. Value loss: 29.110867. Entropy: 0.678555.\n",
      "Iteration 896: Policy loss: 0.355732. Value loss: 26.010105. Entropy: 0.672977.\n",
      "Iteration 897: Policy loss: 0.223353. Value loss: 20.715302. Entropy: 0.670748.\n",
      "episode: 368   score: 150.0  epsilon: 1.0    steps: 292  evaluation reward: 218.75\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: -0.776726. Value loss: 125.053787. Entropy: 0.688578.\n",
      "Iteration 899: Policy loss: -0.826659. Value loss: 84.056145. Entropy: 0.708831.\n",
      "Iteration 900: Policy loss: -1.130264. Value loss: 87.283157. Entropy: 0.687646.\n",
      "episode: 369   score: 100.0  epsilon: 1.0    steps: 684  evaluation reward: 218.7\n",
      "episode: 370   score: 410.0  epsilon: 1.0    steps: 948  evaluation reward: 221.2\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: -0.464540. Value loss: 37.825394. Entropy: 0.815272.\n",
      "Iteration 902: Policy loss: -0.449965. Value loss: 25.487301. Entropy: 0.819330.\n",
      "Iteration 903: Policy loss: -0.527715. Value loss: 20.321699. Entropy: 0.814388.\n",
      "episode: 371   score: 290.0  epsilon: 1.0    steps: 122  evaluation reward: 223.35\n",
      "episode: 372   score: 120.0  epsilon: 1.0    steps: 471  evaluation reward: 221.35\n",
      "episode: 373   score: 300.0  epsilon: 1.0    steps: 556  evaluation reward: 223.25\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: 2.558523. Value loss: 35.529179. Entropy: 1.024273.\n",
      "Iteration 905: Policy loss: 2.180625. Value loss: 21.824219. Entropy: 1.047236.\n",
      "Iteration 906: Policy loss: 2.243326. Value loss: 21.387686. Entropy: 1.075675.\n",
      "episode: 374   score: 360.0  epsilon: 1.0    steps: 851  evaluation reward: 223.8\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: 0.204853. Value loss: 24.600788. Entropy: 1.102560.\n",
      "Iteration 908: Policy loss: -0.023449. Value loss: 19.029100. Entropy: 1.098383.\n",
      "Iteration 909: Policy loss: -0.221304. Value loss: 15.777134. Entropy: 1.089890.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: 2.265067. Value loss: 17.384796. Entropy: 0.920130.\n",
      "Iteration 911: Policy loss: 1.963671. Value loss: 11.821898. Entropy: 0.931557.\n",
      "Iteration 912: Policy loss: 2.039954. Value loss: 9.713765. Entropy: 0.876896.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: 1.336528. Value loss: 23.706877. Entropy: 0.921263.\n",
      "Iteration 914: Policy loss: 1.289865. Value loss: 13.069208. Entropy: 0.940267.\n",
      "Iteration 915: Policy loss: 1.228268. Value loss: 10.968940. Entropy: 0.941525.\n",
      "episode: 375   score: 80.0  epsilon: 1.0    steps: 27  evaluation reward: 223.5\n",
      "episode: 376   score: 590.0  epsilon: 1.0    steps: 155  evaluation reward: 228.0\n",
      "episode: 377   score: 210.0  epsilon: 1.0    steps: 329  evaluation reward: 226.25\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 1.086912. Value loss: 12.203672. Entropy: 1.059852.\n",
      "Iteration 917: Policy loss: 1.188188. Value loss: 7.900269. Entropy: 1.052721.\n",
      "Iteration 918: Policy loss: 1.048786. Value loss: 7.780447. Entropy: 1.047220.\n",
      "episode: 378   score: 120.0  epsilon: 1.0    steps: 958  evaluation reward: 225.15\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: 1.623259. Value loss: 30.361286. Entropy: 1.040922.\n",
      "Iteration 920: Policy loss: 1.856261. Value loss: 17.919172. Entropy: 1.074895.\n",
      "Iteration 921: Policy loss: 1.752541. Value loss: 14.414336. Entropy: 1.103870.\n",
      "episode: 379   score: 155.0  epsilon: 1.0    steps: 604  evaluation reward: 225.95\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: 0.570735. Value loss: 52.063591. Entropy: 1.186488.\n",
      "Iteration 923: Policy loss: 0.069106. Value loss: 27.088346. Entropy: 1.159485.\n",
      "Iteration 924: Policy loss: 0.201040. Value loss: 25.938614. Entropy: 1.181356.\n",
      "episode: 380   score: 215.0  epsilon: 1.0    steps: 415  evaluation reward: 226.9\n",
      "episode: 381   score: 105.0  epsilon: 1.0    steps: 775  evaluation reward: 226.15\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: 0.012632. Value loss: 257.347107. Entropy: 1.137737.\n",
      "Iteration 926: Policy loss: -0.063658. Value loss: 147.810349. Entropy: 1.038276.\n",
      "Iteration 927: Policy loss: 0.985461. Value loss: 142.913345. Entropy: 1.109056.\n",
      "episode: 382   score: 155.0  epsilon: 1.0    steps: 977  evaluation reward: 225.6\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 928: Policy loss: 0.796331. Value loss: 40.124706. Entropy: 1.132399.\n",
      "Iteration 929: Policy loss: 0.803744. Value loss: 22.343201. Entropy: 1.124887.\n",
      "Iteration 930: Policy loss: 0.761764. Value loss: 19.323626. Entropy: 1.137771.\n",
      "episode: 383   score: 75.0  epsilon: 1.0    steps: 326  evaluation reward: 223.7\n",
      "episode: 384   score: 340.0  epsilon: 1.0    steps: 743  evaluation reward: 223.75\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: 2.746538. Value loss: 44.619652. Entropy: 1.053182.\n",
      "Iteration 932: Policy loss: 2.520579. Value loss: 28.838268. Entropy: 1.076714.\n",
      "Iteration 933: Policy loss: 2.594801. Value loss: 23.592506. Entropy: 1.093925.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: 1.900922. Value loss: 18.559658. Entropy: 1.107223.\n",
      "Iteration 935: Policy loss: 2.016693. Value loss: 13.067571. Entropy: 1.133336.\n",
      "Iteration 936: Policy loss: 1.915696. Value loss: 9.287585. Entropy: 1.133418.\n",
      "episode: 385   score: 220.0  epsilon: 1.0    steps: 111  evaluation reward: 224.15\n",
      "episode: 386   score: 120.0  epsilon: 1.0    steps: 625  evaluation reward: 223.25\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 2.392152. Value loss: 42.516968. Entropy: 1.166137.\n",
      "Iteration 938: Policy loss: 2.393460. Value loss: 28.925995. Entropy: 1.146242.\n",
      "Iteration 939: Policy loss: 2.572314. Value loss: 22.720938. Entropy: 1.171669.\n",
      "episode: 387   score: 435.0  epsilon: 1.0    steps: 178  evaluation reward: 225.5\n",
      "episode: 388   score: 75.0  epsilon: 1.0    steps: 754  evaluation reward: 224.15\n",
      "episode: 389   score: 55.0  epsilon: 1.0    steps: 844  evaluation reward: 222.6\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 2.926893. Value loss: 26.924932. Entropy: 1.014001.\n",
      "Iteration 941: Policy loss: 2.694308. Value loss: 13.753195. Entropy: 1.008101.\n",
      "Iteration 942: Policy loss: 2.926732. Value loss: 12.389254. Entropy: 1.043433.\n",
      "episode: 390   score: 155.0  epsilon: 1.0    steps: 436  evaluation reward: 219.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: -1.544457. Value loss: 37.581783. Entropy: 1.027341.\n",
      "Iteration 944: Policy loss: -1.377006. Value loss: 29.591251. Entropy: 1.005334.\n",
      "Iteration 945: Policy loss: -1.569440. Value loss: 23.646238. Entropy: 1.006262.\n",
      "episode: 391   score: 105.0  epsilon: 1.0    steps: 123  evaluation reward: 218.8\n",
      "episode: 392   score: 75.0  epsilon: 1.0    steps: 623  evaluation reward: 217.9\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: -2.922121. Value loss: 248.575119. Entropy: 1.048223.\n",
      "Iteration 947: Policy loss: -3.038126. Value loss: 125.905167. Entropy: 1.047870.\n",
      "Iteration 948: Policy loss: -2.309164. Value loss: 109.002419. Entropy: 1.059996.\n",
      "episode: 393   score: 180.0  epsilon: 1.0    steps: 361  evaluation reward: 218.35\n",
      "episode: 394   score: 125.0  epsilon: 1.0    steps: 746  evaluation reward: 217.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 395   score: 400.0  epsilon: 1.0    steps: 897  evaluation reward: 219.7\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 1.540264. Value loss: 37.846848. Entropy: 1.076899.\n",
      "Iteration 950: Policy loss: 1.575651. Value loss: 22.940815. Entropy: 1.088716.\n",
      "Iteration 951: Policy loss: 1.522644. Value loss: 15.462969. Entropy: 1.071392.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 3.073817. Value loss: 32.611641. Entropy: 1.099908.\n",
      "Iteration 953: Policy loss: 3.034723. Value loss: 21.897478. Entropy: 1.100896.\n",
      "Iteration 954: Policy loss: 3.342264. Value loss: 20.614176. Entropy: 1.116136.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: -0.699732. Value loss: 18.683393. Entropy: 1.096446.\n",
      "Iteration 956: Policy loss: -0.518999. Value loss: 13.494078. Entropy: 1.080412.\n",
      "Iteration 957: Policy loss: -0.846726. Value loss: 14.934360. Entropy: 1.071246.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: -0.780443. Value loss: 24.085629. Entropy: 0.855185.\n",
      "Iteration 959: Policy loss: -0.616723. Value loss: 17.987293. Entropy: 0.886800.\n",
      "Iteration 960: Policy loss: -0.797976. Value loss: 16.216406. Entropy: 0.883581.\n",
      "episode: 396   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 220.0\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: 0.698498. Value loss: 252.356995. Entropy: 0.869771.\n",
      "Iteration 962: Policy loss: 0.719883. Value loss: 183.791656. Entropy: 0.921908.\n",
      "Iteration 963: Policy loss: 0.367389. Value loss: 144.446381. Entropy: 0.925553.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: 1.213391. Value loss: 23.383562. Entropy: 0.623571.\n",
      "Iteration 965: Policy loss: 1.386841. Value loss: 17.163092. Entropy: 0.627103.\n",
      "Iteration 966: Policy loss: 1.393307. Value loss: 13.961119. Entropy: 0.596513.\n",
      "episode: 397   score: 260.0  epsilon: 1.0    steps: 413  evaluation reward: 218.5\n",
      "episode: 398   score: 180.0  epsilon: 1.0    steps: 527  evaluation reward: 219.1\n",
      "episode: 399   score: 110.0  epsilon: 1.0    steps: 654  evaluation reward: 218.4\n",
      "episode: 400   score: 240.0  epsilon: 1.0    steps: 962  evaluation reward: 219.45\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: -0.438429. Value loss: 23.938354. Entropy: 1.039918.\n",
      "Iteration 968: Policy loss: -1.004606. Value loss: 20.198246. Entropy: 1.045056.\n",
      "Iteration 969: Policy loss: -0.423041. Value loss: 16.295662. Entropy: 1.053664.\n",
      "now time :  2019-02-25 18:57:54.707277\n",
      "episode: 401   score: 355.0  epsilon: 1.0    steps: 45  evaluation reward: 221.35\n",
      "episode: 402   score: 210.0  epsilon: 1.0    steps: 277  evaluation reward: 221.65\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: 0.588076. Value loss: 18.533646. Entropy: 1.172397.\n",
      "Iteration 971: Policy loss: 0.479939. Value loss: 16.285542. Entropy: 1.120386.\n",
      "Iteration 972: Policy loss: 0.272000. Value loss: 14.705318. Entropy: 1.179027.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: 0.572129. Value loss: 39.862766. Entropy: 1.202658.\n",
      "Iteration 974: Policy loss: 0.745915. Value loss: 25.657993. Entropy: 1.189608.\n",
      "Iteration 975: Policy loss: 0.622991. Value loss: 20.578169. Entropy: 1.209640.\n",
      "episode: 403   score: 335.0  epsilon: 1.0    steps: 217  evaluation reward: 223.9\n",
      "episode: 404   score: 90.0  epsilon: 1.0    steps: 383  evaluation reward: 223.6\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 1.825423. Value loss: 18.700905. Entropy: 0.919017.\n",
      "Iteration 977: Policy loss: 1.753282. Value loss: 13.363669. Entropy: 0.931526.\n",
      "Iteration 978: Policy loss: 1.806203. Value loss: 12.176778. Entropy: 0.950427.\n",
      "episode: 405   score: 110.0  epsilon: 1.0    steps: 68  evaluation reward: 220.3\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: 0.679007. Value loss: 15.761767. Entropy: 0.718365.\n",
      "Iteration 980: Policy loss: 0.782020. Value loss: 9.975101. Entropy: 0.725466.\n",
      "Iteration 981: Policy loss: 0.467297. Value loss: 7.294622. Entropy: 0.722132.\n",
      "episode: 406   score: 110.0  epsilon: 1.0    steps: 436  evaluation reward: 220.35\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: 0.741652. Value loss: 39.249401. Entropy: 0.873355.\n",
      "Iteration 983: Policy loss: 0.733800. Value loss: 21.605267. Entropy: 0.871585.\n",
      "Iteration 984: Policy loss: 0.617877. Value loss: 20.385393. Entropy: 0.861580.\n",
      "episode: 407   score: 225.0  epsilon: 1.0    steps: 739  evaluation reward: 221.55\n",
      "episode: 408   score: 270.0  epsilon: 1.0    steps: 874  evaluation reward: 223.2\n",
      "episode: 409   score: 210.0  epsilon: 1.0    steps: 1013  evaluation reward: 219.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: 0.577215. Value loss: 28.532644. Entropy: 0.992313.\n",
      "Iteration 986: Policy loss: 0.670752. Value loss: 22.006836. Entropy: 0.984215.\n",
      "Iteration 987: Policy loss: 0.679552. Value loss: 18.834564. Entropy: 0.987588.\n",
      "episode: 410   score: 75.0  epsilon: 1.0    steps: 288  evaluation reward: 219.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: -1.375460. Value loss: 34.048363. Entropy: 1.104411.\n",
      "Iteration 989: Policy loss: -1.697965. Value loss: 23.586800. Entropy: 1.126233.\n",
      "Iteration 990: Policy loss: -1.603658. Value loss: 19.973883. Entropy: 1.115556.\n",
      "episode: 411   score: 80.0  epsilon: 1.0    steps: 425  evaluation reward: 216.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 991: Policy loss: -0.197976. Value loss: 17.993031. Entropy: 1.080515.\n",
      "Iteration 992: Policy loss: -0.044881. Value loss: 13.741363. Entropy: 1.050458.\n",
      "Iteration 993: Policy loss: -0.311302. Value loss: 12.652888. Entropy: 1.087315.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: -0.841065. Value loss: 13.448107. Entropy: 1.096933.\n",
      "Iteration 995: Policy loss: -0.656694. Value loss: 9.801185. Entropy: 1.105791.\n",
      "Iteration 996: Policy loss: -0.754830. Value loss: 9.961101. Entropy: 1.099239.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: -0.963418. Value loss: 12.264050. Entropy: 0.759724.\n",
      "Iteration 998: Policy loss: -1.035745. Value loss: 10.317405. Entropy: 0.771216.\n",
      "Iteration 999: Policy loss: -1.018709. Value loss: 8.465518. Entropy: 0.786513.\n",
      "episode: 412   score: 225.0  epsilon: 1.0    steps: 5  evaluation reward: 216.5\n",
      "episode: 413   score: 270.0  epsilon: 1.0    steps: 157  evaluation reward: 215.85\n",
      "episode: 414   score: 350.0  epsilon: 1.0    steps: 541  evaluation reward: 216.45\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: -1.412527. Value loss: 201.838608. Entropy: 0.852947.\n",
      "Iteration 1001: Policy loss: -1.619251. Value loss: 141.879913. Entropy: 0.818647.\n",
      "Iteration 1002: Policy loss: -1.104853. Value loss: 50.041801. Entropy: 0.861565.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 0.093593. Value loss: 31.831158. Entropy: 0.859412.\n",
      "Iteration 1004: Policy loss: -0.018522. Value loss: 23.975336. Entropy: 0.869055.\n",
      "Iteration 1005: Policy loss: 0.055483. Value loss: 21.564480. Entropy: 0.886344.\n",
      "episode: 415   score: 185.0  epsilon: 1.0    steps: 648  evaluation reward: 216.5\n",
      "episode: 416   score: 190.0  epsilon: 1.0    steps: 782  evaluation reward: 216.7\n",
      "episode: 417   score: 180.0  epsilon: 1.0    steps: 935  evaluation reward: 213.35\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: -1.330726. Value loss: 17.993284. Entropy: 1.155384.\n",
      "Iteration 1007: Policy loss: -1.395577. Value loss: 15.610743. Entropy: 1.157321.\n",
      "Iteration 1008: Policy loss: -1.468234. Value loss: 15.108512. Entropy: 1.150522.\n",
      "episode: 418   score: 240.0  epsilon: 1.0    steps: 345  evaluation reward: 211.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: 2.045638. Value loss: 23.075422. Entropy: 0.769189.\n",
      "Iteration 1010: Policy loss: 2.042981. Value loss: 16.920214. Entropy: 0.792810.\n",
      "Iteration 1011: Policy loss: 1.747924. Value loss: 14.649141. Entropy: 0.767363.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: -0.527591. Value loss: 12.295908. Entropy: 0.996660.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1013: Policy loss: -0.660030. Value loss: 11.512567. Entropy: 1.013139.\n",
      "Iteration 1014: Policy loss: -0.680587. Value loss: 10.246143. Entropy: 1.007048.\n",
      "episode: 419   score: 460.0  epsilon: 1.0    steps: 385  evaluation reward: 214.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: 0.691698. Value loss: 18.292500. Entropy: 1.025226.\n",
      "Iteration 1016: Policy loss: 0.636466. Value loss: 13.883596. Entropy: 1.054290.\n",
      "Iteration 1017: Policy loss: 0.559956. Value loss: 12.658310. Entropy: 1.041582.\n",
      "episode: 420   score: 210.0  epsilon: 1.0    steps: 627  evaluation reward: 215.5\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: 0.256221. Value loss: 14.251858. Entropy: 0.885123.\n",
      "Iteration 1019: Policy loss: 0.284134. Value loss: 10.828312. Entropy: 0.888162.\n",
      "Iteration 1020: Policy loss: 0.312022. Value loss: 9.566157. Entropy: 0.882923.\n",
      "episode: 421   score: 240.0  epsilon: 1.0    steps: 90  evaluation reward: 216.7\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: -2.524664. Value loss: 318.032623. Entropy: 1.097317.\n",
      "Iteration 1022: Policy loss: -2.223536. Value loss: 222.689606. Entropy: 1.101144.\n",
      "Iteration 1023: Policy loss: -2.475114. Value loss: 203.929214. Entropy: 1.077345.\n",
      "episode: 422   score: 210.0  epsilon: 1.0    steps: 726  evaluation reward: 216.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: 1.220546. Value loss: 30.339336. Entropy: 0.893400.\n",
      "Iteration 1025: Policy loss: 0.832593. Value loss: 19.848368. Entropy: 0.851423.\n",
      "Iteration 1026: Policy loss: 1.125195. Value loss: 16.823832. Entropy: 0.857543.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: -1.023172. Value loss: 30.542515. Entropy: 0.836308.\n",
      "Iteration 1028: Policy loss: -1.335539. Value loss: 19.526188. Entropy: 0.867625.\n",
      "Iteration 1029: Policy loss: -1.333405. Value loss: 17.186604. Entropy: 0.847825.\n",
      "episode: 423   score: 285.0  epsilon: 1.0    steps: 253  evaluation reward: 218.7\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: -0.135830. Value loss: 21.654150. Entropy: 0.870871.\n",
      "Iteration 1031: Policy loss: -0.150145. Value loss: 15.312881. Entropy: 0.883944.\n",
      "Iteration 1032: Policy loss: -0.129595. Value loss: 13.561563. Entropy: 0.862344.\n",
      "episode: 424   score: 200.0  epsilon: 1.0    steps: 415  evaluation reward: 216.9\n",
      "episode: 425   score: 710.0  epsilon: 1.0    steps: 835  evaluation reward: 222.2\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: -1.779706. Value loss: 251.494354. Entropy: 1.006943.\n",
      "Iteration 1034: Policy loss: -1.594895. Value loss: 126.118027. Entropy: 0.955936.\n",
      "Iteration 1035: Policy loss: -1.397213. Value loss: 163.136444. Entropy: 0.947142.\n",
      "episode: 426   score: 110.0  epsilon: 1.0    steps: 513  evaluation reward: 220.9\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: -0.250534. Value loss: 24.710337. Entropy: 0.909215.\n",
      "Iteration 1037: Policy loss: -0.224603. Value loss: 15.532247. Entropy: 0.914880.\n",
      "Iteration 1038: Policy loss: -0.233629. Value loss: 13.018998. Entropy: 0.886818.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: -1.417444. Value loss: 28.569939. Entropy: 0.919113.\n",
      "Iteration 1040: Policy loss: -1.422208. Value loss: 20.455641. Entropy: 0.977569.\n",
      "Iteration 1041: Policy loss: -1.380725. Value loss: 19.130669. Entropy: 0.952092.\n",
      "episode: 427   score: 580.0  epsilon: 1.0    steps: 962  evaluation reward: 221.05\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: 0.798330. Value loss: 14.524240. Entropy: 0.903036.\n",
      "Iteration 1043: Policy loss: 0.801476. Value loss: 10.628978. Entropy: 0.901918.\n",
      "Iteration 1044: Policy loss: 0.953552. Value loss: 8.827750. Entropy: 0.912177.\n",
      "episode: 428   score: 240.0  epsilon: 1.0    steps: 57  evaluation reward: 222.35\n",
      "episode: 429   score: 450.0  epsilon: 1.0    steps: 298  evaluation reward: 225.0\n",
      "episode: 430   score: 75.0  epsilon: 1.0    steps: 542  evaluation reward: 224.65\n",
      "episode: 431   score: 210.0  epsilon: 1.0    steps: 698  evaluation reward: 225.65\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: 0.813756. Value loss: 17.090475. Entropy: 0.794500.\n",
      "Iteration 1046: Policy loss: 0.919412. Value loss: 10.666580. Entropy: 0.796820.\n",
      "Iteration 1047: Policy loss: 0.752912. Value loss: 9.491282. Entropy: 0.797639.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: -0.055908. Value loss: 17.199474. Entropy: 0.857201.\n",
      "Iteration 1049: Policy loss: 0.075051. Value loss: 12.772930. Entropy: 0.863264.\n",
      "Iteration 1050: Policy loss: 0.070347. Value loss: 10.944223. Entropy: 0.916716.\n",
      "episode: 432   score: 135.0  epsilon: 1.0    steps: 165  evaluation reward: 225.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: 2.163732. Value loss: 29.253481. Entropy: 0.953656.\n",
      "Iteration 1052: Policy loss: 2.221557. Value loss: 19.763182. Entropy: 0.969952.\n",
      "Iteration 1053: Policy loss: 2.205863. Value loss: 17.237970. Entropy: 0.937570.\n",
      "episode: 433   score: 180.0  epsilon: 1.0    steps: 408  evaluation reward: 225.35\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: 0.342074. Value loss: 9.111773. Entropy: 0.598535.\n",
      "Iteration 1055: Policy loss: 0.307693. Value loss: 4.721909. Entropy: 0.598171.\n",
      "Iteration 1056: Policy loss: 0.411913. Value loss: 4.623844. Entropy: 0.624347.\n",
      "episode: 434   score: 275.0  epsilon: 1.0    steps: 822  evaluation reward: 226.9\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: -3.100486. Value loss: 220.843353. Entropy: 0.379993.\n",
      "Iteration 1058: Policy loss: -2.579717. Value loss: 95.932854. Entropy: 0.448774.\n",
      "Iteration 1059: Policy loss: -3.557676. Value loss: 85.553200. Entropy: 0.399107.\n",
      "episode: 435   score: 110.0  epsilon: 1.0    steps: 739  evaluation reward: 226.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1060: Policy loss: 4.445690. Value loss: 75.518532. Entropy: 0.555452.\n",
      "Iteration 1061: Policy loss: 3.630629. Value loss: 45.125416. Entropy: 0.543652.\n",
      "Iteration 1062: Policy loss: 3.945163. Value loss: 36.792278. Entropy: 0.565844.\n",
      "episode: 436   score: 105.0  epsilon: 1.0    steps: 259  evaluation reward: 224.8\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: -2.062510. Value loss: 25.288128. Entropy: 0.596938.\n",
      "Iteration 1064: Policy loss: -2.069413. Value loss: 18.967236. Entropy: 0.573141.\n",
      "Iteration 1065: Policy loss: -2.037792. Value loss: 16.865419. Entropy: 0.573336.\n",
      "episode: 437   score: 535.0  epsilon: 1.0    steps: 1021  evaluation reward: 228.25\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: -0.506502. Value loss: 31.469021. Entropy: 0.457296.\n",
      "Iteration 1067: Policy loss: -0.503307. Value loss: 19.942125. Entropy: 0.439454.\n",
      "Iteration 1068: Policy loss: -0.537161. Value loss: 16.128490. Entropy: 0.417609.\n",
      "episode: 438   score: 105.0  epsilon: 1.0    steps: 186  evaluation reward: 226.8\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: 1.392755. Value loss: 41.967670. Entropy: 0.706376.\n",
      "Iteration 1070: Policy loss: 1.338706. Value loss: 21.149620. Entropy: 0.655689.\n",
      "Iteration 1071: Policy loss: 1.377520. Value loss: 17.917393. Entropy: 0.676572.\n",
      "episode: 439   score: 110.0  epsilon: 1.0    steps: 431  evaluation reward: 226.45\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: -2.706004. Value loss: 275.973572. Entropy: 0.543446.\n",
      "Iteration 1073: Policy loss: -1.941358. Value loss: 138.194916. Entropy: 0.465413.\n",
      "Iteration 1074: Policy loss: -3.034162. Value loss: 215.679749. Entropy: 0.514515.\n",
      "episode: 440   score: 335.0  epsilon: 1.0    steps: 635  evaluation reward: 228.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: -2.191569. Value loss: 306.801483. Entropy: 0.512266.\n",
      "Iteration 1076: Policy loss: -1.322402. Value loss: 189.117310. Entropy: 0.545924.\n",
      "Iteration 1077: Policy loss: -2.293512. Value loss: 181.047440. Entropy: 0.538095.\n",
      "episode: 441   score: 405.0  epsilon: 1.0    steps: 50  evaluation reward: 227.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1078: Policy loss: 2.597067. Value loss: 34.537670. Entropy: 0.505972.\n",
      "Iteration 1079: Policy loss: 2.695229. Value loss: 18.129896. Entropy: 0.462568.\n",
      "Iteration 1080: Policy loss: 2.451393. Value loss: 14.572277. Entropy: 0.516532.\n",
      "episode: 442   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 227.25\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: 3.176476. Value loss: 60.609287. Entropy: 0.522584.\n",
      "Iteration 1082: Policy loss: 3.234039. Value loss: 34.799408. Entropy: 0.522631.\n",
      "Iteration 1083: Policy loss: 2.936750. Value loss: 28.235989. Entropy: 0.540583.\n",
      "episode: 443   score: 400.0  epsilon: 1.0    steps: 642  evaluation reward: 228.3\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: 1.833363. Value loss: 24.859383. Entropy: 0.620413.\n",
      "Iteration 1085: Policy loss: 1.676916. Value loss: 18.056005. Entropy: 0.621156.\n",
      "Iteration 1086: Policy loss: 1.766652. Value loss: 14.603519. Entropy: 0.627277.\n",
      "episode: 444   score: 500.0  epsilon: 1.0    steps: 867  evaluation reward: 230.15\n",
      "episode: 445   score: 210.0  epsilon: 1.0    steps: 979  evaluation reward: 231.15\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: -2.590842. Value loss: 225.521683. Entropy: 0.557308.\n",
      "Iteration 1088: Policy loss: -3.436278. Value loss: 203.217773. Entropy: 0.511660.\n",
      "Iteration 1089: Policy loss: -2.368577. Value loss: 120.580864. Entropy: 0.489831.\n",
      "episode: 446   score: 75.0  epsilon: 1.0    steps: 341  evaluation reward: 227.15\n",
      "episode: 447   score: 425.0  epsilon: 1.0    steps: 475  evaluation reward: 226.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: 0.273890. Value loss: 26.572792. Entropy: 0.659973.\n",
      "Iteration 1091: Policy loss: 0.170600. Value loss: 17.454933. Entropy: 0.613975.\n",
      "Iteration 1092: Policy loss: 0.192681. Value loss: 14.647043. Entropy: 0.656883.\n",
      "episode: 448   score: 285.0  epsilon: 1.0    steps: 145  evaluation reward: 226.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: -0.323346. Value loss: 47.737778. Entropy: 0.669929.\n",
      "Iteration 1094: Policy loss: -0.514133. Value loss: 33.503178. Entropy: 0.702606.\n",
      "Iteration 1095: Policy loss: -0.222352. Value loss: 28.229666. Entropy: 0.714574.\n",
      "episode: 449   score: 170.0  epsilon: 1.0    steps: 24  evaluation reward: 226.4\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 1.428300. Value loss: 80.266571. Entropy: 0.858510.\n",
      "Iteration 1097: Policy loss: 1.513114. Value loss: 34.407860. Entropy: 0.966835.\n",
      "Iteration 1098: Policy loss: 1.469391. Value loss: 27.687988. Entropy: 0.910914.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1099: Policy loss: 1.170036. Value loss: 47.934341. Entropy: 0.661752.\n",
      "Iteration 1100: Policy loss: 1.514704. Value loss: 26.750128. Entropy: 0.683059.\n",
      "Iteration 1101: Policy loss: 1.172006. Value loss: 20.647461. Entropy: 0.709372.\n",
      "episode: 450   score: 135.0  epsilon: 1.0    steps: 153  evaluation reward: 225.65\n",
      "now time :  2019-02-25 19:00:24.172848\n",
      "episode: 451   score: 125.0  epsilon: 1.0    steps: 710  evaluation reward: 225.55\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: -0.409451. Value loss: 32.330433. Entropy: 0.698882.\n",
      "Iteration 1103: Policy loss: -0.155045. Value loss: 24.596247. Entropy: 0.657468.\n",
      "Iteration 1104: Policy loss: -0.474676. Value loss: 18.573904. Entropy: 0.661764.\n",
      "episode: 452   score: 135.0  epsilon: 1.0    steps: 935  evaluation reward: 223.7\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: 1.548643. Value loss: 33.548439. Entropy: 0.877729.\n",
      "Iteration 1106: Policy loss: 1.902362. Value loss: 21.941628. Entropy: 0.869928.\n",
      "Iteration 1107: Policy loss: 1.610596. Value loss: 17.450472. Entropy: 0.881685.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: 1.549718. Value loss: 44.879471. Entropy: 0.857147.\n",
      "Iteration 1109: Policy loss: 0.943948. Value loss: 27.941959. Entropy: 0.869650.\n",
      "Iteration 1110: Policy loss: 1.477216. Value loss: 24.180429. Entropy: 0.829241.\n",
      "episode: 453   score: 135.0  epsilon: 1.0    steps: 43  evaluation reward: 224.7\n",
      "episode: 454   score: 210.0  epsilon: 1.0    steps: 442  evaluation reward: 225.65\n",
      "episode: 455   score: 270.0  epsilon: 1.0    steps: 816  evaluation reward: 225.7\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: 1.024969. Value loss: 19.921778. Entropy: 0.881109.\n",
      "Iteration 1112: Policy loss: 1.001910. Value loss: 12.632879. Entropy: 0.865256.\n",
      "Iteration 1113: Policy loss: 1.069424. Value loss: 11.709672. Entropy: 0.854615.\n",
      "episode: 456   score: 225.0  epsilon: 1.0    steps: 283  evaluation reward: 223.7\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: 1.693393. Value loss: 35.162315. Entropy: 0.904359.\n",
      "Iteration 1115: Policy loss: 1.591589. Value loss: 21.750124. Entropy: 0.852297.\n",
      "Iteration 1116: Policy loss: 1.582169. Value loss: 17.227606. Entropy: 0.869977.\n",
      "episode: 457   score: 460.0  epsilon: 1.0    steps: 550  evaluation reward: 226.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: 0.741409. Value loss: 21.487806. Entropy: 0.792587.\n",
      "Iteration 1118: Policy loss: 0.788320. Value loss: 16.473335. Entropy: 0.799123.\n",
      "Iteration 1119: Policy loss: 0.837604. Value loss: 14.202844. Entropy: 0.829321.\n",
      "episode: 458   score: 120.0  epsilon: 1.0    steps: 73  evaluation reward: 225.1\n",
      "episode: 459   score: 225.0  epsilon: 1.0    steps: 759  evaluation reward: 225.0\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: -0.292718. Value loss: 14.310876. Entropy: 0.855183.\n",
      "Iteration 1121: Policy loss: -0.175003. Value loss: 11.477426. Entropy: 0.816112.\n",
      "Iteration 1122: Policy loss: -0.202312. Value loss: 10.437460. Entropy: 0.828939.\n",
      "episode: 460   score: 290.0  epsilon: 1.0    steps: 225  evaluation reward: 227.1\n",
      "episode: 461   score: 155.0  epsilon: 1.0    steps: 973  evaluation reward: 227.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 0.752718. Value loss: 16.257364. Entropy: 0.668412.\n",
      "Iteration 1124: Policy loss: 1.080145. Value loss: 10.837182. Entropy: 0.648342.\n",
      "Iteration 1125: Policy loss: 0.872613. Value loss: 10.206335. Entropy: 0.683445.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: 0.772180. Value loss: 15.320212. Entropy: 0.732641.\n",
      "Iteration 1127: Policy loss: 0.687755. Value loss: 11.323462. Entropy: 0.739069.\n",
      "Iteration 1128: Policy loss: 0.631446. Value loss: 10.053849. Entropy: 0.718098.\n",
      "episode: 462   score: 160.0  epsilon: 1.0    steps: 477  evaluation reward: 227.15\n",
      "episode: 463   score: 155.0  epsilon: 1.0    steps: 845  evaluation reward: 227.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: 0.555351. Value loss: 14.532042. Entropy: 0.809948.\n",
      "Iteration 1130: Policy loss: 0.618306. Value loss: 11.477819. Entropy: 0.782134.\n",
      "Iteration 1131: Policy loss: 0.763886. Value loss: 10.088444. Entropy: 0.808544.\n",
      "episode: 464   score: 210.0  epsilon: 1.0    steps: 345  evaluation reward: 226.7\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: 0.142515. Value loss: 9.213518. Entropy: 0.664615.\n",
      "Iteration 1133: Policy loss: 0.187580. Value loss: 8.828437. Entropy: 0.613556.\n",
      "Iteration 1134: Policy loss: 0.143257. Value loss: 8.762878. Entropy: 0.668065.\n",
      "episode: 465   score: 135.0  epsilon: 1.0    steps: 597  evaluation reward: 225.45\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: 0.943401. Value loss: 14.348286. Entropy: 0.650498.\n",
      "Iteration 1136: Policy loss: 1.178469. Value loss: 11.785795. Entropy: 0.701736.\n",
      "Iteration 1137: Policy loss: 0.764463. Value loss: 12.361161. Entropy: 0.659560.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: -0.307765. Value loss: 14.935135. Entropy: 0.792418.\n",
      "Iteration 1139: Policy loss: -0.304213. Value loss: 11.443830. Entropy: 0.786816.\n",
      "Iteration 1140: Policy loss: -0.333748. Value loss: 11.626880. Entropy: 0.781531.\n",
      "episode: 466   score: 210.0  epsilon: 1.0    steps: 707  evaluation reward: 225.45\n",
      "episode: 467   score: 155.0  epsilon: 1.0    steps: 1004  evaluation reward: 224.6\n",
      "Training network. lr: 0.000242. clip: 0.096627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1141: Policy loss: 0.629284. Value loss: 16.453812. Entropy: 0.763552.\n",
      "Iteration 1142: Policy loss: 0.532892. Value loss: 14.070022. Entropy: 0.730369.\n",
      "Iteration 1143: Policy loss: 0.510721. Value loss: 10.818964. Entropy: 0.733134.\n",
      "episode: 468   score: 260.0  epsilon: 1.0    steps: 29  evaluation reward: 225.7\n",
      "episode: 469   score: 210.0  epsilon: 1.0    steps: 139  evaluation reward: 226.8\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1144: Policy loss: -0.656915. Value loss: 7.790002. Entropy: 0.713660.\n",
      "Iteration 1145: Policy loss: -0.724861. Value loss: 6.161287. Entropy: 0.720631.\n",
      "Iteration 1146: Policy loss: -0.705813. Value loss: 5.209171. Entropy: 0.720766.\n",
      "episode: 470   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 224.8\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: 0.889707. Value loss: 13.978276. Entropy: 0.610010.\n",
      "Iteration 1148: Policy loss: 1.123841. Value loss: 15.464385. Entropy: 0.595360.\n",
      "Iteration 1149: Policy loss: 1.142296. Value loss: 13.044613. Entropy: 0.574662.\n",
      "episode: 471   score: 185.0  epsilon: 1.0    steps: 375  evaluation reward: 223.75\n",
      "episode: 472   score: 180.0  epsilon: 1.0    steps: 413  evaluation reward: 224.35\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: -0.071460. Value loss: 7.958634. Entropy: 1.035463.\n",
      "Iteration 1151: Policy loss: -0.064973. Value loss: 5.915977. Entropy: 1.048863.\n",
      "Iteration 1152: Policy loss: -0.167795. Value loss: 4.690860. Entropy: 1.019458.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.082929. Value loss: 9.983865. Entropy: 0.755716.\n",
      "Iteration 1154: Policy loss: 0.019829. Value loss: 7.619821. Entropy: 0.702619.\n",
      "Iteration 1155: Policy loss: 0.070267. Value loss: 7.187493. Entropy: 0.746427.\n",
      "episode: 473   score: 180.0  epsilon: 1.0    steps: 520  evaluation reward: 223.15\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: 0.458391. Value loss: 7.272767. Entropy: 0.741332.\n",
      "Iteration 1157: Policy loss: 0.438680. Value loss: 5.488125. Entropy: 0.734296.\n",
      "Iteration 1158: Policy loss: 0.476561. Value loss: 5.186334. Entropy: 0.717985.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: -1.258061. Value loss: 12.515577. Entropy: 0.881229.\n",
      "Iteration 1160: Policy loss: -1.114893. Value loss: 10.822685. Entropy: 0.873361.\n",
      "Iteration 1161: Policy loss: -1.184264. Value loss: 8.991010. Entropy: 0.839981.\n",
      "episode: 474   score: 180.0  epsilon: 1.0    steps: 62  evaluation reward: 221.35\n",
      "episode: 475   score: 155.0  epsilon: 1.0    steps: 166  evaluation reward: 222.1\n",
      "episode: 476   score: 210.0  epsilon: 1.0    steps: 666  evaluation reward: 218.3\n",
      "episode: 477   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 217.75\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: 0.803512. Value loss: 11.062856. Entropy: 0.915831.\n",
      "Iteration 1163: Policy loss: 0.562724. Value loss: 7.525205. Entropy: 0.900233.\n",
      "Iteration 1164: Policy loss: 0.710992. Value loss: 7.794737. Entropy: 0.893436.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: 0.291552. Value loss: 18.637331. Entropy: 0.743970.\n",
      "Iteration 1166: Policy loss: 0.138295. Value loss: 14.420325. Entropy: 0.765981.\n",
      "Iteration 1167: Policy loss: -0.126693. Value loss: 11.869778. Entropy: 0.737232.\n",
      "episode: 478   score: 135.0  epsilon: 1.0    steps: 458  evaluation reward: 217.9\n",
      "episode: 479   score: 135.0  epsilon: 1.0    steps: 807  evaluation reward: 217.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: 0.136840. Value loss: 17.034985. Entropy: 0.953850.\n",
      "Iteration 1169: Policy loss: 0.299587. Value loss: 13.025105. Entropy: 0.962109.\n",
      "Iteration 1170: Policy loss: 0.139869. Value loss: 10.864371. Entropy: 1.019992.\n",
      "episode: 480   score: 160.0  epsilon: 1.0    steps: 282  evaluation reward: 217.15\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: -2.558858. Value loss: 274.115356. Entropy: 0.855410.\n",
      "Iteration 1172: Policy loss: -2.055312. Value loss: 153.196381. Entropy: 0.882924.\n",
      "Iteration 1173: Policy loss: -1.459464. Value loss: 94.679291. Entropy: 0.819292.\n",
      "episode: 481   score: 145.0  epsilon: 1.0    steps: 547  evaluation reward: 217.55\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: 1.549680. Value loss: 12.308225. Entropy: 0.700266.\n",
      "Iteration 1175: Policy loss: 1.779666. Value loss: 9.332021. Entropy: 0.714695.\n",
      "Iteration 1176: Policy loss: 1.579481. Value loss: 7.346549. Entropy: 0.700166.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: 1.940233. Value loss: 21.886076. Entropy: 0.761604.\n",
      "Iteration 1178: Policy loss: 1.951639. Value loss: 10.265981. Entropy: 0.746406.\n",
      "Iteration 1179: Policy loss: 2.292757. Value loss: 7.643517. Entropy: 0.752074.\n",
      "episode: 482   score: 210.0  epsilon: 1.0    steps: 745  evaluation reward: 218.1\n",
      "episode: 483   score: 155.0  epsilon: 1.0    steps: 959  evaluation reward: 218.9\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: 0.157498. Value loss: 14.975057. Entropy: 0.629542.\n",
      "Iteration 1181: Policy loss: -0.171645. Value loss: 10.345757. Entropy: 0.624774.\n",
      "Iteration 1182: Policy loss: 0.137217. Value loss: 8.804238. Entropy: 0.626778.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 0.049742. Value loss: 10.274847. Entropy: 0.695006.\n",
      "Iteration 1184: Policy loss: 0.036262. Value loss: 7.369138. Entropy: 0.701200.\n",
      "Iteration 1185: Policy loss: 0.051954. Value loss: 6.911335. Entropy: 0.724104.\n",
      "episode: 484   score: 275.0  epsilon: 1.0    steps: 57  evaluation reward: 218.25\n",
      "episode: 485   score: 500.0  epsilon: 1.0    steps: 233  evaluation reward: 221.05\n",
      "episode: 486   score: 155.0  epsilon: 1.0    steps: 479  evaluation reward: 221.4\n",
      "episode: 487   score: 165.0  epsilon: 1.0    steps: 830  evaluation reward: 218.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: 1.789867. Value loss: 16.949091. Entropy: 0.967576.\n",
      "Iteration 1187: Policy loss: 1.747721. Value loss: 12.666121. Entropy: 0.992865.\n",
      "Iteration 1188: Policy loss: 1.868231. Value loss: 9.391429. Entropy: 0.998064.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: 0.446288. Value loss: 10.925107. Entropy: 0.881462.\n",
      "Iteration 1190: Policy loss: 0.313881. Value loss: 7.947215. Entropy: 0.918167.\n",
      "Iteration 1191: Policy loss: 0.366034. Value loss: 6.558748. Entropy: 0.877045.\n",
      "episode: 488   score: 180.0  epsilon: 1.0    steps: 610  evaluation reward: 219.75\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: 0.347022. Value loss: 14.842257. Entropy: 0.825152.\n",
      "Iteration 1193: Policy loss: 0.452435. Value loss: 9.488061. Entropy: 0.819550.\n",
      "Iteration 1194: Policy loss: 0.364860. Value loss: 9.959489. Entropy: 0.849159.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: 0.003965. Value loss: 5.259579. Entropy: 0.962596.\n",
      "Iteration 1196: Policy loss: -0.021857. Value loss: 3.531005. Entropy: 0.959723.\n",
      "Iteration 1197: Policy loss: -0.044817. Value loss: 2.299789. Entropy: 0.981484.\n",
      "episode: 489   score: 80.0  epsilon: 1.0    steps: 132  evaluation reward: 220.0\n",
      "episode: 490   score: 225.0  epsilon: 1.0    steps: 335  evaluation reward: 220.7\n",
      "episode: 491   score: 180.0  epsilon: 1.0    steps: 1009  evaluation reward: 221.45\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: -1.575251. Value loss: 24.107782. Entropy: 0.846141.\n",
      "Iteration 1199: Policy loss: -1.430670. Value loss: 13.396914. Entropy: 0.821188.\n",
      "Iteration 1200: Policy loss: -1.431812. Value loss: 11.848744. Entropy: 0.822840.\n",
      "episode: 492   score: 210.0  epsilon: 1.0    steps: 716  evaluation reward: 222.8\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 0.236060. Value loss: 8.203889. Entropy: 0.926088.\n",
      "Iteration 1202: Policy loss: 0.087453. Value loss: 6.641128. Entropy: 0.929810.\n",
      "Iteration 1203: Policy loss: 0.240227. Value loss: 5.853441. Entropy: 0.922908.\n",
      "episode: 493   score: 155.0  epsilon: 1.0    steps: 108  evaluation reward: 222.55\n",
      "episode: 494   score: 155.0  epsilon: 1.0    steps: 871  evaluation reward: 222.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: 0.227674. Value loss: 14.158964. Entropy: 0.948468.\n",
      "Iteration 1205: Policy loss: 0.183245. Value loss: 11.116944. Entropy: 0.956901.\n",
      "Iteration 1206: Policy loss: 0.412201. Value loss: 10.800515. Entropy: 0.955293.\n",
      "episode: 495   score: 185.0  epsilon: 1.0    steps: 396  evaluation reward: 220.7\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: -0.260593. Value loss: 5.282744. Entropy: 0.869424.\n",
      "Iteration 1208: Policy loss: -0.257699. Value loss: 4.371811. Entropy: 0.868844.\n",
      "Iteration 1209: Policy loss: -0.370678. Value loss: 4.608997. Entropy: 0.878084.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: -0.463615. Value loss: 10.746820. Entropy: 0.751513.\n",
      "Iteration 1211: Policy loss: -0.649870. Value loss: 7.924883. Entropy: 0.719921.\n",
      "Iteration 1212: Policy loss: -0.394264. Value loss: 6.916552. Entropy: 0.741171.\n",
      "episode: 496   score: 210.0  epsilon: 1.0    steps: 569  evaluation reward: 220.7\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: -0.033551. Value loss: 9.374175. Entropy: 0.910806.\n",
      "Iteration 1214: Policy loss: 0.039655. Value loss: 6.994960. Entropy: 0.886365.\n",
      "Iteration 1215: Policy loss: 0.039469. Value loss: 6.627116. Entropy: 0.897784.\n",
      "episode: 497   score: 165.0  epsilon: 1.0    steps: 167  evaluation reward: 219.75\n",
      "episode: 498   score: 155.0  epsilon: 1.0    steps: 368  evaluation reward: 219.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: -0.143114. Value loss: 10.244745. Entropy: 0.769240.\n",
      "Iteration 1217: Policy loss: -0.011919. Value loss: 6.424325. Entropy: 0.760340.\n",
      "Iteration 1218: Policy loss: -0.231027. Value loss: 5.868639. Entropy: 0.818948.\n",
      "episode: 499   score: 210.0  epsilon: 1.0    steps: 932  evaluation reward: 220.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: -0.034735. Value loss: 8.047680. Entropy: 0.785391.\n",
      "Iteration 1220: Policy loss: -0.031225. Value loss: 6.645037. Entropy: 0.769578.\n",
      "Iteration 1221: Policy loss: -0.098961. Value loss: 6.361797. Entropy: 0.793180.\n",
      "episode: 500   score: 180.0  epsilon: 1.0    steps: 647  evaluation reward: 219.9\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: -1.267191. Value loss: 17.709663. Entropy: 0.865486.\n",
      "Iteration 1223: Policy loss: -1.403421. Value loss: 13.913610. Entropy: 0.831333.\n",
      "Iteration 1224: Policy loss: -1.354675. Value loss: 12.285318. Entropy: 0.803893.\n",
      "now time :  2019-02-25 19:02:40.755109\n",
      "episode: 501   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 218.45\n",
      "episode: 502   score: 185.0  epsilon: 1.0    steps: 443  evaluation reward: 218.2\n",
      "episode: 503   score: 185.0  epsilon: 1.0    steps: 788  evaluation reward: 216.7\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 1.153509. Value loss: 6.929267. Entropy: 0.907302.\n",
      "Iteration 1226: Policy loss: 1.127993. Value loss: 6.361090. Entropy: 0.911326.\n",
      "Iteration 1227: Policy loss: 1.241597. Value loss: 3.996581. Entropy: 0.923582.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: 0.708000. Value loss: 7.716163. Entropy: 0.665141.\n",
      "Iteration 1229: Policy loss: 0.668067. Value loss: 6.029715. Entropy: 0.652110.\n",
      "Iteration 1230: Policy loss: 0.734284. Value loss: 5.176232. Entropy: 0.641952.\n",
      "episode: 504   score: 155.0  epsilon: 1.0    steps: 611  evaluation reward: 217.35\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 1.732245. Value loss: 10.780360. Entropy: 0.867265.\n",
      "Iteration 1232: Policy loss: 1.679791. Value loss: 7.616560. Entropy: 0.862157.\n",
      "Iteration 1233: Policy loss: 1.766068. Value loss: 6.989949. Entropy: 0.896255.\n",
      "episode: 505   score: 135.0  epsilon: 1.0    steps: 218  evaluation reward: 217.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: 0.236510. Value loss: 13.256136. Entropy: 0.925039.\n",
      "Iteration 1235: Policy loss: 0.512835. Value loss: 10.295448. Entropy: 0.949489.\n",
      "Iteration 1236: Policy loss: 0.435444. Value loss: 7.150163. Entropy: 0.902473.\n",
      "episode: 506   score: 210.0  epsilon: 1.0    steps: 1020  evaluation reward: 218.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: -0.734759. Value loss: 16.340837. Entropy: 0.712779.\n",
      "Iteration 1238: Policy loss: -0.747408. Value loss: 13.804072. Entropy: 0.667940.\n",
      "Iteration 1239: Policy loss: -0.667988. Value loss: 10.913286. Entropy: 0.686743.\n",
      "episode: 507   score: 285.0  epsilon: 1.0    steps: 314  evaluation reward: 219.2\n",
      "episode: 508   score: 210.0  epsilon: 1.0    steps: 684  evaluation reward: 218.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: -0.425691. Value loss: 11.660233. Entropy: 1.097181.\n",
      "Iteration 1241: Policy loss: -0.316052. Value loss: 8.190419. Entropy: 1.124057.\n",
      "Iteration 1242: Policy loss: -0.545451. Value loss: 6.597659. Entropy: 1.101046.\n",
      "episode: 509   score: 155.0  epsilon: 1.0    steps: 55  evaluation reward: 218.05\n",
      "episode: 510   score: 155.0  epsilon: 1.0    steps: 486  evaluation reward: 218.85\n",
      "episode: 511   score: 210.0  epsilon: 1.0    steps: 885  evaluation reward: 220.15\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: 0.834762. Value loss: 15.628953. Entropy: 1.101133.\n",
      "Iteration 1244: Policy loss: 1.011306. Value loss: 10.373027. Entropy: 1.072419.\n",
      "Iteration 1245: Policy loss: 0.970207. Value loss: 8.516384. Entropy: 1.135372.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.113005. Value loss: 3.144828. Entropy: 1.049477.\n",
      "Iteration 1247: Policy loss: 0.134414. Value loss: 2.472427. Entropy: 1.022387.\n",
      "Iteration 1248: Policy loss: 0.116251. Value loss: 2.372676. Entropy: 1.036314.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: -2.258854. Value loss: 123.527733. Entropy: 1.028345.\n",
      "Iteration 1250: Policy loss: -2.101612. Value loss: 66.823112. Entropy: 0.972677.\n",
      "Iteration 1251: Policy loss: -2.664509. Value loss: 73.460938. Entropy: 0.992995.\n",
      "episode: 512   score: 410.0  epsilon: 1.0    steps: 554  evaluation reward: 222.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: 0.041614. Value loss: 6.735512. Entropy: 0.922840.\n",
      "Iteration 1253: Policy loss: -0.043859. Value loss: 4.491802. Entropy: 0.883881.\n",
      "Iteration 1254: Policy loss: 0.080685. Value loss: 4.101239. Entropy: 0.883691.\n",
      "episode: 513   score: 180.0  epsilon: 1.0    steps: 130  evaluation reward: 221.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: -1.016670. Value loss: 8.844216. Entropy: 0.770939.\n",
      "Iteration 1256: Policy loss: -0.700627. Value loss: 6.568718. Entropy: 0.754880.\n",
      "Iteration 1257: Policy loss: -0.899336. Value loss: 6.137926. Entropy: 0.741503.\n",
      "episode: 514   score: 180.0  epsilon: 1.0    steps: 365  evaluation reward: 219.4\n",
      "episode: 515   score: 180.0  epsilon: 1.0    steps: 729  evaluation reward: 219.35\n",
      "episode: 516   score: 180.0  epsilon: 1.0    steps: 1011  evaluation reward: 219.25\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: -1.955082. Value loss: 225.900436. Entropy: 0.842251.\n",
      "Iteration 1259: Policy loss: -1.648711. Value loss: 87.636589. Entropy: 0.800716.\n",
      "Iteration 1260: Policy loss: -1.404493. Value loss: 47.664341. Entropy: 0.779346.\n",
      "episode: 517   score: 385.0  epsilon: 1.0    steps: 108  evaluation reward: 221.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: 1.271410. Value loss: 14.794429. Entropy: 0.858021.\n",
      "Iteration 1262: Policy loss: 1.136950. Value loss: 11.255540. Entropy: 0.893494.\n",
      "Iteration 1263: Policy loss: 1.296742. Value loss: 9.890481. Entropy: 0.903498.\n",
      "episode: 518   score: 180.0  epsilon: 1.0    steps: 457  evaluation reward: 220.7\n",
      "episode: 519   score: 120.0  epsilon: 1.0    steps: 796  evaluation reward: 217.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: 1.768579. Value loss: 15.750067. Entropy: 1.052637.\n",
      "Iteration 1265: Policy loss: 1.477921. Value loss: 9.728882. Entropy: 1.030895.\n",
      "Iteration 1266: Policy loss: 1.426547. Value loss: 9.678039. Entropy: 1.067509.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1267: Policy loss: -1.775961. Value loss: 194.359756. Entropy: 0.968517.\n",
      "Iteration 1268: Policy loss: -2.308502. Value loss: 159.412918. Entropy: 0.961327.\n",
      "Iteration 1269: Policy loss: -1.901428. Value loss: 72.207649. Entropy: 0.971311.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: 1.791493. Value loss: 37.060562. Entropy: 0.885870.\n",
      "Iteration 1271: Policy loss: 0.937382. Value loss: 13.799290. Entropy: 0.952855.\n",
      "Iteration 1272: Policy loss: 1.578555. Value loss: 13.241053. Entropy: 0.921158.\n",
      "episode: 520   score: 180.0  epsilon: 1.0    steps: 236  evaluation reward: 217.0\n",
      "episode: 521   score: 80.0  epsilon: 1.0    steps: 494  evaluation reward: 215.4\n",
      "episode: 522   score: 110.0  epsilon: 1.0    steps: 732  evaluation reward: 214.4\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: 1.215452. Value loss: 17.205956. Entropy: 0.884510.\n",
      "Iteration 1274: Policy loss: 1.281350. Value loss: 12.647974. Entropy: 0.862204.\n",
      "Iteration 1275: Policy loss: 1.214897. Value loss: 10.337755. Entropy: 0.852757.\n",
      "episode: 523   score: 105.0  epsilon: 1.0    steps: 259  evaluation reward: 212.6\n",
      "episode: 524   score: 440.0  epsilon: 1.0    steps: 531  evaluation reward: 215.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.986657. Value loss: 26.462936. Entropy: 0.945912.\n",
      "Iteration 1277: Policy loss: 1.120839. Value loss: 12.342800. Entropy: 0.946078.\n",
      "Iteration 1278: Policy loss: 0.973711. Value loss: 9.294326. Entropy: 0.985666.\n",
      "episode: 525   score: 110.0  epsilon: 1.0    steps: 905  evaluation reward: 209.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 1.518626. Value loss: 36.119617. Entropy: 1.011055.\n",
      "Iteration 1280: Policy loss: 1.671548. Value loss: 24.738583. Entropy: 1.041184.\n",
      "Iteration 1281: Policy loss: 1.202926. Value loss: 22.491766. Entropy: 1.036108.\n",
      "episode: 526   score: 155.0  epsilon: 1.0    steps: 13  evaluation reward: 209.45\n",
      "episode: 527   score: 75.0  epsilon: 1.0    steps: 512  evaluation reward: 204.4\n",
      "episode: 528   score: 135.0  epsilon: 1.0    steps: 829  evaluation reward: 203.35\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: -0.415749. Value loss: 15.381050. Entropy: 1.218685.\n",
      "Iteration 1283: Policy loss: -0.561809. Value loss: 13.284671. Entropy: 1.188254.\n",
      "Iteration 1284: Policy loss: -0.432244. Value loss: 11.437566. Entropy: 1.200160.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: 1.112218. Value loss: 4.166755. Entropy: 0.930072.\n",
      "Iteration 1286: Policy loss: 1.163633. Value loss: 3.319039. Entropy: 0.907456.\n",
      "Iteration 1287: Policy loss: 1.069455. Value loss: 3.832956. Entropy: 0.932609.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 0.621278. Value loss: 7.782996. Entropy: 0.778923.\n",
      "Iteration 1289: Policy loss: 0.740894. Value loss: 4.817744. Entropy: 0.779129.\n",
      "Iteration 1290: Policy loss: 0.618012. Value loss: 5.408669. Entropy: 0.814124.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: 0.451204. Value loss: 16.829828. Entropy: 0.780908.\n",
      "Iteration 1292: Policy loss: 0.556612. Value loss: 11.180411. Entropy: 0.786108.\n",
      "Iteration 1293: Policy loss: 0.537187. Value loss: 9.728525. Entropy: 0.802609.\n",
      "episode: 529   score: 155.0  epsilon: 1.0    steps: 141  evaluation reward: 200.4\n",
      "episode: 530   score: 210.0  epsilon: 1.0    steps: 345  evaluation reward: 201.75\n",
      "episode: 531   score: 165.0  epsilon: 1.0    steps: 568  evaluation reward: 201.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1294: Policy loss: -1.578563. Value loss: 16.537933. Entropy: 0.968716.\n",
      "Iteration 1295: Policy loss: -1.796873. Value loss: 12.447207. Entropy: 0.932670.\n",
      "Iteration 1296: Policy loss: -1.652516. Value loss: 11.447221. Entropy: 0.979163.\n",
      "episode: 532   score: 260.0  epsilon: 1.0    steps: 696  evaluation reward: 202.55\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: 0.600569. Value loss: 11.963043. Entropy: 0.846410.\n",
      "Iteration 1298: Policy loss: 0.622772. Value loss: 9.346283. Entropy: 0.832573.\n",
      "Iteration 1299: Policy loss: 0.460542. Value loss: 7.647355. Entropy: 0.870785.\n",
      "episode: 533   score: 135.0  epsilon: 1.0    steps: 35  evaluation reward: 202.1\n",
      "episode: 534   score: 140.0  epsilon: 1.0    steps: 847  evaluation reward: 200.75\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 1.792210. Value loss: 15.026160. Entropy: 1.042140.\n",
      "Iteration 1301: Policy loss: 1.087141. Value loss: 11.856821. Entropy: 1.050424.\n",
      "Iteration 1302: Policy loss: 1.301802. Value loss: 10.007070. Entropy: 1.020259.\n",
      "episode: 535   score: 155.0  epsilon: 1.0    steps: 421  evaluation reward: 201.2\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: 0.748533. Value loss: 11.559791. Entropy: 1.030163.\n",
      "Iteration 1304: Policy loss: 0.848245. Value loss: 7.106850. Entropy: 1.022017.\n",
      "Iteration 1305: Policy loss: 0.689284. Value loss: 6.735450. Entropy: 1.009267.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: 0.792610. Value loss: 7.597501. Entropy: 0.860820.\n",
      "Iteration 1307: Policy loss: 0.709097. Value loss: 5.542944. Entropy: 0.877473.\n",
      "Iteration 1308: Policy loss: 0.662230. Value loss: 4.936705. Entropy: 0.883589.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: -0.114312. Value loss: 5.578348. Entropy: 0.984404.\n",
      "Iteration 1310: Policy loss: -0.123540. Value loss: 3.816949. Entropy: 0.958937.\n",
      "Iteration 1311: Policy loss: -0.030547. Value loss: 2.909014. Entropy: 0.935060.\n",
      "episode: 536   score: 180.0  epsilon: 1.0    steps: 183  evaluation reward: 201.95\n",
      "episode: 537   score: 155.0  epsilon: 1.0    steps: 603  evaluation reward: 198.15\n",
      "episode: 538   score: 315.0  epsilon: 1.0    steps: 910  evaluation reward: 200.25\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: -0.490994. Value loss: 12.154104. Entropy: 0.953368.\n",
      "Iteration 1313: Policy loss: -0.668578. Value loss: 8.284025. Entropy: 0.948721.\n",
      "Iteration 1314: Policy loss: -0.674844. Value loss: 9.320180. Entropy: 0.973263.\n",
      "episode: 539   score: 210.0  epsilon: 1.0    steps: 278  evaluation reward: 201.25\n",
      "episode: 540   score: 180.0  epsilon: 1.0    steps: 753  evaluation reward: 199.7\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: 0.037678. Value loss: 10.781479. Entropy: 0.870085.\n",
      "Iteration 1316: Policy loss: -0.172861. Value loss: 10.701540. Entropy: 0.859789.\n",
      "Iteration 1317: Policy loss: -0.082263. Value loss: 9.146585. Entropy: 0.865629.\n",
      "episode: 541   score: 155.0  epsilon: 1.0    steps: 80  evaluation reward: 197.2\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: 0.075352. Value loss: 13.656083. Entropy: 1.054873.\n",
      "Iteration 1319: Policy loss: 0.141165. Value loss: 9.675598. Entropy: 1.058546.\n",
      "Iteration 1320: Policy loss: 0.124745. Value loss: 8.645793. Entropy: 1.052806.\n",
      "episode: 542   score: 155.0  epsilon: 1.0    steps: 472  evaluation reward: 196.95\n",
      "episode: 543   score: 180.0  epsilon: 1.0    steps: 796  evaluation reward: 194.75\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: 0.304342. Value loss: 6.770184. Entropy: 1.038336.\n",
      "Iteration 1322: Policy loss: 0.239465. Value loss: 5.056497. Entropy: 1.028105.\n",
      "Iteration 1323: Policy loss: 0.319003. Value loss: 5.076284. Entropy: 1.019370.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: 0.983983. Value loss: 6.398587. Entropy: 0.885352.\n",
      "Iteration 1325: Policy loss: 0.983542. Value loss: 5.468344. Entropy: 0.905050.\n",
      "Iteration 1326: Policy loss: 0.986206. Value loss: 3.818141. Entropy: 0.855377.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: -2.686403. Value loss: 274.413025. Entropy: 0.987063.\n",
      "Iteration 1328: Policy loss: -2.896096. Value loss: 65.456673. Entropy: 0.957961.\n",
      "Iteration 1329: Policy loss: -2.319089. Value loss: 30.593328. Entropy: 0.940043.\n",
      "episode: 544   score: 355.0  epsilon: 1.0    steps: 637  evaluation reward: 193.3\n",
      "episode: 545   score: 135.0  epsilon: 1.0    steps: 944  evaluation reward: 192.55\n",
      "Training network. lr: 0.000240. clip: 0.096009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1330: Policy loss: 0.282090. Value loss: 14.938943. Entropy: 0.915774.\n",
      "Iteration 1331: Policy loss: 0.121744. Value loss: 11.722298. Entropy: 0.933922.\n",
      "Iteration 1332: Policy loss: 0.093708. Value loss: 9.660933. Entropy: 0.952747.\n",
      "episode: 546   score: 210.0  epsilon: 1.0    steps: 346  evaluation reward: 193.9\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: -0.731429. Value loss: 25.707735. Entropy: 0.934123.\n",
      "Iteration 1334: Policy loss: -0.767914. Value loss: 13.874340. Entropy: 0.957352.\n",
      "Iteration 1335: Policy loss: -0.846737. Value loss: 11.732559. Entropy: 0.950907.\n",
      "episode: 547   score: 275.0  epsilon: 1.0    steps: 178  evaluation reward: 192.4\n",
      "episode: 548   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 191.35\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1336: Policy loss: 0.026422. Value loss: 16.804605. Entropy: 1.040371.\n",
      "Iteration 1337: Policy loss: -0.190538. Value loss: 12.350044. Entropy: 1.029408.\n",
      "Iteration 1338: Policy loss: -0.140683. Value loss: 9.543520. Entropy: 1.022948.\n",
      "episode: 549   score: 210.0  epsilon: 1.0    steps: 6  evaluation reward: 191.75\n",
      "episode: 550   score: 180.0  epsilon: 1.0    steps: 511  evaluation reward: 192.2\n",
      "now time :  2019-02-25 19:04:51.333746\n",
      "episode: 551   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 193.05\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: -0.302195. Value loss: 11.869293. Entropy: 0.960995.\n",
      "Iteration 1340: Policy loss: -0.315081. Value loss: 8.047320. Entropy: 0.963801.\n",
      "Iteration 1341: Policy loss: -0.231308. Value loss: 6.983106. Entropy: 0.949983.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: 0.406694. Value loss: 4.158808. Entropy: 0.921869.\n",
      "Iteration 1343: Policy loss: 0.424560. Value loss: 3.621979. Entropy: 0.907338.\n",
      "Iteration 1344: Policy loss: 0.374619. Value loss: 3.208910. Entropy: 0.876048.\n",
      "episode: 552   score: 105.0  epsilon: 1.0    steps: 964  evaluation reward: 192.75\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: 0.055463. Value loss: 15.959914. Entropy: 0.977060.\n",
      "Iteration 1346: Policy loss: 0.334495. Value loss: 8.646293. Entropy: 1.010895.\n",
      "Iteration 1347: Policy loss: 0.337548. Value loss: 9.470016. Entropy: 0.971192.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: -0.820829. Value loss: 9.623335. Entropy: 0.859346.\n",
      "Iteration 1349: Policy loss: -0.796570. Value loss: 7.008981. Entropy: 0.871992.\n",
      "Iteration 1350: Policy loss: -0.727187. Value loss: 6.180264. Entropy: 0.835691.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: -0.330074. Value loss: 6.738024. Entropy: 0.963769.\n",
      "Iteration 1352: Policy loss: -0.281759. Value loss: 4.461402. Entropy: 0.959861.\n",
      "Iteration 1353: Policy loss: -0.415285. Value loss: 3.644164. Entropy: 0.953662.\n",
      "episode: 553   score: 180.0  epsilon: 1.0    steps: 235  evaluation reward: 193.2\n",
      "episode: 554   score: 210.0  epsilon: 1.0    steps: 269  evaluation reward: 193.2\n",
      "episode: 555   score: 285.0  epsilon: 1.0    steps: 596  evaluation reward: 193.35\n",
      "episode: 556   score: 210.0  epsilon: 1.0    steps: 721  evaluation reward: 193.2\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: 0.056807. Value loss: 12.111303. Entropy: 1.035476.\n",
      "Iteration 1355: Policy loss: 0.067485. Value loss: 9.997205. Entropy: 1.038391.\n",
      "Iteration 1356: Policy loss: -0.011128. Value loss: 9.101579. Entropy: 1.045066.\n",
      "episode: 557   score: 155.0  epsilon: 1.0    steps: 39  evaluation reward: 190.15\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: -0.466898. Value loss: 7.847702. Entropy: 0.758674.\n",
      "Iteration 1358: Policy loss: -0.493672. Value loss: 6.329479. Entropy: 0.745282.\n",
      "Iteration 1359: Policy loss: -0.457976. Value loss: 5.932018. Entropy: 0.749030.\n",
      "episode: 558   score: 210.0  epsilon: 1.0    steps: 446  evaluation reward: 191.05\n",
      "episode: 559   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 190.9\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 0.017573. Value loss: 14.208262. Entropy: 0.879629.\n",
      "Iteration 1361: Policy loss: 0.197128. Value loss: 9.342395. Entropy: 0.868379.\n",
      "Iteration 1362: Policy loss: -0.118909. Value loss: 9.303398. Entropy: 0.878067.\n",
      "episode: 560   score: 180.0  epsilon: 1.0    steps: 1017  evaluation reward: 189.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.472961. Value loss: 5.058171. Entropy: 0.843496.\n",
      "Iteration 1364: Policy loss: 0.457892. Value loss: 3.908187. Entropy: 0.832806.\n",
      "Iteration 1365: Policy loss: 0.491926. Value loss: 3.558266. Entropy: 0.825054.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: 0.464050. Value loss: 3.623825. Entropy: 0.822817.\n",
      "Iteration 1367: Policy loss: 0.482239. Value loss: 2.761325. Entropy: 0.833128.\n",
      "Iteration 1368: Policy loss: 0.459130. Value loss: 2.982118. Entropy: 0.851979.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: 0.291870. Value loss: 4.229763. Entropy: 0.815266.\n",
      "Iteration 1370: Policy loss: 0.329912. Value loss: 3.015513. Entropy: 0.816041.\n",
      "Iteration 1371: Policy loss: 0.249240. Value loss: 2.986775. Entropy: 0.847069.\n",
      "episode: 561   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 190.35\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: -0.723179. Value loss: 12.938252. Entropy: 0.877716.\n",
      "Iteration 1373: Policy loss: -0.662577. Value loss: 11.032538. Entropy: 0.884313.\n",
      "Iteration 1374: Policy loss: -0.602756. Value loss: 8.855244. Entropy: 0.916001.\n",
      "episode: 562   score: 210.0  epsilon: 1.0    steps: 164  evaluation reward: 190.85\n",
      "episode: 563   score: 210.0  epsilon: 1.0    steps: 535  evaluation reward: 191.4\n",
      "episode: 564   score: 180.0  epsilon: 1.0    steps: 650  evaluation reward: 191.1\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: -0.419742. Value loss: 10.526839. Entropy: 1.002951.\n",
      "Iteration 1376: Policy loss: -0.556965. Value loss: 5.718177. Entropy: 0.997982.\n",
      "Iteration 1377: Policy loss: -0.404024. Value loss: 5.760837. Entropy: 0.985485.\n",
      "episode: 565   score: 210.0  epsilon: 1.0    steps: 14  evaluation reward: 191.85\n",
      "episode: 566   score: 210.0  epsilon: 1.0    steps: 497  evaluation reward: 191.85\n",
      "episode: 567   score: 185.0  epsilon: 1.0    steps: 843  evaluation reward: 192.15\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: 0.390419. Value loss: 11.628289. Entropy: 0.935274.\n",
      "Iteration 1379: Policy loss: 0.321321. Value loss: 9.906346. Entropy: 0.909445.\n",
      "Iteration 1380: Policy loss: 0.318190. Value loss: 8.797039. Entropy: 0.927785.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: -1.664469. Value loss: 14.674086. Entropy: 0.965220.\n",
      "Iteration 1382: Policy loss: -1.739761. Value loss: 12.053761. Entropy: 0.969765.\n",
      "Iteration 1383: Policy loss: -1.786287. Value loss: 11.461086. Entropy: 0.950332.\n",
      "episode: 568   score: 210.0  epsilon: 1.0    steps: 940  evaluation reward: 191.65\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: -0.604685. Value loss: 8.643464. Entropy: 1.104593.\n",
      "Iteration 1385: Policy loss: -0.633730. Value loss: 6.305170. Entropy: 1.077938.\n",
      "Iteration 1386: Policy loss: -0.701685. Value loss: 6.065452. Entropy: 1.084656.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 0.091924. Value loss: 3.449645. Entropy: 0.947003.\n",
      "Iteration 1388: Policy loss: 0.169791. Value loss: 3.215331. Entropy: 0.945180.\n",
      "Iteration 1389: Policy loss: 0.047915. Value loss: 2.269809. Entropy: 0.943109.\n",
      "episode: 569   score: 105.0  epsilon: 1.0    steps: 172  evaluation reward: 190.6\n",
      "episode: 570   score: 155.0  epsilon: 1.0    steps: 375  evaluation reward: 190.05\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: -0.498619. Value loss: 177.593536. Entropy: 0.980580.\n",
      "Iteration 1391: Policy loss: -1.012704. Value loss: 39.096474. Entropy: 1.048493.\n",
      "Iteration 1392: Policy loss: -0.209549. Value loss: 14.167501. Entropy: 1.043115.\n",
      "episode: 571   score: 155.0  epsilon: 1.0    steps: 685  evaluation reward: 189.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: -0.838581. Value loss: 16.291447. Entropy: 1.047614.\n",
      "Iteration 1394: Policy loss: -0.750995. Value loss: 10.786757. Entropy: 1.015644.\n",
      "Iteration 1395: Policy loss: -0.832066. Value loss: 8.485952. Entropy: 1.009982.\n",
      "episode: 572   score: 180.0  epsilon: 1.0    steps: 71  evaluation reward: 189.75\n",
      "episode: 573   score: 410.0  epsilon: 1.0    steps: 528  evaluation reward: 192.05\n",
      "episode: 574   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 191.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: 0.095845. Value loss: 16.305729. Entropy: 0.994394.\n",
      "Iteration 1397: Policy loss: 0.083018. Value loss: 12.122435. Entropy: 0.981064.\n",
      "Iteration 1398: Policy loss: 0.108393. Value loss: 11.233398. Entropy: 1.004308.\n",
      "episode: 575   score: 180.0  epsilon: 1.0    steps: 426  evaluation reward: 192.05\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: -0.725581. Value loss: 8.269903. Entropy: 1.034185.\n",
      "Iteration 1400: Policy loss: -0.693565. Value loss: 6.429962. Entropy: 1.020376.\n",
      "Iteration 1401: Policy loss: -0.826349. Value loss: 6.584324. Entropy: 1.024652.\n",
      "episode: 576   score: 210.0  epsilon: 1.0    steps: 997  evaluation reward: 192.05\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: -0.150231. Value loss: 9.956490. Entropy: 1.013878.\n",
      "Iteration 1403: Policy loss: -0.390065. Value loss: 7.618833. Entropy: 1.007375.\n",
      "Iteration 1404: Policy loss: -0.231518. Value loss: 7.091588. Entropy: 0.989747.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: 0.553633. Value loss: 3.730318. Entropy: 1.004042.\n",
      "Iteration 1406: Policy loss: 0.558708. Value loss: 2.152818. Entropy: 1.003100.\n",
      "Iteration 1407: Policy loss: 0.507577. Value loss: 1.631742. Entropy: 1.019986.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: -2.643352. Value loss: 109.803726. Entropy: 1.020231.\n",
      "Iteration 1409: Policy loss: -3.078169. Value loss: 53.650227. Entropy: 1.012736.\n",
      "Iteration 1410: Policy loss: -3.012188. Value loss: 45.808434. Entropy: 1.010988.\n",
      "episode: 577   score: 210.0  epsilon: 1.0    steps: 131  evaluation reward: 192.6\n",
      "episode: 578   score: 160.0  epsilon: 1.0    steps: 281  evaluation reward: 192.85\n",
      "episode: 579   score: 355.0  epsilon: 1.0    steps: 734  evaluation reward: 195.05\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: -2.315678. Value loss: 108.427292. Entropy: 0.990921.\n",
      "Iteration 1412: Policy loss: -2.616857. Value loss: 77.634140. Entropy: 1.016873.\n",
      "Iteration 1413: Policy loss: -2.460092. Value loss: 49.085091. Entropy: 1.011625.\n",
      "episode: 580   score: 410.0  epsilon: 1.0    steps: 610  evaluation reward: 197.55\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: 3.225781. Value loss: 58.066833. Entropy: 0.873571.\n",
      "Iteration 1415: Policy loss: 3.145295. Value loss: 18.158339. Entropy: 0.824328.\n",
      "Iteration 1416: Policy loss: 2.683225. Value loss: 12.552867. Entropy: 0.843436.\n",
      "episode: 581   score: 180.0  epsilon: 1.0    steps: 10  evaluation reward: 197.9\n",
      "episode: 582   score: 155.0  epsilon: 1.0    steps: 791  evaluation reward: 197.35\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: -0.219077. Value loss: 20.375072. Entropy: 0.982248.\n",
      "Iteration 1418: Policy loss: -0.029955. Value loss: 13.612752. Entropy: 0.950532.\n",
      "Iteration 1419: Policy loss: -0.218152. Value loss: 12.220865. Entropy: 0.980121.\n",
      "episode: 583   score: 180.0  epsilon: 1.0    steps: 389  evaluation reward: 197.6\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: 0.017109. Value loss: 9.539380. Entropy: 0.911025.\n",
      "Iteration 1421: Policy loss: -0.018862. Value loss: 8.656653. Entropy: 0.949562.\n",
      "Iteration 1422: Policy loss: -0.044256. Value loss: 7.695014. Entropy: 0.902325.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: 0.354337. Value loss: 6.316272. Entropy: 0.963250.\n",
      "Iteration 1424: Policy loss: 0.324272. Value loss: 4.320097. Entropy: 0.979938.\n",
      "Iteration 1425: Policy loss: 0.287821. Value loss: 3.885073. Entropy: 0.985716.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: -1.480565. Value loss: 18.555136. Entropy: 0.967943.\n",
      "Iteration 1427: Policy loss: -1.456948. Value loss: 8.168064. Entropy: 0.973855.\n",
      "Iteration 1428: Policy loss: -1.511747. Value loss: 6.370900. Entropy: 0.968114.\n",
      "episode: 584   score: 155.0  epsilon: 1.0    steps: 171  evaluation reward: 196.4\n",
      "episode: 585   score: 210.0  epsilon: 1.0    steps: 338  evaluation reward: 193.5\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: 1.103219. Value loss: 18.258032. Entropy: 1.007510.\n",
      "Iteration 1430: Policy loss: 0.993704. Value loss: 9.382243. Entropy: 1.015669.\n",
      "Iteration 1431: Policy loss: 1.023385. Value loss: 8.155392. Entropy: 0.986087.\n",
      "episode: 586   score: 180.0  epsilon: 1.0    steps: 657  evaluation reward: 193.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: -0.658096. Value loss: 20.958759. Entropy: 0.909621.\n",
      "Iteration 1433: Policy loss: -0.201579. Value loss: 9.447114. Entropy: 0.933362.\n",
      "Iteration 1434: Policy loss: -0.363751. Value loss: 8.934441. Entropy: 0.912118.\n",
      "episode: 587   score: 180.0  epsilon: 1.0    steps: 610  evaluation reward: 193.9\n",
      "episode: 588   score: 180.0  epsilon: 1.0    steps: 882  evaluation reward: 193.9\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: 2.553318. Value loss: 21.342087. Entropy: 1.061746.\n",
      "Iteration 1436: Policy loss: 2.333358. Value loss: 11.700994. Entropy: 1.000595.\n",
      "Iteration 1437: Policy loss: 2.632833. Value loss: 10.257209. Entropy: 1.051733.\n",
      "episode: 589   score: 195.0  epsilon: 1.0    steps: 83  evaluation reward: 195.05\n",
      "episode: 590   score: 180.0  epsilon: 1.0    steps: 446  evaluation reward: 194.6\n",
      "episode: 591   score: 345.0  epsilon: 1.0    steps: 907  evaluation reward: 196.25\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: 0.545502. Value loss: 9.176582. Entropy: 1.055760.\n",
      "Iteration 1439: Policy loss: 0.493411. Value loss: 6.390480. Entropy: 1.064752.\n",
      "Iteration 1440: Policy loss: 0.541791. Value loss: 5.679128. Entropy: 1.060846.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: 0.508545. Value loss: 8.947122. Entropy: 0.872261.\n",
      "Iteration 1442: Policy loss: 0.497522. Value loss: 6.420639. Entropy: 0.890586.\n",
      "Iteration 1443: Policy loss: 0.605196. Value loss: 6.861455. Entropy: 0.885884.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: -1.474289. Value loss: 104.933533. Entropy: 1.043475.\n",
      "Iteration 1445: Policy loss: -1.072896. Value loss: 30.222933. Entropy: 1.029055.\n",
      "Iteration 1446: Policy loss: -1.489801. Value loss: 12.543297. Entropy: 0.964397.\n",
      "episode: 592   score: 155.0  epsilon: 1.0    steps: 222  evaluation reward: 195.7\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: 2.206882. Value loss: 72.229393. Entropy: 0.839566.\n",
      "Iteration 1448: Policy loss: 1.975698. Value loss: 8.529814. Entropy: 0.816158.\n",
      "Iteration 1449: Policy loss: 2.450763. Value loss: 8.140647. Entropy: 0.827206.\n",
      "episode: 593   score: 380.0  epsilon: 1.0    steps: 257  evaluation reward: 197.95\n",
      "episode: 594   score: 155.0  epsilon: 1.0    steps: 710  evaluation reward: 197.95\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: 0.677600. Value loss: 16.069315. Entropy: 0.625385.\n",
      "Iteration 1451: Policy loss: 0.559054. Value loss: 10.181076. Entropy: 0.601007.\n",
      "Iteration 1452: Policy loss: 0.681734. Value loss: 7.561608. Entropy: 0.633011.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: 1.393724. Value loss: 17.529491. Entropy: 0.770106.\n",
      "Iteration 1454: Policy loss: 1.195881. Value loss: 8.867036. Entropy: 0.901937.\n",
      "Iteration 1455: Policy loss: 1.206352. Value loss: 7.689758. Entropy: 0.959885.\n",
      "episode: 595   score: 180.0  epsilon: 1.0    steps: 118  evaluation reward: 197.9\n",
      "episode: 596   score: 155.0  epsilon: 1.0    steps: 479  evaluation reward: 197.35\n",
      "episode: 597   score: 155.0  epsilon: 1.0    steps: 531  evaluation reward: 197.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 598   score: 155.0  epsilon: 1.0    steps: 806  evaluation reward: 197.25\n",
      "episode: 599   score: 120.0  epsilon: 1.0    steps: 936  evaluation reward: 196.35\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: 0.042386. Value loss: 10.187260. Entropy: 1.109315.\n",
      "Iteration 1457: Policy loss: -0.003726. Value loss: 6.250106. Entropy: 1.131262.\n",
      "Iteration 1458: Policy loss: 0.013805. Value loss: 5.917012. Entropy: 1.150441.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: 1.267827. Value loss: 11.745013. Entropy: 0.814507.\n",
      "Iteration 1460: Policy loss: 0.824212. Value loss: 6.630285. Entropy: 0.850136.\n",
      "Iteration 1461: Policy loss: 1.312910. Value loss: 4.966064. Entropy: 0.825470.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: -1.562211. Value loss: 135.922897. Entropy: 0.912763.\n",
      "Iteration 1463: Policy loss: -0.997242. Value loss: 24.769762. Entropy: 0.933532.\n",
      "Iteration 1464: Policy loss: -1.502657. Value loss: 14.665008. Entropy: 0.914412.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: 0.521158. Value loss: 12.311706. Entropy: 1.030222.\n",
      "Iteration 1466: Policy loss: 1.051905. Value loss: 4.672050. Entropy: 1.042569.\n",
      "Iteration 1467: Policy loss: 0.703472. Value loss: 4.153853. Entropy: 1.037195.\n",
      "episode: 600   score: 380.0  epsilon: 1.0    steps: 141  evaluation reward: 198.35\n",
      "now time :  2019-02-25 19:07:15.666069\n",
      "episode: 601   score: 155.0  epsilon: 1.0    steps: 289  evaluation reward: 197.8\n",
      "episode: 602   score: 125.0  epsilon: 1.0    steps: 737  evaluation reward: 197.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: 1.886437. Value loss: 11.169493. Entropy: 0.845211.\n",
      "Iteration 1469: Policy loss: 1.786176. Value loss: 7.692301. Entropy: 0.869390.\n",
      "Iteration 1470: Policy loss: 1.781803. Value loss: 6.092282. Entropy: 0.877506.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: 1.386495. Value loss: 27.272646. Entropy: 0.844642.\n",
      "Iteration 1472: Policy loss: 1.281631. Value loss: 12.183809. Entropy: 0.811693.\n",
      "Iteration 1473: Policy loss: 1.604896. Value loss: 10.763358. Entropy: 0.822816.\n",
      "episode: 603   score: 120.0  epsilon: 1.0    steps: 562  evaluation reward: 196.55\n",
      "episode: 604   score: 135.0  epsilon: 1.0    steps: 839  evaluation reward: 196.35\n",
      "episode: 605   score: 155.0  epsilon: 1.0    steps: 977  evaluation reward: 196.55\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: 1.082166. Value loss: 22.992483. Entropy: 1.173782.\n",
      "Iteration 1475: Policy loss: 0.789447. Value loss: 16.197865. Entropy: 1.154780.\n",
      "Iteration 1476: Policy loss: 1.037371. Value loss: 10.384258. Entropy: 1.154315.\n",
      "episode: 606   score: 180.0  epsilon: 1.0    steps: 35  evaluation reward: 196.25\n",
      "episode: 607   score: 180.0  epsilon: 1.0    steps: 450  evaluation reward: 195.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: -0.021218. Value loss: 6.914368. Entropy: 1.027303.\n",
      "Iteration 1478: Policy loss: -0.134095. Value loss: 6.595872. Entropy: 0.965946.\n",
      "Iteration 1479: Policy loss: -0.196696. Value loss: 5.150985. Entropy: 1.009136.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 0.654761. Value loss: 14.910476. Entropy: 0.995640.\n",
      "Iteration 1481: Policy loss: 0.536740. Value loss: 9.430931. Entropy: 1.024713.\n",
      "Iteration 1482: Policy loss: 0.402257. Value loss: 7.710205. Entropy: 1.020502.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: -0.208105. Value loss: 154.215408. Entropy: 0.986571.\n",
      "Iteration 1484: Policy loss: 0.250783. Value loss: 69.995461. Entropy: 1.035571.\n",
      "Iteration 1485: Policy loss: -0.326219. Value loss: 44.238674. Entropy: 1.108342.\n",
      "episode: 608   score: 210.0  epsilon: 1.0    steps: 380  evaluation reward: 195.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: -0.266009. Value loss: 16.623123. Entropy: 1.082814.\n",
      "Iteration 1487: Policy loss: -0.289029. Value loss: 11.766138. Entropy: 1.119947.\n",
      "Iteration 1488: Policy loss: -0.264251. Value loss: 10.520475. Entropy: 1.086503.\n",
      "episode: 609   score: 215.0  epsilon: 1.0    steps: 174  evaluation reward: 195.8\n",
      "episode: 610   score: 410.0  epsilon: 1.0    steps: 694  evaluation reward: 198.35\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: 1.006868. Value loss: 11.548582. Entropy: 1.006721.\n",
      "Iteration 1490: Policy loss: 1.184899. Value loss: 6.941938. Entropy: 1.012611.\n",
      "Iteration 1491: Policy loss: 1.143799. Value loss: 5.695226. Entropy: 1.011340.\n",
      "episode: 611   score: 155.0  epsilon: 1.0    steps: 1011  evaluation reward: 197.8\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: -0.342124. Value loss: 16.388693. Entropy: 0.905101.\n",
      "Iteration 1493: Policy loss: -0.624406. Value loss: 11.820765. Entropy: 0.892103.\n",
      "Iteration 1494: Policy loss: -0.215812. Value loss: 9.480968. Entropy: 0.904616.\n",
      "episode: 612   score: 180.0  epsilon: 1.0    steps: 94  evaluation reward: 195.5\n",
      "episode: 613   score: 180.0  epsilon: 1.0    steps: 507  evaluation reward: 195.5\n",
      "episode: 614   score: 210.0  epsilon: 1.0    steps: 799  evaluation reward: 195.8\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: -1.120563. Value loss: 20.009752. Entropy: 1.134971.\n",
      "Iteration 1496: Policy loss: -1.231594. Value loss: 12.375713. Entropy: 1.129994.\n",
      "Iteration 1497: Policy loss: -1.116078. Value loss: 9.468885. Entropy: 1.142856.\n",
      "episode: 615   score: 270.0  epsilon: 1.0    steps: 580  evaluation reward: 196.7\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.134615. Value loss: 7.380302. Entropy: 0.959784.\n",
      "Iteration 1499: Policy loss: 0.127032. Value loss: 6.325437. Entropy: 0.971033.\n",
      "Iteration 1500: Policy loss: 0.137156. Value loss: 5.603812. Entropy: 0.992003.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: 0.442412. Value loss: 6.200077. Entropy: 0.914845.\n",
      "Iteration 1502: Policy loss: 0.445543. Value loss: 4.702010. Entropy: 0.923977.\n",
      "Iteration 1503: Policy loss: 0.360570. Value loss: 5.023907. Entropy: 0.923840.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: -0.077174. Value loss: 3.825073. Entropy: 1.122924.\n",
      "Iteration 1505: Policy loss: -0.047403. Value loss: 2.487199. Entropy: 1.115456.\n",
      "Iteration 1506: Policy loss: -0.072675. Value loss: 1.660944. Entropy: 1.107834.\n",
      "episode: 616   score: 155.0  epsilon: 1.0    steps: 219  evaluation reward: 196.45\n",
      "episode: 617   score: 210.0  epsilon: 1.0    steps: 321  evaluation reward: 194.7\n",
      "episode: 618   score: 210.0  epsilon: 1.0    steps: 750  evaluation reward: 195.0\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: -0.269852. Value loss: 11.097799. Entropy: 1.025306.\n",
      "Iteration 1508: Policy loss: 0.084123. Value loss: 7.652812. Entropy: 1.051560.\n",
      "Iteration 1509: Policy loss: -0.455473. Value loss: 7.652989. Entropy: 1.025289.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: -0.258156. Value loss: 7.678640. Entropy: 0.767042.\n",
      "Iteration 1511: Policy loss: -0.281124. Value loss: 4.500685. Entropy: 0.762311.\n",
      "Iteration 1512: Policy loss: -0.257505. Value loss: 3.443851. Entropy: 0.774437.\n",
      "episode: 619   score: 180.0  epsilon: 1.0    steps: 850  evaluation reward: 195.6\n",
      "episode: 620   score: 180.0  epsilon: 1.0    steps: 940  evaluation reward: 195.6\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 0.766789. Value loss: 11.978099. Entropy: 1.053983.\n",
      "Iteration 1514: Policy loss: 0.548410. Value loss: 9.780404. Entropy: 1.041800.\n",
      "Iteration 1515: Policy loss: 0.662592. Value loss: 8.098550. Entropy: 1.032286.\n",
      "episode: 621   score: 180.0  epsilon: 1.0    steps: 11  evaluation reward: 196.6\n",
      "episode: 622   score: 155.0  epsilon: 1.0    steps: 430  evaluation reward: 197.05\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: -1.114868. Value loss: 8.682227. Entropy: 0.994953.\n",
      "Iteration 1517: Policy loss: -1.126110. Value loss: 6.036262. Entropy: 0.972854.\n",
      "Iteration 1518: Policy loss: -1.174878. Value loss: 6.056189. Entropy: 0.998315.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1519: Policy loss: 0.011023. Value loss: 10.754158. Entropy: 0.729171.\n",
      "Iteration 1520: Policy loss: 0.060859. Value loss: 6.676558. Entropy: 0.716486.\n",
      "Iteration 1521: Policy loss: 0.077636. Value loss: 6.186719. Entropy: 0.761490.\n",
      "episode: 623   score: 265.0  epsilon: 1.0    steps: 557  evaluation reward: 198.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: 0.149054. Value loss: 3.968026. Entropy: 1.079093.\n",
      "Iteration 1523: Policy loss: 0.250163. Value loss: 2.696465. Entropy: 1.015222.\n",
      "Iteration 1524: Policy loss: 0.173003. Value loss: 2.786646. Entropy: 1.035712.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 0.137095. Value loss: 13.264050. Entropy: 0.866165.\n",
      "Iteration 1526: Policy loss: 0.107875. Value loss: 8.314763. Entropy: 0.833132.\n",
      "Iteration 1527: Policy loss: 0.292400. Value loss: 7.014889. Entropy: 0.898522.\n",
      "episode: 624   score: 155.0  epsilon: 1.0    steps: 132  evaluation reward: 195.8\n",
      "episode: 625   score: 180.0  epsilon: 1.0    steps: 292  evaluation reward: 196.5\n",
      "episode: 626   score: 180.0  epsilon: 1.0    steps: 681  evaluation reward: 196.75\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1528: Policy loss: 0.820525. Value loss: 5.981696. Entropy: 0.898615.\n",
      "Iteration 1529: Policy loss: 0.769127. Value loss: 3.519383. Entropy: 0.874281.\n",
      "Iteration 1530: Policy loss: 0.791264. Value loss: 3.279239. Entropy: 0.893956.\n",
      "episode: 627   score: 120.0  epsilon: 1.0    steps: 23  evaluation reward: 197.2\n",
      "episode: 628   score: 180.0  epsilon: 1.0    steps: 991  evaluation reward: 197.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: 0.607931. Value loss: 8.168599. Entropy: 0.782228.\n",
      "Iteration 1532: Policy loss: 0.696052. Value loss: 5.039684. Entropy: 0.774879.\n",
      "Iteration 1533: Policy loss: 0.582681. Value loss: 4.908065. Entropy: 0.792980.\n",
      "episode: 629   score: 210.0  epsilon: 1.0    steps: 481  evaluation reward: 198.2\n",
      "episode: 630   score: 155.0  epsilon: 1.0    steps: 771  evaluation reward: 197.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: 0.204985. Value loss: 5.015233. Entropy: 0.828501.\n",
      "Iteration 1535: Policy loss: 0.048532. Value loss: 4.354911. Entropy: 0.793469.\n",
      "Iteration 1536: Policy loss: 0.243774. Value loss: 3.019463. Entropy: 0.832944.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: 0.180317. Value loss: 3.653599. Entropy: 0.802494.\n",
      "Iteration 1538: Policy loss: 0.038368. Value loss: 3.622884. Entropy: 0.784576.\n",
      "Iteration 1539: Policy loss: 0.080000. Value loss: 3.162748. Entropy: 0.798762.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: -0.000486. Value loss: 7.520434. Entropy: 0.749200.\n",
      "Iteration 1541: Policy loss: -0.105479. Value loss: 4.601554. Entropy: 0.759017.\n",
      "Iteration 1542: Policy loss: -0.053491. Value loss: 4.750128. Entropy: 0.719639.\n",
      "episode: 631   score: 210.0  epsilon: 1.0    steps: 526  evaluation reward: 198.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: 0.232866. Value loss: 6.062537. Entropy: 0.833637.\n",
      "Iteration 1544: Policy loss: 0.181468. Value loss: 6.395010. Entropy: 0.825129.\n",
      "Iteration 1545: Policy loss: 0.228939. Value loss: 5.489146. Entropy: 0.840352.\n",
      "episode: 632   score: 155.0  epsilon: 1.0    steps: 177  evaluation reward: 197.05\n",
      "episode: 633   score: 180.0  epsilon: 1.0    steps: 343  evaluation reward: 197.5\n",
      "episode: 634   score: 155.0  epsilon: 1.0    steps: 732  evaluation reward: 197.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: 0.123969. Value loss: 6.890892. Entropy: 0.782036.\n",
      "Iteration 1547: Policy loss: 0.160765. Value loss: 3.693443. Entropy: 0.742019.\n",
      "Iteration 1548: Policy loss: 0.143434. Value loss: 3.372794. Entropy: 0.803601.\n",
      "episode: 635   score: 210.0  epsilon: 1.0    steps: 74  evaluation reward: 198.2\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: -1.025144. Value loss: 8.198401. Entropy: 0.734722.\n",
      "Iteration 1550: Policy loss: -1.065328. Value loss: 6.607295. Entropy: 0.705045.\n",
      "Iteration 1551: Policy loss: -1.071928. Value loss: 5.807970. Entropy: 0.734758.\n",
      "episode: 636   score: 180.0  epsilon: 1.0    steps: 822  evaluation reward: 198.2\n",
      "episode: 637   score: 210.0  epsilon: 1.0    steps: 914  evaluation reward: 198.75\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: 0.220067. Value loss: 10.829927. Entropy: 0.825050.\n",
      "Iteration 1553: Policy loss: 0.240946. Value loss: 5.643737. Entropy: 0.850073.\n",
      "Iteration 1554: Policy loss: 0.185455. Value loss: 6.166991. Entropy: 0.830443.\n",
      "episode: 638   score: 155.0  epsilon: 1.0    steps: 404  evaluation reward: 197.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: -0.214995. Value loss: 3.847569. Entropy: 0.827023.\n",
      "Iteration 1556: Policy loss: -0.207912. Value loss: 4.064338. Entropy: 0.808982.\n",
      "Iteration 1557: Policy loss: -0.066174. Value loss: 4.021841. Entropy: 0.804771.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: -0.717003. Value loss: 7.136176. Entropy: 0.851254.\n",
      "Iteration 1559: Policy loss: -0.786061. Value loss: 7.485688. Entropy: 0.868082.\n",
      "Iteration 1560: Policy loss: -0.807983. Value loss: 5.399403. Entropy: 0.863185.\n",
      "episode: 639   score: 180.0  epsilon: 1.0    steps: 577  evaluation reward: 196.85\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: 0.044599. Value loss: 3.472047. Entropy: 1.033054.\n",
      "Iteration 1562: Policy loss: 0.029996. Value loss: 2.819811. Entropy: 1.010541.\n",
      "Iteration 1563: Policy loss: 0.078925. Value loss: 3.003593. Entropy: 1.017567.\n",
      "episode: 640   score: 150.0  epsilon: 1.0    steps: 197  evaluation reward: 196.55\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: 0.264068. Value loss: 11.038259. Entropy: 1.036914.\n",
      "Iteration 1565: Policy loss: 0.336576. Value loss: 5.658810. Entropy: 1.006688.\n",
      "Iteration 1566: Policy loss: 0.278127. Value loss: 5.012331. Entropy: 1.002599.\n",
      "episode: 641   score: 160.0  epsilon: 1.0    steps: 101  evaluation reward: 196.6\n",
      "episode: 642   score: 155.0  epsilon: 1.0    steps: 266  evaluation reward: 196.6\n",
      "episode: 643   score: 210.0  epsilon: 1.0    steps: 657  evaluation reward: 196.9\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: 0.495015. Value loss: 8.694674. Entropy: 0.886543.\n",
      "Iteration 1568: Policy loss: 0.424873. Value loss: 4.566220. Entropy: 0.898726.\n",
      "Iteration 1569: Policy loss: 0.445897. Value loss: 4.507285. Entropy: 0.872775.\n",
      "episode: 644   score: 155.0  epsilon: 1.0    steps: 855  evaluation reward: 194.9\n",
      "episode: 645   score: 180.0  epsilon: 1.0    steps: 950  evaluation reward: 195.35\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: 0.462587. Value loss: 7.224647. Entropy: 0.917482.\n",
      "Iteration 1571: Policy loss: 0.450495. Value loss: 5.426644. Entropy: 0.910415.\n",
      "Iteration 1572: Policy loss: 0.415641. Value loss: 4.978913. Entropy: 0.893325.\n",
      "episode: 646   score: 180.0  epsilon: 1.0    steps: 452  evaluation reward: 195.05\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: -0.141067. Value loss: 9.114939. Entropy: 0.867995.\n",
      "Iteration 1574: Policy loss: -0.180293. Value loss: 6.723296. Entropy: 0.874774.\n",
      "Iteration 1575: Policy loss: -0.187968. Value loss: 6.129651. Entropy: 0.869949.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: 0.467509. Value loss: 3.073234. Entropy: 0.827164.\n",
      "Iteration 1577: Policy loss: 0.480964. Value loss: 2.291935. Entropy: 0.839861.\n",
      "Iteration 1578: Policy loss: 0.495139. Value loss: 2.373945. Entropy: 0.842694.\n",
      "episode: 647   score: 155.0  epsilon: 1.0    steps: 628  evaluation reward: 193.85\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: 0.552690. Value loss: 3.626544. Entropy: 0.875379.\n",
      "Iteration 1580: Policy loss: 0.577676. Value loss: 2.219563. Entropy: 0.907494.\n",
      "Iteration 1581: Policy loss: 0.569802. Value loss: 2.026282. Entropy: 0.889340.\n",
      "episode: 648   score: 180.0  epsilon: 1.0    steps: 250  evaluation reward: 193.85\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: -0.557691. Value loss: 5.215019. Entropy: 0.825241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1583: Policy loss: -0.437589. Value loss: 3.928986. Entropy: 0.835755.\n",
      "Iteration 1584: Policy loss: -0.523152. Value loss: 4.533579. Entropy: 0.832917.\n",
      "episode: 649   score: 145.0  epsilon: 1.0    steps: 114  evaluation reward: 193.2\n",
      "episode: 650   score: 180.0  epsilon: 1.0    steps: 310  evaluation reward: 193.2\n",
      "now time :  2019-02-25 19:09:28.685877\n",
      "episode: 651   score: 155.0  epsilon: 1.0    steps: 690  evaluation reward: 192.65\n",
      "episode: 652   score: 105.0  epsilon: 1.0    steps: 956  evaluation reward: 192.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: 0.274248. Value loss: 9.717252. Entropy: 0.940720.\n",
      "Iteration 1586: Policy loss: 0.136815. Value loss: 7.937088. Entropy: 0.951505.\n",
      "Iteration 1587: Policy loss: 0.055931. Value loss: 7.129829. Entropy: 0.930873.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: -0.906621. Value loss: 5.496384. Entropy: 0.662111.\n",
      "Iteration 1589: Policy loss: -0.916894. Value loss: 4.242833. Entropy: 0.627214.\n",
      "Iteration 1590: Policy loss: -0.941888. Value loss: 4.479698. Entropy: 0.632759.\n",
      "episode: 653   score: 155.0  epsilon: 1.0    steps: 503  evaluation reward: 192.4\n",
      "episode: 654   score: 210.0  epsilon: 1.0    steps: 777  evaluation reward: 192.4\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.759943. Value loss: 6.615066. Entropy: 0.987179.\n",
      "Iteration 1592: Policy loss: 0.776287. Value loss: 5.218515. Entropy: 1.002072.\n",
      "Iteration 1593: Policy loss: 0.716044. Value loss: 4.725302. Entropy: 1.016287.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: -0.222939. Value loss: 3.531793. Entropy: 0.790085.\n",
      "Iteration 1595: Policy loss: -0.185145. Value loss: 1.861118. Entropy: 0.843145.\n",
      "Iteration 1596: Policy loss: -0.121399. Value loss: 2.077017. Entropy: 0.827007.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: -1.886331. Value loss: 69.972458. Entropy: 0.602241.\n",
      "Iteration 1598: Policy loss: -3.331990. Value loss: 79.886658. Entropy: 0.678704.\n",
      "Iteration 1599: Policy loss: -2.158058. Value loss: 30.623798. Entropy: 0.747845.\n",
      "episode: 655   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 191.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: -0.600256. Value loss: 14.240485. Entropy: 0.748513.\n",
      "Iteration 1601: Policy loss: -0.557923. Value loss: 7.570684. Entropy: 0.751743.\n",
      "Iteration 1602: Policy loss: -0.584879. Value loss: 6.323390. Entropy: 0.755406.\n",
      "episode: 656   score: 180.0  epsilon: 1.0    steps: 159  evaluation reward: 191.35\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: 0.131359. Value loss: 23.173903. Entropy: 0.653581.\n",
      "Iteration 1604: Policy loss: 0.366106. Value loss: 13.092768. Entropy: 0.591479.\n",
      "Iteration 1605: Policy loss: 0.317157. Value loss: 11.727214. Entropy: 0.622023.\n",
      "episode: 657   score: 155.0  epsilon: 1.0    steps: 19  evaluation reward: 191.35\n",
      "episode: 658   score: 155.0  epsilon: 1.0    steps: 259  evaluation reward: 190.8\n",
      "episode: 659   score: 410.0  epsilon: 1.0    steps: 643  evaluation reward: 192.8\n",
      "episode: 660   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 193.1\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1606: Policy loss: -0.143670. Value loss: 8.514394. Entropy: 0.606641.\n",
      "Iteration 1607: Policy loss: -0.251920. Value loss: 7.049746. Entropy: 0.604853.\n",
      "Iteration 1608: Policy loss: -0.189779. Value loss: 6.044651. Entropy: 0.577349.\n",
      "episode: 661   score: 135.0  epsilon: 1.0    steps: 810  evaluation reward: 192.35\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1609: Policy loss: 0.066295. Value loss: 7.924404. Entropy: 0.661559.\n",
      "Iteration 1610: Policy loss: 0.254568. Value loss: 4.978230. Entropy: 0.654319.\n",
      "Iteration 1611: Policy loss: 0.115340. Value loss: 5.457418. Entropy: 0.660873.\n",
      "episode: 662   score: 180.0  epsilon: 1.0    steps: 426  evaluation reward: 192.05\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1612: Policy loss: -0.411068. Value loss: 4.801948. Entropy: 1.020966.\n",
      "Iteration 1613: Policy loss: -0.333254. Value loss: 4.014154. Entropy: 1.012228.\n",
      "Iteration 1614: Policy loss: -0.431764. Value loss: 2.516541. Entropy: 0.991334.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1615: Policy loss: -0.724975. Value loss: 4.055875. Entropy: 0.655097.\n",
      "Iteration 1616: Policy loss: -0.698491. Value loss: 3.539170. Entropy: 0.667146.\n",
      "Iteration 1617: Policy loss: -0.798303. Value loss: 2.041139. Entropy: 0.640745.\n",
      "episode: 663   score: 180.0  epsilon: 1.0    steps: 633  evaluation reward: 191.75\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1618: Policy loss: -0.176541. Value loss: 5.862220. Entropy: 0.585286.\n",
      "Iteration 1619: Policy loss: -0.203592. Value loss: 3.830886. Entropy: 0.593735.\n",
      "Iteration 1620: Policy loss: -0.222581. Value loss: 3.432190. Entropy: 0.617579.\n",
      "episode: 664   score: 180.0  epsilon: 1.0    steps: 202  evaluation reward: 191.75\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1621: Policy loss: 0.175898. Value loss: 8.104734. Entropy: 0.741415.\n",
      "Iteration 1622: Policy loss: 0.299592. Value loss: 6.205547. Entropy: 0.756144.\n",
      "Iteration 1623: Policy loss: 0.167205. Value loss: 5.730565. Entropy: 0.722245.\n",
      "episode: 665   score: 210.0  epsilon: 1.0    steps: 71  evaluation reward: 191.75\n",
      "episode: 666   score: 180.0  epsilon: 1.0    steps: 310  evaluation reward: 191.45\n",
      "episode: 667   score: 180.0  epsilon: 1.0    steps: 689  evaluation reward: 191.4\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1624: Policy loss: -1.255530. Value loss: 15.438324. Entropy: 0.856278.\n",
      "Iteration 1625: Policy loss: -1.418076. Value loss: 11.022002. Entropy: 0.841003.\n",
      "Iteration 1626: Policy loss: -1.243076. Value loss: 11.017667. Entropy: 0.834185.\n",
      "episode: 668   score: 260.0  epsilon: 1.0    steps: 1005  evaluation reward: 191.9\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1627: Policy loss: 0.404388. Value loss: 9.718366. Entropy: 0.700012.\n",
      "Iteration 1628: Policy loss: 0.471482. Value loss: 5.588861. Entropy: 0.688014.\n",
      "Iteration 1629: Policy loss: 0.462515. Value loss: 6.482076. Entropy: 0.693000.\n",
      "episode: 669   score: 240.0  epsilon: 1.0    steps: 894  evaluation reward: 193.25\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1630: Policy loss: 0.381288. Value loss: 11.164236. Entropy: 0.876675.\n",
      "Iteration 1631: Policy loss: 0.446764. Value loss: 9.546643. Entropy: 0.885110.\n",
      "Iteration 1632: Policy loss: 0.169840. Value loss: 6.795946. Entropy: 0.930171.\n",
      "episode: 670   score: 210.0  epsilon: 1.0    steps: 395  evaluation reward: 193.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1633: Policy loss: -0.755011. Value loss: 5.110445. Entropy: 0.786863.\n",
      "Iteration 1634: Policy loss: -0.677410. Value loss: 3.110447. Entropy: 0.811042.\n",
      "Iteration 1635: Policy loss: -0.774057. Value loss: 2.912557. Entropy: 0.829786.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1636: Policy loss: 0.201174. Value loss: 6.202769. Entropy: 0.675438.\n",
      "Iteration 1637: Policy loss: 0.281888. Value loss: 3.633930. Entropy: 0.666066.\n",
      "Iteration 1638: Policy loss: 0.228671. Value loss: 3.276344. Entropy: 0.663199.\n",
      "episode: 671   score: 155.0  epsilon: 1.0    steps: 249  evaluation reward: 193.8\n",
      "episode: 672   score: 155.0  epsilon: 1.0    steps: 556  evaluation reward: 193.55\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1639: Policy loss: 0.715645. Value loss: 3.879263. Entropy: 0.977589.\n",
      "Iteration 1640: Policy loss: 0.743528. Value loss: 1.768484. Entropy: 0.967668.\n",
      "Iteration 1641: Policy loss: 0.678854. Value loss: 1.892832. Entropy: 0.975246.\n",
      "episode: 673   score: 180.0  epsilon: 1.0    steps: 126  evaluation reward: 191.25\n",
      "episode: 674   score: 180.0  epsilon: 1.0    steps: 359  evaluation reward: 191.5\n",
      "episode: 675   score: 155.0  epsilon: 1.0    steps: 722  evaluation reward: 191.25\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1642: Policy loss: -0.381701. Value loss: 9.049372. Entropy: 0.843364.\n",
      "Iteration 1643: Policy loss: -0.321243. Value loss: 6.506533. Entropy: 0.839630.\n",
      "Iteration 1644: Policy loss: -0.330347. Value loss: 6.179257. Entropy: 0.881863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1645: Policy loss: 0.193395. Value loss: 5.921730. Entropy: 0.591835.\n",
      "Iteration 1646: Policy loss: 0.154041. Value loss: 3.841994. Entropy: 0.634391.\n",
      "Iteration 1647: Policy loss: 0.224866. Value loss: 4.383388. Entropy: 0.609459.\n",
      "episode: 676   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 190.7\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1648: Policy loss: 0.648089. Value loss: 10.299220. Entropy: 0.908935.\n",
      "Iteration 1649: Policy loss: 0.637681. Value loss: 7.506359. Entropy: 0.919133.\n",
      "Iteration 1650: Policy loss: 0.627466. Value loss: 7.725567. Entropy: 0.933959.\n",
      "episode: 677   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 190.7\n",
      "episode: 678   score: 210.0  epsilon: 1.0    steps: 817  evaluation reward: 191.2\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1651: Policy loss: 0.473869. Value loss: 10.432400. Entropy: 1.040957.\n",
      "Iteration 1652: Policy loss: 0.376654. Value loss: 6.365030. Entropy: 1.073986.\n",
      "Iteration 1653: Policy loss: 0.462967. Value loss: 5.000731. Entropy: 1.065531.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1654: Policy loss: 0.220625. Value loss: 2.313030. Entropy: 0.761370.\n",
      "Iteration 1655: Policy loss: 0.187930. Value loss: 1.572980. Entropy: 0.770289.\n",
      "Iteration 1656: Policy loss: 0.245357. Value loss: 1.356190. Entropy: 0.780119.\n",
      "episode: 679   score: 155.0  epsilon: 1.0    steps: 581  evaluation reward: 189.2\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1657: Policy loss: 1.155133. Value loss: 9.354350. Entropy: 0.941756.\n",
      "Iteration 1658: Policy loss: 1.078495. Value loss: 5.732278. Entropy: 0.917784.\n",
      "Iteration 1659: Policy loss: 1.274688. Value loss: 4.397831. Entropy: 0.913074.\n",
      "episode: 680   score: 210.0  epsilon: 1.0    steps: 156  evaluation reward: 187.2\n",
      "episode: 681   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 186.95\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1660: Policy loss: 0.353344. Value loss: 9.633296. Entropy: 0.973453.\n",
      "Iteration 1661: Policy loss: 0.329488. Value loss: 6.730638. Entropy: 0.993371.\n",
      "Iteration 1662: Policy loss: 0.307399. Value loss: 6.405131. Entropy: 1.008087.\n",
      "episode: 682   score: 180.0  epsilon: 1.0    steps: 39  evaluation reward: 187.2\n",
      "episode: 683   score: 180.0  epsilon: 1.0    steps: 270  evaluation reward: 187.2\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1663: Policy loss: 0.931404. Value loss: 12.350715. Entropy: 0.785154.\n",
      "Iteration 1664: Policy loss: 1.021137. Value loss: 7.562149. Entropy: 0.767299.\n",
      "Iteration 1665: Policy loss: 0.964464. Value loss: 7.039303. Entropy: 0.779941.\n",
      "episode: 684   score: 210.0  epsilon: 1.0    steps: 970  evaluation reward: 187.75\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1666: Policy loss: -3.411737. Value loss: 210.278336. Entropy: 0.764095.\n",
      "Iteration 1667: Policy loss: -3.142607. Value loss: 58.141724. Entropy: 0.842420.\n",
      "Iteration 1668: Policy loss: -3.560397. Value loss: 65.881149. Entropy: 0.826262.\n",
      "episode: 685   score: 410.0  epsilon: 1.0    steps: 511  evaluation reward: 189.75\n",
      "episode: 686   score: 155.0  epsilon: 1.0    steps: 866  evaluation reward: 189.5\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1669: Policy loss: -0.176483. Value loss: 9.758217. Entropy: 0.965025.\n",
      "Iteration 1670: Policy loss: -0.196216. Value loss: 7.095686. Entropy: 0.986544.\n",
      "Iteration 1671: Policy loss: -0.037987. Value loss: 5.886934. Entropy: 0.977693.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1672: Policy loss: 0.236501. Value loss: 24.031166. Entropy: 0.679077.\n",
      "Iteration 1673: Policy loss: 0.300930. Value loss: 8.597235. Entropy: 0.743211.\n",
      "Iteration 1674: Policy loss: 0.203977. Value loss: 5.980291. Entropy: 0.720930.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1675: Policy loss: 0.727144. Value loss: 12.140637. Entropy: 0.984475.\n",
      "Iteration 1676: Policy loss: 1.040503. Value loss: 6.337009. Entropy: 0.943719.\n",
      "Iteration 1677: Policy loss: 0.933939. Value loss: 4.368630. Entropy: 0.981450.\n",
      "episode: 687   score: 180.0  epsilon: 1.0    steps: 192  evaluation reward: 189.5\n",
      "episode: 688   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 189.5\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1678: Policy loss: 1.206711. Value loss: 23.929205. Entropy: 1.288150.\n",
      "Iteration 1679: Policy loss: 1.359967. Value loss: 8.060502. Entropy: 1.235206.\n",
      "Iteration 1680: Policy loss: 1.113135. Value loss: 6.502430. Entropy: 1.233711.\n",
      "episode: 689   score: 210.0  epsilon: 1.0    steps: 124  evaluation reward: 189.65\n",
      "episode: 690   score: 180.0  epsilon: 1.0    steps: 319  evaluation reward: 189.65\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1681: Policy loss: -0.369608. Value loss: 14.901276. Entropy: 0.870226.\n",
      "Iteration 1682: Policy loss: -0.405704. Value loss: 8.992899. Entropy: 0.911925.\n",
      "Iteration 1683: Policy loss: -0.445277. Value loss: 8.980838. Entropy: 0.858293.\n",
      "episode: 691   score: 275.0  epsilon: 1.0    steps: 742  evaluation reward: 188.95\n",
      "episode: 692   score: 180.0  epsilon: 1.0    steps: 1023  evaluation reward: 189.2\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1684: Policy loss: 1.333899. Value loss: 13.344203. Entropy: 0.997154.\n",
      "Iteration 1685: Policy loss: 1.521780. Value loss: 7.827575. Entropy: 0.996664.\n",
      "Iteration 1686: Policy loss: 1.297333. Value loss: 6.413678. Entropy: 1.025929.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1687: Policy loss: 0.644351. Value loss: 14.160166. Entropy: 0.954211.\n",
      "Iteration 1688: Policy loss: 0.785139. Value loss: 10.582221. Entropy: 0.930235.\n",
      "Iteration 1689: Policy loss: 0.620326. Value loss: 8.230781. Entropy: 0.939402.\n",
      "episode: 693   score: 210.0  epsilon: 1.0    steps: 476  evaluation reward: 187.5\n",
      "episode: 694   score: 180.0  epsilon: 1.0    steps: 787  evaluation reward: 187.75\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1690: Policy loss: 0.591155. Value loss: 6.638301. Entropy: 0.838589.\n",
      "Iteration 1691: Policy loss: 0.489058. Value loss: 5.748491. Entropy: 0.852830.\n",
      "Iteration 1692: Policy loss: 0.494395. Value loss: 5.895245. Entropy: 0.821065.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1693: Policy loss: -0.623229. Value loss: 9.358004. Entropy: 1.034056.\n",
      "Iteration 1694: Policy loss: -0.684613. Value loss: 5.816881. Entropy: 1.048152.\n",
      "Iteration 1695: Policy loss: -0.720595. Value loss: 5.648309. Entropy: 1.062236.\n",
      "episode: 695   score: 120.0  epsilon: 1.0    steps: 339  evaluation reward: 187.15\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1696: Policy loss: 0.546965. Value loss: 11.530889. Entropy: 1.050865.\n",
      "Iteration 1697: Policy loss: 0.634190. Value loss: 7.712330. Entropy: 1.062570.\n",
      "Iteration 1698: Policy loss: 0.534701. Value loss: 7.285666. Entropy: 1.048805.\n",
      "episode: 696   score: 210.0  epsilon: 1.0    steps: 156  evaluation reward: 187.7\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1699: Policy loss: -2.028875. Value loss: 184.301605. Entropy: 0.904447.\n",
      "Iteration 1700: Policy loss: -2.926729. Value loss: 139.467194. Entropy: 0.797571.\n",
      "Iteration 1701: Policy loss: -1.565338. Value loss: 126.738419. Entropy: 0.859990.\n",
      "episode: 697   score: 210.0  epsilon: 1.0    steps: 51  evaluation reward: 188.25\n",
      "episode: 698   score: 260.0  epsilon: 1.0    steps: 525  evaluation reward: 189.3\n",
      "episode: 699   score: 120.0  epsilon: 1.0    steps: 954  evaluation reward: 189.3\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1702: Policy loss: 0.347838. Value loss: 20.475750. Entropy: 0.911992.\n",
      "Iteration 1703: Policy loss: 0.189302. Value loss: 14.085814. Entropy: 0.905626.\n",
      "Iteration 1704: Policy loss: 0.424716. Value loss: 12.565380. Entropy: 0.917517.\n",
      "episode: 700   score: 180.0  epsilon: 1.0    steps: 663  evaluation reward: 187.3\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1705: Policy loss: 0.866643. Value loss: 19.400883. Entropy: 0.902175.\n",
      "Iteration 1706: Policy loss: 0.747826. Value loss: 12.362512. Entropy: 0.878467.\n",
      "Iteration 1707: Policy loss: 0.918364. Value loss: 12.037757. Entropy: 0.883972.\n",
      "now time :  2019-02-25 19:11:46.047149\n",
      "episode: 701   score: 210.0  epsilon: 1.0    steps: 511  evaluation reward: 187.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1708: Policy loss: -0.031790. Value loss: 8.206283. Entropy: 0.894950.\n",
      "Iteration 1709: Policy loss: -0.144885. Value loss: 6.137877. Entropy: 0.887691.\n",
      "Iteration 1710: Policy loss: -0.023842. Value loss: 5.103371. Entropy: 0.884032.\n",
      "episode: 702   score: 460.0  epsilon: 1.0    steps: 896  evaluation reward: 191.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1711: Policy loss: 1.968519. Value loss: 30.163197. Entropy: 0.908613.\n",
      "Iteration 1712: Policy loss: 1.885305. Value loss: 10.311728. Entropy: 0.906859.\n",
      "Iteration 1713: Policy loss: 1.738776. Value loss: 9.123777. Entropy: 0.929765.\n",
      "episode: 703   score: 180.0  epsilon: 1.0    steps: 374  evaluation reward: 191.8\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1714: Policy loss: -2.907147. Value loss: 291.408508. Entropy: 0.558947.\n",
      "Iteration 1715: Policy loss: -2.716036. Value loss: 195.142914. Entropy: 0.530236.\n",
      "Iteration 1716: Policy loss: -2.964897. Value loss: 180.016052. Entropy: 0.396840.\n",
      "episode: 704   score: 410.0  epsilon: 1.0    steps: 245  evaluation reward: 194.55\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1717: Policy loss: -3.565298. Value loss: 244.945969. Entropy: 0.429161.\n",
      "Iteration 1718: Policy loss: -3.123008. Value loss: 95.492950. Entropy: 0.465000.\n",
      "Iteration 1719: Policy loss: -4.181140. Value loss: 82.891296. Entropy: 0.489736.\n",
      "episode: 705   score: 410.0  epsilon: 1.0    steps: 98  evaluation reward: 197.1\n",
      "episode: 706   score: 185.0  epsilon: 1.0    steps: 987  evaluation reward: 197.15\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1720: Policy loss: 0.998755. Value loss: 19.093906. Entropy: 0.522466.\n",
      "Iteration 1721: Policy loss: 1.029092. Value loss: 12.663855. Entropy: 0.514422.\n",
      "Iteration 1722: Policy loss: 1.092169. Value loss: 10.941578. Entropy: 0.510267.\n",
      "episode: 707   score: 210.0  epsilon: 1.0    steps: 701  evaluation reward: 197.45\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1723: Policy loss: -0.463189. Value loss: 15.981616. Entropy: 0.474751.\n",
      "Iteration 1724: Policy loss: -0.466560. Value loss: 11.873036. Entropy: 0.465758.\n",
      "Iteration 1725: Policy loss: -0.355236. Value loss: 10.542897. Entropy: 0.460642.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1726: Policy loss: 1.672263. Value loss: 50.984741. Entropy: 0.499102.\n",
      "Iteration 1727: Policy loss: 1.661741. Value loss: 16.598827. Entropy: 0.445780.\n",
      "Iteration 1728: Policy loss: 1.509033. Value loss: 12.151393. Entropy: 0.454856.\n",
      "episode: 708   score: 155.0  epsilon: 1.0    steps: 426  evaluation reward: 196.9\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1729: Policy loss: -1.116131. Value loss: 110.487076. Entropy: 0.665930.\n",
      "Iteration 1730: Policy loss: -0.528758. Value loss: 53.635468. Entropy: 0.654299.\n",
      "Iteration 1731: Policy loss: -1.088415. Value loss: 34.512371. Entropy: 0.652011.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1732: Policy loss: -0.802934. Value loss: 19.771656. Entropy: 0.419308.\n",
      "Iteration 1733: Policy loss: -0.965275. Value loss: 10.505336. Entropy: 0.409161.\n",
      "Iteration 1734: Policy loss: -1.018136. Value loss: 9.674348. Entropy: 0.408786.\n",
      "episode: 709   score: 395.0  epsilon: 1.0    steps: 616  evaluation reward: 198.7\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1735: Policy loss: 2.690597. Value loss: 47.005486. Entropy: 0.489331.\n",
      "Iteration 1736: Policy loss: 2.624882. Value loss: 19.040257. Entropy: 0.455374.\n",
      "Iteration 1737: Policy loss: 2.558800. Value loss: 13.334426. Entropy: 0.536198.\n",
      "episode: 710   score: 180.0  epsilon: 1.0    steps: 164  evaluation reward: 196.4\n",
      "episode: 711   score: 470.0  epsilon: 1.0    steps: 331  evaluation reward: 199.55\n",
      "episode: 712   score: 335.0  epsilon: 1.0    steps: 849  evaluation reward: 201.1\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1738: Policy loss: 1.163229. Value loss: 18.640762. Entropy: 0.748550.\n",
      "Iteration 1739: Policy loss: 1.243896. Value loss: 11.951041. Entropy: 0.760672.\n",
      "Iteration 1740: Policy loss: 1.258700. Value loss: 9.557578. Entropy: 0.753111.\n",
      "episode: 713   score: 210.0  epsilon: 1.0    steps: 766  evaluation reward: 201.4\n",
      "episode: 714   score: 180.0  epsilon: 1.0    steps: 967  evaluation reward: 201.1\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1741: Policy loss: -0.342064. Value loss: 20.470163. Entropy: 0.482732.\n",
      "Iteration 1742: Policy loss: -0.465097. Value loss: 12.240027. Entropy: 0.510358.\n",
      "Iteration 1743: Policy loss: -0.305414. Value loss: 11.862358. Entropy: 0.458219.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1744: Policy loss: 0.505996. Value loss: 18.843077. Entropy: 0.651169.\n",
      "Iteration 1745: Policy loss: 0.345309. Value loss: 10.782350. Entropy: 0.633736.\n",
      "Iteration 1746: Policy loss: 0.568005. Value loss: 9.287237. Entropy: 0.644839.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1747: Policy loss: 0.147273. Value loss: 8.163956. Entropy: 0.754381.\n",
      "Iteration 1748: Policy loss: 0.064498. Value loss: 6.123983. Entropy: 0.758408.\n",
      "Iteration 1749: Policy loss: -0.147536. Value loss: 5.262927. Entropy: 0.739715.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1750: Policy loss: 1.228460. Value loss: 7.912951. Entropy: 0.460503.\n",
      "Iteration 1751: Policy loss: 1.103250. Value loss: 3.880559. Entropy: 0.533589.\n",
      "Iteration 1752: Policy loss: 1.157118. Value loss: 2.749785. Entropy: 0.533532.\n",
      "episode: 715   score: 340.0  epsilon: 1.0    steps: 113  evaluation reward: 201.8\n",
      "episode: 716   score: 210.0  epsilon: 1.0    steps: 403  evaluation reward: 202.35\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1753: Policy loss: -1.801291. Value loss: 161.784256. Entropy: 0.423797.\n",
      "Iteration 1754: Policy loss: -1.255001. Value loss: 63.347610. Entropy: 0.537614.\n",
      "Iteration 1755: Policy loss: -2.022172. Value loss: 49.541222. Entropy: 0.530928.\n",
      "episode: 717   score: 210.0  epsilon: 1.0    steps: 221  evaluation reward: 202.35\n",
      "episode: 718   score: 180.0  epsilon: 1.0    steps: 537  evaluation reward: 202.05\n",
      "episode: 719   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 201.8\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1756: Policy loss: -4.554304. Value loss: 143.892578. Entropy: 0.562935.\n",
      "Iteration 1757: Policy loss: -4.359921. Value loss: 78.468147. Entropy: 0.587431.\n",
      "Iteration 1758: Policy loss: -4.716497. Value loss: 67.776756. Entropy: 0.552984.\n",
      "episode: 720   score: 410.0  epsilon: 1.0    steps: 274  evaluation reward: 204.1\n",
      "episode: 721   score: 355.0  epsilon: 1.0    steps: 998  evaluation reward: 205.85\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1759: Policy loss: -0.645781. Value loss: 24.779642. Entropy: 0.669692.\n",
      "Iteration 1760: Policy loss: -0.726492. Value loss: 20.118364. Entropy: 0.645515.\n",
      "Iteration 1761: Policy loss: -0.838442. Value loss: 17.934450. Entropy: 0.662119.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1762: Policy loss: 0.514309. Value loss: 16.135057. Entropy: 0.771840.\n",
      "Iteration 1763: Policy loss: 0.301129. Value loss: 10.913510. Entropy: 0.814482.\n",
      "Iteration 1764: Policy loss: 0.464980. Value loss: 7.923578. Entropy: 0.780262.\n",
      "episode: 722   score: 475.0  epsilon: 1.0    steps: 751  evaluation reward: 209.05\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1765: Policy loss: 1.430611. Value loss: 7.048931. Entropy: 0.863096.\n",
      "Iteration 1766: Policy loss: 1.246316. Value loss: 4.948914. Entropy: 0.887193.\n",
      "Iteration 1767: Policy loss: 1.415693. Value loss: 4.469891. Entropy: 0.890574.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1768: Policy loss: 1.276696. Value loss: 7.826498. Entropy: 0.576238.\n",
      "Iteration 1769: Policy loss: 1.286775. Value loss: 4.222695. Entropy: 0.608349.\n",
      "Iteration 1770: Policy loss: 1.288500. Value loss: 3.053138. Entropy: 0.605326.\n",
      "episode: 723   score: 135.0  epsilon: 1.0    steps: 427  evaluation reward: 207.75\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1771: Policy loss: 0.624681. Value loss: 78.009758. Entropy: 0.616358.\n",
      "Iteration 1772: Policy loss: 0.566954. Value loss: 28.916759. Entropy: 0.622482.\n",
      "Iteration 1773: Policy loss: 0.313597. Value loss: 19.847692. Entropy: 0.621543.\n",
      "episode: 724   score: 180.0  epsilon: 1.0    steps: 11  evaluation reward: 208.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 725   score: 120.0  epsilon: 1.0    steps: 1012  evaluation reward: 207.4\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1774: Policy loss: -2.418637. Value loss: 241.569016. Entropy: 0.671922.\n",
      "Iteration 1775: Policy loss: -1.629223. Value loss: 118.254181. Entropy: 0.671819.\n",
      "Iteration 1776: Policy loss: -3.164479. Value loss: 87.922188. Entropy: 0.698680.\n",
      "episode: 726   score: 210.0  epsilon: 1.0    steps: 166  evaluation reward: 207.7\n",
      "episode: 727   score: 410.0  epsilon: 1.0    steps: 350  evaluation reward: 210.6\n",
      "episode: 728   score: 285.0  epsilon: 1.0    steps: 614  evaluation reward: 211.65\n",
      "episode: 729   score: 410.0  epsilon: 1.0    steps: 803  evaluation reward: 213.65\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1777: Policy loss: -0.384958. Value loss: 16.973204. Entropy: 0.988249.\n",
      "Iteration 1778: Policy loss: -0.406169. Value loss: 14.874666. Entropy: 0.952480.\n",
      "Iteration 1779: Policy loss: -0.339337. Value loss: 10.297225. Entropy: 0.941431.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1780: Policy loss: 0.175840. Value loss: 13.303480. Entropy: 0.715503.\n",
      "Iteration 1781: Policy loss: -0.257646. Value loss: 8.582885. Entropy: 0.725970.\n",
      "Iteration 1782: Policy loss: 0.099899. Value loss: 7.745743. Entropy: 0.734878.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1783: Policy loss: 0.117653. Value loss: 12.658979. Entropy: 0.978067.\n",
      "Iteration 1784: Policy loss: 0.122482. Value loss: 10.931288. Entropy: 0.957849.\n",
      "Iteration 1785: Policy loss: 0.038320. Value loss: 10.047596. Entropy: 0.958737.\n",
      "episode: 730   score: 180.0  epsilon: 1.0    steps: 676  evaluation reward: 213.9\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1786: Policy loss: 1.479128. Value loss: 52.662075. Entropy: 0.895852.\n",
      "Iteration 1787: Policy loss: 1.235287. Value loss: 7.450361. Entropy: 0.904179.\n",
      "Iteration 1788: Policy loss: 1.459834. Value loss: 5.704970. Entropy: 0.885988.\n",
      "episode: 731   score: 180.0  epsilon: 1.0    steps: 474  evaluation reward: 213.6\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1789: Policy loss: 0.434666. Value loss: 16.792393. Entropy: 0.643742.\n",
      "Iteration 1790: Policy loss: 0.503937. Value loss: 9.135404. Entropy: 0.646062.\n",
      "Iteration 1791: Policy loss: 0.439184. Value loss: 6.373336. Entropy: 0.670335.\n",
      "episode: 732   score: 180.0  epsilon: 1.0    steps: 86  evaluation reward: 213.85\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: 1.546384. Value loss: 15.561932. Entropy: 0.731253.\n",
      "Iteration 1793: Policy loss: 1.673321. Value loss: 10.487965. Entropy: 0.725938.\n",
      "Iteration 1794: Policy loss: 1.628989. Value loss: 9.016489. Entropy: 0.751284.\n",
      "episode: 733   score: 210.0  epsilon: 1.0    steps: 246  evaluation reward: 214.15\n",
      "episode: 734   score: 155.0  epsilon: 1.0    steps: 846  evaluation reward: 214.15\n",
      "episode: 735   score: 210.0  epsilon: 1.0    steps: 975  evaluation reward: 214.15\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1795: Policy loss: -0.017015. Value loss: 23.921436. Entropy: 0.834353.\n",
      "Iteration 1796: Policy loss: -0.551274. Value loss: 15.034505. Entropy: 0.838566.\n",
      "Iteration 1797: Policy loss: -0.146125. Value loss: 11.999603. Entropy: 0.831306.\n",
      "episode: 736   score: 210.0  epsilon: 1.0    steps: 537  evaluation reward: 214.45\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1798: Policy loss: 0.510521. Value loss: 18.551529. Entropy: 0.545531.\n",
      "Iteration 1799: Policy loss: 0.352715. Value loss: 9.666116. Entropy: 0.511678.\n",
      "Iteration 1800: Policy loss: 0.414765. Value loss: 9.645211. Entropy: 0.529759.\n",
      "episode: 737   score: 260.0  epsilon: 1.0    steps: 302  evaluation reward: 214.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1801: Policy loss: 1.146048. Value loss: 11.134855. Entropy: 0.576807.\n",
      "Iteration 1802: Policy loss: 1.111614. Value loss: 6.429842. Entropy: 0.661184.\n",
      "Iteration 1803: Policy loss: 1.087797. Value loss: 6.880008. Entropy: 0.651389.\n",
      "episode: 738   score: 185.0  epsilon: 1.0    steps: 719  evaluation reward: 215.25\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1804: Policy loss: 0.261211. Value loss: 3.645463. Entropy: 0.715724.\n",
      "Iteration 1805: Policy loss: 0.309007. Value loss: 2.621760. Entropy: 0.692817.\n",
      "Iteration 1806: Policy loss: 0.290896. Value loss: 2.074458. Entropy: 0.716259.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1807: Policy loss: 0.050137. Value loss: 4.819643. Entropy: 0.586802.\n",
      "Iteration 1808: Policy loss: 0.263922. Value loss: 2.983013. Entropy: 0.561885.\n",
      "Iteration 1809: Policy loss: 0.181048. Value loss: 3.097899. Entropy: 0.617138.\n",
      "episode: 739   score: 180.0  epsilon: 1.0    steps: 404  evaluation reward: 215.25\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1810: Policy loss: 0.137337. Value loss: 7.534588. Entropy: 0.587854.\n",
      "Iteration 1811: Policy loss: 0.082423. Value loss: 4.293504. Entropy: 0.630234.\n",
      "Iteration 1812: Policy loss: 0.142774. Value loss: 3.609625. Entropy: 0.599207.\n",
      "episode: 740   score: 180.0  epsilon: 1.0    steps: 9  evaluation reward: 215.55\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1813: Policy loss: -3.370523. Value loss: 193.050537. Entropy: 0.661052.\n",
      "Iteration 1814: Policy loss: -2.723340. Value loss: 63.519669. Entropy: 0.437155.\n",
      "Iteration 1815: Policy loss: -2.938352. Value loss: 38.697578. Entropy: 0.634324.\n",
      "episode: 741   score: 210.0  epsilon: 1.0    steps: 169  evaluation reward: 216.05\n",
      "episode: 742   score: 210.0  epsilon: 1.0    steps: 771  evaluation reward: 216.6\n",
      "episode: 743   score: 210.0  epsilon: 1.0    steps: 923  evaluation reward: 216.6\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1816: Policy loss: -1.174948. Value loss: 8.529172. Entropy: 0.725547.\n",
      "Iteration 1817: Policy loss: -1.208566. Value loss: 5.854501. Entropy: 0.701269.\n",
      "Iteration 1818: Policy loss: -1.239801. Value loss: 5.900106. Entropy: 0.713512.\n",
      "episode: 744   score: 260.0  epsilon: 1.0    steps: 625  evaluation reward: 217.65\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1819: Policy loss: 1.502856. Value loss: 18.348595. Entropy: 0.654575.\n",
      "Iteration 1820: Policy loss: 1.353737. Value loss: 14.549019. Entropy: 0.648704.\n",
      "Iteration 1821: Policy loss: 1.233570. Value loss: 12.465978. Entropy: 0.614500.\n",
      "episode: 745   score: 410.0  epsilon: 1.0    steps: 257  evaluation reward: 219.95\n",
      "episode: 746   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 219.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1822: Policy loss: -0.094689. Value loss: 5.669416. Entropy: 0.713908.\n",
      "Iteration 1823: Policy loss: -0.170903. Value loss: 4.117051. Entropy: 0.724322.\n",
      "Iteration 1824: Policy loss: -0.183334. Value loss: 3.243273. Entropy: 0.726353.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1825: Policy loss: -0.407076. Value loss: 7.483798. Entropy: 0.712793.\n",
      "Iteration 1826: Policy loss: -0.430864. Value loss: 5.238276. Entropy: 0.709374.\n",
      "Iteration 1827: Policy loss: -0.382429. Value loss: 5.505731. Entropy: 0.725925.\n",
      "episode: 747   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 220.2\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1828: Policy loss: 0.951475. Value loss: 9.232265. Entropy: 0.772884.\n",
      "Iteration 1829: Policy loss: 0.831400. Value loss: 5.509142. Entropy: 0.768618.\n",
      "Iteration 1830: Policy loss: 0.861152. Value loss: 4.286113. Entropy: 0.776867.\n",
      "episode: 748   score: 135.0  epsilon: 1.0    steps: 32  evaluation reward: 219.75\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1831: Policy loss: 0.681289. Value loss: 8.251264. Entropy: 0.856122.\n",
      "Iteration 1832: Policy loss: 0.713240. Value loss: 4.422783. Entropy: 0.876435.\n",
      "Iteration 1833: Policy loss: 0.662493. Value loss: 3.715054. Entropy: 0.872442.\n",
      "episode: 749   score: 180.0  epsilon: 1.0    steps: 198  evaluation reward: 220.1\n",
      "episode: 750   score: 180.0  epsilon: 1.0    steps: 806  evaluation reward: 220.1\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1834: Policy loss: -0.343827. Value loss: 18.571194. Entropy: 0.920011.\n",
      "Iteration 1835: Policy loss: -0.566822. Value loss: 10.186321. Entropy: 0.922665.\n",
      "Iteration 1836: Policy loss: -0.407078. Value loss: 9.149226. Entropy: 0.951267.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2019-02-25 19:14:11.423108\n",
      "episode: 751   score: 210.0  epsilon: 1.0    steps: 899  evaluation reward: 220.65\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1837: Policy loss: -0.380307. Value loss: 11.371275. Entropy: 0.722403.\n",
      "Iteration 1838: Policy loss: -0.421842. Value loss: 8.958973. Entropy: 0.685571.\n",
      "Iteration 1839: Policy loss: -0.446278. Value loss: 7.856825. Entropy: 0.732918.\n",
      "episode: 752   score: 185.0  epsilon: 1.0    steps: 294  evaluation reward: 221.45\n",
      "episode: 753   score: 180.0  epsilon: 1.0    steps: 532  evaluation reward: 221.7\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1840: Policy loss: -0.973042. Value loss: 9.494258. Entropy: 0.832595.\n",
      "Iteration 1841: Policy loss: -0.938946. Value loss: 7.417164. Entropy: 0.843729.\n",
      "Iteration 1842: Policy loss: -0.964166. Value loss: 6.309573. Entropy: 0.834751.\n",
      "episode: 754   score: 180.0  epsilon: 1.0    steps: 671  evaluation reward: 221.4\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1843: Policy loss: 1.013441. Value loss: 25.494709. Entropy: 0.720948.\n",
      "Iteration 1844: Policy loss: 0.807951. Value loss: 17.995344. Entropy: 0.678472.\n",
      "Iteration 1845: Policy loss: 0.781610. Value loss: 12.937605. Entropy: 0.673241.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1846: Policy loss: 0.050940. Value loss: 11.450232. Entropy: 0.818760.\n",
      "Iteration 1847: Policy loss: 0.103101. Value loss: 7.324884. Entropy: 0.808903.\n",
      "Iteration 1848: Policy loss: -0.050164. Value loss: 6.547288. Entropy: 0.801881.\n",
      "episode: 755   score: 210.0  epsilon: 1.0    steps: 91  evaluation reward: 221.4\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1849: Policy loss: 0.780412. Value loss: 12.801212. Entropy: 0.824146.\n",
      "Iteration 1850: Policy loss: 0.561595. Value loss: 6.380925. Entropy: 0.842913.\n",
      "Iteration 1851: Policy loss: 0.537096. Value loss: 5.641039. Entropy: 0.868033.\n",
      "episode: 756   score: 180.0  epsilon: 1.0    steps: 215  evaluation reward: 221.4\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1852: Policy loss: 0.709660. Value loss: 17.854626. Entropy: 0.769089.\n",
      "Iteration 1853: Policy loss: 0.757242. Value loss: 8.844113. Entropy: 0.773704.\n",
      "Iteration 1854: Policy loss: 0.814146. Value loss: 7.882624. Entropy: 0.771456.\n",
      "episode: 757   score: 180.0  epsilon: 1.0    steps: 789  evaluation reward: 221.65\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1855: Policy loss: 0.128868. Value loss: 51.347950. Entropy: 0.735750.\n",
      "Iteration 1856: Policy loss: -0.592649. Value loss: 136.090714. Entropy: 0.737235.\n",
      "Iteration 1857: Policy loss: 0.127297. Value loss: 36.143845. Entropy: 0.795409.\n",
      "episode: 758   score: 210.0  epsilon: 1.0    steps: 369  evaluation reward: 222.2\n",
      "episode: 759   score: 410.0  epsilon: 1.0    steps: 621  evaluation reward: 222.2\n",
      "episode: 760   score: 260.0  epsilon: 1.0    steps: 988  evaluation reward: 222.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1858: Policy loss: -1.972822. Value loss: 166.372971. Entropy: 0.878296.\n",
      "Iteration 1859: Policy loss: -1.753671. Value loss: 44.699261. Entropy: 0.879426.\n",
      "Iteration 1860: Policy loss: -1.870010. Value loss: 26.326057. Entropy: 0.902203.\n",
      "episode: 761   score: 520.0  epsilon: 1.0    steps: 427  evaluation reward: 226.55\n",
      "episode: 762   score: 210.0  epsilon: 1.0    steps: 764  evaluation reward: 226.85\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1861: Policy loss: -0.074476. Value loss: 14.927885. Entropy: 0.866069.\n",
      "Iteration 1862: Policy loss: -0.266063. Value loss: 10.261133. Entropy: 0.880396.\n",
      "Iteration 1863: Policy loss: -0.097745. Value loss: 8.798252. Entropy: 0.861999.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1864: Policy loss: 1.373566. Value loss: 14.586407. Entropy: 0.750249.\n",
      "Iteration 1865: Policy loss: 1.045522. Value loss: 8.962419. Entropy: 0.763022.\n",
      "Iteration 1866: Policy loss: 1.163100. Value loss: 9.550371. Entropy: 0.751812.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1867: Policy loss: 0.567334. Value loss: 11.383984. Entropy: 0.840726.\n",
      "Iteration 1868: Policy loss: 0.599675. Value loss: 9.106404. Entropy: 0.848397.\n",
      "Iteration 1869: Policy loss: 0.710543. Value loss: 6.310396. Entropy: 0.824763.\n",
      "episode: 763   score: 210.0  epsilon: 1.0    steps: 35  evaluation reward: 227.15\n",
      "episode: 764   score: 180.0  epsilon: 1.0    steps: 247  evaluation reward: 227.15\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1870: Policy loss: 0.242310. Value loss: 7.053251. Entropy: 0.946055.\n",
      "Iteration 1871: Policy loss: 0.167192. Value loss: 5.456495. Entropy: 0.931410.\n",
      "Iteration 1872: Policy loss: 0.244497. Value loss: 4.535960. Entropy: 0.964262.\n",
      "episode: 765   score: 155.0  epsilon: 1.0    steps: 818  evaluation reward: 226.6\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1873: Policy loss: 0.234480. Value loss: 7.310620. Entropy: 0.887561.\n",
      "Iteration 1874: Policy loss: 0.096477. Value loss: 5.563226. Entropy: 0.842412.\n",
      "Iteration 1875: Policy loss: 0.216084. Value loss: 4.782646. Entropy: 0.868804.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1876: Policy loss: -1.462695. Value loss: 262.796997. Entropy: 0.723740.\n",
      "Iteration 1877: Policy loss: -0.898492. Value loss: 57.360924. Entropy: 0.690843.\n",
      "Iteration 1878: Policy loss: -1.145661. Value loss: 36.064526. Entropy: 0.796528.\n",
      "episode: 766   score: 180.0  epsilon: 1.0    steps: 277  evaluation reward: 226.6\n",
      "episode: 767   score: 380.0  epsilon: 1.0    steps: 468  evaluation reward: 228.6\n",
      "episode: 768   score: 155.0  epsilon: 1.0    steps: 524  evaluation reward: 227.55\n",
      "episode: 769   score: 180.0  epsilon: 1.0    steps: 907  evaluation reward: 226.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1879: Policy loss: 0.473514. Value loss: 11.198014. Entropy: 0.770325.\n",
      "Iteration 1880: Policy loss: 0.431970. Value loss: 9.981446. Entropy: 0.803688.\n",
      "Iteration 1881: Policy loss: 0.307109. Value loss: 9.831653. Entropy: 0.865973.\n",
      "episode: 770   score: 210.0  epsilon: 1.0    steps: 665  evaluation reward: 226.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1882: Policy loss: 0.869767. Value loss: 13.031539. Entropy: 0.604543.\n",
      "Iteration 1883: Policy loss: 0.828637. Value loss: 9.342141. Entropy: 0.634006.\n",
      "Iteration 1884: Policy loss: 0.697252. Value loss: 8.960786. Entropy: 0.617386.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1885: Policy loss: -0.219757. Value loss: 15.601074. Entropy: 0.602927.\n",
      "Iteration 1886: Policy loss: 0.084345. Value loss: 7.658963. Entropy: 0.559959.\n",
      "Iteration 1887: Policy loss: -0.068938. Value loss: 5.747138. Entropy: 0.590555.\n",
      "episode: 771   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 227.5\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1888: Policy loss: 1.116165. Value loss: 25.961103. Entropy: 0.759941.\n",
      "Iteration 1889: Policy loss: 1.041011. Value loss: 7.471254. Entropy: 0.735586.\n",
      "Iteration 1890: Policy loss: 1.044562. Value loss: 5.721010. Entropy: 0.748979.\n",
      "episode: 772   score: 180.0  epsilon: 1.0    steps: 162  evaluation reward: 227.75\n",
      "episode: 773   score: 180.0  epsilon: 1.0    steps: 869  evaluation reward: 227.75\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1891: Policy loss: -0.218794. Value loss: 12.976549. Entropy: 0.727670.\n",
      "Iteration 1892: Policy loss: -0.160680. Value loss: 9.249295. Entropy: 0.707594.\n",
      "Iteration 1893: Policy loss: -0.274207. Value loss: 6.573050. Entropy: 0.708235.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1894: Policy loss: -3.182596. Value loss: 193.213806. Entropy: 0.471030.\n",
      "Iteration 1895: Policy loss: -1.584856. Value loss: 109.420326. Entropy: 0.511871.\n",
      "Iteration 1896: Policy loss: -2.736360. Value loss: 50.693886. Entropy: 0.507025.\n",
      "episode: 774   score: 410.0  epsilon: 1.0    steps: 354  evaluation reward: 230.05\n",
      "episode: 775   score: 210.0  epsilon: 1.0    steps: 575  evaluation reward: 230.6\n",
      "episode: 776   score: 180.0  epsilon: 1.0    steps: 954  evaluation reward: 230.85\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1897: Policy loss: 0.986270. Value loss: 17.363113. Entropy: 0.760872.\n",
      "Iteration 1898: Policy loss: 1.080210. Value loss: 15.816888. Entropy: 0.733716.\n",
      "Iteration 1899: Policy loss: 0.970627. Value loss: 11.638775. Entropy: 0.750467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 777   score: 180.0  epsilon: 1.0    steps: 391  evaluation reward: 230.55\n",
      "episode: 778   score: 180.0  epsilon: 1.0    steps: 702  evaluation reward: 230.25\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1900: Policy loss: 1.093903. Value loss: 7.788144. Entropy: 0.646493.\n",
      "Iteration 1901: Policy loss: 0.954424. Value loss: 5.812388. Entropy: 0.662646.\n",
      "Iteration 1902: Policy loss: 0.939326. Value loss: 4.994433. Entropy: 0.724749.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1903: Policy loss: 1.422650. Value loss: 12.450649. Entropy: 0.651102.\n",
      "Iteration 1904: Policy loss: 1.644214. Value loss: 10.919611. Entropy: 0.659101.\n",
      "Iteration 1905: Policy loss: 1.610463. Value loss: 7.637060. Entropy: 0.664256.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1906: Policy loss: 0.943855. Value loss: 6.878540. Entropy: 0.886176.\n",
      "Iteration 1907: Policy loss: 0.897034. Value loss: 5.283123. Entropy: 0.919588.\n",
      "Iteration 1908: Policy loss: 0.863737. Value loss: 5.292035. Entropy: 0.925849.\n",
      "episode: 779   score: 210.0  epsilon: 1.0    steps: 77  evaluation reward: 230.8\n",
      "episode: 780   score: 210.0  epsilon: 1.0    steps: 255  evaluation reward: 230.8\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1909: Policy loss: 0.307359. Value loss: 6.395628. Entropy: 0.938541.\n",
      "Iteration 1910: Policy loss: 0.215260. Value loss: 5.881401. Entropy: 0.933693.\n",
      "Iteration 1911: Policy loss: 0.313847. Value loss: 3.293849. Entropy: 0.913474.\n",
      "episode: 781   score: 120.0  epsilon: 1.0    steps: 371  evaluation reward: 230.45\n",
      "episode: 782   score: 210.0  epsilon: 1.0    steps: 844  evaluation reward: 230.75\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1912: Policy loss: 0.992676. Value loss: 9.401954. Entropy: 0.767431.\n",
      "Iteration 1913: Policy loss: 1.080260. Value loss: 5.871089. Entropy: 0.753695.\n",
      "Iteration 1914: Policy loss: 1.141035. Value loss: 4.083378. Entropy: 0.766041.\n",
      "episode: 783   score: 180.0  epsilon: 1.0    steps: 613  evaluation reward: 230.75\n",
      "episode: 784   score: 180.0  epsilon: 1.0    steps: 1003  evaluation reward: 230.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1915: Policy loss: 0.905672. Value loss: 14.523190. Entropy: 0.702135.\n",
      "Iteration 1916: Policy loss: 1.263241. Value loss: 9.457970. Entropy: 0.734767.\n",
      "Iteration 1917: Policy loss: 1.081053. Value loss: 7.734812. Entropy: 0.675298.\n",
      "episode: 785   score: 155.0  epsilon: 1.0    steps: 721  evaluation reward: 227.9\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1918: Policy loss: 1.121262. Value loss: 13.186513. Entropy: 0.910184.\n",
      "Iteration 1919: Policy loss: 0.949184. Value loss: 8.093746. Entropy: 0.909126.\n",
      "Iteration 1920: Policy loss: 0.977238. Value loss: 7.980715. Entropy: 0.921265.\n",
      "episode: 786   score: 260.0  epsilon: 1.0    steps: 469  evaluation reward: 228.95\n",
      "episode: 787   score: 105.0  epsilon: 1.0    steps: 873  evaluation reward: 228.2\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1921: Policy loss: -3.095559. Value loss: 92.135551. Entropy: 0.846814.\n",
      "Iteration 1922: Policy loss: -2.589274. Value loss: 46.580921. Entropy: 0.829782.\n",
      "Iteration 1923: Policy loss: -3.214747. Value loss: 48.103516. Entropy: 0.840083.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1924: Policy loss: 1.004009. Value loss: 6.898331. Entropy: 0.872718.\n",
      "Iteration 1925: Policy loss: 1.005577. Value loss: 4.681119. Entropy: 0.891846.\n",
      "Iteration 1926: Policy loss: 0.958246. Value loss: 4.213093. Entropy: 0.870090.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1927: Policy loss: 1.429578. Value loss: 19.366184. Entropy: 0.723961.\n",
      "Iteration 1928: Policy loss: 1.414517. Value loss: 9.440824. Entropy: 0.743104.\n",
      "Iteration 1929: Policy loss: 1.435423. Value loss: 7.411316. Entropy: 0.756093.\n",
      "episode: 788   score: 410.0  epsilon: 1.0    steps: 37  evaluation reward: 230.5\n",
      "episode: 789   score: 155.0  epsilon: 1.0    steps: 155  evaluation reward: 229.95\n",
      "episode: 790   score: 105.0  epsilon: 1.0    steps: 458  evaluation reward: 229.2\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1930: Policy loss: 0.134384. Value loss: 16.056293. Entropy: 0.997492.\n",
      "Iteration 1931: Policy loss: 0.073002. Value loss: 9.655153. Entropy: 0.965875.\n",
      "Iteration 1932: Policy loss: -0.024584. Value loss: 7.042283. Entropy: 0.996783.\n",
      "episode: 791   score: 180.0  epsilon: 1.0    steps: 288  evaluation reward: 228.25\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1933: Policy loss: 0.411006. Value loss: 17.364841. Entropy: 0.673674.\n",
      "Iteration 1934: Policy loss: 0.433201. Value loss: 9.670204. Entropy: 0.638833.\n",
      "Iteration 1935: Policy loss: 0.432909. Value loss: 8.165929. Entropy: 0.670768.\n",
      "episode: 792   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 228.25\n",
      "episode: 793   score: 210.0  epsilon: 1.0    steps: 922  evaluation reward: 228.25\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1936: Policy loss: 1.182725. Value loss: 15.321492. Entropy: 0.824798.\n",
      "Iteration 1937: Policy loss: 1.304100. Value loss: 9.770638. Entropy: 0.834199.\n",
      "Iteration 1938: Policy loss: 1.181189. Value loss: 8.044281. Entropy: 0.829106.\n",
      "episode: 794   score: 210.0  epsilon: 1.0    steps: 676  evaluation reward: 228.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1939: Policy loss: 0.329119. Value loss: 6.382054. Entropy: 0.866826.\n",
      "Iteration 1940: Policy loss: 0.470742. Value loss: 5.270741. Entropy: 0.879398.\n",
      "Iteration 1941: Policy loss: 0.438995. Value loss: 4.470287. Entropy: 0.861929.\n",
      "episode: 795   score: 210.0  epsilon: 1.0    steps: 836  evaluation reward: 229.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1942: Policy loss: 0.587367. Value loss: 3.528232. Entropy: 0.802648.\n",
      "Iteration 1943: Policy loss: 0.543038. Value loss: 2.703159. Entropy: 0.810214.\n",
      "Iteration 1944: Policy loss: 0.454216. Value loss: 3.826095. Entropy: 0.818844.\n",
      "episode: 796   score: 110.0  epsilon: 1.0    steps: 460  evaluation reward: 228.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1945: Policy loss: 1.096298. Value loss: 10.222428. Entropy: 0.840211.\n",
      "Iteration 1946: Policy loss: 1.028316. Value loss: 6.835192. Entropy: 0.814274.\n",
      "Iteration 1947: Policy loss: 0.990645. Value loss: 4.961232. Entropy: 0.844099.\n",
      "episode: 797   score: 210.0  epsilon: 1.0    steps: 93  evaluation reward: 228.45\n",
      "episode: 798   score: 210.0  epsilon: 1.0    steps: 247  evaluation reward: 227.95\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1948: Policy loss: 0.260609. Value loss: 10.055568. Entropy: 0.883454.\n",
      "Iteration 1949: Policy loss: 0.073256. Value loss: 6.068479. Entropy: 0.903483.\n",
      "Iteration 1950: Policy loss: 0.247869. Value loss: 6.729941. Entropy: 0.906956.\n",
      "episode: 799   score: 155.0  epsilon: 1.0    steps: 339  evaluation reward: 228.3\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1951: Policy loss: 0.720442. Value loss: 10.529424. Entropy: 0.862095.\n",
      "Iteration 1952: Policy loss: 0.669612. Value loss: 6.261758. Entropy: 0.862680.\n",
      "Iteration 1953: Policy loss: 0.690360. Value loss: 5.620378. Entropy: 0.888573.\n",
      "episode: 800   score: 180.0  epsilon: 1.0    steps: 585  evaluation reward: 228.3\n",
      "now time :  2019-02-25 19:16:22.855586\n",
      "episode: 801   score: 180.0  epsilon: 1.0    steps: 957  evaluation reward: 228.0\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1954: Policy loss: 0.433652. Value loss: 10.695536. Entropy: 0.864011.\n",
      "Iteration 1955: Policy loss: 0.463735. Value loss: 8.078787. Entropy: 0.873846.\n",
      "Iteration 1956: Policy loss: 0.389466. Value loss: 6.466385. Entropy: 0.899095.\n",
      "episode: 802   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 225.2\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1957: Policy loss: 1.102551. Value loss: 11.065627. Entropy: 0.900146.\n",
      "Iteration 1958: Policy loss: 0.788354. Value loss: 7.059494. Entropy: 0.930086.\n",
      "Iteration 1959: Policy loss: 1.122156. Value loss: 5.536696. Entropy: 0.940761.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1960: Policy loss: -0.771342. Value loss: 7.590889. Entropy: 0.842074.\n",
      "Iteration 1961: Policy loss: -0.949604. Value loss: 4.833167. Entropy: 0.889308.\n",
      "Iteration 1962: Policy loss: -1.050037. Value loss: 4.221418. Entropy: 0.886576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 803   score: 180.0  epsilon: 1.0    steps: 492  evaluation reward: 225.2\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1963: Policy loss: -0.253485. Value loss: 7.181131. Entropy: 0.959949.\n",
      "Iteration 1964: Policy loss: -0.349792. Value loss: 4.903742. Entropy: 0.943788.\n",
      "Iteration 1965: Policy loss: -0.347830. Value loss: 4.016987. Entropy: 0.943767.\n",
      "episode: 804   score: 260.0  epsilon: 1.0    steps: 796  evaluation reward: 223.7\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1966: Policy loss: -0.379604. Value loss: 8.860384. Entropy: 0.923637.\n",
      "Iteration 1967: Policy loss: -0.341980. Value loss: 6.980501. Entropy: 0.921169.\n",
      "Iteration 1968: Policy loss: -0.479643. Value loss: 4.198288. Entropy: 0.933924.\n",
      "episode: 805   score: 210.0  epsilon: 1.0    steps: 28  evaluation reward: 221.7\n",
      "episode: 806   score: 155.0  epsilon: 1.0    steps: 143  evaluation reward: 221.4\n",
      "episode: 807   score: 155.0  epsilon: 1.0    steps: 380  evaluation reward: 220.85\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1969: Policy loss: -2.479927. Value loss: 219.974823. Entropy: 0.933665.\n",
      "Iteration 1970: Policy loss: -1.776061. Value loss: 54.612164. Entropy: 0.899924.\n",
      "Iteration 1971: Policy loss: -2.016310. Value loss: 44.994259. Entropy: 0.849419.\n",
      "episode: 808   score: 155.0  epsilon: 1.0    steps: 618  evaluation reward: 220.85\n",
      "episode: 809   score: 380.0  epsilon: 1.0    steps: 995  evaluation reward: 220.7\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1972: Policy loss: 2.684235. Value loss: 43.415432. Entropy: 0.893198.\n",
      "Iteration 1973: Policy loss: 2.515779. Value loss: 13.307111. Entropy: 0.922055.\n",
      "Iteration 1974: Policy loss: 2.011485. Value loss: 10.182102. Entropy: 0.939201.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1975: Policy loss: -0.557151. Value loss: 13.103766. Entropy: 0.841674.\n",
      "Iteration 1976: Policy loss: -0.582936. Value loss: 6.421962. Entropy: 0.828790.\n",
      "Iteration 1977: Policy loss: -0.454610. Value loss: 5.707900. Entropy: 0.835601.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1978: Policy loss: -0.093446. Value loss: 11.332130. Entropy: 0.835042.\n",
      "Iteration 1979: Policy loss: -0.112954. Value loss: 7.303332. Entropy: 0.847249.\n",
      "Iteration 1980: Policy loss: -0.137093. Value loss: 6.058588. Entropy: 0.854381.\n",
      "episode: 810   score: 260.0  epsilon: 1.0    steps: 673  evaluation reward: 221.5\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1981: Policy loss: 0.109844. Value loss: 9.823318. Entropy: 0.918089.\n",
      "Iteration 1982: Policy loss: 0.109442. Value loss: 4.381926. Entropy: 0.870262.\n",
      "Iteration 1983: Policy loss: 0.226758. Value loss: 3.448642. Entropy: 0.925921.\n",
      "episode: 811   score: 180.0  epsilon: 1.0    steps: 417  evaluation reward: 218.6\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1984: Policy loss: 0.835570. Value loss: 16.722261. Entropy: 0.796897.\n",
      "Iteration 1985: Policy loss: 0.919447. Value loss: 10.768559. Entropy: 0.788805.\n",
      "Iteration 1986: Policy loss: 0.907840. Value loss: 7.041966. Entropy: 0.794782.\n",
      "episode: 812   score: 180.0  epsilon: 1.0    steps: 75  evaluation reward: 217.05\n",
      "episode: 813   score: 180.0  epsilon: 1.0    steps: 177  evaluation reward: 216.75\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1987: Policy loss: 0.150069. Value loss: 9.952384. Entropy: 0.849582.\n",
      "Iteration 1988: Policy loss: -0.021630. Value loss: 6.766964. Entropy: 0.871026.\n",
      "Iteration 1989: Policy loss: 0.014441. Value loss: 5.397112. Entropy: 0.846429.\n",
      "episode: 814   score: 210.0  epsilon: 1.0    steps: 303  evaluation reward: 217.05\n",
      "episode: 815   score: 285.0  epsilon: 1.0    steps: 789  evaluation reward: 216.5\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1990: Policy loss: 0.395426. Value loss: 11.303107. Entropy: 0.893145.\n",
      "Iteration 1991: Policy loss: 0.437835. Value loss: 6.702277. Entropy: 0.870445.\n",
      "Iteration 1992: Policy loss: 0.422317. Value loss: 6.167734. Entropy: 0.828500.\n",
      "episode: 816   score: 210.0  epsilon: 1.0    steps: 556  evaluation reward: 216.5\n",
      "episode: 817   score: 210.0  epsilon: 1.0    steps: 927  evaluation reward: 216.5\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1993: Policy loss: 0.059238. Value loss: 4.273649. Entropy: 0.742407.\n",
      "Iteration 1994: Policy loss: 0.126293. Value loss: 4.112891. Entropy: 0.740042.\n",
      "Iteration 1995: Policy loss: 0.117968. Value loss: 3.289120. Entropy: 0.738079.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1996: Policy loss: -0.505671. Value loss: 6.183640. Entropy: 0.716582.\n",
      "Iteration 1997: Policy loss: -0.541621. Value loss: 5.073209. Entropy: 0.720313.\n",
      "Iteration 1998: Policy loss: -0.535844. Value loss: 3.928383. Entropy: 0.710323.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1999: Policy loss: -0.665990. Value loss: 4.288640. Entropy: 0.859291.\n",
      "Iteration 2000: Policy loss: -0.699086. Value loss: 3.082049. Entropy: 0.851562.\n",
      "Iteration 2001: Policy loss: -0.714608. Value loss: 1.623414. Entropy: 0.853862.\n",
      "episode: 818   score: 180.0  epsilon: 1.0    steps: 468  evaluation reward: 216.5\n",
      "episode: 819   score: 260.0  epsilon: 1.0    steps: 758  evaluation reward: 217.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2002: Policy loss: 0.719526. Value loss: 7.079442. Entropy: 0.764534.\n",
      "Iteration 2003: Policy loss: 0.367502. Value loss: 3.190037. Entropy: 0.807225.\n",
      "Iteration 2004: Policy loss: 0.570667. Value loss: 3.297446. Entropy: 0.793154.\n",
      "episode: 820   score: 180.0  epsilon: 1.0    steps: 124  evaluation reward: 215.25\n",
      "episode: 821   score: 185.0  epsilon: 1.0    steps: 227  evaluation reward: 213.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2005: Policy loss: 0.382690. Value loss: 9.625832. Entropy: 0.675732.\n",
      "Iteration 2006: Policy loss: 0.494524. Value loss: 6.830441. Entropy: 0.711451.\n",
      "Iteration 2007: Policy loss: 0.480206. Value loss: 5.769539. Entropy: 0.688503.\n",
      "episode: 822   score: 180.0  epsilon: 1.0    steps: 341  evaluation reward: 210.6\n",
      "episode: 823   score: 155.0  epsilon: 1.0    steps: 839  evaluation reward: 210.8\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2008: Policy loss: 0.561598. Value loss: 14.673364. Entropy: 0.879050.\n",
      "Iteration 2009: Policy loss: 0.525426. Value loss: 9.808138. Entropy: 0.839398.\n",
      "Iteration 2010: Policy loss: 0.544602. Value loss: 8.511249. Entropy: 0.861090.\n",
      "episode: 824   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 210.8\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2011: Policy loss: 1.298645. Value loss: 25.818584. Entropy: 0.904238.\n",
      "Iteration 2012: Policy loss: 1.233282. Value loss: 21.411736. Entropy: 0.922485.\n",
      "Iteration 2013: Policy loss: 1.298195. Value loss: 19.237688. Entropy: 0.913651.\n",
      "episode: 825   score: 210.0  epsilon: 1.0    steps: 527  evaluation reward: 211.7\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2014: Policy loss: -2.950501. Value loss: 170.155273. Entropy: 1.055213.\n",
      "Iteration 2015: Policy loss: -3.217992. Value loss: 121.981476. Entropy: 0.971087.\n",
      "Iteration 2016: Policy loss: -3.514885. Value loss: 122.047737. Entropy: 0.996493.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2017: Policy loss: 0.481155. Value loss: 10.188854. Entropy: 0.921298.\n",
      "Iteration 2018: Policy loss: 0.405226. Value loss: 6.571057. Entropy: 0.905120.\n",
      "Iteration 2019: Policy loss: 0.491517. Value loss: 5.374303. Entropy: 0.917438.\n",
      "episode: 826   score: 210.0  epsilon: 1.0    steps: 503  evaluation reward: 211.7\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2020: Policy loss: -0.522595. Value loss: 11.761326. Entropy: 0.938356.\n",
      "Iteration 2021: Policy loss: -0.403932. Value loss: 7.791602. Entropy: 0.894495.\n",
      "Iteration 2022: Policy loss: -0.430311. Value loss: 6.487090. Entropy: 0.929671.\n",
      "episode: 827   score: 75.0  epsilon: 1.0    steps: 901  evaluation reward: 208.35\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2023: Policy loss: -0.785068. Value loss: 12.714426. Entropy: 0.756223.\n",
      "Iteration 2024: Policy loss: -0.716020. Value loss: 7.841455. Entropy: 0.728376.\n",
      "Iteration 2025: Policy loss: -0.659947. Value loss: 6.394571. Entropy: 0.726315.\n",
      "episode: 828   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 207.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 829   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 205.05\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2026: Policy loss: 0.716743. Value loss: 17.288038. Entropy: 0.688575.\n",
      "Iteration 2027: Policy loss: 0.729206. Value loss: 12.823090. Entropy: 0.663490.\n",
      "Iteration 2028: Policy loss: 0.608604. Value loss: 9.735865. Entropy: 0.684888.\n",
      "episode: 830   score: 210.0  epsilon: 1.0    steps: 294  evaluation reward: 205.35\n",
      "episode: 831   score: 490.0  epsilon: 1.0    steps: 701  evaluation reward: 208.45\n",
      "episode: 832   score: 210.0  epsilon: 1.0    steps: 781  evaluation reward: 208.75\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2029: Policy loss: -0.426120. Value loss: 10.171779. Entropy: 0.816848.\n",
      "Iteration 2030: Policy loss: -0.478565. Value loss: 7.578917. Entropy: 0.824629.\n",
      "Iteration 2031: Policy loss: -0.486942. Value loss: 7.198960. Entropy: 0.817542.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2032: Policy loss: 0.529324. Value loss: 11.640022. Entropy: 0.718767.\n",
      "Iteration 2033: Policy loss: 0.643596. Value loss: 8.809658. Entropy: 0.735458.\n",
      "Iteration 2034: Policy loss: 0.503316. Value loss: 9.960263. Entropy: 0.726940.\n",
      "episode: 833   score: 260.0  epsilon: 1.0    steps: 610  evaluation reward: 209.25\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2035: Policy loss: 0.430900. Value loss: 6.920416. Entropy: 0.985600.\n",
      "Iteration 2036: Policy loss: 0.565324. Value loss: 5.860863. Entropy: 1.008198.\n",
      "Iteration 2037: Policy loss: 0.453109. Value loss: 4.543563. Entropy: 0.995317.\n",
      "episode: 834   score: 75.0  epsilon: 1.0    steps: 326  evaluation reward: 208.45\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2038: Policy loss: 0.809092. Value loss: 5.420125. Entropy: 0.862627.\n",
      "Iteration 2039: Policy loss: 0.639465. Value loss: 2.834474. Entropy: 0.869034.\n",
      "Iteration 2040: Policy loss: 0.724157. Value loss: 2.079648. Entropy: 0.860605.\n",
      "episode: 835   score: 210.0  epsilon: 1.0    steps: 459  evaluation reward: 208.45\n",
      "episode: 836   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 208.45\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2041: Policy loss: -0.162957. Value loss: 9.281158. Entropy: 0.810280.\n",
      "Iteration 2042: Policy loss: -0.135994. Value loss: 5.976639. Entropy: 0.806084.\n",
      "Iteration 2043: Policy loss: -0.130074. Value loss: 5.445161. Entropy: 0.815650.\n",
      "episode: 837   score: 180.0  epsilon: 1.0    steps: 223  evaluation reward: 207.65\n",
      "episode: 838   score: 105.0  epsilon: 1.0    steps: 633  evaluation reward: 206.85\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2044: Policy loss: -0.316625. Value loss: 14.564969. Entropy: 0.729568.\n",
      "Iteration 2045: Policy loss: -0.245452. Value loss: 10.463700. Entropy: 0.735872.\n",
      "Iteration 2046: Policy loss: -0.200089. Value loss: 8.760215. Entropy: 0.705013.\n",
      "episode: 839   score: 210.0  epsilon: 1.0    steps: 58  evaluation reward: 207.15\n",
      "episode: 840   score: 155.0  epsilon: 1.0    steps: 737  evaluation reward: 206.9\n",
      "episode: 841   score: 180.0  epsilon: 1.0    steps: 846  evaluation reward: 206.6\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: 0.581590. Value loss: 15.741740. Entropy: 0.835098.\n",
      "Iteration 2048: Policy loss: 1.065892. Value loss: 9.575474. Entropy: 0.832275.\n",
      "Iteration 2049: Policy loss: 0.738769. Value loss: 8.262822. Entropy: 0.828125.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2050: Policy loss: 0.967900. Value loss: 6.539481. Entropy: 0.816331.\n",
      "Iteration 2051: Policy loss: 0.965606. Value loss: 4.359231. Entropy: 0.859362.\n",
      "Iteration 2052: Policy loss: 0.889763. Value loss: 3.258164. Entropy: 0.877726.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2053: Policy loss: 0.527237. Value loss: 23.527693. Entropy: 0.939342.\n",
      "Iteration 2054: Policy loss: 0.310289. Value loss: 9.908698. Entropy: 1.043064.\n",
      "Iteration 2055: Policy loss: 0.364205. Value loss: 7.970937. Entropy: 1.032734.\n",
      "episode: 842   score: 75.0  epsilon: 1.0    steps: 873  evaluation reward: 205.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2056: Policy loss: 0.231499. Value loss: 6.712725. Entropy: 1.020235.\n",
      "Iteration 2057: Policy loss: 0.279013. Value loss: 3.353379. Entropy: 0.981546.\n",
      "Iteration 2058: Policy loss: 0.208888. Value loss: 2.328959. Entropy: 0.996328.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2059: Policy loss: 1.151610. Value loss: 21.208916. Entropy: 0.946452.\n",
      "Iteration 2060: Policy loss: 1.179678. Value loss: 11.562340. Entropy: 0.978278.\n",
      "Iteration 2061: Policy loss: 1.224946. Value loss: 9.903400. Entropy: 0.967515.\n",
      "episode: 843   score: 255.0  epsilon: 1.0    steps: 274  evaluation reward: 205.7\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2062: Policy loss: 0.675636. Value loss: 12.809254. Entropy: 0.833124.\n",
      "Iteration 2063: Policy loss: 0.811276. Value loss: 7.381457. Entropy: 0.808341.\n",
      "Iteration 2064: Policy loss: 0.740193. Value loss: 5.414275. Entropy: 0.828961.\n",
      "episode: 844   score: 180.0  epsilon: 1.0    steps: 105  evaluation reward: 204.9\n",
      "episode: 845   score: 180.0  epsilon: 1.0    steps: 166  evaluation reward: 202.6\n",
      "episode: 846   score: 180.0  epsilon: 1.0    steps: 530  evaluation reward: 202.6\n",
      "episode: 847   score: 265.0  epsilon: 1.0    steps: 921  evaluation reward: 203.45\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2065: Policy loss: 0.309084. Value loss: 24.752056. Entropy: 1.104343.\n",
      "Iteration 2066: Policy loss: -0.253238. Value loss: 15.869129. Entropy: 1.095695.\n",
      "Iteration 2067: Policy loss: -0.184513. Value loss: 12.011422. Entropy: 1.130073.\n",
      "episode: 848   score: 180.0  epsilon: 1.0    steps: 707  evaluation reward: 203.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2068: Policy loss: 1.731962. Value loss: 23.584141. Entropy: 1.091061.\n",
      "Iteration 2069: Policy loss: 2.074115. Value loss: 15.149227. Entropy: 1.102969.\n",
      "Iteration 2070: Policy loss: 1.862601. Value loss: 13.445616. Entropy: 1.101596.\n",
      "episode: 849   score: 80.0  epsilon: 1.0    steps: 277  evaluation reward: 202.9\n",
      "episode: 850   score: 315.0  epsilon: 1.0    steps: 502  evaluation reward: 204.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2071: Policy loss: 0.538958. Value loss: 22.971104. Entropy: 1.244282.\n",
      "Iteration 2072: Policy loss: 0.633864. Value loss: 12.913235. Entropy: 1.248913.\n",
      "Iteration 2073: Policy loss: 0.584767. Value loss: 9.748623. Entropy: 1.256482.\n",
      "now time :  2019-02-25 19:18:36.367246\n",
      "episode: 851   score: 115.0  epsilon: 1.0    steps: 543  evaluation reward: 203.3\n",
      "episode: 852   score: 85.0  epsilon: 1.0    steps: 944  evaluation reward: 202.3\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2074: Policy loss: 0.975721. Value loss: 21.537260. Entropy: 1.236659.\n",
      "Iteration 2075: Policy loss: 0.532458. Value loss: 15.057117. Entropy: 1.228258.\n",
      "Iteration 2076: Policy loss: 0.924598. Value loss: 12.233891. Entropy: 1.254027.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2077: Policy loss: 0.113563. Value loss: 27.943834. Entropy: 1.033520.\n",
      "Iteration 2078: Policy loss: 0.319995. Value loss: 21.609474. Entropy: 1.053920.\n",
      "Iteration 2079: Policy loss: 0.084825. Value loss: 14.122473. Entropy: 1.025731.\n",
      "episode: 853   score: 285.0  epsilon: 1.0    steps: 864  evaluation reward: 203.35\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2080: Policy loss: -1.379124. Value loss: 153.160431. Entropy: 1.088631.\n",
      "Iteration 2081: Policy loss: -1.911643. Value loss: 125.944260. Entropy: 1.129944.\n",
      "Iteration 2082: Policy loss: -0.888912. Value loss: 31.505995. Entropy: 1.096016.\n",
      "episode: 854   score: 215.0  epsilon: 1.0    steps: 203  evaluation reward: 203.7\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2083: Policy loss: -3.047217. Value loss: 125.351654. Entropy: 1.080626.\n",
      "Iteration 2084: Policy loss: -3.387997. Value loss: 43.745739. Entropy: 1.075329.\n",
      "Iteration 2085: Policy loss: -3.081393. Value loss: 35.547287. Entropy: 1.073739.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2086: Policy loss: 0.912955. Value loss: 141.873154. Entropy: 0.853665.\n",
      "Iteration 2087: Policy loss: 1.117677. Value loss: 106.739456. Entropy: 0.855836.\n",
      "Iteration 2088: Policy loss: 0.914610. Value loss: 94.126007. Entropy: 0.819490.\n",
      "episode: 855   score: 260.0  epsilon: 1.0    steps: 62  evaluation reward: 204.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 856   score: 245.0  epsilon: 1.0    steps: 648  evaluation reward: 204.85\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2089: Policy loss: -2.210908. Value loss: 176.249054. Entropy: 0.771775.\n",
      "Iteration 2090: Policy loss: -0.994274. Value loss: 50.640610. Entropy: 0.804296.\n",
      "Iteration 2091: Policy loss: -1.896159. Value loss: 33.721294. Entropy: 0.843877.\n",
      "episode: 857   score: 465.0  epsilon: 1.0    steps: 280  evaluation reward: 207.7\n",
      "episode: 858   score: 415.0  epsilon: 1.0    steps: 427  evaluation reward: 209.75\n",
      "episode: 859   score: 125.0  epsilon: 1.0    steps: 774  evaluation reward: 206.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2092: Policy loss: 1.641539. Value loss: 26.507797. Entropy: 1.074663.\n",
      "Iteration 2093: Policy loss: 1.721892. Value loss: 17.862719. Entropy: 1.044716.\n",
      "Iteration 2094: Policy loss: 1.635293. Value loss: 17.530212. Entropy: 1.071164.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2095: Policy loss: 1.647604. Value loss: 87.973236. Entropy: 0.870690.\n",
      "Iteration 2096: Policy loss: 1.477107. Value loss: 36.962822. Entropy: 0.856845.\n",
      "Iteration 2097: Policy loss: 0.919271. Value loss: 26.324705. Entropy: 0.890461.\n",
      "episode: 860   score: 110.0  epsilon: 1.0    steps: 212  evaluation reward: 205.4\n",
      "episode: 861   score: 535.0  epsilon: 1.0    steps: 518  evaluation reward: 205.55\n",
      "episode: 862   score: 445.0  epsilon: 1.0    steps: 907  evaluation reward: 207.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2098: Policy loss: 2.778220. Value loss: 50.772282. Entropy: 1.050042.\n",
      "Iteration 2099: Policy loss: 2.995253. Value loss: 30.745951. Entropy: 1.085577.\n",
      "Iteration 2100: Policy loss: 2.605093. Value loss: 22.865139. Entropy: 1.142870.\n",
      "episode: 863   score: 90.0  epsilon: 1.0    steps: 311  evaluation reward: 206.7\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2101: Policy loss: 0.832525. Value loss: 26.407457. Entropy: 0.900677.\n",
      "Iteration 2102: Policy loss: 0.825146. Value loss: 14.702303. Entropy: 0.896109.\n",
      "Iteration 2103: Policy loss: 0.598742. Value loss: 12.730311. Entropy: 0.863291.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2104: Policy loss: 3.249773. Value loss: 35.626194. Entropy: 0.888094.\n",
      "Iteration 2105: Policy loss: 3.331574. Value loss: 15.693203. Entropy: 0.913256.\n",
      "Iteration 2106: Policy loss: 3.182227. Value loss: 15.131876. Entropy: 0.906896.\n",
      "episode: 864   score: 215.0  epsilon: 1.0    steps: 125  evaluation reward: 207.05\n",
      "episode: 865   score: 185.0  epsilon: 1.0    steps: 741  evaluation reward: 207.35\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2107: Policy loss: 1.284257. Value loss: 16.558336. Entropy: 0.985199.\n",
      "Iteration 2108: Policy loss: 1.528667. Value loss: 10.676553. Entropy: 0.977858.\n",
      "Iteration 2109: Policy loss: 1.261162. Value loss: 7.387326. Entropy: 0.985323.\n",
      "episode: 866   score: 185.0  epsilon: 1.0    steps: 463  evaluation reward: 207.4\n",
      "episode: 867   score: 185.0  epsilon: 1.0    steps: 821  evaluation reward: 205.45\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2110: Policy loss: 0.871744. Value loss: 9.596539. Entropy: 0.839461.\n",
      "Iteration 2111: Policy loss: 0.660954. Value loss: 6.759983. Entropy: 0.869060.\n",
      "Iteration 2112: Policy loss: 0.957405. Value loss: 6.396652. Entropy: 0.863950.\n",
      "episode: 868   score: 115.0  epsilon: 1.0    steps: 229  evaluation reward: 205.05\n",
      "episode: 869   score: 150.0  epsilon: 1.0    steps: 543  evaluation reward: 204.75\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2113: Policy loss: 0.494497. Value loss: 31.359127. Entropy: 0.797108.\n",
      "Iteration 2114: Policy loss: 0.310755. Value loss: 20.695932. Entropy: 0.839028.\n",
      "Iteration 2115: Policy loss: 0.526388. Value loss: 17.131626. Entropy: 0.872467.\n",
      "episode: 870   score: 210.0  epsilon: 1.0    steps: 954  evaluation reward: 204.75\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2116: Policy loss: 0.761445. Value loss: 14.944328. Entropy: 1.085957.\n",
      "Iteration 2117: Policy loss: 0.718074. Value loss: 10.755985. Entropy: 1.064102.\n",
      "Iteration 2118: Policy loss: 0.679632. Value loss: 8.710167. Entropy: 1.087428.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2119: Policy loss: 1.002276. Value loss: 19.717905. Entropy: 0.946856.\n",
      "Iteration 2120: Policy loss: 0.467053. Value loss: 10.648688. Entropy: 0.948489.\n",
      "Iteration 2121: Policy loss: 0.637730. Value loss: 8.953576. Entropy: 0.938653.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2122: Policy loss: 0.575874. Value loss: 9.198280. Entropy: 1.087838.\n",
      "Iteration 2123: Policy loss: 0.528914. Value loss: 6.327049. Entropy: 1.085930.\n",
      "Iteration 2124: Policy loss: 0.655658. Value loss: 5.379633. Entropy: 1.110246.\n",
      "episode: 871   score: 500.0  epsilon: 1.0    steps: 368  evaluation reward: 207.65\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2125: Policy loss: -1.632507. Value loss: 303.631226. Entropy: 1.123272.\n",
      "Iteration 2126: Policy loss: -0.921102. Value loss: 191.520340. Entropy: 1.266291.\n",
      "Iteration 2127: Policy loss: -1.002931. Value loss: 132.082245. Entropy: 1.167045.\n",
      "episode: 872   score: 210.0  epsilon: 1.0    steps: 106  evaluation reward: 207.95\n",
      "episode: 873   score: 115.0  epsilon: 1.0    steps: 543  evaluation reward: 207.3\n",
      "episode: 874   score: 210.0  epsilon: 1.0    steps: 676  evaluation reward: 205.3\n",
      "episode: 875   score: 190.0  epsilon: 1.0    steps: 833  evaluation reward: 205.1\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2128: Policy loss: 2.561499. Value loss: 36.063301. Entropy: 0.976309.\n",
      "Iteration 2129: Policy loss: 2.463065. Value loss: 17.616894. Entropy: 0.885159.\n",
      "Iteration 2130: Policy loss: 2.791773. Value loss: 16.526814. Entropy: 0.937417.\n",
      "episode: 876   score: 210.0  epsilon: 1.0    steps: 396  evaluation reward: 205.4\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2131: Policy loss: 1.232972. Value loss: 40.488697. Entropy: 0.993155.\n",
      "Iteration 2132: Policy loss: 1.090451. Value loss: 25.037474. Entropy: 1.008699.\n",
      "Iteration 2133: Policy loss: 1.103702. Value loss: 22.801407. Entropy: 0.998855.\n",
      "episode: 877   score: 200.0  epsilon: 1.0    steps: 208  evaluation reward: 205.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2134: Policy loss: -0.478811. Value loss: 273.332153. Entropy: 0.969556.\n",
      "Iteration 2135: Policy loss: 0.422201. Value loss: 136.320526. Entropy: 0.806898.\n",
      "Iteration 2136: Policy loss: -0.720115. Value loss: 131.007599. Entropy: 0.911519.\n",
      "episode: 878   score: 55.0  epsilon: 1.0    steps: 542  evaluation reward: 204.35\n",
      "episode: 879   score: 50.0  epsilon: 1.0    steps: 826  evaluation reward: 202.75\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2137: Policy loss: 0.394138. Value loss: 33.066132. Entropy: 0.706439.\n",
      "Iteration 2138: Policy loss: 0.771133. Value loss: 22.522158. Entropy: 0.711829.\n",
      "Iteration 2139: Policy loss: 0.346557. Value loss: 13.177276. Entropy: 0.707938.\n",
      "episode: 880   score: 320.0  epsilon: 1.0    steps: 1001  evaluation reward: 203.85\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2140: Policy loss: 2.583531. Value loss: 50.112885. Entropy: 0.723464.\n",
      "Iteration 2141: Policy loss: 2.234360. Value loss: 25.889135. Entropy: 0.765332.\n",
      "Iteration 2142: Policy loss: 1.707267. Value loss: 20.457422. Entropy: 0.749615.\n",
      "episode: 881   score: 90.0  epsilon: 1.0    steps: 190  evaluation reward: 203.55\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2143: Policy loss: 0.474791. Value loss: 38.357998. Entropy: 0.781294.\n",
      "Iteration 2144: Policy loss: 0.321438. Value loss: 24.485176. Entropy: 0.768514.\n",
      "Iteration 2145: Policy loss: 0.345310. Value loss: 19.625885. Entropy: 0.767813.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2146: Policy loss: 0.479355. Value loss: 16.170317. Entropy: 0.721940.\n",
      "Iteration 2147: Policy loss: 0.503984. Value loss: 10.599619. Entropy: 0.763134.\n",
      "Iteration 2148: Policy loss: 0.419974. Value loss: 7.695667. Entropy: 0.777961.\n",
      "episode: 882   score: 155.0  epsilon: 1.0    steps: 10  evaluation reward: 203.0\n",
      "episode: 883   score: 230.0  epsilon: 1.0    steps: 485  evaluation reward: 203.5\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2149: Policy loss: -2.955544. Value loss: 211.330200. Entropy: 0.792499.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2150: Policy loss: -2.576711. Value loss: 50.838917. Entropy: 0.804672.\n",
      "Iteration 2151: Policy loss: -2.050696. Value loss: 41.731827. Entropy: 0.836205.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2152: Policy loss: -0.578362. Value loss: 173.082443. Entropy: 0.904788.\n",
      "Iteration 2153: Policy loss: -0.749674. Value loss: 55.912552. Entropy: 0.849973.\n",
      "Iteration 2154: Policy loss: -1.176742. Value loss: 47.793255. Entropy: 0.849598.\n",
      "episode: 884   score: 475.0  epsilon: 1.0    steps: 259  evaluation reward: 206.45\n",
      "episode: 885   score: 230.0  epsilon: 1.0    steps: 618  evaluation reward: 207.2\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2155: Policy loss: 1.350469. Value loss: 24.033319. Entropy: 0.928273.\n",
      "Iteration 2156: Policy loss: 1.477335. Value loss: 19.760885. Entropy: 0.939573.\n",
      "Iteration 2157: Policy loss: 1.366175. Value loss: 16.029533. Entropy: 0.950647.\n",
      "episode: 886   score: 570.0  epsilon: 1.0    steps: 711  evaluation reward: 210.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2158: Policy loss: 0.753567. Value loss: 27.410028. Entropy: 0.968471.\n",
      "Iteration 2159: Policy loss: 0.446468. Value loss: 17.845974. Entropy: 0.998440.\n",
      "Iteration 2160: Policy loss: 0.439689. Value loss: 14.089486. Entropy: 0.970722.\n",
      "episode: 887   score: 70.0  epsilon: 1.0    steps: 503  evaluation reward: 209.95\n",
      "episode: 888   score: 270.0  epsilon: 1.0    steps: 818  evaluation reward: 208.55\n",
      "episode: 889   score: 485.0  epsilon: 1.0    steps: 1013  evaluation reward: 211.85\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2161: Policy loss: 1.626469. Value loss: 24.478731. Entropy: 0.951346.\n",
      "Iteration 2162: Policy loss: 1.662923. Value loss: 15.165844. Entropy: 0.935519.\n",
      "Iteration 2163: Policy loss: 1.767880. Value loss: 9.733564. Entropy: 0.933501.\n",
      "episode: 890   score: 220.0  epsilon: 1.0    steps: 157  evaluation reward: 213.0\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2164: Policy loss: 1.009235. Value loss: 8.817067. Entropy: 0.904154.\n",
      "Iteration 2165: Policy loss: 1.014183. Value loss: 5.392294. Entropy: 0.933159.\n",
      "Iteration 2166: Policy loss: 0.853847. Value loss: 4.216710. Entropy: 0.958370.\n",
      "episode: 891   score: 210.0  epsilon: 1.0    steps: 39  evaluation reward: 213.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2167: Policy loss: -0.643497. Value loss: 35.110706. Entropy: 0.946010.\n",
      "Iteration 2168: Policy loss: -0.359825. Value loss: 23.001797. Entropy: 0.949246.\n",
      "Iteration 2169: Policy loss: -0.809543. Value loss: 19.536386. Entropy: 0.996378.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2170: Policy loss: -0.004680. Value loss: 24.858982. Entropy: 1.034885.\n",
      "Iteration 2171: Policy loss: -0.044264. Value loss: 13.414888. Entropy: 1.013927.\n",
      "Iteration 2172: Policy loss: 0.037387. Value loss: 10.723353. Entropy: 1.052132.\n",
      "episode: 892   score: 80.0  epsilon: 1.0    steps: 497  evaluation reward: 212.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2173: Policy loss: -1.581954. Value loss: 176.588913. Entropy: 0.980710.\n",
      "Iteration 2174: Policy loss: -1.503759. Value loss: 60.564236. Entropy: 1.038017.\n",
      "Iteration 2175: Policy loss: -1.627774. Value loss: 29.836845. Entropy: 1.069350.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2176: Policy loss: 1.592014. Value loss: 45.452759. Entropy: 0.942045.\n",
      "Iteration 2177: Policy loss: 1.506866. Value loss: 13.977492. Entropy: 0.933615.\n",
      "Iteration 2178: Policy loss: 1.367157. Value loss: 9.184060. Entropy: 0.944669.\n",
      "episode: 893   score: 135.0  epsilon: 1.0    steps: 172  evaluation reward: 211.55\n",
      "episode: 894   score: 190.0  epsilon: 1.0    steps: 655  evaluation reward: 211.35\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2179: Policy loss: -1.712253. Value loss: 57.014629. Entropy: 0.828146.\n",
      "Iteration 2180: Policy loss: -1.275141. Value loss: 28.007545. Entropy: 0.805763.\n",
      "Iteration 2181: Policy loss: -1.457231. Value loss: 20.199646. Entropy: 0.840681.\n",
      "episode: 895   score: 75.0  epsilon: 1.0    steps: 476  evaluation reward: 210.0\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2182: Policy loss: 2.350723. Value loss: 35.679356. Entropy: 0.791316.\n",
      "Iteration 2183: Policy loss: 2.256222. Value loss: 16.804689. Entropy: 0.780814.\n",
      "Iteration 2184: Policy loss: 2.298592. Value loss: 13.376503. Entropy: 0.799622.\n",
      "episode: 896   score: 135.0  epsilon: 1.0    steps: 65  evaluation reward: 210.25\n",
      "episode: 897   score: 375.0  epsilon: 1.0    steps: 328  evaluation reward: 211.9\n",
      "episode: 898   score: 535.0  epsilon: 1.0    steps: 638  evaluation reward: 215.15\n",
      "episode: 899   score: 350.0  epsilon: 1.0    steps: 878  evaluation reward: 217.1\n",
      "episode: 900   score: 260.0  epsilon: 1.0    steps: 963  evaluation reward: 217.9\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2185: Policy loss: 2.143102. Value loss: 31.963236. Entropy: 1.022313.\n",
      "Iteration 2186: Policy loss: 1.937955. Value loss: 17.108118. Entropy: 1.042661.\n",
      "Iteration 2187: Policy loss: 2.056230. Value loss: 12.964405. Entropy: 1.047800.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2188: Policy loss: -0.443231. Value loss: 19.142614. Entropy: 0.863978.\n",
      "Iteration 2189: Policy loss: -0.678693. Value loss: 15.740759. Entropy: 0.834691.\n",
      "Iteration 2190: Policy loss: -0.653802. Value loss: 12.854335. Entropy: 0.821645.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2191: Policy loss: 0.385073. Value loss: 24.139639. Entropy: 1.047684.\n",
      "Iteration 2192: Policy loss: 0.228899. Value loss: 17.840195. Entropy: 1.042759.\n",
      "Iteration 2193: Policy loss: 0.498622. Value loss: 13.439283. Entropy: 1.021019.\n",
      "now time :  2019-02-25 19:20:50.733830\n",
      "episode: 901   score: 105.0  epsilon: 1.0    steps: 187  evaluation reward: 217.15\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2194: Policy loss: 3.213694. Value loss: 28.553082. Entropy: 1.073789.\n",
      "Iteration 2195: Policy loss: 3.038439. Value loss: 8.762853. Entropy: 1.142654.\n",
      "Iteration 2196: Policy loss: 2.844373. Value loss: 6.403437. Entropy: 1.120644.\n",
      "episode: 902   score: 155.0  epsilon: 1.0    steps: 702  evaluation reward: 216.9\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2197: Policy loss: 1.319211. Value loss: 8.319559. Entropy: 0.751733.\n",
      "Iteration 2198: Policy loss: 1.237319. Value loss: 4.810774. Entropy: 0.755566.\n",
      "Iteration 2199: Policy loss: 1.365296. Value loss: 4.040370. Entropy: 0.761587.\n",
      "episode: 903   score: 140.0  epsilon: 1.0    steps: 978  evaluation reward: 216.5\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2200: Policy loss: 0.178812. Value loss: 14.884092. Entropy: 0.902490.\n",
      "Iteration 2201: Policy loss: -0.085635. Value loss: 9.951486. Entropy: 0.935879.\n",
      "Iteration 2202: Policy loss: -0.019905. Value loss: 7.450356. Entropy: 0.897793.\n",
      "episode: 904   score: 210.0  epsilon: 1.0    steps: 413  evaluation reward: 216.0\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2203: Policy loss: -1.154301. Value loss: 22.198065. Entropy: 0.698746.\n",
      "Iteration 2204: Policy loss: -1.059647. Value loss: 15.539480. Entropy: 0.670440.\n",
      "Iteration 2205: Policy loss: -1.256324. Value loss: 12.867621. Entropy: 0.668795.\n",
      "episode: 905   score: 215.0  epsilon: 1.0    steps: 300  evaluation reward: 216.05\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2206: Policy loss: -0.311227. Value loss: 171.128082. Entropy: 0.786174.\n",
      "Iteration 2207: Policy loss: -0.023348. Value loss: 81.098640. Entropy: 0.782365.\n",
      "Iteration 2208: Policy loss: -0.731466. Value loss: 63.358818. Entropy: 0.749769.\n",
      "episode: 906   score: 260.0  epsilon: 1.0    steps: 30  evaluation reward: 217.1\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2209: Policy loss: 0.550883. Value loss: 42.647736. Entropy: 0.854044.\n",
      "Iteration 2210: Policy loss: 0.380607. Value loss: 26.507437. Entropy: 0.834200.\n",
      "Iteration 2211: Policy loss: 0.384292. Value loss: 21.511694. Entropy: 0.826026.\n",
      "episode: 907   score: 100.0  epsilon: 1.0    steps: 434  evaluation reward: 216.55\n",
      "episode: 908   score: 305.0  epsilon: 1.0    steps: 578  evaluation reward: 218.05\n",
      "episode: 909   score: 300.0  epsilon: 1.0    steps: 825  evaluation reward: 217.25\n",
      "Training network. lr: 0.000233. clip: 0.093245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2212: Policy loss: 0.452387. Value loss: 28.656990. Entropy: 0.801350.\n",
      "Iteration 2213: Policy loss: 0.508657. Value loss: 16.590565. Entropy: 0.823994.\n",
      "Iteration 2214: Policy loss: 0.402384. Value loss: 13.331876. Entropy: 0.812683.\n",
      "episode: 910   score: 85.0  epsilon: 1.0    steps: 312  evaluation reward: 215.5\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2215: Policy loss: 2.552080. Value loss: 41.001514. Entropy: 0.727548.\n",
      "Iteration 2216: Policy loss: 2.656335. Value loss: 21.315384. Entropy: 0.688725.\n",
      "Iteration 2217: Policy loss: 2.754773. Value loss: 16.730436. Entropy: 0.761861.\n",
      "episode: 911   score: 460.0  epsilon: 1.0    steps: 146  evaluation reward: 218.3\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2218: Policy loss: 1.468108. Value loss: 24.615143. Entropy: 0.836215.\n",
      "Iteration 2219: Policy loss: 1.333639. Value loss: 18.083662. Entropy: 0.877080.\n",
      "Iteration 2220: Policy loss: 1.311131. Value loss: 14.401433. Entropy: 0.858513.\n",
      "episode: 912   score: 105.0  epsilon: 1.0    steps: 599  evaluation reward: 217.55\n",
      "episode: 913   score: 230.0  epsilon: 1.0    steps: 925  evaluation reward: 218.05\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2221: Policy loss: 0.386921. Value loss: 24.390942. Entropy: 0.900592.\n",
      "Iteration 2222: Policy loss: 0.314593. Value loss: 13.119667. Entropy: 0.882355.\n",
      "Iteration 2223: Policy loss: 0.431266. Value loss: 10.163268. Entropy: 0.884277.\n",
      "episode: 914   score: 295.0  epsilon: 1.0    steps: 734  evaluation reward: 218.9\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2224: Policy loss: 2.715589. Value loss: 34.438400. Entropy: 0.781044.\n",
      "Iteration 2225: Policy loss: 2.656478. Value loss: 12.523541. Entropy: 0.821094.\n",
      "Iteration 2226: Policy loss: 2.420630. Value loss: 10.125788. Entropy: 0.820709.\n",
      "episode: 915   score: 230.0  epsilon: 1.0    steps: 93  evaluation reward: 218.35\n",
      "episode: 916   score: 55.0  epsilon: 1.0    steps: 157  evaluation reward: 216.8\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2227: Policy loss: -1.218017. Value loss: 28.936148. Entropy: 0.783166.\n",
      "Iteration 2228: Policy loss: -1.057114. Value loss: 17.377470. Entropy: 0.747507.\n",
      "Iteration 2229: Policy loss: -1.073899. Value loss: 13.948984. Entropy: 0.748864.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2230: Policy loss: 1.029332. Value loss: 16.019913. Entropy: 0.944789.\n",
      "Iteration 2231: Policy loss: 1.012006. Value loss: 10.343654. Entropy: 0.965937.\n",
      "Iteration 2232: Policy loss: 0.900775. Value loss: 7.836951. Entropy: 0.986944.\n",
      "episode: 917   score: 180.0  epsilon: 1.0    steps: 346  evaluation reward: 216.5\n",
      "episode: 918   score: 265.0  epsilon: 1.0    steps: 485  evaluation reward: 217.35\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2233: Policy loss: 1.490211. Value loss: 26.001720. Entropy: 0.974321.\n",
      "Iteration 2234: Policy loss: 1.719866. Value loss: 13.812859. Entropy: 0.969752.\n",
      "Iteration 2235: Policy loss: 1.592395. Value loss: 11.890014. Entropy: 0.962771.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2236: Policy loss: 0.453863. Value loss: 26.184973. Entropy: 0.904052.\n",
      "Iteration 2237: Policy loss: 0.424660. Value loss: 10.971704. Entropy: 0.901276.\n",
      "Iteration 2238: Policy loss: 0.270700. Value loss: 7.161312. Entropy: 0.885381.\n",
      "episode: 919   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 216.3\n",
      "episode: 920   score: 240.0  epsilon: 1.0    steps: 966  evaluation reward: 216.9\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2239: Policy loss: -2.773746. Value loss: 182.243805. Entropy: 0.986962.\n",
      "Iteration 2240: Policy loss: -3.437361. Value loss: 102.523804. Entropy: 0.991170.\n",
      "Iteration 2241: Policy loss: -2.845874. Value loss: 89.341019. Entropy: 0.951909.\n",
      "episode: 921   score: 515.0  epsilon: 1.0    steps: 886  evaluation reward: 220.2\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2242: Policy loss: -1.667969. Value loss: 243.453140. Entropy: 0.978132.\n",
      "Iteration 2243: Policy loss: -1.334123. Value loss: 148.330826. Entropy: 0.957536.\n",
      "Iteration 2244: Policy loss: -1.779414. Value loss: 60.680798. Entropy: 0.949070.\n",
      "episode: 922   score: 450.0  epsilon: 1.0    steps: 691  evaluation reward: 222.9\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2245: Policy loss: 1.786959. Value loss: 35.046707. Entropy: 0.907411.\n",
      "Iteration 2246: Policy loss: 2.152594. Value loss: 16.342222. Entropy: 0.931650.\n",
      "Iteration 2247: Policy loss: 1.522638. Value loss: 13.811212. Entropy: 0.944147.\n",
      "episode: 923   score: 220.0  epsilon: 1.0    steps: 15  evaluation reward: 223.55\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2248: Policy loss: 3.504273. Value loss: 72.478149. Entropy: 0.967391.\n",
      "Iteration 2249: Policy loss: 3.167900. Value loss: 36.143139. Entropy: 0.967222.\n",
      "Iteration 2250: Policy loss: 3.123357. Value loss: 28.278912. Entropy: 0.997874.\n",
      "episode: 924   score: 315.0  epsilon: 1.0    steps: 162  evaluation reward: 224.9\n",
      "episode: 925   score: 95.0  epsilon: 1.0    steps: 565  evaluation reward: 223.75\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2251: Policy loss: -0.483373. Value loss: 50.123341. Entropy: 1.054911.\n",
      "Iteration 2252: Policy loss: -0.388824. Value loss: 30.021896. Entropy: 1.051497.\n",
      "Iteration 2253: Policy loss: -0.441442. Value loss: 24.868006. Entropy: 1.069221.\n",
      "episode: 926   score: 185.0  epsilon: 1.0    steps: 263  evaluation reward: 223.5\n",
      "episode: 927   score: 220.0  epsilon: 1.0    steps: 388  evaluation reward: 224.95\n",
      "episode: 928   score: 200.0  epsilon: 1.0    steps: 1013  evaluation reward: 224.85\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2254: Policy loss: 1.069907. Value loss: 30.697739. Entropy: 1.144247.\n",
      "Iteration 2255: Policy loss: 0.982367. Value loss: 18.260738. Entropy: 1.159513.\n",
      "Iteration 2256: Policy loss: 1.206651. Value loss: 14.790817. Entropy: 1.203148.\n",
      "episode: 929   score: 60.0  epsilon: 1.0    steps: 67  evaluation reward: 223.9\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2257: Policy loss: 0.869069. Value loss: 36.716892. Entropy: 1.195322.\n",
      "Iteration 2258: Policy loss: 1.372485. Value loss: 23.291395. Entropy: 1.209358.\n",
      "Iteration 2259: Policy loss: 1.099390. Value loss: 18.923868. Entropy: 1.190315.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2260: Policy loss: -0.726559. Value loss: 35.092682. Entropy: 1.287327.\n",
      "Iteration 2261: Policy loss: -0.393686. Value loss: 20.891989. Entropy: 1.257226.\n",
      "Iteration 2262: Policy loss: -0.355114. Value loss: 13.232090. Entropy: 1.262662.\n",
      "episode: 930   score: 65.0  epsilon: 1.0    steps: 454  evaluation reward: 222.45\n",
      "episode: 931   score: 165.0  epsilon: 1.0    steps: 629  evaluation reward: 219.2\n",
      "episode: 932   score: 215.0  epsilon: 1.0    steps: 811  evaluation reward: 219.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2263: Policy loss: 0.398003. Value loss: 52.836540. Entropy: 1.283049.\n",
      "Iteration 2264: Policy loss: 0.557682. Value loss: 32.428978. Entropy: 1.247146.\n",
      "Iteration 2265: Policy loss: 0.484916. Value loss: 29.421309. Entropy: 1.239676.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2266: Policy loss: 0.973570. Value loss: 40.232033. Entropy: 1.084245.\n",
      "Iteration 2267: Policy loss: 1.039345. Value loss: 25.563202. Entropy: 1.058552.\n",
      "Iteration 2268: Policy loss: 0.914550. Value loss: 21.027779. Entropy: 1.092301.\n",
      "episode: 933   score: 215.0  epsilon: 1.0    steps: 214  evaluation reward: 218.8\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2269: Policy loss: 0.832467. Value loss: 26.855093. Entropy: 0.998950.\n",
      "Iteration 2270: Policy loss: 0.733029. Value loss: 14.462900. Entropy: 1.020396.\n",
      "Iteration 2271: Policy loss: 0.624665. Value loss: 12.287895. Entropy: 1.060872.\n",
      "episode: 934   score: 225.0  epsilon: 1.0    steps: 303  evaluation reward: 220.3\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2272: Policy loss: 1.010123. Value loss: 21.311762. Entropy: 1.236822.\n",
      "Iteration 2273: Policy loss: 1.241207. Value loss: 11.098129. Entropy: 1.234092.\n",
      "Iteration 2274: Policy loss: 0.824976. Value loss: 8.847642. Entropy: 1.242333.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2275: Policy loss: 0.817344. Value loss: 24.281885. Entropy: 1.069149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2276: Policy loss: 1.009256. Value loss: 14.479310. Entropy: 1.050959.\n",
      "Iteration 2277: Policy loss: 0.820468. Value loss: 10.712809. Entropy: 1.087166.\n",
      "episode: 935   score: 225.0  epsilon: 1.0    steps: 4  evaluation reward: 220.45\n",
      "episode: 936   score: 105.0  epsilon: 1.0    steps: 594  evaluation reward: 219.4\n",
      "episode: 937   score: 240.0  epsilon: 1.0    steps: 954  evaluation reward: 220.0\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2278: Policy loss: -3.858988. Value loss: 458.168518. Entropy: 1.094912.\n",
      "Iteration 2279: Policy loss: -2.944789. Value loss: 276.496521. Entropy: 1.009365.\n",
      "Iteration 2280: Policy loss: -4.006985. Value loss: 300.274445. Entropy: 0.902378.\n",
      "episode: 938   score: 75.0  epsilon: 1.0    steps: 358  evaluation reward: 219.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2281: Policy loss: 0.695332. Value loss: 76.599388. Entropy: 0.979156.\n",
      "Iteration 2282: Policy loss: 0.807225. Value loss: 47.286701. Entropy: 0.950157.\n",
      "Iteration 2283: Policy loss: 1.005093. Value loss: 38.893845. Entropy: 0.935509.\n",
      "episode: 939   score: 800.0  epsilon: 1.0    steps: 706  evaluation reward: 225.6\n",
      "episode: 940   score: 415.0  epsilon: 1.0    steps: 799  evaluation reward: 228.2\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2284: Policy loss: 2.376496. Value loss: 27.247400. Entropy: 0.845206.\n",
      "Iteration 2285: Policy loss: 2.646072. Value loss: 15.219777. Entropy: 0.891691.\n",
      "Iteration 2286: Policy loss: 2.301517. Value loss: 13.586402. Entropy: 0.910599.\n",
      "episode: 941   score: 80.0  epsilon: 1.0    steps: 69  evaluation reward: 227.2\n",
      "episode: 942   score: 175.0  epsilon: 1.0    steps: 129  evaluation reward: 228.2\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2287: Policy loss: 0.288152. Value loss: 38.550545. Entropy: 0.859205.\n",
      "Iteration 2288: Policy loss: 0.383287. Value loss: 26.694950. Entropy: 0.875457.\n",
      "Iteration 2289: Policy loss: 0.439056. Value loss: 20.520706. Entropy: 0.848403.\n",
      "episode: 943   score: 95.0  epsilon: 1.0    steps: 623  evaluation reward: 226.6\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2290: Policy loss: 0.763337. Value loss: 42.757496. Entropy: 0.888374.\n",
      "Iteration 2291: Policy loss: -0.085265. Value loss: 30.208818. Entropy: 0.855068.\n",
      "Iteration 2292: Policy loss: 0.491481. Value loss: 19.785934. Entropy: 0.847518.\n",
      "episode: 944   score: 305.0  epsilon: 1.0    steps: 423  evaluation reward: 227.85\n",
      "episode: 945   score: 130.0  epsilon: 1.0    steps: 973  evaluation reward: 227.35\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2293: Policy loss: 2.081766. Value loss: 38.849178. Entropy: 0.914367.\n",
      "Iteration 2294: Policy loss: 2.096669. Value loss: 21.555189. Entropy: 0.920792.\n",
      "Iteration 2295: Policy loss: 2.108068. Value loss: 16.944260. Entropy: 0.936220.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2296: Policy loss: 2.358605. Value loss: 46.693058. Entropy: 0.881925.\n",
      "Iteration 2297: Policy loss: 2.122038. Value loss: 25.787859. Entropy: 0.880584.\n",
      "Iteration 2298: Policy loss: 2.545933. Value loss: 21.419195. Entropy: 0.896426.\n",
      "episode: 946   score: 185.0  epsilon: 1.0    steps: 838  evaluation reward: 227.4\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2299: Policy loss: 0.480801. Value loss: 40.881657. Entropy: 1.070220.\n",
      "Iteration 2300: Policy loss: 0.250151. Value loss: 25.172117. Entropy: 1.096985.\n",
      "Iteration 2301: Policy loss: 0.738630. Value loss: 18.748402. Entropy: 1.084968.\n",
      "episode: 947   score: 150.0  epsilon: 1.0    steps: 57  evaluation reward: 226.25\n",
      "episode: 948   score: 110.0  epsilon: 1.0    steps: 136  evaluation reward: 225.55\n",
      "episode: 949   score: 255.0  epsilon: 1.0    steps: 299  evaluation reward: 227.3\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2302: Policy loss: 2.428678. Value loss: 40.051552. Entropy: 1.046464.\n",
      "Iteration 2303: Policy loss: 2.797133. Value loss: 26.644760. Entropy: 1.032266.\n",
      "Iteration 2304: Policy loss: 2.659592. Value loss: 22.941284. Entropy: 1.063657.\n",
      "episode: 950   score: 165.0  epsilon: 1.0    steps: 621  evaluation reward: 225.8\n",
      "now time :  2019-02-25 19:22:56.434629\n",
      "episode: 951   score: 285.0  epsilon: 1.0    steps: 661  evaluation reward: 227.5\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2305: Policy loss: 0.150098. Value loss: 36.833786. Entropy: 0.715992.\n",
      "Iteration 2306: Policy loss: 0.076114. Value loss: 20.859215. Entropy: 0.730669.\n",
      "Iteration 2307: Policy loss: -0.309224. Value loss: 18.191845. Entropy: 0.738017.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2308: Policy loss: 0.993887. Value loss: 45.058777. Entropy: 0.994608.\n",
      "Iteration 2309: Policy loss: 0.552503. Value loss: 28.615871. Entropy: 1.024227.\n",
      "Iteration 2310: Policy loss: 0.608646. Value loss: 23.515554. Entropy: 1.060502.\n",
      "episode: 952   score: 230.0  epsilon: 1.0    steps: 458  evaluation reward: 228.95\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2311: Policy loss: -0.951492. Value loss: 34.779549. Entropy: 0.965277.\n",
      "Iteration 2312: Policy loss: -0.978157. Value loss: 22.055584. Entropy: 0.982835.\n",
      "Iteration 2313: Policy loss: -0.657569. Value loss: 15.178135. Entropy: 0.954765.\n",
      "episode: 953   score: 85.0  epsilon: 1.0    steps: 753  evaluation reward: 226.95\n",
      "episode: 954   score: 175.0  epsilon: 1.0    steps: 824  evaluation reward: 226.55\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2314: Policy loss: 0.047704. Value loss: 27.336266. Entropy: 1.068521.\n",
      "Iteration 2315: Policy loss: 0.019534. Value loss: 14.907660. Entropy: 1.069324.\n",
      "Iteration 2316: Policy loss: 0.450378. Value loss: 11.092839. Entropy: 1.054288.\n",
      "episode: 955   score: 205.0  epsilon: 1.0    steps: 94  evaluation reward: 226.0\n",
      "episode: 956   score: 155.0  epsilon: 1.0    steps: 310  evaluation reward: 225.1\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2317: Policy loss: -0.394788. Value loss: 56.172604. Entropy: 1.067144.\n",
      "Iteration 2318: Policy loss: -0.094257. Value loss: 27.202456. Entropy: 1.066761.\n",
      "Iteration 2319: Policy loss: -0.205093. Value loss: 21.499792. Entropy: 1.090729.\n",
      "episode: 957   score: 400.0  epsilon: 1.0    steps: 1004  evaluation reward: 224.45\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2320: Policy loss: 1.188195. Value loss: 39.120506. Entropy: 1.091239.\n",
      "Iteration 2321: Policy loss: 1.287809. Value loss: 21.231731. Entropy: 1.097603.\n",
      "Iteration 2322: Policy loss: 1.243667. Value loss: 16.225117. Entropy: 1.094214.\n",
      "episode: 958   score: 450.0  epsilon: 1.0    steps: 165  evaluation reward: 224.8\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2323: Policy loss: 0.363453. Value loss: 191.731644. Entropy: 1.160842.\n",
      "Iteration 2324: Policy loss: -0.263057. Value loss: 210.124100. Entropy: 1.090296.\n",
      "Iteration 2325: Policy loss: 0.447206. Value loss: 128.477692. Entropy: 1.058199.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2326: Policy loss: 1.450647. Value loss: 35.685635. Entropy: 1.019134.\n",
      "Iteration 2327: Policy loss: 1.723128. Value loss: 19.646841. Entropy: 1.028785.\n",
      "Iteration 2328: Policy loss: 1.692587. Value loss: 15.498281. Entropy: 1.054914.\n",
      "episode: 959   score: 60.0  epsilon: 1.0    steps: 67  evaluation reward: 224.15\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2329: Policy loss: 1.633797. Value loss: 43.898987. Entropy: 1.224242.\n",
      "Iteration 2330: Policy loss: 2.064759. Value loss: 28.137974. Entropy: 1.225725.\n",
      "Iteration 2331: Policy loss: 1.992050. Value loss: 21.687363. Entropy: 1.213467.\n",
      "episode: 960   score: 160.0  epsilon: 1.0    steps: 292  evaluation reward: 224.65\n",
      "episode: 961   score: 280.0  epsilon: 1.0    steps: 503  evaluation reward: 222.1\n",
      "episode: 962   score: 330.0  epsilon: 1.0    steps: 582  evaluation reward: 220.95\n",
      "episode: 963   score: 160.0  epsilon: 1.0    steps: 696  evaluation reward: 221.65\n",
      "episode: 964   score: 225.0  epsilon: 1.0    steps: 886  evaluation reward: 221.75\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2332: Policy loss: 2.554330. Value loss: 25.325012. Entropy: 1.065847.\n",
      "Iteration 2333: Policy loss: 2.756274. Value loss: 12.451519. Entropy: 1.089655.\n",
      "Iteration 2334: Policy loss: 2.499202. Value loss: 9.032361. Entropy: 1.090477.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2335: Policy loss: 0.010577. Value loss: 23.815159. Entropy: 0.933968.\n",
      "Iteration 2336: Policy loss: 0.058831. Value loss: 19.610117. Entropy: 0.892359.\n",
      "Iteration 2337: Policy loss: 0.084140. Value loss: 17.117313. Entropy: 0.902812.\n",
      "episode: 965   score: 205.0  epsilon: 1.0    steps: 200  evaluation reward: 221.95\n",
      "episode: 966   score: 175.0  epsilon: 1.0    steps: 917  evaluation reward: 221.85\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2338: Policy loss: 0.949565. Value loss: 22.100723. Entropy: 0.907023.\n",
      "Iteration 2339: Policy loss: 1.015906. Value loss: 14.079199. Entropy: 0.880201.\n",
      "Iteration 2340: Policy loss: 1.040607. Value loss: 12.531670. Entropy: 0.873869.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2341: Policy loss: -0.733909. Value loss: 52.962578. Entropy: 0.969188.\n",
      "Iteration 2342: Policy loss: -0.993907. Value loss: 37.562981. Entropy: 0.975548.\n",
      "Iteration 2343: Policy loss: -1.202017. Value loss: 30.395012. Entropy: 0.956221.\n",
      "episode: 967   score: 105.0  epsilon: 1.0    steps: 756  evaluation reward: 221.05\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2344: Policy loss: 1.121066. Value loss: 40.522129. Entropy: 1.132292.\n",
      "Iteration 2345: Policy loss: 1.165498. Value loss: 23.389786. Entropy: 1.138718.\n",
      "Iteration 2346: Policy loss: 1.022954. Value loss: 19.342062. Entropy: 1.144602.\n",
      "episode: 968   score: 240.0  epsilon: 1.0    steps: 96  evaluation reward: 222.3\n",
      "episode: 969   score: 200.0  epsilon: 1.0    steps: 629  evaluation reward: 222.8\n",
      "episode: 970   score: 150.0  epsilon: 1.0    steps: 798  evaluation reward: 222.2\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2347: Policy loss: 1.235549. Value loss: 35.258163. Entropy: 1.176591.\n",
      "Iteration 2348: Policy loss: 1.466320. Value loss: 18.309753. Entropy: 1.178357.\n",
      "Iteration 2349: Policy loss: 1.157319. Value loss: 15.978710. Entropy: 1.192155.\n",
      "episode: 971   score: 135.0  epsilon: 1.0    steps: 963  evaluation reward: 218.55\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2350: Policy loss: -1.498578. Value loss: 54.230881. Entropy: 0.989646.\n",
      "Iteration 2351: Policy loss: -1.518614. Value loss: 35.363895. Entropy: 0.953116.\n",
      "Iteration 2352: Policy loss: -1.419198. Value loss: 23.667250. Entropy: 0.957270.\n",
      "episode: 972   score: 145.0  epsilon: 1.0    steps: 407  evaluation reward: 217.9\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2353: Policy loss: 0.192351. Value loss: 30.508896. Entropy: 1.040004.\n",
      "Iteration 2354: Policy loss: 0.487418. Value loss: 17.352991. Entropy: 1.050944.\n",
      "Iteration 2355: Policy loss: 0.207620. Value loss: 13.326442. Entropy: 1.035416.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2356: Policy loss: 1.019650. Value loss: 35.602932. Entropy: 1.163606.\n",
      "Iteration 2357: Policy loss: 0.957272. Value loss: 19.798717. Entropy: 1.146263.\n",
      "Iteration 2358: Policy loss: 1.085521. Value loss: 15.884383. Entropy: 1.164113.\n",
      "episode: 973   score: 485.0  epsilon: 1.0    steps: 384  evaluation reward: 221.6\n",
      "episode: 974   score: 80.0  epsilon: 1.0    steps: 506  evaluation reward: 220.3\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2359: Policy loss: -4.506647. Value loss: 231.559891. Entropy: 1.156122.\n",
      "Iteration 2360: Policy loss: -4.045173. Value loss: 159.178848. Entropy: 0.991251.\n",
      "Iteration 2361: Policy loss: -3.562721. Value loss: 64.986359. Entropy: 0.941576.\n",
      "episode: 975   score: 400.0  epsilon: 1.0    steps: 223  evaluation reward: 222.4\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2362: Policy loss: -1.540622. Value loss: 69.590721. Entropy: 0.984998.\n",
      "Iteration 2363: Policy loss: -1.076479. Value loss: 45.135334. Entropy: 1.039959.\n",
      "Iteration 2364: Policy loss: -1.504244. Value loss: 27.554762. Entropy: 1.022645.\n",
      "episode: 976   score: 585.0  epsilon: 1.0    steps: 826  evaluation reward: 226.15\n",
      "episode: 977   score: 190.0  epsilon: 1.0    steps: 962  evaluation reward: 226.05\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2365: Policy loss: 2.441612. Value loss: 52.564373. Entropy: 0.854328.\n",
      "Iteration 2366: Policy loss: 2.728836. Value loss: 30.335405. Entropy: 0.877771.\n",
      "Iteration 2367: Policy loss: 2.151093. Value loss: 27.409693. Entropy: 0.874484.\n",
      "episode: 978   score: 250.0  epsilon: 1.0    steps: 109  evaluation reward: 228.0\n",
      "episode: 979   score: 315.0  epsilon: 1.0    steps: 612  evaluation reward: 230.65\n",
      "episode: 980   score: 465.0  epsilon: 1.0    steps: 643  evaluation reward: 232.1\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2368: Policy loss: 2.291595. Value loss: 46.280792. Entropy: 0.782425.\n",
      "Iteration 2369: Policy loss: 2.753926. Value loss: 26.564806. Entropy: 0.764037.\n",
      "Iteration 2370: Policy loss: 2.370781. Value loss: 22.420151. Entropy: 0.755108.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2371: Policy loss: 1.717609. Value loss: 278.514191. Entropy: 0.785304.\n",
      "Iteration 2372: Policy loss: 2.351727. Value loss: 189.137711. Entropy: 0.768233.\n",
      "Iteration 2373: Policy loss: 2.415677. Value loss: 173.675323. Entropy: 0.705671.\n",
      "episode: 981   score: 55.0  epsilon: 1.0    steps: 971  evaluation reward: 231.75\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2374: Policy loss: 1.786146. Value loss: 58.226166. Entropy: 0.895467.\n",
      "Iteration 2375: Policy loss: 1.446657. Value loss: 28.082766. Entropy: 0.912045.\n",
      "Iteration 2376: Policy loss: 1.543369. Value loss: 20.908350. Entropy: 0.908191.\n",
      "episode: 982   score: 140.0  epsilon: 1.0    steps: 204  evaluation reward: 231.6\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2377: Policy loss: 4.201257. Value loss: 83.246925. Entropy: 0.934777.\n",
      "Iteration 2378: Policy loss: 3.352958. Value loss: 36.278927. Entropy: 0.928125.\n",
      "Iteration 2379: Policy loss: 3.430327. Value loss: 24.335968. Entropy: 0.977603.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2380: Policy loss: 1.040331. Value loss: 59.399506. Entropy: 0.939583.\n",
      "Iteration 2381: Policy loss: 1.081958. Value loss: 29.307787. Entropy: 0.951658.\n",
      "Iteration 2382: Policy loss: 0.936839. Value loss: 21.771723. Entropy: 0.966640.\n",
      "episode: 983   score: 115.0  epsilon: 1.0    steps: 117  evaluation reward: 230.45\n",
      "episode: 984   score: 495.0  epsilon: 1.0    steps: 314  evaluation reward: 230.65\n",
      "episode: 985   score: 360.0  epsilon: 1.0    steps: 470  evaluation reward: 231.95\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2383: Policy loss: 0.795931. Value loss: 52.143082. Entropy: 0.969126.\n",
      "Iteration 2384: Policy loss: 0.644690. Value loss: 32.642860. Entropy: 0.994314.\n",
      "Iteration 2385: Policy loss: 0.908513. Value loss: 22.546799. Entropy: 0.982708.\n",
      "episode: 986   score: 205.0  epsilon: 1.0    steps: 717  evaluation reward: 228.3\n",
      "episode: 987   score: 305.0  epsilon: 1.0    steps: 798  evaluation reward: 230.65\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2386: Policy loss: 0.568375. Value loss: 40.619041. Entropy: 0.793599.\n",
      "Iteration 2387: Policy loss: 0.164026. Value loss: 18.050730. Entropy: 0.800418.\n",
      "Iteration 2388: Policy loss: 0.324187. Value loss: 12.740504. Entropy: 0.792535.\n",
      "episode: 988   score: 315.0  epsilon: 1.0    steps: 591  evaluation reward: 231.1\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2389: Policy loss: 0.550232. Value loss: 30.081398. Entropy: 0.759413.\n",
      "Iteration 2390: Policy loss: 0.712254. Value loss: 24.776115. Entropy: 0.739797.\n",
      "Iteration 2391: Policy loss: 0.505355. Value loss: 17.492649. Entropy: 0.734596.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2392: Policy loss: 0.770704. Value loss: 30.096550. Entropy: 1.090382.\n",
      "Iteration 2393: Policy loss: 0.863167. Value loss: 17.229780. Entropy: 1.103905.\n",
      "Iteration 2394: Policy loss: 0.720069. Value loss: 13.822979. Entropy: 1.083218.\n",
      "episode: 989   score: 150.0  epsilon: 1.0    steps: 147  evaluation reward: 227.75\n",
      "episode: 990   score: 135.0  epsilon: 1.0    steps: 417  evaluation reward: 226.9\n",
      "episode: 991   score: 210.0  epsilon: 1.0    steps: 917  evaluation reward: 226.9\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2395: Policy loss: 1.063123. Value loss: 26.523695. Entropy: 0.927201.\n",
      "Iteration 2396: Policy loss: 1.191700. Value loss: 16.206310. Entropy: 0.934414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2397: Policy loss: 1.169144. Value loss: 14.268110. Entropy: 0.903669.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2398: Policy loss: -0.416749. Value loss: 36.099316. Entropy: 0.772958.\n",
      "Iteration 2399: Policy loss: -0.255421. Value loss: 26.599281. Entropy: 0.765316.\n",
      "Iteration 2400: Policy loss: -0.261977. Value loss: 21.351925. Entropy: 0.774187.\n",
      "episode: 992   score: 205.0  epsilon: 1.0    steps: 81  evaluation reward: 228.15\n",
      "episode: 993   score: 370.0  epsilon: 1.0    steps: 307  evaluation reward: 230.5\n",
      "episode: 994   score: 165.0  epsilon: 1.0    steps: 804  evaluation reward: 230.25\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2401: Policy loss: 1.594190. Value loss: 213.593643. Entropy: 1.033839.\n",
      "Iteration 2402: Policy loss: 1.286244. Value loss: 199.741714. Entropy: 1.016686.\n",
      "Iteration 2403: Policy loss: 1.141465. Value loss: 156.140488. Entropy: 1.001948.\n",
      "episode: 995   score: 155.0  epsilon: 1.0    steps: 591  evaluation reward: 231.05\n",
      "episode: 996   score: 150.0  epsilon: 1.0    steps: 648  evaluation reward: 231.2\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2404: Policy loss: -0.209840. Value loss: 42.261799. Entropy: 0.731242.\n",
      "Iteration 2405: Policy loss: 0.036633. Value loss: 23.607889. Entropy: 0.708427.\n",
      "Iteration 2406: Policy loss: 0.260080. Value loss: 22.635965. Entropy: 0.740397.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2407: Policy loss: -1.666115. Value loss: 32.443592. Entropy: 0.659002.\n",
      "Iteration 2408: Policy loss: -1.256554. Value loss: 22.126287. Entropy: 0.688629.\n",
      "Iteration 2409: Policy loss: -1.616587. Value loss: 20.692410. Entropy: 0.674871.\n",
      "episode: 997   score: 160.0  epsilon: 1.0    steps: 154  evaluation reward: 229.05\n",
      "episode: 998   score: 60.0  epsilon: 1.0    steps: 329  evaluation reward: 224.3\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2410: Policy loss: 0.549323. Value loss: 12.299314. Entropy: 1.114318.\n",
      "Iteration 2411: Policy loss: 0.321932. Value loss: 8.348204. Entropy: 1.104239.\n",
      "Iteration 2412: Policy loss: 0.482703. Value loss: 7.047890. Entropy: 1.109024.\n",
      "episode: 999   score: 235.0  epsilon: 1.0    steps: 461  evaluation reward: 223.15\n",
      "episode: 1000   score: 220.0  epsilon: 1.0    steps: 958  evaluation reward: 222.75\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2413: Policy loss: 0.232682. Value loss: 20.439512. Entropy: 0.666666.\n",
      "Iteration 2414: Policy loss: 0.240482. Value loss: 12.080559. Entropy: 0.672556.\n",
      "Iteration 2415: Policy loss: 0.290108. Value loss: 9.889005. Entropy: 0.679648.\n",
      "now time :  2019-02-25 19:25:01.416659\n",
      "episode: 1001   score: 170.0  epsilon: 1.0    steps: 817  evaluation reward: 223.4\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2416: Policy loss: 1.017722. Value loss: 28.651554. Entropy: 0.889651.\n",
      "Iteration 2417: Policy loss: 0.966355. Value loss: 20.034971. Entropy: 0.881072.\n",
      "Iteration 2418: Policy loss: 0.876172. Value loss: 13.263696. Entropy: 0.870342.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2419: Policy loss: -1.393049. Value loss: 31.076872. Entropy: 0.826744.\n",
      "Iteration 2420: Policy loss: -1.304178. Value loss: 18.851290. Entropy: 0.790653.\n",
      "Iteration 2421: Policy loss: -1.443134. Value loss: 16.587486. Entropy: 0.777549.\n",
      "episode: 1002   score: 225.0  epsilon: 1.0    steps: 39  evaluation reward: 224.1\n",
      "episode: 1003   score: 240.0  epsilon: 1.0    steps: 686  evaluation reward: 225.1\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2422: Policy loss: 1.973879. Value loss: 40.201725. Entropy: 0.924565.\n",
      "Iteration 2423: Policy loss: 2.088889. Value loss: 17.652460. Entropy: 0.955638.\n",
      "Iteration 2424: Policy loss: 2.226165. Value loss: 12.257980. Entropy: 0.948944.\n",
      "episode: 1004   score: 70.0  epsilon: 1.0    steps: 978  evaluation reward: 223.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2425: Policy loss: 0.329233. Value loss: 25.180464. Entropy: 0.864057.\n",
      "Iteration 2426: Policy loss: 0.600473. Value loss: 17.937021. Entropy: 0.858502.\n",
      "Iteration 2427: Policy loss: 0.320214. Value loss: 12.737078. Entropy: 0.854215.\n",
      "episode: 1005   score: 190.0  epsilon: 1.0    steps: 176  evaluation reward: 223.45\n",
      "episode: 1006   score: 130.0  epsilon: 1.0    steps: 474  evaluation reward: 222.15\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2428: Policy loss: 1.364776. Value loss: 48.266552. Entropy: 0.910527.\n",
      "Iteration 2429: Policy loss: 1.353649. Value loss: 22.953680. Entropy: 0.893335.\n",
      "Iteration 2430: Policy loss: 1.212670. Value loss: 18.309185. Entropy: 0.908020.\n",
      "episode: 1007   score: 340.0  epsilon: 1.0    steps: 537  evaluation reward: 224.55\n",
      "episode: 1008   score: 95.0  epsilon: 1.0    steps: 708  evaluation reward: 222.45\n",
      "episode: 1009   score: 165.0  epsilon: 1.0    steps: 880  evaluation reward: 221.1\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2431: Policy loss: 1.349293. Value loss: 31.097866. Entropy: 0.867232.\n",
      "Iteration 2432: Policy loss: 1.670042. Value loss: 18.489170. Entropy: 0.873594.\n",
      "Iteration 2433: Policy loss: 1.321466. Value loss: 16.542736. Entropy: 0.858538.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2434: Policy loss: 0.496577. Value loss: 40.748600. Entropy: 0.810392.\n",
      "Iteration 2435: Policy loss: 0.573570. Value loss: 23.716684. Entropy: 0.836817.\n",
      "Iteration 2436: Policy loss: 0.571626. Value loss: 20.795277. Entropy: 0.819179.\n",
      "episode: 1010   score: 370.0  epsilon: 1.0    steps: 366  evaluation reward: 223.95\n",
      "episode: 1011   score: 135.0  epsilon: 1.0    steps: 926  evaluation reward: 220.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2437: Policy loss: 1.379781. Value loss: 22.593187. Entropy: 0.831496.\n",
      "Iteration 2438: Policy loss: 1.162785. Value loss: 15.269337. Entropy: 0.825966.\n",
      "Iteration 2439: Policy loss: 1.533805. Value loss: 10.714491. Entropy: 0.841723.\n",
      "episode: 1012   score: 150.0  epsilon: 1.0    steps: 30  evaluation reward: 221.15\n",
      "episode: 1013   score: 50.0  epsilon: 1.0    steps: 880  evaluation reward: 219.35\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2440: Policy loss: 1.397206. Value loss: 46.642017. Entropy: 0.802762.\n",
      "Iteration 2441: Policy loss: 1.141065. Value loss: 21.274059. Entropy: 0.833451.\n",
      "Iteration 2442: Policy loss: 1.559536. Value loss: 18.258978. Entropy: 0.836736.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2443: Policy loss: 0.685212. Value loss: 28.269199. Entropy: 0.867511.\n",
      "Iteration 2444: Policy loss: 0.535546. Value loss: 19.326925. Entropy: 0.826779.\n",
      "Iteration 2445: Policy loss: 0.590856. Value loss: 15.939476. Entropy: 0.851233.\n",
      "episode: 1014   score: 250.0  epsilon: 1.0    steps: 512  evaluation reward: 218.9\n",
      "episode: 1015   score: 115.0  epsilon: 1.0    steps: 583  evaluation reward: 217.75\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2446: Policy loss: 0.471329. Value loss: 24.294502. Entropy: 1.061026.\n",
      "Iteration 2447: Policy loss: 0.419932. Value loss: 12.681698. Entropy: 1.065889.\n",
      "Iteration 2448: Policy loss: 0.472109. Value loss: 9.772822. Entropy: 1.045647.\n",
      "episode: 1016   score: 185.0  epsilon: 1.0    steps: 760  evaluation reward: 219.05\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2449: Policy loss: -0.375900. Value loss: 26.644293. Entropy: 0.913833.\n",
      "Iteration 2450: Policy loss: -0.700356. Value loss: 17.337444. Entropy: 0.898673.\n",
      "Iteration 2451: Policy loss: -0.648788. Value loss: 14.079353. Entropy: 0.913524.\n",
      "episode: 1017   score: 375.0  epsilon: 1.0    steps: 141  evaluation reward: 221.0\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2452: Policy loss: 1.026166. Value loss: 12.717423. Entropy: 0.840646.\n",
      "Iteration 2453: Policy loss: 1.144565. Value loss: 9.227021. Entropy: 0.847210.\n",
      "Iteration 2454: Policy loss: 1.170794. Value loss: 6.612716. Entropy: 0.817983.\n",
      "episode: 1018   score: 130.0  epsilon: 1.0    steps: 329  evaluation reward: 219.65\n",
      "episode: 1019   score: 185.0  epsilon: 1.0    steps: 971  evaluation reward: 219.95\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2455: Policy loss: 0.565017. Value loss: 37.713642. Entropy: 0.876359.\n",
      "Iteration 2456: Policy loss: 0.661348. Value loss: 19.131725. Entropy: 0.869942.\n",
      "Iteration 2457: Policy loss: 0.513408. Value loss: 14.286477. Entropy: 0.889621.\n",
      "episode: 1020   score: 160.0  epsilon: 1.0    steps: 20  evaluation reward: 219.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1021   score: 75.0  epsilon: 1.0    steps: 247  evaluation reward: 214.75\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2458: Policy loss: 1.480857. Value loss: 32.177254. Entropy: 0.861093.\n",
      "Iteration 2459: Policy loss: 1.493356. Value loss: 13.737444. Entropy: 0.842431.\n",
      "Iteration 2460: Policy loss: 1.380728. Value loss: 10.709804. Entropy: 0.830824.\n",
      "episode: 1022   score: 205.0  epsilon: 1.0    steps: 602  evaluation reward: 212.3\n",
      "episode: 1023   score: 150.0  epsilon: 1.0    steps: 859  evaluation reward: 211.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2461: Policy loss: -0.647640. Value loss: 28.765938. Entropy: 0.771653.\n",
      "Iteration 2462: Policy loss: -0.807406. Value loss: 18.571646. Entropy: 0.762622.\n",
      "Iteration 2463: Policy loss: -0.883497. Value loss: 14.780798. Entropy: 0.757125.\n",
      "episode: 1024   score: 185.0  epsilon: 1.0    steps: 407  evaluation reward: 210.3\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2464: Policy loss: 0.083628. Value loss: 18.401827. Entropy: 0.916180.\n",
      "Iteration 2465: Policy loss: 0.173208. Value loss: 11.962122. Entropy: 0.927217.\n",
      "Iteration 2466: Policy loss: 0.146980. Value loss: 9.120312. Entropy: 0.946625.\n",
      "episode: 1025   score: 225.0  epsilon: 1.0    steps: 718  evaluation reward: 211.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2467: Policy loss: 3.540305. Value loss: 23.574471. Entropy: 0.897743.\n",
      "Iteration 2468: Policy loss: 3.526911. Value loss: 13.533597. Entropy: 0.903632.\n",
      "Iteration 2469: Policy loss: 3.644087. Value loss: 10.204477. Entropy: 0.932298.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2470: Policy loss: 0.250300. Value loss: 21.298632. Entropy: 0.911361.\n",
      "Iteration 2471: Policy loss: 0.086516. Value loss: 10.382510. Entropy: 0.947152.\n",
      "Iteration 2472: Policy loss: 0.090390. Value loss: 7.744998. Entropy: 0.949670.\n",
      "episode: 1026   score: 160.0  epsilon: 1.0    steps: 78  evaluation reward: 211.35\n",
      "episode: 1027   score: 30.0  epsilon: 1.0    steps: 420  evaluation reward: 209.45\n",
      "episode: 1028   score: 160.0  epsilon: 1.0    steps: 605  evaluation reward: 209.05\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2473: Policy loss: -0.312334. Value loss: 19.850618. Entropy: 1.009894.\n",
      "Iteration 2474: Policy loss: -0.408344. Value loss: 10.692232. Entropy: 1.001856.\n",
      "Iteration 2475: Policy loss: -0.327359. Value loss: 8.475774. Entropy: 0.992978.\n",
      "episode: 1029   score: 125.0  epsilon: 1.0    steps: 896  evaluation reward: 209.7\n",
      "episode: 1030   score: 250.0  epsilon: 1.0    steps: 904  evaluation reward: 211.55\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2476: Policy loss: -1.825203. Value loss: 27.143665. Entropy: 0.788442.\n",
      "Iteration 2477: Policy loss: -1.985056. Value loss: 16.094837. Entropy: 0.760248.\n",
      "Iteration 2478: Policy loss: -1.888957. Value loss: 13.369583. Entropy: 0.768652.\n",
      "episode: 1031   score: 165.0  epsilon: 1.0    steps: 382  evaluation reward: 211.55\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2479: Policy loss: -0.091971. Value loss: 32.116646. Entropy: 0.938071.\n",
      "Iteration 2480: Policy loss: 0.032732. Value loss: 15.359730. Entropy: 0.918267.\n",
      "Iteration 2481: Policy loss: 0.185788. Value loss: 12.914852. Entropy: 0.906721.\n",
      "episode: 1032   score: 260.0  epsilon: 1.0    steps: 181  evaluation reward: 212.0\n",
      "episode: 1033   score: 195.0  epsilon: 1.0    steps: 733  evaluation reward: 211.8\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2482: Policy loss: -3.272014. Value loss: 198.790451. Entropy: 0.853784.\n",
      "Iteration 2483: Policy loss: -3.826574. Value loss: 59.382374. Entropy: 0.873500.\n",
      "Iteration 2484: Policy loss: -2.993005. Value loss: 21.927874. Entropy: 0.909840.\n",
      "episode: 1034   score: 80.0  epsilon: 1.0    steps: 914  evaluation reward: 210.35\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2485: Policy loss: -0.572436. Value loss: 58.730328. Entropy: 0.725757.\n",
      "Iteration 2486: Policy loss: -0.916634. Value loss: 24.301620. Entropy: 0.772264.\n",
      "Iteration 2487: Policy loss: -0.485982. Value loss: 19.329357. Entropy: 0.733752.\n",
      "episode: 1035   score: 125.0  epsilon: 1.0    steps: 69  evaluation reward: 209.35\n",
      "episode: 1036   score: 190.0  epsilon: 1.0    steps: 500  evaluation reward: 210.2\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2488: Policy loss: 0.146542. Value loss: 29.726171. Entropy: 1.059396.\n",
      "Iteration 2489: Policy loss: 0.115919. Value loss: 19.277086. Entropy: 1.047968.\n",
      "Iteration 2490: Policy loss: 0.153328. Value loss: 15.003964. Entropy: 1.071898.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2491: Policy loss: 0.362046. Value loss: 36.546715. Entropy: 1.030847.\n",
      "Iteration 2492: Policy loss: 0.239684. Value loss: 21.279709. Entropy: 1.034425.\n",
      "Iteration 2493: Policy loss: 0.236246. Value loss: 18.636391. Entropy: 1.033214.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2494: Policy loss: 0.403181. Value loss: 38.026028. Entropy: 1.008312.\n",
      "Iteration 2495: Policy loss: 0.416280. Value loss: 20.774439. Entropy: 0.999700.\n",
      "Iteration 2496: Policy loss: 0.040884. Value loss: 16.160618. Entropy: 0.971366.\n",
      "episode: 1037   score: 80.0  epsilon: 1.0    steps: 67  evaluation reward: 208.6\n",
      "episode: 1038   score: 155.0  epsilon: 1.0    steps: 254  evaluation reward: 209.4\n",
      "episode: 1039   score: 250.0  epsilon: 1.0    steps: 327  evaluation reward: 203.9\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2497: Policy loss: 3.212372. Value loss: 14.357909. Entropy: 1.169634.\n",
      "Iteration 2498: Policy loss: 2.936838. Value loss: 9.094837. Entropy: 1.183977.\n",
      "Iteration 2499: Policy loss: 3.091810. Value loss: 7.084386. Entropy: 1.182038.\n",
      "episode: 1040   score: 210.0  epsilon: 1.0    steps: 993  evaluation reward: 201.85\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2500: Policy loss: -0.177608. Value loss: 33.452732. Entropy: 0.888728.\n",
      "Iteration 2501: Policy loss: -0.114855. Value loss: 16.518425. Entropy: 0.909180.\n",
      "Iteration 2502: Policy loss: 0.012648. Value loss: 12.167836. Entropy: 0.910216.\n",
      "episode: 1041   score: 200.0  epsilon: 1.0    steps: 503  evaluation reward: 203.05\n",
      "episode: 1042   score: 505.0  epsilon: 1.0    steps: 623  evaluation reward: 206.35\n",
      "episode: 1043   score: 315.0  epsilon: 1.0    steps: 724  evaluation reward: 208.55\n",
      "episode: 1044   score: 325.0  epsilon: 1.0    steps: 806  evaluation reward: 208.75\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2503: Policy loss: 2.127983. Value loss: 29.752705. Entropy: 0.880803.\n",
      "Iteration 2504: Policy loss: 1.858086. Value loss: 17.248631. Entropy: 0.860048.\n",
      "Iteration 2505: Policy loss: 2.070336. Value loss: 13.355219. Entropy: 0.843232.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2506: Policy loss: 0.009251. Value loss: 17.756147. Entropy: 0.853998.\n",
      "Iteration 2507: Policy loss: 0.165306. Value loss: 11.240359. Entropy: 0.933556.\n",
      "Iteration 2508: Policy loss: 0.021049. Value loss: 8.695909. Entropy: 0.901925.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2509: Policy loss: 0.266884. Value loss: 21.329712. Entropy: 1.052720.\n",
      "Iteration 2510: Policy loss: 0.681403. Value loss: 15.243259. Entropy: 1.014758.\n",
      "Iteration 2511: Policy loss: 0.502583. Value loss: 11.602806. Entropy: 1.006674.\n",
      "episode: 1045   score: 105.0  epsilon: 1.0    steps: 74  evaluation reward: 208.5\n",
      "episode: 1046   score: 175.0  epsilon: 1.0    steps: 249  evaluation reward: 208.4\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2512: Policy loss: -0.478197. Value loss: 41.596409. Entropy: 1.102337.\n",
      "Iteration 2513: Policy loss: -0.576455. Value loss: 22.427181. Entropy: 1.120111.\n",
      "Iteration 2514: Policy loss: -0.309616. Value loss: 15.377945. Entropy: 1.098397.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2515: Policy loss: 2.077212. Value loss: 38.234394. Entropy: 1.079412.\n",
      "Iteration 2516: Policy loss: 2.430000. Value loss: 19.041407. Entropy: 1.079628.\n",
      "Iteration 2517: Policy loss: 2.303970. Value loss: 15.377604. Entropy: 1.079490.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2518: Policy loss: -1.108288. Value loss: 41.627563. Entropy: 1.071697.\n",
      "Iteration 2519: Policy loss: -0.690856. Value loss: 20.466005. Entropy: 1.057880.\n",
      "Iteration 2520: Policy loss: -0.885665. Value loss: 17.723953. Entropy: 1.078502.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1047   score: 190.0  epsilon: 1.0    steps: 386  evaluation reward: 208.8\n",
      "episode: 1048   score: 230.0  epsilon: 1.0    steps: 936  evaluation reward: 210.0\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2521: Policy loss: 0.725672. Value loss: 25.292866. Entropy: 1.143489.\n",
      "Iteration 2522: Policy loss: 0.564760. Value loss: 11.624939. Entropy: 1.151441.\n",
      "Iteration 2523: Policy loss: 0.679261. Value loss: 9.844065. Entropy: 1.164894.\n",
      "episode: 1049   score: 70.0  epsilon: 1.0    steps: 58  evaluation reward: 208.15\n",
      "episode: 1050   score: 270.0  epsilon: 1.0    steps: 279  evaluation reward: 209.2\n",
      "now time :  2019-02-25 19:27:02.144937\n",
      "episode: 1051   score: 235.0  epsilon: 1.0    steps: 533  evaluation reward: 208.7\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2524: Policy loss: 2.060709. Value loss: 24.253767. Entropy: 0.841724.\n",
      "Iteration 2525: Policy loss: 1.883155. Value loss: 14.340004. Entropy: 0.842929.\n",
      "Iteration 2526: Policy loss: 2.218159. Value loss: 12.833455. Entropy: 0.838065.\n",
      "episode: 1052   score: 80.0  epsilon: 1.0    steps: 487  evaluation reward: 207.2\n",
      "episode: 1053   score: 370.0  epsilon: 1.0    steps: 659  evaluation reward: 210.05\n",
      "episode: 1054   score: 330.0  epsilon: 1.0    steps: 796  evaluation reward: 211.6\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2527: Policy loss: 0.770967. Value loss: 24.999794. Entropy: 0.814792.\n",
      "Iteration 2528: Policy loss: 1.119519. Value loss: 12.899059. Entropy: 0.881304.\n",
      "Iteration 2529: Policy loss: 0.734595. Value loss: 10.525365. Entropy: 0.847140.\n",
      "episode: 1055   score: 220.0  epsilon: 1.0    steps: 246  evaluation reward: 211.75\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2530: Policy loss: 0.409384. Value loss: 20.867683. Entropy: 0.989041.\n",
      "Iteration 2531: Policy loss: 0.734195. Value loss: 15.229421. Entropy: 1.028786.\n",
      "Iteration 2532: Policy loss: 0.447083. Value loss: 12.694940. Entropy: 1.042748.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2533: Policy loss: 0.399154. Value loss: 15.262938. Entropy: 1.165401.\n",
      "Iteration 2534: Policy loss: 0.393361. Value loss: 11.288082. Entropy: 1.131648.\n",
      "Iteration 2535: Policy loss: 0.467005. Value loss: 8.956470. Entropy: 1.124624.\n",
      "episode: 1056   score: 130.0  epsilon: 1.0    steps: 1007  evaluation reward: 211.5\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2536: Policy loss: 2.501217. Value loss: 29.214430. Entropy: 1.074483.\n",
      "Iteration 2537: Policy loss: 2.448219. Value loss: 16.172646. Entropy: 1.064361.\n",
      "Iteration 2538: Policy loss: 2.417013. Value loss: 14.689208. Entropy: 1.093130.\n",
      "episode: 1057   score: 165.0  epsilon: 1.0    steps: 111  evaluation reward: 209.15\n",
      "episode: 1058   score: 140.0  epsilon: 1.0    steps: 302  evaluation reward: 206.05\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2539: Policy loss: 0.354016. Value loss: 15.517130. Entropy: 1.052403.\n",
      "Iteration 2540: Policy loss: 0.412384. Value loss: 10.801284. Entropy: 1.061530.\n",
      "Iteration 2541: Policy loss: 0.280960. Value loss: 8.886259. Entropy: 1.051587.\n",
      "episode: 1059   score: 195.0  epsilon: 1.0    steps: 578  evaluation reward: 207.4\n",
      "episode: 1060   score: 145.0  epsilon: 1.0    steps: 788  evaluation reward: 207.25\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2542: Policy loss: 0.588269. Value loss: 32.393425. Entropy: 0.931456.\n",
      "Iteration 2543: Policy loss: 0.474851. Value loss: 18.641066. Entropy: 0.908200.\n",
      "Iteration 2544: Policy loss: 0.424510. Value loss: 18.148796. Entropy: 0.929733.\n",
      "episode: 1061   score: 80.0  epsilon: 1.0    steps: 148  evaluation reward: 205.25\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2545: Policy loss: -0.052478. Value loss: 30.554667. Entropy: 0.826371.\n",
      "Iteration 2546: Policy loss: -0.118474. Value loss: 16.747747. Entropy: 0.829711.\n",
      "Iteration 2547: Policy loss: -0.134092. Value loss: 13.976957. Entropy: 0.823207.\n",
      "episode: 1062   score: 85.0  epsilon: 1.0    steps: 297  evaluation reward: 202.8\n",
      "episode: 1063   score: 215.0  epsilon: 1.0    steps: 388  evaluation reward: 203.35\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2548: Policy loss: 0.277761. Value loss: 13.888551. Entropy: 0.983315.\n",
      "Iteration 2549: Policy loss: 0.119618. Value loss: 8.880053. Entropy: 1.002654.\n",
      "Iteration 2550: Policy loss: 0.192438. Value loss: 7.042706. Entropy: 0.974322.\n",
      "episode: 1064   score: 140.0  epsilon: 1.0    steps: 1010  evaluation reward: 202.5\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2551: Policy loss: -1.309394. Value loss: 48.661572. Entropy: 0.934466.\n",
      "Iteration 2552: Policy loss: -1.133890. Value loss: 28.367506. Entropy: 0.940972.\n",
      "Iteration 2553: Policy loss: -1.311938. Value loss: 22.989439. Entropy: 0.945218.\n",
      "episode: 1065   score: 80.0  epsilon: 1.0    steps: 516  evaluation reward: 201.25\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2554: Policy loss: 0.984994. Value loss: 25.397818. Entropy: 1.145687.\n",
      "Iteration 2555: Policy loss: 0.945299. Value loss: 14.150375. Entropy: 1.128002.\n",
      "Iteration 2556: Policy loss: 0.936957. Value loss: 11.584069. Entropy: 1.149398.\n",
      "episode: 1066   score: 205.0  epsilon: 1.0    steps: 851  evaluation reward: 201.55\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2557: Policy loss: 1.625949. Value loss: 26.174583. Entropy: 0.988607.\n",
      "Iteration 2558: Policy loss: 1.488866. Value loss: 13.545835. Entropy: 0.987357.\n",
      "Iteration 2559: Policy loss: 1.965943. Value loss: 12.265319. Entropy: 1.006691.\n",
      "episode: 1067   score: 245.0  epsilon: 1.0    steps: 40  evaluation reward: 202.95\n",
      "episode: 1068   score: 80.0  epsilon: 1.0    steps: 997  evaluation reward: 201.35\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2560: Policy loss: 1.510347. Value loss: 14.871920. Entropy: 1.084383.\n",
      "Iteration 2561: Policy loss: 1.334286. Value loss: 9.643273. Entropy: 1.091155.\n",
      "Iteration 2562: Policy loss: 1.563968. Value loss: 8.784927. Entropy: 1.087293.\n",
      "episode: 1069   score: 265.0  epsilon: 1.0    steps: 194  evaluation reward: 202.0\n",
      "episode: 1070   score: 80.0  epsilon: 1.0    steps: 603  evaluation reward: 201.3\n",
      "episode: 1071   score: 470.0  epsilon: 1.0    steps: 662  evaluation reward: 204.65\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2563: Policy loss: 0.060117. Value loss: 24.991972. Entropy: 0.854064.\n",
      "Iteration 2564: Policy loss: 0.190823. Value loss: 14.238033. Entropy: 0.880741.\n",
      "Iteration 2565: Policy loss: 0.191528. Value loss: 11.902979. Entropy: 0.867148.\n",
      "episode: 1072   score: 235.0  epsilon: 1.0    steps: 341  evaluation reward: 205.55\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2566: Policy loss: -0.824044. Value loss: 18.964012. Entropy: 0.969641.\n",
      "Iteration 2567: Policy loss: -0.658445. Value loss: 13.256022. Entropy: 0.936039.\n",
      "Iteration 2568: Policy loss: -0.813012. Value loss: 11.839687. Entropy: 0.967426.\n",
      "episode: 1073   score: 240.0  epsilon: 1.0    steps: 386  evaluation reward: 203.1\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2569: Policy loss: -0.693595. Value loss: 23.892046. Entropy: 1.156866.\n",
      "Iteration 2570: Policy loss: -0.745747. Value loss: 13.426106. Entropy: 1.148656.\n",
      "Iteration 2571: Policy loss: -0.961588. Value loss: 12.268463. Entropy: 1.154303.\n",
      "episode: 1074   score: 185.0  epsilon: 1.0    steps: 875  evaluation reward: 204.15\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2572: Policy loss: -1.781113. Value loss: 40.946632. Entropy: 0.956954.\n",
      "Iteration 2573: Policy loss: -1.271541. Value loss: 23.008825. Entropy: 0.985425.\n",
      "Iteration 2574: Policy loss: -1.508727. Value loss: 22.300016. Entropy: 0.948543.\n",
      "episode: 1075   score: 75.0  epsilon: 1.0    steps: 544  evaluation reward: 200.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2575: Policy loss: 1.504947. Value loss: 21.432014. Entropy: 1.196596.\n",
      "Iteration 2576: Policy loss: 1.752227. Value loss: 11.476622. Entropy: 1.202958.\n",
      "Iteration 2577: Policy loss: 1.493659. Value loss: 9.725437. Entropy: 1.198840.\n",
      "episode: 1076   score: 130.0  epsilon: 1.0    steps: 11  evaluation reward: 196.35\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2578: Policy loss: -4.449844. Value loss: 287.491455. Entropy: 0.999953.\n",
      "Iteration 2579: Policy loss: -3.862834. Value loss: 139.732422. Entropy: 0.970705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2580: Policy loss: -3.998431. Value loss: 136.410858. Entropy: 0.910669.\n",
      "episode: 1077   score: 120.0  epsilon: 1.0    steps: 347  evaluation reward: 195.65\n",
      "episode: 1078   score: 115.0  epsilon: 1.0    steps: 860  evaluation reward: 194.3\n",
      "episode: 1079   score: 300.0  epsilon: 1.0    steps: 907  evaluation reward: 194.15\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2581: Policy loss: 0.880338. Value loss: 19.211895. Entropy: 0.781353.\n",
      "Iteration 2582: Policy loss: 0.868049. Value loss: 10.751359. Entropy: 0.814033.\n",
      "Iteration 2583: Policy loss: 0.887824. Value loss: 8.339018. Entropy: 0.822752.\n",
      "episode: 1080   score: 310.0  epsilon: 1.0    steps: 659  evaluation reward: 192.6\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2584: Policy loss: -0.189596. Value loss: 27.343336. Entropy: 0.776964.\n",
      "Iteration 2585: Policy loss: -0.145922. Value loss: 14.545242. Entropy: 0.833082.\n",
      "Iteration 2586: Policy loss: -0.259262. Value loss: 10.997845. Entropy: 0.811780.\n",
      "episode: 1081   score: 430.0  epsilon: 1.0    steps: 178  evaluation reward: 196.35\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2587: Policy loss: -1.915753. Value loss: 241.779587. Entropy: 0.893309.\n",
      "Iteration 2588: Policy loss: -1.553188. Value loss: 177.550766. Entropy: 0.825584.\n",
      "Iteration 2589: Policy loss: -1.597588. Value loss: 100.930939. Entropy: 0.796835.\n",
      "episode: 1082   score: 435.0  epsilon: 1.0    steps: 467  evaluation reward: 199.3\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2590: Policy loss: 0.693875. Value loss: 41.487446. Entropy: 0.899626.\n",
      "Iteration 2591: Policy loss: 1.107594. Value loss: 22.401508. Entropy: 0.919206.\n",
      "Iteration 2592: Policy loss: 0.658274. Value loss: 18.155363. Entropy: 0.904983.\n",
      "episode: 1083   score: 300.0  epsilon: 1.0    steps: 591  evaluation reward: 201.15\n",
      "episode: 1084   score: 100.0  epsilon: 1.0    steps: 751  evaluation reward: 197.2\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2593: Policy loss: -0.046329. Value loss: 45.386486. Entropy: 0.933378.\n",
      "Iteration 2594: Policy loss: -0.235582. Value loss: 25.624428. Entropy: 0.930887.\n",
      "Iteration 2595: Policy loss: -0.049112. Value loss: 19.569389. Entropy: 0.924083.\n",
      "episode: 1085   score: 115.0  epsilon: 1.0    steps: 384  evaluation reward: 194.75\n",
      "episode: 1086   score: 225.0  epsilon: 1.0    steps: 853  evaluation reward: 194.95\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2596: Policy loss: 0.992741. Value loss: 38.024086. Entropy: 0.777543.\n",
      "Iteration 2597: Policy loss: 0.954896. Value loss: 22.300293. Entropy: 0.769802.\n",
      "Iteration 2598: Policy loss: 1.190774. Value loss: 16.874372. Entropy: 0.758836.\n",
      "episode: 1087   score: 295.0  epsilon: 1.0    steps: 80  evaluation reward: 194.85\n",
      "episode: 1088   score: 195.0  epsilon: 1.0    steps: 974  evaluation reward: 193.65\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2599: Policy loss: 0.582012. Value loss: 13.691428. Entropy: 0.773267.\n",
      "Iteration 2600: Policy loss: 0.529251. Value loss: 10.189879. Entropy: 0.754573.\n",
      "Iteration 2601: Policy loss: 0.437731. Value loss: 8.269943. Entropy: 0.767108.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2602: Policy loss: 1.640754. Value loss: 28.932644. Entropy: 0.745403.\n",
      "Iteration 2603: Policy loss: 2.131441. Value loss: 14.158311. Entropy: 0.742788.\n",
      "Iteration 2604: Policy loss: 1.770019. Value loss: 10.976741. Entropy: 0.788382.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2605: Policy loss: 1.233873. Value loss: 33.530212. Entropy: 1.026835.\n",
      "Iteration 2606: Policy loss: 0.797909. Value loss: 22.579660. Entropy: 1.006521.\n",
      "Iteration 2607: Policy loss: 1.112592. Value loss: 16.693594. Entropy: 0.992264.\n",
      "episode: 1089   score: 230.0  epsilon: 1.0    steps: 480  evaluation reward: 194.45\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2608: Policy loss: -0.537088. Value loss: 42.433884. Entropy: 1.032396.\n",
      "Iteration 2609: Policy loss: -0.354797. Value loss: 19.020542. Entropy: 1.005035.\n",
      "Iteration 2610: Policy loss: -0.382212. Value loss: 16.088173. Entropy: 0.994445.\n",
      "episode: 1090   score: 205.0  epsilon: 1.0    steps: 607  evaluation reward: 195.15\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2611: Policy loss: 2.604435. Value loss: 32.684719. Entropy: 1.075027.\n",
      "Iteration 2612: Policy loss: 2.622820. Value loss: 17.997034. Entropy: 1.076152.\n",
      "Iteration 2613: Policy loss: 2.345392. Value loss: 15.981576. Entropy: 1.104168.\n",
      "episode: 1091   score: 185.0  epsilon: 1.0    steps: 107  evaluation reward: 194.9\n",
      "episode: 1092   score: 180.0  epsilon: 1.0    steps: 884  evaluation reward: 194.65\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2614: Policy loss: 1.565589. Value loss: 31.199724. Entropy: 0.988086.\n",
      "Iteration 2615: Policy loss: 1.201672. Value loss: 13.301709. Entropy: 0.960240.\n",
      "Iteration 2616: Policy loss: 1.700849. Value loss: 10.843110. Entropy: 1.004857.\n",
      "episode: 1093   score: 165.0  epsilon: 1.0    steps: 259  evaluation reward: 192.6\n",
      "episode: 1094   score: 315.0  epsilon: 1.0    steps: 966  evaluation reward: 194.1\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2617: Policy loss: 1.783323. Value loss: 18.878572. Entropy: 0.895610.\n",
      "Iteration 2618: Policy loss: 1.575206. Value loss: 10.526043. Entropy: 0.925262.\n",
      "Iteration 2619: Policy loss: 1.547724. Value loss: 8.861117. Entropy: 0.928469.\n",
      "episode: 1095   score: 325.0  epsilon: 1.0    steps: 658  evaluation reward: 195.8\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2620: Policy loss: -1.763730. Value loss: 30.796871. Entropy: 0.813391.\n",
      "Iteration 2621: Policy loss: -1.975232. Value loss: 20.697178. Entropy: 0.833881.\n",
      "Iteration 2622: Policy loss: -1.925275. Value loss: 16.442055. Entropy: 0.815912.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2623: Policy loss: 1.584101. Value loss: 18.006275. Entropy: 0.941111.\n",
      "Iteration 2624: Policy loss: 1.412732. Value loss: 11.848595. Entropy: 0.943730.\n",
      "Iteration 2625: Policy loss: 1.633960. Value loss: 8.956323. Entropy: 0.942392.\n",
      "episode: 1096   score: 135.0  epsilon: 1.0    steps: 437  evaluation reward: 195.65\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2626: Policy loss: 1.220511. Value loss: 38.298901. Entropy: 0.952258.\n",
      "Iteration 2627: Policy loss: 1.361450. Value loss: 21.987782. Entropy: 0.975524.\n",
      "Iteration 2628: Policy loss: 1.260517. Value loss: 14.731684. Entropy: 0.932113.\n",
      "episode: 1097   score: 475.0  epsilon: 1.0    steps: 151  evaluation reward: 198.8\n",
      "episode: 1098   score: 205.0  epsilon: 1.0    steps: 636  evaluation reward: 200.25\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2629: Policy loss: -0.033317. Value loss: 27.049049. Entropy: 0.837517.\n",
      "Iteration 2630: Policy loss: -0.179994. Value loss: 13.494460. Entropy: 0.831102.\n",
      "Iteration 2631: Policy loss: -0.125087. Value loss: 8.869279. Entropy: 0.833918.\n",
      "episode: 1099   score: 220.0  epsilon: 1.0    steps: 299  evaluation reward: 200.1\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2632: Policy loss: 2.468216. Value loss: 26.583416. Entropy: 0.840719.\n",
      "Iteration 2633: Policy loss: 2.247996. Value loss: 17.518087. Entropy: 0.881342.\n",
      "Iteration 2634: Policy loss: 2.579637. Value loss: 15.132261. Entropy: 0.897370.\n",
      "episode: 1100   score: 195.0  epsilon: 1.0    steps: 680  evaluation reward: 199.85\n",
      "now time :  2019-02-25 19:29:08.356900\n",
      "episode: 1101   score: 250.0  epsilon: 1.0    steps: 919  evaluation reward: 200.65\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2635: Policy loss: -0.410049. Value loss: 25.020945. Entropy: 0.790955.\n",
      "Iteration 2636: Policy loss: -0.392916. Value loss: 12.476810. Entropy: 0.796508.\n",
      "Iteration 2637: Policy loss: -0.404268. Value loss: 10.296586. Entropy: 0.795694.\n",
      "episode: 1102   score: 75.0  epsilon: 1.0    steps: 478  evaluation reward: 199.15\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2638: Policy loss: -3.352230. Value loss: 314.353943. Entropy: 0.963401.\n",
      "Iteration 2639: Policy loss: -3.030112. Value loss: 176.323883. Entropy: 0.816822.\n",
      "Iteration 2640: Policy loss: -2.836662. Value loss: 129.398773. Entropy: 0.737930.\n",
      "episode: 1103   score: 350.0  epsilon: 1.0    steps: 825  evaluation reward: 200.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2641: Policy loss: 0.463513. Value loss: 42.761135. Entropy: 0.791910.\n",
      "Iteration 2642: Policy loss: 0.342273. Value loss: 24.863064. Entropy: 0.790358.\n",
      "Iteration 2643: Policy loss: 0.135857. Value loss: 20.978130. Entropy: 0.781976.\n",
      "episode: 1104   score: 220.0  epsilon: 1.0    steps: 239  evaluation reward: 201.75\n",
      "episode: 1105   score: 60.0  epsilon: 1.0    steps: 356  evaluation reward: 200.45\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2644: Policy loss: 1.328859. Value loss: 40.781490. Entropy: 0.846687.\n",
      "Iteration 2645: Policy loss: 1.929872. Value loss: 21.421097. Entropy: 0.811046.\n",
      "Iteration 2646: Policy loss: 1.724007. Value loss: 19.500700. Entropy: 0.833626.\n",
      "episode: 1106   score: 170.0  epsilon: 1.0    steps: 519  evaluation reward: 200.85\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2647: Policy loss: 1.793515. Value loss: 34.793861. Entropy: 0.843584.\n",
      "Iteration 2648: Policy loss: 1.310735. Value loss: 15.329463. Entropy: 0.828305.\n",
      "Iteration 2649: Policy loss: 1.540020. Value loss: 11.632495. Entropy: 0.850672.\n",
      "episode: 1107   score: 685.0  epsilon: 1.0    steps: 64  evaluation reward: 204.3\n",
      "episode: 1108   score: 120.0  epsilon: 1.0    steps: 824  evaluation reward: 204.55\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2650: Policy loss: -0.001709. Value loss: 23.467262. Entropy: 0.605229.\n",
      "Iteration 2651: Policy loss: 0.280465. Value loss: 18.114212. Entropy: 0.615857.\n",
      "Iteration 2652: Policy loss: 0.142294. Value loss: 14.018578. Entropy: 0.616798.\n",
      "episode: 1109   score: 75.0  epsilon: 1.0    steps: 333  evaluation reward: 203.65\n",
      "episode: 1110   score: 205.0  epsilon: 1.0    steps: 715  evaluation reward: 202.0\n",
      "episode: 1111   score: 300.0  epsilon: 1.0    steps: 966  evaluation reward: 203.65\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2653: Policy loss: -0.249379. Value loss: 18.802490. Entropy: 0.716518.\n",
      "Iteration 2654: Policy loss: -0.400007. Value loss: 11.814573. Entropy: 0.728642.\n",
      "Iteration 2655: Policy loss: -0.475938. Value loss: 9.771963. Entropy: 0.721042.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2656: Policy loss: 1.342338. Value loss: 28.854479. Entropy: 0.777420.\n",
      "Iteration 2657: Policy loss: 0.984207. Value loss: 15.005971. Entropy: 0.805910.\n",
      "Iteration 2658: Policy loss: 1.135876. Value loss: 11.955153. Entropy: 0.806411.\n",
      "episode: 1112   score: 315.0  epsilon: 1.0    steps: 428  evaluation reward: 205.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2659: Policy loss: -1.506665. Value loss: 34.508659. Entropy: 0.709531.\n",
      "Iteration 2660: Policy loss: -1.209455. Value loss: 18.976974. Entropy: 0.680585.\n",
      "Iteration 2661: Policy loss: -1.575826. Value loss: 15.996175. Entropy: 0.704367.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2662: Policy loss: -0.353845. Value loss: 195.933914. Entropy: 0.662846.\n",
      "Iteration 2663: Policy loss: -0.112891. Value loss: 180.429382. Entropy: 0.618501.\n",
      "Iteration 2664: Policy loss: 0.293069. Value loss: 100.790924. Entropy: 0.505186.\n",
      "episode: 1113   score: 215.0  epsilon: 1.0    steps: 561  evaluation reward: 206.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2665: Policy loss: 0.698983. Value loss: 29.447512. Entropy: 0.668713.\n",
      "Iteration 2666: Policy loss: 0.837086. Value loss: 18.798626. Entropy: 0.680462.\n",
      "Iteration 2667: Policy loss: 1.247349. Value loss: 14.460720. Entropy: 0.675635.\n",
      "episode: 1114   score: 50.0  epsilon: 1.0    steps: 395  evaluation reward: 204.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2668: Policy loss: 2.491036. Value loss: 20.358503. Entropy: 0.894002.\n",
      "Iteration 2669: Policy loss: 2.539365. Value loss: 12.683816. Entropy: 0.907212.\n",
      "Iteration 2670: Policy loss: 2.464174. Value loss: 9.654065. Entropy: 0.916495.\n",
      "episode: 1115   score: 165.0  epsilon: 1.0    steps: 39  evaluation reward: 205.45\n",
      "episode: 1116   score: 205.0  epsilon: 1.0    steps: 232  evaluation reward: 205.65\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2671: Policy loss: -0.979217. Value loss: 25.097387. Entropy: 0.703718.\n",
      "Iteration 2672: Policy loss: -1.028244. Value loss: 13.858440. Entropy: 0.704444.\n",
      "Iteration 2673: Policy loss: -1.085081. Value loss: 12.027405. Entropy: 0.714817.\n",
      "episode: 1117   score: 500.0  epsilon: 1.0    steps: 722  evaluation reward: 206.9\n",
      "episode: 1118   score: 230.0  epsilon: 1.0    steps: 780  evaluation reward: 207.9\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2674: Policy loss: -1.152327. Value loss: 36.219025. Entropy: 0.740029.\n",
      "Iteration 2675: Policy loss: -0.635635. Value loss: 17.936443. Entropy: 0.725906.\n",
      "Iteration 2676: Policy loss: -1.122000. Value loss: 15.011425. Entropy: 0.731246.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2677: Policy loss: -0.724055. Value loss: 46.855583. Entropy: 0.724332.\n",
      "Iteration 2678: Policy loss: -0.627957. Value loss: 27.966177. Entropy: 0.738350.\n",
      "Iteration 2679: Policy loss: -1.020662. Value loss: 20.893606. Entropy: 0.726857.\n",
      "episode: 1119   score: 340.0  epsilon: 1.0    steps: 290  evaluation reward: 209.45\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2680: Policy loss: 0.310163. Value loss: 23.978844. Entropy: 0.822886.\n",
      "Iteration 2681: Policy loss: 0.451299. Value loss: 16.453587. Entropy: 0.824212.\n",
      "Iteration 2682: Policy loss: 0.251267. Value loss: 12.337783. Entropy: 0.833033.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2683: Policy loss: -0.304575. Value loss: 37.775673. Entropy: 0.760808.\n",
      "Iteration 2684: Policy loss: -0.187992. Value loss: 24.326225. Entropy: 0.790241.\n",
      "Iteration 2685: Policy loss: -0.194890. Value loss: 17.385572. Entropy: 0.787574.\n",
      "episode: 1120   score: 115.0  epsilon: 1.0    steps: 232  evaluation reward: 209.0\n",
      "episode: 1121   score: 130.0  epsilon: 1.0    steps: 799  evaluation reward: 209.55\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2686: Policy loss: -0.752397. Value loss: 32.530598. Entropy: 0.967648.\n",
      "Iteration 2687: Policy loss: -0.699132. Value loss: 18.275415. Entropy: 0.968825.\n",
      "Iteration 2688: Policy loss: -0.584554. Value loss: 14.020427. Entropy: 0.964226.\n",
      "episode: 1122   score: 415.0  epsilon: 1.0    steps: 626  evaluation reward: 211.65\n",
      "episode: 1123   score: 155.0  epsilon: 1.0    steps: 733  evaluation reward: 211.7\n",
      "episode: 1124   score: 405.0  epsilon: 1.0    steps: 924  evaluation reward: 213.9\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2689: Policy loss: 0.702201. Value loss: 27.537737. Entropy: 0.764793.\n",
      "Iteration 2690: Policy loss: 0.869598. Value loss: 17.047207. Entropy: 0.752850.\n",
      "Iteration 2691: Policy loss: 1.028013. Value loss: 13.963121. Entropy: 0.752714.\n",
      "episode: 1125   score: 415.0  epsilon: 1.0    steps: 488  evaluation reward: 215.8\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2692: Policy loss: 0.821254. Value loss: 29.787565. Entropy: 0.740307.\n",
      "Iteration 2693: Policy loss: 1.099134. Value loss: 22.997366. Entropy: 0.740192.\n",
      "Iteration 2694: Policy loss: 0.808018. Value loss: 18.767836. Entropy: 0.735563.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2695: Policy loss: -3.099077. Value loss: 214.474823. Entropy: 0.696645.\n",
      "Iteration 2696: Policy loss: -2.568127. Value loss: 138.143921. Entropy: 0.657119.\n",
      "Iteration 2697: Policy loss: -2.919248. Value loss: 99.783035. Entropy: 0.655460.\n",
      "episode: 1126   score: 365.0  epsilon: 1.0    steps: 120  evaluation reward: 217.85\n",
      "episode: 1127   score: 210.0  epsilon: 1.0    steps: 359  evaluation reward: 219.65\n",
      "episode: 1128   score: 65.0  epsilon: 1.0    steps: 588  evaluation reward: 218.7\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2698: Policy loss: 4.048288. Value loss: 36.759045. Entropy: 0.641697.\n",
      "Iteration 2699: Policy loss: 4.058156. Value loss: 17.725288. Entropy: 0.649580.\n",
      "Iteration 2700: Policy loss: 3.903341. Value loss: 15.232964. Entropy: 0.671298.\n",
      "episode: 1129   score: 125.0  epsilon: 1.0    steps: 240  evaluation reward: 218.7\n",
      "episode: 1130   score: 90.0  epsilon: 1.0    steps: 475  evaluation reward: 217.1\n",
      "episode: 1131   score: 125.0  epsilon: 1.0    steps: 722  evaluation reward: 216.7\n",
      "Training network. lr: 0.000229. clip: 0.091715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2701: Policy loss: 2.114470. Value loss: 29.891884. Entropy: 0.614609.\n",
      "Iteration 2702: Policy loss: 1.890383. Value loss: 14.359991. Entropy: 0.631566.\n",
      "Iteration 2703: Policy loss: 2.171020. Value loss: 11.216838. Entropy: 0.606051.\n",
      "episode: 1132   score: 225.0  epsilon: 1.0    steps: 990  evaluation reward: 216.35\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2704: Policy loss: 0.307891. Value loss: 31.277531. Entropy: 0.541500.\n",
      "Iteration 2705: Policy loss: 0.460410. Value loss: 21.174202. Entropy: 0.529276.\n",
      "Iteration 2706: Policy loss: 0.533165. Value loss: 15.966892. Entropy: 0.536110.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2707: Policy loss: -0.248437. Value loss: 38.403591. Entropy: 0.783012.\n",
      "Iteration 2708: Policy loss: -0.680093. Value loss: 25.648161. Entropy: 0.816609.\n",
      "Iteration 2709: Policy loss: -0.003983. Value loss: 24.752537. Entropy: 0.785753.\n",
      "episode: 1133   score: 105.0  epsilon: 1.0    steps: 291  evaluation reward: 215.45\n",
      "episode: 1134   score: 105.0  epsilon: 1.0    steps: 737  evaluation reward: 215.7\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2710: Policy loss: 2.434216. Value loss: 34.199684. Entropy: 0.849607.\n",
      "Iteration 2711: Policy loss: 2.611617. Value loss: 19.454798. Entropy: 0.854145.\n",
      "Iteration 2712: Policy loss: 2.142183. Value loss: 15.500319. Entropy: 0.901564.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2713: Policy loss: 1.741566. Value loss: 34.318432. Entropy: 0.861033.\n",
      "Iteration 2714: Policy loss: 1.883212. Value loss: 17.440512. Entropy: 0.865940.\n",
      "Iteration 2715: Policy loss: 1.634053. Value loss: 15.300384. Entropy: 0.848681.\n",
      "episode: 1135   score: 95.0  epsilon: 1.0    steps: 171  evaluation reward: 215.4\n",
      "episode: 1136   score: 85.0  epsilon: 1.0    steps: 386  evaluation reward: 214.35\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2716: Policy loss: 0.107896. Value loss: 49.532345. Entropy: 0.810540.\n",
      "Iteration 2717: Policy loss: 0.342901. Value loss: 24.921490. Entropy: 0.801152.\n",
      "Iteration 2718: Policy loss: 0.306882. Value loss: 18.469172. Entropy: 0.838613.\n",
      "episode: 1137   score: 755.0  epsilon: 1.0    steps: 895  evaluation reward: 221.1\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2719: Policy loss: 1.197021. Value loss: 65.294250. Entropy: 0.972264.\n",
      "Iteration 2720: Policy loss: 1.053297. Value loss: 28.670658. Entropy: 0.945579.\n",
      "Iteration 2721: Policy loss: 1.238142. Value loss: 21.825617. Entropy: 0.944946.\n",
      "episode: 1138   score: 110.0  epsilon: 1.0    steps: 278  evaluation reward: 220.65\n",
      "episode: 1139   score: 340.0  epsilon: 1.0    steps: 558  evaluation reward: 221.55\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2722: Policy loss: -1.073240. Value loss: 233.130981. Entropy: 0.900685.\n",
      "Iteration 2723: Policy loss: -0.105930. Value loss: 114.799026. Entropy: 0.859085.\n",
      "Iteration 2724: Policy loss: -0.585343. Value loss: 73.771873. Entropy: 0.812946.\n",
      "episode: 1140   score: 180.0  epsilon: 1.0    steps: 742  evaluation reward: 221.25\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2725: Policy loss: 0.133007. Value loss: 43.127583. Entropy: 0.797585.\n",
      "Iteration 2726: Policy loss: 0.140017. Value loss: 25.531195. Entropy: 0.797579.\n",
      "Iteration 2727: Policy loss: 0.128528. Value loss: 21.234232. Entropy: 0.793339.\n",
      "episode: 1141   score: 620.0  epsilon: 1.0    steps: 121  evaluation reward: 225.45\n",
      "episode: 1142   score: 105.0  epsilon: 1.0    steps: 415  evaluation reward: 221.45\n",
      "episode: 1143   score: 255.0  epsilon: 1.0    steps: 897  evaluation reward: 220.85\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2728: Policy loss: -1.892604. Value loss: 30.021297. Entropy: 0.798420.\n",
      "Iteration 2729: Policy loss: -1.797357. Value loss: 18.634060. Entropy: 0.745655.\n",
      "Iteration 2730: Policy loss: -1.736818. Value loss: 15.558502. Entropy: 0.772617.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2731: Policy loss: -0.365072. Value loss: 30.345209. Entropy: 0.784356.\n",
      "Iteration 2732: Policy loss: -0.704171. Value loss: 19.286978. Entropy: 0.816817.\n",
      "Iteration 2733: Policy loss: -0.252170. Value loss: 19.117783. Entropy: 0.786612.\n",
      "episode: 1144   score: 175.0  epsilon: 1.0    steps: 213  evaluation reward: 219.35\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2734: Policy loss: 1.812663. Value loss: 25.625563. Entropy: 0.879890.\n",
      "Iteration 2735: Policy loss: 2.089373. Value loss: 12.451681. Entropy: 0.889226.\n",
      "Iteration 2736: Policy loss: 2.189063. Value loss: 10.064325. Entropy: 0.897877.\n",
      "episode: 1145   score: 75.0  epsilon: 1.0    steps: 956  evaluation reward: 219.05\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2737: Policy loss: 1.250339. Value loss: 22.950706. Entropy: 1.004290.\n",
      "Iteration 2738: Policy loss: 0.979541. Value loss: 11.313297. Entropy: 1.010636.\n",
      "Iteration 2739: Policy loss: 1.273888. Value loss: 10.387740. Entropy: 0.995162.\n",
      "episode: 1146   score: 240.0  epsilon: 1.0    steps: 613  evaluation reward: 219.7\n",
      "episode: 1147   score: 250.0  epsilon: 1.0    steps: 782  evaluation reward: 220.3\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2740: Policy loss: 1.082262. Value loss: 15.443227. Entropy: 0.899111.\n",
      "Iteration 2741: Policy loss: 1.241251. Value loss: 9.783584. Entropy: 0.927840.\n",
      "Iteration 2742: Policy loss: 1.041770. Value loss: 8.326268. Entropy: 0.917642.\n",
      "episode: 1148   score: 115.0  epsilon: 1.0    steps: 121  evaluation reward: 219.15\n",
      "episode: 1149   score: 225.0  epsilon: 1.0    steps: 313  evaluation reward: 220.7\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2743: Policy loss: -0.259226. Value loss: 36.258785. Entropy: 0.905739.\n",
      "Iteration 2744: Policy loss: -0.371343. Value loss: 22.585522. Entropy: 0.938573.\n",
      "Iteration 2745: Policy loss: -0.199343. Value loss: 18.229887. Entropy: 0.954944.\n",
      "episode: 1150   score: 230.0  epsilon: 1.0    steps: 650  evaluation reward: 220.3\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2746: Policy loss: 2.233017. Value loss: 21.192413. Entropy: 0.898608.\n",
      "Iteration 2747: Policy loss: 2.154209. Value loss: 11.982727. Entropy: 0.891726.\n",
      "Iteration 2748: Policy loss: 2.261566. Value loss: 10.509089. Entropy: 0.883744.\n",
      "now time :  2019-02-25 19:31:14.094601\n",
      "episode: 1151   score: 200.0  epsilon: 1.0    steps: 217  evaluation reward: 219.95\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2749: Policy loss: -0.087096. Value loss: 48.656460. Entropy: 0.777055.\n",
      "Iteration 2750: Policy loss: -0.173186. Value loss: 34.227077. Entropy: 0.747654.\n",
      "Iteration 2751: Policy loss: -0.483957. Value loss: 28.194937. Entropy: 0.788456.\n",
      "episode: 1152   score: 175.0  epsilon: 1.0    steps: 496  evaluation reward: 220.9\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2752: Policy loss: -0.795069. Value loss: 206.837112. Entropy: 1.002535.\n",
      "Iteration 2753: Policy loss: -2.091344. Value loss: 211.238937. Entropy: 0.991701.\n",
      "Iteration 2754: Policy loss: -1.459937. Value loss: 128.459122. Entropy: 0.956686.\n",
      "episode: 1153   score: 110.0  epsilon: 1.0    steps: 4  evaluation reward: 218.3\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2755: Policy loss: 3.267303. Value loss: 30.966301. Entropy: 0.812607.\n",
      "Iteration 2756: Policy loss: 3.216544. Value loss: 20.477167. Entropy: 0.823503.\n",
      "Iteration 2757: Policy loss: 2.766506. Value loss: 14.788544. Entropy: 0.832200.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2758: Policy loss: 1.489399. Value loss: 40.533852. Entropy: 0.925890.\n",
      "Iteration 2759: Policy loss: 1.623509. Value loss: 18.116863. Entropy: 0.940034.\n",
      "Iteration 2760: Policy loss: 1.559217. Value loss: 16.586800. Entropy: 0.918190.\n",
      "episode: 1154   score: 75.0  epsilon: 1.0    steps: 142  evaluation reward: 215.75\n",
      "episode: 1155   score: 105.0  epsilon: 1.0    steps: 499  evaluation reward: 214.6\n",
      "episode: 1156   score: 245.0  epsilon: 1.0    steps: 556  evaluation reward: 215.75\n",
      "episode: 1157   score: 465.0  epsilon: 1.0    steps: 1016  evaluation reward: 218.75\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2761: Policy loss: 1.193866. Value loss: 40.860691. Entropy: 0.968462.\n",
      "Iteration 2762: Policy loss: 1.253431. Value loss: 26.752434. Entropy: 0.953722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2763: Policy loss: 1.126503. Value loss: 20.062756. Entropy: 0.953265.\n",
      "episode: 1158   score: 230.0  epsilon: 1.0    steps: 258  evaluation reward: 219.65\n",
      "episode: 1159   score: 185.0  epsilon: 1.0    steps: 718  evaluation reward: 219.55\n",
      "episode: 1160   score: 275.0  epsilon: 1.0    steps: 773  evaluation reward: 220.85\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2764: Policy loss: -0.191535. Value loss: 14.256827. Entropy: 0.671655.\n",
      "Iteration 2765: Policy loss: 0.041501. Value loss: 8.403933. Entropy: 0.651636.\n",
      "Iteration 2766: Policy loss: 0.002474. Value loss: 8.401904. Entropy: 0.650463.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2767: Policy loss: 1.792115. Value loss: 21.700211. Entropy: 0.891616.\n",
      "Iteration 2768: Policy loss: 2.205096. Value loss: 17.571115. Entropy: 0.920692.\n",
      "Iteration 2769: Policy loss: 2.008843. Value loss: 16.087856. Entropy: 0.893884.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2770: Policy loss: -1.253142. Value loss: 24.275229. Entropy: 1.117282.\n",
      "Iteration 2771: Policy loss: -1.098260. Value loss: 16.690407. Entropy: 1.126341.\n",
      "Iteration 2772: Policy loss: -1.247008. Value loss: 14.236117. Entropy: 1.118884.\n",
      "episode: 1161   score: 170.0  epsilon: 1.0    steps: 103  evaluation reward: 221.75\n",
      "episode: 1162   score: 125.0  epsilon: 1.0    steps: 436  evaluation reward: 222.15\n",
      "episode: 1163   score: 105.0  epsilon: 1.0    steps: 841  evaluation reward: 221.05\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2773: Policy loss: 0.200312. Value loss: 8.478806. Entropy: 1.038274.\n",
      "Iteration 2774: Policy loss: 0.190760. Value loss: 5.998953. Entropy: 1.048766.\n",
      "Iteration 2775: Policy loss: 0.169387. Value loss: 4.935252. Entropy: 1.024642.\n",
      "episode: 1164   score: 110.0  epsilon: 1.0    steps: 611  evaluation reward: 220.75\n",
      "episode: 1165   score: 110.0  epsilon: 1.0    steps: 665  evaluation reward: 221.05\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2776: Policy loss: -0.663604. Value loss: 15.665293. Entropy: 0.823249.\n",
      "Iteration 2777: Policy loss: -0.709022. Value loss: 10.724255. Entropy: 0.824569.\n",
      "Iteration 2778: Policy loss: -0.653871. Value loss: 8.874093. Entropy: 0.824389.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2779: Policy loss: -0.536885. Value loss: 22.578087. Entropy: 1.012621.\n",
      "Iteration 2780: Policy loss: -0.630728. Value loss: 14.714940. Entropy: 1.012151.\n",
      "Iteration 2781: Policy loss: -0.445119. Value loss: 11.340284. Entropy: 0.980991.\n",
      "episode: 1166   score: 265.0  epsilon: 1.0    steps: 213  evaluation reward: 221.65\n",
      "episode: 1167   score: 215.0  epsilon: 1.0    steps: 371  evaluation reward: 221.35\n",
      "episode: 1168   score: 210.0  epsilon: 1.0    steps: 1020  evaluation reward: 222.65\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2782: Policy loss: 1.593793. Value loss: 20.304976. Entropy: 1.165756.\n",
      "Iteration 2783: Policy loss: 1.952849. Value loss: 12.746184. Entropy: 1.194311.\n",
      "Iteration 2784: Policy loss: 1.455972. Value loss: 11.388497. Entropy: 1.192675.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2785: Policy loss: 0.266017. Value loss: 7.949882. Entropy: 0.979810.\n",
      "Iteration 2786: Policy loss: 0.274477. Value loss: 5.921485. Entropy: 0.994318.\n",
      "Iteration 2787: Policy loss: 0.117515. Value loss: 4.651297. Entropy: 0.992202.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2788: Policy loss: 2.455637. Value loss: 20.719910. Entropy: 0.940183.\n",
      "Iteration 2789: Policy loss: 2.299148. Value loss: 11.736193. Entropy: 0.963675.\n",
      "Iteration 2790: Policy loss: 2.294394. Value loss: 10.101908. Entropy: 0.967779.\n",
      "episode: 1169   score: 105.0  epsilon: 1.0    steps: 503  evaluation reward: 221.05\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2791: Policy loss: 1.303005. Value loss: 28.240892. Entropy: 1.040711.\n",
      "Iteration 2792: Policy loss: 1.393702. Value loss: 15.242201. Entropy: 1.044545.\n",
      "Iteration 2793: Policy loss: 1.293992. Value loss: 12.660706. Entropy: 1.029742.\n",
      "episode: 1170   score: 215.0  epsilon: 1.0    steps: 782  evaluation reward: 222.4\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2794: Policy loss: -3.198855. Value loss: 199.221893. Entropy: 0.901271.\n",
      "Iteration 2795: Policy loss: -3.047380. Value loss: 125.850563. Entropy: 0.861393.\n",
      "Iteration 2796: Policy loss: -2.596552. Value loss: 116.057976. Entropy: 0.833960.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2797: Policy loss: -0.834837. Value loss: 26.905872. Entropy: 0.837519.\n",
      "Iteration 2798: Policy loss: -1.047882. Value loss: 16.718218. Entropy: 0.810660.\n",
      "Iteration 2799: Policy loss: -0.935955. Value loss: 13.952042. Entropy: 0.826183.\n",
      "episode: 1171   score: 55.0  epsilon: 1.0    steps: 465  evaluation reward: 218.25\n",
      "episode: 1172   score: 260.0  epsilon: 1.0    steps: 727  evaluation reward: 218.5\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2800: Policy loss: 1.627929. Value loss: 50.071053. Entropy: 0.855502.\n",
      "Iteration 2801: Policy loss: 2.110730. Value loss: 18.877617. Entropy: 0.841531.\n",
      "Iteration 2802: Policy loss: 1.800769. Value loss: 13.268914. Entropy: 0.875026.\n",
      "episode: 1173   score: 185.0  epsilon: 1.0    steps: 911  evaluation reward: 217.95\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2803: Policy loss: 0.316676. Value loss: 31.396717. Entropy: 0.801681.\n",
      "Iteration 2804: Policy loss: 0.441897. Value loss: 14.769263. Entropy: 0.810459.\n",
      "Iteration 2805: Policy loss: 0.326698. Value loss: 10.997883. Entropy: 0.807191.\n",
      "episode: 1174   score: 275.0  epsilon: 1.0    steps: 16  evaluation reward: 218.85\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2806: Policy loss: 0.329761. Value loss: 26.819790. Entropy: 0.937457.\n",
      "Iteration 2807: Policy loss: 0.510212. Value loss: 14.158840. Entropy: 0.975336.\n",
      "Iteration 2808: Policy loss: 0.288210. Value loss: 10.611990. Entropy: 0.966812.\n",
      "episode: 1175   score: 210.0  epsilon: 1.0    steps: 174  evaluation reward: 220.2\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2809: Policy loss: -1.728319. Value loss: 32.501125. Entropy: 1.100876.\n",
      "Iteration 2810: Policy loss: -1.887099. Value loss: 19.148455. Entropy: 1.079983.\n",
      "Iteration 2811: Policy loss: -2.161434. Value loss: 15.132360. Entropy: 1.109934.\n",
      "episode: 1176   score: 320.0  epsilon: 1.0    steps: 364  evaluation reward: 222.1\n",
      "episode: 1177   score: 225.0  epsilon: 1.0    steps: 846  evaluation reward: 223.15\n",
      "episode: 1178   score: 105.0  epsilon: 1.0    steps: 940  evaluation reward: 223.05\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2812: Policy loss: 1.989729. Value loss: 38.371391. Entropy: 0.932806.\n",
      "Iteration 2813: Policy loss: 2.279658. Value loss: 17.747324. Entropy: 0.921197.\n",
      "Iteration 2814: Policy loss: 2.245124. Value loss: 13.917378. Entropy: 0.914728.\n",
      "episode: 1179   score: 135.0  epsilon: 1.0    steps: 507  evaluation reward: 221.4\n",
      "episode: 1180   score: 550.0  epsilon: 1.0    steps: 570  evaluation reward: 223.8\n",
      "episode: 1181   score: 135.0  epsilon: 1.0    steps: 746  evaluation reward: 220.85\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2815: Policy loss: 0.844022. Value loss: 28.636017. Entropy: 0.900686.\n",
      "Iteration 2816: Policy loss: 1.154080. Value loss: 16.798023. Entropy: 0.932600.\n",
      "Iteration 2817: Policy loss: 0.760335. Value loss: 16.049496. Entropy: 0.910868.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2818: Policy loss: -0.874584. Value loss: 20.896929. Entropy: 0.923473.\n",
      "Iteration 2819: Policy loss: -0.802483. Value loss: 12.081439. Entropy: 0.908862.\n",
      "Iteration 2820: Policy loss: -0.781262. Value loss: 9.642882. Entropy: 0.906131.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2821: Policy loss: 1.992695. Value loss: 31.241625. Entropy: 1.089986.\n",
      "Iteration 2822: Policy loss: 1.897175. Value loss: 22.443033. Entropy: 1.078419.\n",
      "Iteration 2823: Policy loss: 1.895089. Value loss: 16.459366. Entropy: 1.095940.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2824: Policy loss: 0.557207. Value loss: 14.849117. Entropy: 1.068979.\n",
      "Iteration 2825: Policy loss: 0.383081. Value loss: 12.312648. Entropy: 1.090095.\n",
      "Iteration 2826: Policy loss: 0.369835. Value loss: 11.588268. Entropy: 1.084491.\n",
      "episode: 1182   score: 115.0  epsilon: 1.0    steps: 42  evaluation reward: 217.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2827: Policy loss: 1.753661. Value loss: 8.652190. Entropy: 0.810483.\n",
      "Iteration 2828: Policy loss: 1.824307. Value loss: 6.486760. Entropy: 0.807893.\n",
      "Iteration 2829: Policy loss: 1.941023. Value loss: 4.523484. Entropy: 0.816754.\n",
      "episode: 1183   score: 105.0  epsilon: 1.0    steps: 770  evaluation reward: 215.7\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2830: Policy loss: -2.835052. Value loss: 225.153717. Entropy: 0.834421.\n",
      "Iteration 2831: Policy loss: -2.921358. Value loss: 229.287338. Entropy: 0.809865.\n",
      "Iteration 2832: Policy loss: -2.442691. Value loss: 104.330894. Entropy: 0.441035.\n",
      "episode: 1184   score: 225.0  epsilon: 1.0    steps: 246  evaluation reward: 216.95\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2833: Policy loss: 0.006310. Value loss: 29.713276. Entropy: 0.711425.\n",
      "Iteration 2834: Policy loss: -0.154356. Value loss: 16.631912. Entropy: 0.724316.\n",
      "Iteration 2835: Policy loss: 0.157265. Value loss: 12.469250. Entropy: 0.723517.\n",
      "episode: 1185   score: 170.0  epsilon: 1.0    steps: 472  evaluation reward: 217.5\n",
      "episode: 1186   score: 340.0  epsilon: 1.0    steps: 555  evaluation reward: 218.65\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2836: Policy loss: -1.064024. Value loss: 19.795938. Entropy: 0.766065.\n",
      "Iteration 2837: Policy loss: -1.267081. Value loss: 10.320282. Entropy: 0.782363.\n",
      "Iteration 2838: Policy loss: -1.020153. Value loss: 8.602292. Entropy: 0.788954.\n",
      "episode: 1187   score: 250.0  epsilon: 1.0    steps: 317  evaluation reward: 218.2\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2839: Policy loss: 0.936972. Value loss: 25.455292. Entropy: 0.650351.\n",
      "Iteration 2840: Policy loss: 1.219830. Value loss: 14.646513. Entropy: 0.673136.\n",
      "Iteration 2841: Policy loss: 0.876018. Value loss: 13.641652. Entropy: 0.667063.\n",
      "episode: 1188   score: 445.0  epsilon: 1.0    steps: 908  evaluation reward: 220.7\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2842: Policy loss: 0.356902. Value loss: 18.160311. Entropy: 0.848591.\n",
      "Iteration 2843: Policy loss: 0.451116. Value loss: 10.401978. Entropy: 0.823625.\n",
      "Iteration 2844: Policy loss: 0.309888. Value loss: 8.905401. Entropy: 0.844447.\n",
      "episode: 1189   score: 100.0  epsilon: 1.0    steps: 603  evaluation reward: 219.4\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2845: Policy loss: 2.281445. Value loss: 28.845303. Entropy: 0.648002.\n",
      "Iteration 2846: Policy loss: 2.367501. Value loss: 16.345123. Entropy: 0.675964.\n",
      "Iteration 2847: Policy loss: 2.697829. Value loss: 13.492434. Entropy: 0.627431.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2848: Policy loss: 1.529438. Value loss: 11.963534. Entropy: 0.845047.\n",
      "Iteration 2849: Policy loss: 1.681831. Value loss: 6.291208. Entropy: 0.856697.\n",
      "Iteration 2850: Policy loss: 1.678120. Value loss: 5.386034. Entropy: 0.853891.\n",
      "episode: 1190   score: 195.0  epsilon: 1.0    steps: 23  evaluation reward: 219.3\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2851: Policy loss: 1.615532. Value loss: 16.322779. Entropy: 0.808236.\n",
      "Iteration 2852: Policy loss: 1.619145. Value loss: 8.899283. Entropy: 0.797240.\n",
      "Iteration 2853: Policy loss: 1.635094. Value loss: 6.717574. Entropy: 0.787702.\n",
      "episode: 1191   score: 245.0  epsilon: 1.0    steps: 148  evaluation reward: 219.9\n",
      "episode: 1192   score: 235.0  epsilon: 1.0    steps: 862  evaluation reward: 220.45\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2854: Policy loss: -0.190780. Value loss: 19.809532. Entropy: 0.819570.\n",
      "Iteration 2855: Policy loss: 0.091512. Value loss: 10.170653. Entropy: 0.832182.\n",
      "Iteration 2856: Policy loss: -0.193677. Value loss: 11.124287. Entropy: 0.809051.\n",
      "episode: 1193   score: 285.0  epsilon: 1.0    steps: 668  evaluation reward: 221.65\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2857: Policy loss: -0.440276. Value loss: 21.802582. Entropy: 0.750463.\n",
      "Iteration 2858: Policy loss: -0.452482. Value loss: 10.738128. Entropy: 0.733481.\n",
      "Iteration 2859: Policy loss: -0.735172. Value loss: 9.593595. Entropy: 0.730336.\n",
      "episode: 1194   score: 140.0  epsilon: 1.0    steps: 268  evaluation reward: 219.9\n",
      "episode: 1195   score: 170.0  epsilon: 1.0    steps: 385  evaluation reward: 218.35\n",
      "episode: 1196   score: 215.0  epsilon: 1.0    steps: 618  evaluation reward: 219.15\n",
      "episode: 1197   score: 120.0  epsilon: 1.0    steps: 979  evaluation reward: 215.6\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2860: Policy loss: 0.598942. Value loss: 13.733140. Entropy: 0.661614.\n",
      "Iteration 2861: Policy loss: 0.558032. Value loss: 8.481194. Entropy: 0.645025.\n",
      "Iteration 2862: Policy loss: 0.553755. Value loss: 7.409060. Entropy: 0.657570.\n",
      "episode: 1198   score: 50.0  epsilon: 1.0    steps: 12  evaluation reward: 214.05\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2863: Policy loss: -0.423009. Value loss: 28.770559. Entropy: 0.892278.\n",
      "Iteration 2864: Policy loss: -0.632035. Value loss: 16.165539. Entropy: 0.911840.\n",
      "Iteration 2865: Policy loss: -0.493887. Value loss: 13.857819. Entropy: 0.897187.\n",
      "episode: 1199   score: 95.0  epsilon: 1.0    steps: 799  evaluation reward: 212.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2866: Policy loss: -0.829081. Value loss: 29.265446. Entropy: 0.867223.\n",
      "Iteration 2867: Policy loss: -1.014797. Value loss: 18.145803. Entropy: 0.856652.\n",
      "Iteration 2868: Policy loss: -0.633742. Value loss: 14.267273. Entropy: 0.846146.\n",
      "episode: 1200   score: 100.0  epsilon: 1.0    steps: 964  evaluation reward: 211.85\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2869: Policy loss: -0.240657. Value loss: 30.956463. Entropy: 0.959681.\n",
      "Iteration 2870: Policy loss: -0.461591. Value loss: 16.252083. Entropy: 0.967268.\n",
      "Iteration 2871: Policy loss: -0.330167. Value loss: 13.954243. Entropy: 0.943026.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2872: Policy loss: 0.598603. Value loss: 25.522364. Entropy: 0.729187.\n",
      "Iteration 2873: Policy loss: 1.152178. Value loss: 12.855895. Entropy: 0.697713.\n",
      "Iteration 2874: Policy loss: 0.586638. Value loss: 10.782536. Entropy: 0.704897.\n",
      "now time :  2019-02-25 19:33:35.055310\n",
      "episode: 1201   score: 225.0  epsilon: 1.0    steps: 213  evaluation reward: 211.6\n",
      "episode: 1202   score: 120.0  epsilon: 1.0    steps: 389  evaluation reward: 212.05\n",
      "episode: 1203   score: 350.0  epsilon: 1.0    steps: 754  evaluation reward: 212.05\n",
      "episode: 1204   score: 110.0  epsilon: 1.0    steps: 774  evaluation reward: 210.95\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2875: Policy loss: 1.520800. Value loss: 12.460193. Entropy: 0.792513.\n",
      "Iteration 2876: Policy loss: 1.324301. Value loss: 6.016561. Entropy: 0.840749.\n",
      "Iteration 2877: Policy loss: 1.368100. Value loss: 5.173867. Entropy: 0.830523.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2878: Policy loss: -0.231588. Value loss: 20.301586. Entropy: 0.780818.\n",
      "Iteration 2879: Policy loss: 0.087639. Value loss: 17.104856. Entropy: 0.780032.\n",
      "Iteration 2880: Policy loss: -0.170195. Value loss: 11.819424. Entropy: 0.793075.\n",
      "episode: 1205   score: 220.0  epsilon: 1.0    steps: 37  evaluation reward: 212.55\n",
      "episode: 1206   score: 195.0  epsilon: 1.0    steps: 281  evaluation reward: 212.8\n",
      "episode: 1207   score: 210.0  epsilon: 1.0    steps: 537  evaluation reward: 208.05\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2881: Policy loss: 0.605366. Value loss: 24.880703. Entropy: 0.880424.\n",
      "Iteration 2882: Policy loss: 0.579628. Value loss: 16.353521. Entropy: 0.864031.\n",
      "Iteration 2883: Policy loss: 0.695617. Value loss: 13.899309. Entropy: 0.858909.\n",
      "episode: 1208   score: 80.0  epsilon: 1.0    steps: 192  evaluation reward: 207.65\n",
      "episode: 1209   score: 135.0  epsilon: 1.0    steps: 487  evaluation reward: 208.25\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2884: Policy loss: 0.451135. Value loss: 27.278484. Entropy: 0.786096.\n",
      "Iteration 2885: Policy loss: 0.265963. Value loss: 17.081835. Entropy: 0.766594.\n",
      "Iteration 2886: Policy loss: 0.234801. Value loss: 13.758688. Entropy: 0.809407.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2887: Policy loss: -1.012082. Value loss: 31.147961. Entropy: 0.841220.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2888: Policy loss: -0.848703. Value loss: 16.655890. Entropy: 0.875377.\n",
      "Iteration 2889: Policy loss: -0.747822. Value loss: 12.307539. Entropy: 0.864075.\n",
      "episode: 1210   score: 90.0  epsilon: 1.0    steps: 66  evaluation reward: 207.1\n",
      "episode: 1211   score: 265.0  epsilon: 1.0    steps: 971  evaluation reward: 206.75\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2890: Policy loss: -0.098091. Value loss: 20.789972. Entropy: 0.920588.\n",
      "Iteration 2891: Policy loss: 0.110379. Value loss: 8.354395. Entropy: 0.919643.\n",
      "Iteration 2892: Policy loss: 0.060425. Value loss: 8.025658. Entropy: 0.914214.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2893: Policy loss: -0.581834. Value loss: 25.228746. Entropy: 1.070351.\n",
      "Iteration 2894: Policy loss: -0.530033. Value loss: 13.493002. Entropy: 1.080003.\n",
      "Iteration 2895: Policy loss: -0.522137. Value loss: 10.619376. Entropy: 1.077194.\n",
      "episode: 1212   score: 180.0  epsilon: 1.0    steps: 796  evaluation reward: 205.4\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2896: Policy loss: -0.841490. Value loss: 30.231060. Entropy: 0.960562.\n",
      "Iteration 2897: Policy loss: -0.980289. Value loss: 16.037838. Entropy: 0.958561.\n",
      "Iteration 2898: Policy loss: -0.893633. Value loss: 11.782022. Entropy: 0.950374.\n",
      "episode: 1213   score: 185.0  epsilon: 1.0    steps: 200  evaluation reward: 205.1\n",
      "episode: 1214   score: 205.0  epsilon: 1.0    steps: 328  evaluation reward: 206.65\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2899: Policy loss: 1.393244. Value loss: 12.538587. Entropy: 0.796156.\n",
      "Iteration 2900: Policy loss: 1.437239. Value loss: 7.754703. Entropy: 0.810355.\n",
      "Iteration 2901: Policy loss: 1.302522. Value loss: 6.194740. Entropy: 0.788531.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2902: Policy loss: -2.590150. Value loss: 34.755020. Entropy: 0.846548.\n",
      "Iteration 2903: Policy loss: -2.359574. Value loss: 18.386614. Entropy: 0.838400.\n",
      "Iteration 2904: Policy loss: -2.554294. Value loss: 15.789857. Entropy: 0.852781.\n",
      "episode: 1215   score: 125.0  epsilon: 1.0    steps: 994  evaluation reward: 206.25\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2905: Policy loss: -0.978174. Value loss: 25.127121. Entropy: 0.987727.\n",
      "Iteration 2906: Policy loss: -0.837836. Value loss: 13.560483. Entropy: 0.986341.\n",
      "Iteration 2907: Policy loss: -1.080892. Value loss: 10.329381. Entropy: 0.997457.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2908: Policy loss: 1.707297. Value loss: 14.766183. Entropy: 1.053673.\n",
      "Iteration 2909: Policy loss: 1.570450. Value loss: 8.471218. Entropy: 1.048463.\n",
      "Iteration 2910: Policy loss: 1.758483. Value loss: 6.311816. Entropy: 1.065330.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2911: Policy loss: 0.137318. Value loss: 28.605688. Entropy: 1.006709.\n",
      "Iteration 2912: Policy loss: 0.209825. Value loss: 15.005289. Entropy: 1.009122.\n",
      "Iteration 2913: Policy loss: 0.220266. Value loss: 11.204115. Entropy: 1.004021.\n",
      "episode: 1216   score: 320.0  epsilon: 1.0    steps: 106  evaluation reward: 207.4\n",
      "episode: 1217   score: 190.0  epsilon: 1.0    steps: 209  evaluation reward: 204.3\n",
      "episode: 1218   score: 445.0  epsilon: 1.0    steps: 582  evaluation reward: 206.45\n",
      "episode: 1219   score: 325.0  epsilon: 1.0    steps: 642  evaluation reward: 206.3\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2914: Policy loss: 0.702994. Value loss: 24.563025. Entropy: 0.832596.\n",
      "Iteration 2915: Policy loss: 0.580815. Value loss: 16.249651. Entropy: 0.849716.\n",
      "Iteration 2916: Policy loss: 0.373008. Value loss: 13.471857. Entropy: 0.841291.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2917: Policy loss: 0.555811. Value loss: 14.300252. Entropy: 0.772379.\n",
      "Iteration 2918: Policy loss: 0.593638. Value loss: 8.543123. Entropy: 0.765374.\n",
      "Iteration 2919: Policy loss: 0.291617. Value loss: 7.276293. Entropy: 0.766446.\n",
      "episode: 1220   score: 290.0  epsilon: 1.0    steps: 361  evaluation reward: 208.05\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2920: Policy loss: 0.018048. Value loss: 196.415924. Entropy: 1.002679.\n",
      "Iteration 2921: Policy loss: 0.527854. Value loss: 112.402016. Entropy: 0.984266.\n",
      "Iteration 2922: Policy loss: 0.653780. Value loss: 74.771179. Entropy: 0.981026.\n",
      "episode: 1221   score: 650.0  epsilon: 1.0    steps: 452  evaluation reward: 213.25\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2923: Policy loss: 0.314992. Value loss: 20.830463. Entropy: 0.841592.\n",
      "Iteration 2924: Policy loss: 0.309402. Value loss: 11.318312. Entropy: 0.847303.\n",
      "Iteration 2925: Policy loss: 0.277561. Value loss: 8.980350. Entropy: 0.836038.\n",
      "episode: 1222   score: 210.0  epsilon: 1.0    steps: 914  evaluation reward: 211.2\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2926: Policy loss: 0.060973. Value loss: 29.503958. Entropy: 0.669001.\n",
      "Iteration 2927: Policy loss: 0.304696. Value loss: 18.323105. Entropy: 0.670410.\n",
      "Iteration 2928: Policy loss: 0.224904. Value loss: 15.401078. Entropy: 0.675244.\n",
      "episode: 1223   score: 235.0  epsilon: 1.0    steps: 868  evaluation reward: 212.0\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2929: Policy loss: 2.740606. Value loss: 24.817408. Entropy: 0.585808.\n",
      "Iteration 2930: Policy loss: 2.655882. Value loss: 12.947791. Entropy: 0.620134.\n",
      "Iteration 2931: Policy loss: 2.884436. Value loss: 10.207863. Entropy: 0.604674.\n",
      "episode: 1224   score: 275.0  epsilon: 1.0    steps: 535  evaluation reward: 210.7\n",
      "episode: 1225   score: 210.0  epsilon: 1.0    steps: 725  evaluation reward: 208.65\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2932: Policy loss: -1.516071. Value loss: 21.639585. Entropy: 0.693165.\n",
      "Iteration 2933: Policy loss: -1.617934. Value loss: 16.027246. Entropy: 0.696694.\n",
      "Iteration 2934: Policy loss: -1.594875. Value loss: 13.122633. Entropy: 0.686865.\n",
      "episode: 1226   score: 220.0  epsilon: 1.0    steps: 49  evaluation reward: 207.2\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2935: Policy loss: -0.241093. Value loss: 19.071550. Entropy: 0.645509.\n",
      "Iteration 2936: Policy loss: -0.292875. Value loss: 10.823861. Entropy: 0.626195.\n",
      "Iteration 2937: Policy loss: -0.306279. Value loss: 10.589524. Entropy: 0.661865.\n",
      "episode: 1227   score: 120.0  epsilon: 1.0    steps: 509  evaluation reward: 206.3\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2938: Policy loss: 1.530818. Value loss: 28.204599. Entropy: 0.816971.\n",
      "Iteration 2939: Policy loss: 1.322735. Value loss: 12.396615. Entropy: 0.803965.\n",
      "Iteration 2940: Policy loss: 1.344955. Value loss: 9.107836. Entropy: 0.824310.\n",
      "episode: 1228   score: 90.0  epsilon: 1.0    steps: 735  evaluation reward: 206.55\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2941: Policy loss: 1.536110. Value loss: 18.401543. Entropy: 0.827363.\n",
      "Iteration 2942: Policy loss: 1.544158. Value loss: 9.016918. Entropy: 0.850184.\n",
      "Iteration 2943: Policy loss: 1.684601. Value loss: 6.328066. Entropy: 0.829058.\n",
      "episode: 1229   score: 340.0  epsilon: 1.0    steps: 227  evaluation reward: 208.7\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2944: Policy loss: 0.844938. Value loss: 19.712942. Entropy: 0.651538.\n",
      "Iteration 2945: Policy loss: 0.774103. Value loss: 11.048961. Entropy: 0.612754.\n",
      "Iteration 2946: Policy loss: 1.050674. Value loss: 7.132490. Entropy: 0.623495.\n",
      "episode: 1230   score: 185.0  epsilon: 1.0    steps: 296  evaluation reward: 209.65\n",
      "episode: 1231   score: 110.0  epsilon: 1.0    steps: 595  evaluation reward: 209.5\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2947: Policy loss: 0.349448. Value loss: 17.436287. Entropy: 0.652636.\n",
      "Iteration 2948: Policy loss: -0.087919. Value loss: 8.523025. Entropy: 0.685430.\n",
      "Iteration 2949: Policy loss: -0.074090. Value loss: 7.270423. Entropy: 0.683381.\n",
      "episode: 1232   score: 195.0  epsilon: 1.0    steps: 970  evaluation reward: 209.2\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2950: Policy loss: 2.345524. Value loss: 20.050823. Entropy: 0.678320.\n",
      "Iteration 2951: Policy loss: 2.231912. Value loss: 15.866206. Entropy: 0.716401.\n",
      "Iteration 2952: Policy loss: 2.011077. Value loss: 13.452834. Entropy: 0.728376.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2953: Policy loss: 0.378140. Value loss: 16.246920. Entropy: 0.832906.\n",
      "Iteration 2954: Policy loss: 0.297964. Value loss: 9.642452. Entropy: 0.813841.\n",
      "Iteration 2955: Policy loss: 0.378633. Value loss: 8.717864. Entropy: 0.823287.\n",
      "episode: 1233   score: 50.0  epsilon: 1.0    steps: 562  evaluation reward: 208.65\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2956: Policy loss: 1.252321. Value loss: 21.291021. Entropy: 0.933554.\n",
      "Iteration 2957: Policy loss: 1.345525. Value loss: 13.951058. Entropy: 0.947580.\n",
      "Iteration 2958: Policy loss: 1.081517. Value loss: 12.324223. Entropy: 0.945593.\n",
      "episode: 1234   score: 135.0  epsilon: 1.0    steps: 496  evaluation reward: 208.95\n",
      "episode: 1235   score: 285.0  epsilon: 1.0    steps: 795  evaluation reward: 210.85\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2959: Policy loss: 1.247323. Value loss: 11.147813. Entropy: 0.828683.\n",
      "Iteration 2960: Policy loss: 1.348488. Value loss: 4.607421. Entropy: 0.872080.\n",
      "Iteration 2961: Policy loss: 1.208114. Value loss: 4.420590. Entropy: 0.852196.\n",
      "episode: 1236   score: 115.0  epsilon: 1.0    steps: 312  evaluation reward: 211.15\n",
      "episode: 1237   score: 240.0  epsilon: 1.0    steps: 678  evaluation reward: 206.0\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2962: Policy loss: -4.497909. Value loss: 270.509308. Entropy: 0.586776.\n",
      "Iteration 2963: Policy loss: -4.039974. Value loss: 129.974960. Entropy: 0.440148.\n",
      "Iteration 2964: Policy loss: -3.988922. Value loss: 110.447853. Entropy: 0.416922.\n",
      "episode: 1238   score: 425.0  epsilon: 1.0    steps: 152  evaluation reward: 209.15\n",
      "episode: 1239   score: 110.0  epsilon: 1.0    steps: 551  evaluation reward: 206.85\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2965: Policy loss: -1.664148. Value loss: 23.440817. Entropy: 0.553533.\n",
      "Iteration 2966: Policy loss: -1.809208. Value loss: 10.923819. Entropy: 0.542598.\n",
      "Iteration 2967: Policy loss: -1.550123. Value loss: 7.325196. Entropy: 0.536920.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2968: Policy loss: 2.001498. Value loss: 20.307253. Entropy: 0.622123.\n",
      "Iteration 2969: Policy loss: 2.237358. Value loss: 14.704396. Entropy: 0.622107.\n",
      "Iteration 2970: Policy loss: 1.992003. Value loss: 13.626593. Entropy: 0.638773.\n",
      "episode: 1240   score: 255.0  epsilon: 1.0    steps: 50  evaluation reward: 207.6\n",
      "episode: 1241   score: 220.0  epsilon: 1.0    steps: 938  evaluation reward: 203.6\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2971: Policy loss: 1.805034. Value loss: 11.470105. Entropy: 0.713893.\n",
      "Iteration 2972: Policy loss: 1.857182. Value loss: 4.201233. Entropy: 0.755977.\n",
      "Iteration 2973: Policy loss: 1.665804. Value loss: 3.217263. Entropy: 0.765358.\n",
      "episode: 1242   score: 75.0  epsilon: 1.0    steps: 190  evaluation reward: 203.3\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2974: Policy loss: 1.068378. Value loss: 14.221066. Entropy: 0.690197.\n",
      "Iteration 2975: Policy loss: 1.208428. Value loss: 7.653994. Entropy: 0.736041.\n",
      "Iteration 2976: Policy loss: 1.347628. Value loss: 6.103989. Entropy: 0.729055.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2977: Policy loss: 1.014166. Value loss: 16.118597. Entropy: 0.729010.\n",
      "Iteration 2978: Policy loss: 1.184940. Value loss: 6.593688. Entropy: 0.744541.\n",
      "Iteration 2979: Policy loss: 1.077712. Value loss: 5.314859. Entropy: 0.740226.\n",
      "episode: 1243   score: 160.0  epsilon: 1.0    steps: 375  evaluation reward: 202.35\n",
      "episode: 1244   score: 105.0  epsilon: 1.0    steps: 567  evaluation reward: 201.65\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2980: Policy loss: 1.720856. Value loss: 15.511668. Entropy: 0.703942.\n",
      "Iteration 2981: Policy loss: 1.846209. Value loss: 8.368320. Entropy: 0.724097.\n",
      "Iteration 2982: Policy loss: 1.692612. Value loss: 6.196716. Entropy: 0.710848.\n",
      "episode: 1245   score: 260.0  epsilon: 1.0    steps: 773  evaluation reward: 203.5\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2983: Policy loss: -0.466140. Value loss: 7.463045. Entropy: 0.834180.\n",
      "Iteration 2984: Policy loss: -0.527645. Value loss: 5.916792. Entropy: 0.830993.\n",
      "Iteration 2985: Policy loss: -0.491049. Value loss: 5.014686. Entropy: 0.826574.\n",
      "episode: 1246   score: 235.0  epsilon: 1.0    steps: 733  evaluation reward: 203.45\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2986: Policy loss: -2.081692. Value loss: 274.897797. Entropy: 0.661431.\n",
      "Iteration 2987: Policy loss: -2.073745. Value loss: 85.289391. Entropy: 0.741520.\n",
      "Iteration 2988: Policy loss: -2.787820. Value loss: 44.983665. Entropy: 0.691533.\n",
      "episode: 1247   score: 105.0  epsilon: 1.0    steps: 35  evaluation reward: 202.0\n",
      "episode: 1248   score: 355.0  epsilon: 1.0    steps: 975  evaluation reward: 204.4\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2989: Policy loss: 0.977962. Value loss: 23.698324. Entropy: 1.029154.\n",
      "Iteration 2990: Policy loss: 1.218555. Value loss: 8.440096. Entropy: 1.030277.\n",
      "Iteration 2991: Policy loss: 1.025190. Value loss: 7.091789. Entropy: 1.030300.\n",
      "episode: 1249   score: 105.0  epsilon: 1.0    steps: 165  evaluation reward: 203.2\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2992: Policy loss: 0.138038. Value loss: 18.677107. Entropy: 0.866614.\n",
      "Iteration 2993: Policy loss: 0.215097. Value loss: 10.172811. Entropy: 0.875994.\n",
      "Iteration 2994: Policy loss: 0.207051. Value loss: 9.173395. Entropy: 0.861235.\n",
      "episode: 1250   score: 500.0  epsilon: 1.0    steps: 456  evaluation reward: 205.9\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2995: Policy loss: -0.445267. Value loss: 12.559559. Entropy: 0.881257.\n",
      "Iteration 2996: Policy loss: -0.382714. Value loss: 7.520077. Entropy: 0.886411.\n",
      "Iteration 2997: Policy loss: -0.302264. Value loss: 5.698323. Entropy: 0.878172.\n",
      "now time :  2019-02-25 19:35:52.067003\n",
      "episode: 1251   score: 75.0  epsilon: 1.0    steps: 65  evaluation reward: 204.65\n",
      "episode: 1252   score: 310.0  epsilon: 1.0    steps: 303  evaluation reward: 206.0\n",
      "episode: 1253   score: 125.0  epsilon: 1.0    steps: 582  evaluation reward: 206.15\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2998: Policy loss: -0.094704. Value loss: 153.361801. Entropy: 1.079031.\n",
      "Iteration 2999: Policy loss: 0.128083. Value loss: 74.895737. Entropy: 1.116399.\n",
      "Iteration 3000: Policy loss: 0.275283. Value loss: 50.829094. Entropy: 1.071835.\n",
      "episode: 1254   score: 120.0  epsilon: 1.0    steps: 814  evaluation reward: 206.6\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3001: Policy loss: -1.203711. Value loss: 157.430328. Entropy: 0.730894.\n",
      "Iteration 3002: Policy loss: -0.749793. Value loss: 74.716835. Entropy: 0.719488.\n",
      "Iteration 3003: Policy loss: -1.177568. Value loss: 24.669683. Entropy: 0.711499.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3004: Policy loss: -0.844283. Value loss: 27.319330. Entropy: 0.884223.\n",
      "Iteration 3005: Policy loss: -0.887501. Value loss: 15.305901. Entropy: 0.896292.\n",
      "Iteration 3006: Policy loss: -0.986406. Value loss: 12.270929. Entropy: 0.886888.\n",
      "episode: 1255   score: 410.0  epsilon: 1.0    steps: 670  evaluation reward: 209.65\n",
      "episode: 1256   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 208.25\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3007: Policy loss: -1.912228. Value loss: 32.307777. Entropy: 0.917895.\n",
      "Iteration 3008: Policy loss: -1.327961. Value loss: 15.218325. Entropy: 0.931059.\n",
      "Iteration 3009: Policy loss: -1.740874. Value loss: 10.470059. Entropy: 0.924945.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3010: Policy loss: 3.638262. Value loss: 71.136261. Entropy: 0.710612.\n",
      "Iteration 3011: Policy loss: 2.257374. Value loss: 15.383442. Entropy: 0.620764.\n",
      "Iteration 3012: Policy loss: 2.486939. Value loss: 10.855359. Entropy: 0.655755.\n",
      "episode: 1257   score: 260.0  epsilon: 1.0    steps: 235  evaluation reward: 206.2\n",
      "episode: 1258   score: 125.0  epsilon: 1.0    steps: 595  evaluation reward: 205.15\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3013: Policy loss: 1.338900. Value loss: 27.017344. Entropy: 0.734056.\n",
      "Iteration 3014: Policy loss: 0.947915. Value loss: 14.283070. Entropy: 0.714377.\n",
      "Iteration 3015: Policy loss: 1.156330. Value loss: 10.808980. Entropy: 0.731849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1259   score: 180.0  epsilon: 1.0    steps: 353  evaluation reward: 205.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3016: Policy loss: 1.425166. Value loss: 15.222254. Entropy: 0.644505.\n",
      "Iteration 3017: Policy loss: 1.346359. Value loss: 10.893215. Entropy: 0.663329.\n",
      "Iteration 3018: Policy loss: 1.401648. Value loss: 9.993553. Entropy: 0.659369.\n",
      "episode: 1260   score: 210.0  epsilon: 1.0    steps: 4  evaluation reward: 204.45\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3019: Policy loss: -3.485860. Value loss: 26.051132. Entropy: 0.663595.\n",
      "Iteration 3020: Policy loss: -3.471474. Value loss: 12.821436. Entropy: 0.653439.\n",
      "Iteration 3021: Policy loss: -3.407848. Value loss: 9.472459. Entropy: 0.647523.\n",
      "episode: 1261   score: 290.0  epsilon: 1.0    steps: 829  evaluation reward: 205.65\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3022: Policy loss: -4.408550. Value loss: 175.415436. Entropy: 0.614439.\n",
      "Iteration 3023: Policy loss: -3.615410. Value loss: 101.528923. Entropy: 0.621467.\n",
      "Iteration 3024: Policy loss: -4.287568. Value loss: 48.332386. Entropy: 0.630373.\n",
      "episode: 1262   score: 410.0  epsilon: 1.0    steps: 767  evaluation reward: 208.5\n",
      "episode: 1263   score: 410.0  epsilon: 1.0    steps: 997  evaluation reward: 211.55\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3025: Policy loss: -1.038009. Value loss: 21.204367. Entropy: 0.705491.\n",
      "Iteration 3026: Policy loss: -0.845278. Value loss: 12.453872. Entropy: 0.720326.\n",
      "Iteration 3027: Policy loss: -0.971850. Value loss: 9.617415. Entropy: 0.709518.\n",
      "episode: 1264   score: 405.0  epsilon: 1.0    steps: 430  evaluation reward: 214.5\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3028: Policy loss: 1.273006. Value loss: 41.008572. Entropy: 0.695059.\n",
      "Iteration 3029: Policy loss: 1.177396. Value loss: 12.955017. Entropy: 0.717783.\n",
      "Iteration 3030: Policy loss: 1.358525. Value loss: 10.473158. Entropy: 0.706848.\n",
      "episode: 1265   score: 125.0  epsilon: 1.0    steps: 222  evaluation reward: 214.65\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3031: Policy loss: 1.522344. Value loss: 23.144341. Entropy: 0.550626.\n",
      "Iteration 3032: Policy loss: 1.567659. Value loss: 12.991711. Entropy: 0.567275.\n",
      "Iteration 3033: Policy loss: 1.411323. Value loss: 11.562297. Entropy: 0.557729.\n",
      "episode: 1266   score: 105.0  epsilon: 1.0    steps: 49  evaluation reward: 213.05\n",
      "episode: 1267   score: 325.0  epsilon: 1.0    steps: 383  evaluation reward: 214.15\n",
      "episode: 1268   score: 210.0  epsilon: 1.0    steps: 526  evaluation reward: 214.15\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3034: Policy loss: -1.064382. Value loss: 293.021545. Entropy: 0.742768.\n",
      "Iteration 3035: Policy loss: -0.862617. Value loss: 148.928436. Entropy: 0.674568.\n",
      "Iteration 3036: Policy loss: -0.850516. Value loss: 97.873940. Entropy: 0.678593.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3037: Policy loss: 0.682241. Value loss: 26.126297. Entropy: 0.576583.\n",
      "Iteration 3038: Policy loss: 0.573012. Value loss: 12.857514. Entropy: 0.574075.\n",
      "Iteration 3039: Policy loss: 0.492098. Value loss: 11.541721. Entropy: 0.597924.\n",
      "episode: 1269   score: 110.0  epsilon: 1.0    steps: 783  evaluation reward: 214.2\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3040: Policy loss: -0.314669. Value loss: 38.913212. Entropy: 0.627042.\n",
      "Iteration 3041: Policy loss: -0.157067. Value loss: 22.539455. Entropy: 0.625716.\n",
      "Iteration 3042: Policy loss: -0.308066. Value loss: 15.371982. Entropy: 0.603293.\n",
      "episode: 1270   score: 105.0  epsilon: 1.0    steps: 518  evaluation reward: 213.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3043: Policy loss: 1.295812. Value loss: 70.095161. Entropy: 0.509718.\n",
      "Iteration 3044: Policy loss: 1.435563. Value loss: 37.252701. Entropy: 0.517524.\n",
      "Iteration 3045: Policy loss: 0.978739. Value loss: 25.371675. Entropy: 0.523659.\n",
      "episode: 1271   score: 180.0  epsilon: 1.0    steps: 484  evaluation reward: 214.35\n",
      "episode: 1272   score: 210.0  epsilon: 1.0    steps: 722  evaluation reward: 213.85\n",
      "episode: 1273   score: 210.0  epsilon: 1.0    steps: 972  evaluation reward: 214.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3046: Policy loss: 1.760712. Value loss: 34.433956. Entropy: 0.638592.\n",
      "Iteration 3047: Policy loss: 1.611927. Value loss: 16.134930. Entropy: 0.609974.\n",
      "Iteration 3048: Policy loss: 1.671670. Value loss: 12.672455. Entropy: 0.624131.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3049: Policy loss: -1.421309. Value loss: 258.100586. Entropy: 0.580414.\n",
      "Iteration 3050: Policy loss: -1.037627. Value loss: 177.972595. Entropy: 0.591372.\n",
      "Iteration 3051: Policy loss: -1.204245. Value loss: 170.515091. Entropy: 0.555520.\n",
      "episode: 1274   score: 180.0  epsilon: 1.0    steps: 132  evaluation reward: 213.15\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3052: Policy loss: 2.457466. Value loss: 35.566387. Entropy: 0.569459.\n",
      "Iteration 3053: Policy loss: 2.659826. Value loss: 16.528210. Entropy: 0.567517.\n",
      "Iteration 3054: Policy loss: 2.817052. Value loss: 13.376898. Entropy: 0.566690.\n",
      "episode: 1275   score: 410.0  epsilon: 1.0    steps: 19  evaluation reward: 215.15\n",
      "episode: 1276   score: 155.0  epsilon: 1.0    steps: 284  evaluation reward: 213.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3055: Policy loss: 1.145569. Value loss: 13.670362. Entropy: 0.752319.\n",
      "Iteration 3056: Policy loss: 1.432050. Value loss: 7.210872. Entropy: 0.754842.\n",
      "Iteration 3057: Policy loss: 1.356626. Value loss: 4.768925. Entropy: 0.778218.\n",
      "episode: 1277   score: 155.0  epsilon: 1.0    steps: 815  evaluation reward: 212.8\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3058: Policy loss: 1.212167. Value loss: 19.658646. Entropy: 0.535939.\n",
      "Iteration 3059: Policy loss: 1.351651. Value loss: 10.599427. Entropy: 0.496179.\n",
      "Iteration 3060: Policy loss: 1.370916. Value loss: 8.743733. Entropy: 0.526342.\n",
      "episode: 1278   score: 180.0  epsilon: 1.0    steps: 554  evaluation reward: 213.55\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3061: Policy loss: 1.985456. Value loss: 13.188625. Entropy: 0.768178.\n",
      "Iteration 3062: Policy loss: 1.947664. Value loss: 7.078159. Entropy: 0.787109.\n",
      "Iteration 3063: Policy loss: 2.062300. Value loss: 5.100581. Entropy: 0.735192.\n",
      "episode: 1279   score: 210.0  epsilon: 1.0    steps: 1018  evaluation reward: 214.3\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3064: Policy loss: -0.268800. Value loss: 22.442974. Entropy: 0.739080.\n",
      "Iteration 3065: Policy loss: -0.313791. Value loss: 14.713834. Entropy: 0.731981.\n",
      "Iteration 3066: Policy loss: -0.223968. Value loss: 14.108729. Entropy: 0.739227.\n",
      "episode: 1280   score: 210.0  epsilon: 1.0    steps: 455  evaluation reward: 210.9\n",
      "episode: 1281   score: 210.0  epsilon: 1.0    steps: 711  evaluation reward: 211.65\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3067: Policy loss: 1.151338. Value loss: 8.299269. Entropy: 0.721664.\n",
      "Iteration 3068: Policy loss: 1.299249. Value loss: 6.166384. Entropy: 0.742587.\n",
      "Iteration 3069: Policy loss: 1.270714. Value loss: 4.039666. Entropy: 0.755526.\n",
      "episode: 1282   score: 210.0  epsilon: 1.0    steps: 249  evaluation reward: 212.6\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3070: Policy loss: 0.595198. Value loss: 15.508273. Entropy: 0.688281.\n",
      "Iteration 3071: Policy loss: 0.633291. Value loss: 7.519236. Entropy: 0.707526.\n",
      "Iteration 3072: Policy loss: 0.507974. Value loss: 9.224655. Entropy: 0.687873.\n",
      "episode: 1283   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 213.65\n",
      "episode: 1284   score: 210.0  epsilon: 1.0    steps: 353  evaluation reward: 213.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3073: Policy loss: 0.607899. Value loss: 11.584094. Entropy: 0.825768.\n",
      "Iteration 3074: Policy loss: 0.509807. Value loss: 8.323385. Entropy: 0.808568.\n",
      "Iteration 3075: Policy loss: 0.548177. Value loss: 7.182713. Entropy: 0.822708.\n",
      "episode: 1285   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 213.9\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3076: Policy loss: 0.114191. Value loss: 10.866222. Entropy: 0.786795.\n",
      "Iteration 3077: Policy loss: 0.142718. Value loss: 7.445395. Entropy: 0.768383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3078: Policy loss: 0.169944. Value loss: 5.868052. Entropy: 0.780933.\n",
      "episode: 1286   score: 210.0  epsilon: 1.0    steps: 629  evaluation reward: 212.6\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3079: Policy loss: 1.008137. Value loss: 8.833015. Entropy: 0.787580.\n",
      "Iteration 3080: Policy loss: 1.040024. Value loss: 5.390838. Entropy: 0.778786.\n",
      "Iteration 3081: Policy loss: 0.906864. Value loss: 5.214595. Entropy: 0.798417.\n",
      "episode: 1287   score: 80.0  epsilon: 1.0    steps: 103  evaluation reward: 210.9\n",
      "episode: 1288   score: 50.0  epsilon: 1.0    steps: 135  evaluation reward: 206.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3082: Policy loss: 0.145028. Value loss: 10.896179. Entropy: 0.840097.\n",
      "Iteration 3083: Policy loss: 0.244910. Value loss: 6.127170. Entropy: 0.862992.\n",
      "Iteration 3084: Policy loss: 0.218970. Value loss: 3.954879. Entropy: 0.854525.\n",
      "episode: 1289   score: 145.0  epsilon: 1.0    steps: 737  evaluation reward: 207.4\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3085: Policy loss: 0.502732. Value loss: 20.777296. Entropy: 0.709857.\n",
      "Iteration 3086: Policy loss: 0.447250. Value loss: 12.506254. Entropy: 0.735758.\n",
      "Iteration 3087: Policy loss: 0.277483. Value loss: 9.170220. Entropy: 0.725164.\n",
      "episode: 1290   score: 215.0  epsilon: 1.0    steps: 401  evaluation reward: 207.6\n",
      "episode: 1291   score: 55.0  epsilon: 1.0    steps: 618  evaluation reward: 205.7\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3088: Policy loss: -0.922727. Value loss: 14.876258. Entropy: 0.745689.\n",
      "Iteration 3089: Policy loss: -0.711848. Value loss: 8.351816. Entropy: 0.776428.\n",
      "Iteration 3090: Policy loss: -0.901150. Value loss: 7.051762. Entropy: 0.766052.\n",
      "episode: 1292   score: 230.0  epsilon: 1.0    steps: 966  evaluation reward: 205.65\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3091: Policy loss: 0.683425. Value loss: 12.718767. Entropy: 0.613467.\n",
      "Iteration 3092: Policy loss: 0.766930. Value loss: 8.561505. Entropy: 0.618546.\n",
      "Iteration 3093: Policy loss: 0.584925. Value loss: 6.044351. Entropy: 0.607041.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3094: Policy loss: -0.124226. Value loss: 12.353553. Entropy: 0.855228.\n",
      "Iteration 3095: Policy loss: -0.325120. Value loss: 5.541968. Entropy: 0.884041.\n",
      "Iteration 3096: Policy loss: -0.313818. Value loss: 4.332008. Entropy: 0.853176.\n",
      "episode: 1293   score: 105.0  epsilon: 1.0    steps: 190  evaluation reward: 203.85\n",
      "episode: 1294   score: 230.0  epsilon: 1.0    steps: 350  evaluation reward: 204.75\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3097: Policy loss: -0.409983. Value loss: 9.089734. Entropy: 0.758209.\n",
      "Iteration 3098: Policy loss: -0.259561. Value loss: 5.327702. Entropy: 0.754849.\n",
      "Iteration 3099: Policy loss: -0.362599. Value loss: 5.537090. Entropy: 0.739422.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3100: Policy loss: 0.082437. Value loss: 8.233186. Entropy: 0.794042.\n",
      "Iteration 3101: Policy loss: 0.128041. Value loss: 6.153340. Entropy: 0.748486.\n",
      "Iteration 3102: Policy loss: 0.162555. Value loss: 4.573697. Entropy: 0.746366.\n",
      "episode: 1295   score: 105.0  epsilon: 1.0    steps: 420  evaluation reward: 204.1\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3103: Policy loss: -0.999840. Value loss: 15.830790. Entropy: 0.778794.\n",
      "Iteration 3104: Policy loss: -1.046230. Value loss: 10.627803. Entropy: 0.784734.\n",
      "Iteration 3105: Policy loss: -1.043453. Value loss: 8.612653. Entropy: 0.766981.\n",
      "episode: 1296   score: 275.0  epsilon: 1.0    steps: 55  evaluation reward: 204.7\n",
      "episode: 1297   score: 305.0  epsilon: 1.0    steps: 587  evaluation reward: 206.55\n",
      "episode: 1298   score: 230.0  epsilon: 1.0    steps: 855  evaluation reward: 208.35\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3106: Policy loss: -1.344269. Value loss: 287.350677. Entropy: 0.699184.\n",
      "Iteration 3107: Policy loss: -1.154056. Value loss: 211.124756. Entropy: 0.676726.\n",
      "Iteration 3108: Policy loss: -1.112834. Value loss: 191.300140. Entropy: 0.649037.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3109: Policy loss: -0.521867. Value loss: 20.111696. Entropy: 0.655534.\n",
      "Iteration 3110: Policy loss: -0.305321. Value loss: 10.348082. Entropy: 0.654012.\n",
      "Iteration 3111: Policy loss: -0.647688. Value loss: 7.081432. Entropy: 0.645103.\n",
      "episode: 1299   score: 110.0  epsilon: 1.0    steps: 204  evaluation reward: 208.5\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3112: Policy loss: 0.848171. Value loss: 9.576519. Entropy: 0.745257.\n",
      "Iteration 3113: Policy loss: 0.743639. Value loss: 6.181264. Entropy: 0.753590.\n",
      "Iteration 3114: Policy loss: 0.854601. Value loss: 5.207496. Entropy: 0.745115.\n",
      "episode: 1300   score: 115.0  epsilon: 1.0    steps: 292  evaluation reward: 208.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3115: Policy loss: 0.016558. Value loss: 10.774305. Entropy: 0.878574.\n",
      "Iteration 3116: Policy loss: -0.120966. Value loss: 3.971180. Entropy: 0.874929.\n",
      "Iteration 3117: Policy loss: 0.097581. Value loss: 3.568333. Entropy: 0.865171.\n",
      "now time :  2019-02-25 19:38:08.613178\n",
      "episode: 1301   score: 315.0  epsilon: 1.0    steps: 760  evaluation reward: 209.55\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3118: Policy loss: -2.950606. Value loss: 160.219345. Entropy: 0.761720.\n",
      "Iteration 3119: Policy loss: -2.847962. Value loss: 48.829437. Entropy: 0.795979.\n",
      "Iteration 3120: Policy loss: -2.930283. Value loss: 35.620716. Entropy: 0.780288.\n",
      "episode: 1302   score: 105.0  epsilon: 1.0    steps: 100  evaluation reward: 209.4\n",
      "episode: 1303   score: 180.0  epsilon: 1.0    steps: 460  evaluation reward: 207.7\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3121: Policy loss: -0.531053. Value loss: 18.958746. Entropy: 0.848292.\n",
      "Iteration 3122: Policy loss: -0.412984. Value loss: 9.729694. Entropy: 0.841969.\n",
      "Iteration 3123: Policy loss: -0.407968. Value loss: 8.417096. Entropy: 0.806172.\n",
      "episode: 1304   score: 175.0  epsilon: 1.0    steps: 624  evaluation reward: 208.35\n",
      "episode: 1305   score: 485.0  epsilon: 1.0    steps: 955  evaluation reward: 211.0\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3124: Policy loss: 0.317338. Value loss: 12.324437. Entropy: 0.535949.\n",
      "Iteration 3125: Policy loss: 0.443702. Value loss: 8.371492. Entropy: 0.502910.\n",
      "Iteration 3126: Policy loss: 0.381523. Value loss: 6.390730. Entropy: 0.538880.\n",
      "episode: 1306   score: 105.0  epsilon: 1.0    steps: 226  evaluation reward: 210.1\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3127: Policy loss: 0.705823. Value loss: 5.817357. Entropy: 0.789909.\n",
      "Iteration 3128: Policy loss: 0.765079. Value loss: 5.186068. Entropy: 0.810827.\n",
      "Iteration 3129: Policy loss: 0.693541. Value loss: 4.608301. Entropy: 0.817745.\n",
      "episode: 1307   score: 110.0  epsilon: 1.0    steps: 345  evaluation reward: 209.1\n",
      "episode: 1308   score: 255.0  epsilon: 1.0    steps: 870  evaluation reward: 210.85\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3130: Policy loss: 0.384243. Value loss: 15.613016. Entropy: 0.876767.\n",
      "Iteration 3131: Policy loss: 0.414248. Value loss: 12.153233. Entropy: 0.862250.\n",
      "Iteration 3132: Policy loss: 0.391699. Value loss: 10.499787. Entropy: 0.864787.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3133: Policy loss: -1.093806. Value loss: 10.697013. Entropy: 0.669758.\n",
      "Iteration 3134: Policy loss: -1.157540. Value loss: 7.077232. Entropy: 0.631471.\n",
      "Iteration 3135: Policy loss: -1.073625. Value loss: 6.254504. Entropy: 0.658034.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3136: Policy loss: -1.212690. Value loss: 14.754243. Entropy: 0.743944.\n",
      "Iteration 3137: Policy loss: -1.179395. Value loss: 10.787148. Entropy: 0.745509.\n",
      "Iteration 3138: Policy loss: -1.180104. Value loss: 9.019835. Entropy: 0.747927.\n",
      "episode: 1309   score: 180.0  epsilon: 1.0    steps: 497  evaluation reward: 211.3\n",
      "episode: 1310   score: 210.0  epsilon: 1.0    steps: 700  evaluation reward: 212.5\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3139: Policy loss: 0.307857. Value loss: 12.286011. Entropy: 0.650348.\n",
      "Iteration 3140: Policy loss: 0.106660. Value loss: 6.398529. Entropy: 0.650630.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3141: Policy loss: 0.364038. Value loss: 6.239238. Entropy: 0.646564.\n",
      "episode: 1311   score: 210.0  epsilon: 1.0    steps: 29  evaluation reward: 211.95\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3142: Policy loss: -3.130781. Value loss: 122.970055. Entropy: 0.721439.\n",
      "Iteration 3143: Policy loss: -2.949334. Value loss: 35.962257. Entropy: 0.759153.\n",
      "Iteration 3144: Policy loss: -2.926271. Value loss: 23.995987. Entropy: 0.686494.\n",
      "episode: 1312   score: 185.0  epsilon: 1.0    steps: 538  evaluation reward: 212.0\n",
      "episode: 1313   score: 210.0  epsilon: 1.0    steps: 919  evaluation reward: 212.25\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3145: Policy loss: -0.172681. Value loss: 34.389004. Entropy: 0.732885.\n",
      "Iteration 3146: Policy loss: 0.006932. Value loss: 17.535055. Entropy: 0.722324.\n",
      "Iteration 3147: Policy loss: -0.037663. Value loss: 13.839320. Entropy: 0.768039.\n",
      "episode: 1314   score: 380.0  epsilon: 1.0    steps: 135  evaluation reward: 214.0\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3148: Policy loss: 0.944821. Value loss: 9.213485. Entropy: 0.743680.\n",
      "Iteration 3149: Policy loss: 0.910442. Value loss: 5.195706. Entropy: 0.716871.\n",
      "Iteration 3150: Policy loss: 0.952675. Value loss: 5.440497. Entropy: 0.730336.\n",
      "episode: 1315   score: 180.0  epsilon: 1.0    steps: 258  evaluation reward: 214.55\n",
      "episode: 1316   score: 180.0  epsilon: 1.0    steps: 791  evaluation reward: 213.15\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3151: Policy loss: 0.674190. Value loss: 4.896885. Entropy: 0.824793.\n",
      "Iteration 3152: Policy loss: 0.616896. Value loss: 3.400386. Entropy: 0.836228.\n",
      "Iteration 3153: Policy loss: 0.617636. Value loss: 3.176652. Entropy: 0.823524.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3154: Policy loss: -1.341048. Value loss: 167.945511. Entropy: 0.681156.\n",
      "Iteration 3155: Policy loss: -1.224940. Value loss: 62.443855. Entropy: 0.645477.\n",
      "Iteration 3156: Policy loss: -0.942309. Value loss: 24.664978. Entropy: 0.577125.\n",
      "episode: 1317   score: 105.0  epsilon: 1.0    steps: 83  evaluation reward: 212.3\n",
      "episode: 1318   score: 110.0  epsilon: 1.0    steps: 462  evaluation reward: 208.95\n",
      "episode: 1319   score: 305.0  epsilon: 1.0    steps: 675  evaluation reward: 208.75\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3157: Policy loss: 5.136875. Value loss: 102.553024. Entropy: 0.723146.\n",
      "Iteration 3158: Policy loss: 4.468436. Value loss: 33.516273. Entropy: 0.730803.\n",
      "Iteration 3159: Policy loss: 4.725357. Value loss: 32.478176. Entropy: 0.751608.\n",
      "episode: 1320   score: 125.0  epsilon: 1.0    steps: 634  evaluation reward: 207.1\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3160: Policy loss: 1.366900. Value loss: 62.096287. Entropy: 0.669930.\n",
      "Iteration 3161: Policy loss: 1.767511. Value loss: 22.819571. Entropy: 0.661405.\n",
      "Iteration 3162: Policy loss: 1.148393. Value loss: 15.984211. Entropy: 0.681975.\n",
      "episode: 1321   score: 160.0  epsilon: 1.0    steps: 969  evaluation reward: 202.2\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3163: Policy loss: 0.830157. Value loss: 12.981961. Entropy: 0.643513.\n",
      "Iteration 3164: Policy loss: 0.947451. Value loss: 7.381999. Entropy: 0.623864.\n",
      "Iteration 3165: Policy loss: 0.984653. Value loss: 6.024662. Entropy: 0.632828.\n",
      "episode: 1322   score: 105.0  epsilon: 1.0    steps: 813  evaluation reward: 201.15\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3166: Policy loss: 0.306924. Value loss: 15.287052. Entropy: 0.646519.\n",
      "Iteration 3167: Policy loss: 0.355254. Value loss: 8.897713. Entropy: 0.672860.\n",
      "Iteration 3168: Policy loss: 0.423736. Value loss: 7.059643. Entropy: 0.692475.\n",
      "episode: 1323   score: 210.0  epsilon: 1.0    steps: 217  evaluation reward: 200.9\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3169: Policy loss: -0.179115. Value loss: 9.963618. Entropy: 0.772320.\n",
      "Iteration 3170: Policy loss: -0.268464. Value loss: 7.481231. Entropy: 0.779906.\n",
      "Iteration 3171: Policy loss: -0.061299. Value loss: 5.741830. Entropy: 0.793475.\n",
      "episode: 1324   score: 220.0  epsilon: 1.0    steps: 325  evaluation reward: 200.35\n",
      "episode: 1325   score: 105.0  epsilon: 1.0    steps: 510  evaluation reward: 199.3\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3172: Policy loss: 1.629674. Value loss: 19.976244. Entropy: 0.771503.\n",
      "Iteration 3173: Policy loss: 1.471941. Value loss: 9.867712. Entropy: 0.761765.\n",
      "Iteration 3174: Policy loss: 1.481764. Value loss: 8.593931. Entropy: 0.756796.\n",
      "episode: 1326   score: 205.0  epsilon: 1.0    steps: 734  evaluation reward: 199.15\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3175: Policy loss: 0.187138. Value loss: 7.892421. Entropy: 0.578344.\n",
      "Iteration 3176: Policy loss: 0.173683. Value loss: 6.343853. Entropy: 0.578992.\n",
      "Iteration 3177: Policy loss: 0.084411. Value loss: 5.441085. Entropy: 0.585001.\n",
      "episode: 1327   score: 165.0  epsilon: 1.0    steps: 60  evaluation reward: 199.6\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3178: Policy loss: 1.139664. Value loss: 11.635397. Entropy: 0.707671.\n",
      "Iteration 3179: Policy loss: 1.036446. Value loss: 6.668462. Entropy: 0.738533.\n",
      "Iteration 3180: Policy loss: 1.032568. Value loss: 5.011465. Entropy: 0.733195.\n",
      "episode: 1328   score: 210.0  epsilon: 1.0    steps: 543  evaluation reward: 200.8\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3181: Policy loss: 1.583969. Value loss: 16.520195. Entropy: 0.779022.\n",
      "Iteration 3182: Policy loss: 1.595652. Value loss: 9.198539. Entropy: 0.798665.\n",
      "Iteration 3183: Policy loss: 1.734505. Value loss: 6.440305. Entropy: 0.793742.\n",
      "episode: 1329   score: 185.0  epsilon: 1.0    steps: 879  evaluation reward: 199.25\n",
      "episode: 1330   score: 160.0  epsilon: 1.0    steps: 902  evaluation reward: 199.0\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3184: Policy loss: 0.497537. Value loss: 6.625580. Entropy: 0.791374.\n",
      "Iteration 3185: Policy loss: 0.560840. Value loss: 4.215788. Entropy: 0.823379.\n",
      "Iteration 3186: Policy loss: 0.548074. Value loss: 4.847425. Entropy: 0.817007.\n",
      "episode: 1331   score: 145.0  epsilon: 1.0    steps: 247  evaluation reward: 199.35\n",
      "episode: 1332   score: 105.0  epsilon: 1.0    steps: 370  evaluation reward: 198.45\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3187: Policy loss: 0.836314. Value loss: 13.852224. Entropy: 0.758393.\n",
      "Iteration 3188: Policy loss: 1.009092. Value loss: 7.178098. Entropy: 0.755246.\n",
      "Iteration 3189: Policy loss: 0.837392. Value loss: 6.393436. Entropy: 0.780522.\n",
      "episode: 1333   score: 105.0  epsilon: 1.0    steps: 440  evaluation reward: 199.0\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3190: Policy loss: -0.976565. Value loss: 12.283330. Entropy: 0.770394.\n",
      "Iteration 3191: Policy loss: -0.913872. Value loss: 6.878928. Entropy: 0.750428.\n",
      "Iteration 3192: Policy loss: -1.094899. Value loss: 6.815780. Entropy: 0.753797.\n",
      "episode: 1334   score: 105.0  epsilon: 1.0    steps: 53  evaluation reward: 198.7\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3193: Policy loss: -2.471239. Value loss: 151.208817. Entropy: 0.624092.\n",
      "Iteration 3194: Policy loss: -3.375016. Value loss: 117.504509. Entropy: 0.584197.\n",
      "Iteration 3195: Policy loss: -2.828759. Value loss: 57.768234. Entropy: 0.550177.\n",
      "episode: 1335   score: 105.0  epsilon: 1.0    steps: 577  evaluation reward: 196.9\n",
      "episode: 1336   score: 360.0  epsilon: 1.0    steps: 647  evaluation reward: 199.35\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3196: Policy loss: 0.679581. Value loss: 12.374353. Entropy: 0.582629.\n",
      "Iteration 3197: Policy loss: 0.511816. Value loss: 4.788925. Entropy: 0.585525.\n",
      "Iteration 3198: Policy loss: 0.687424. Value loss: 3.913402. Entropy: 0.586408.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3199: Policy loss: 0.364820. Value loss: 40.836658. Entropy: 0.584517.\n",
      "Iteration 3200: Policy loss: 0.432102. Value loss: 17.105362. Entropy: 0.584810.\n",
      "Iteration 3201: Policy loss: 0.719854. Value loss: 11.611362. Entropy: 0.606524.\n",
      "episode: 1337   score: 235.0  epsilon: 1.0    steps: 953  evaluation reward: 199.3\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3202: Policy loss: 1.223355. Value loss: 15.732304. Entropy: 0.684432.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3203: Policy loss: 1.217135. Value loss: 9.262335. Entropy: 0.771197.\n",
      "Iteration 3204: Policy loss: 1.145019. Value loss: 6.661269. Entropy: 0.817642.\n",
      "episode: 1338   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 196.1\n",
      "episode: 1339   score: 185.0  epsilon: 1.0    steps: 789  evaluation reward: 196.85\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3205: Policy loss: 1.103035. Value loss: 22.094019. Entropy: 0.789104.\n",
      "Iteration 3206: Policy loss: 1.219581. Value loss: 12.655103. Entropy: 0.796680.\n",
      "Iteration 3207: Policy loss: 0.899889. Value loss: 9.895728. Entropy: 0.776319.\n",
      "episode: 1340   score: 195.0  epsilon: 1.0    steps: 279  evaluation reward: 196.25\n",
      "episode: 1341   score: 105.0  epsilon: 1.0    steps: 401  evaluation reward: 195.1\n",
      "episode: 1342   score: 105.0  epsilon: 1.0    steps: 755  evaluation reward: 195.4\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3208: Policy loss: 2.106619. Value loss: 40.401222. Entropy: 0.670679.\n",
      "Iteration 3209: Policy loss: 2.129223. Value loss: 14.741074. Entropy: 0.686894.\n",
      "Iteration 3210: Policy loss: 1.947277. Value loss: 10.206082. Entropy: 0.671086.\n",
      "episode: 1343   score: 195.0  epsilon: 1.0    steps: 95  evaluation reward: 195.75\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3211: Policy loss: 1.578073. Value loss: 16.447575. Entropy: 0.709855.\n",
      "Iteration 3212: Policy loss: 1.427402. Value loss: 10.288798. Entropy: 0.703563.\n",
      "Iteration 3213: Policy loss: 1.453461. Value loss: 7.405065. Entropy: 0.730503.\n",
      "episode: 1344   score: 75.0  epsilon: 1.0    steps: 231  evaluation reward: 195.45\n",
      "episode: 1345   score: 155.0  epsilon: 1.0    steps: 610  evaluation reward: 194.4\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3214: Policy loss: 1.237105. Value loss: 14.503196. Entropy: 0.840106.\n",
      "Iteration 3215: Policy loss: 1.021850. Value loss: 10.567561. Entropy: 0.838167.\n",
      "Iteration 3216: Policy loss: 1.141528. Value loss: 6.961198. Entropy: 0.830141.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3217: Policy loss: 0.753078. Value loss: 5.655262. Entropy: 0.769795.\n",
      "Iteration 3218: Policy loss: 0.887469. Value loss: 3.789577. Entropy: 0.797086.\n",
      "Iteration 3219: Policy loss: 0.832180. Value loss: 2.881883. Entropy: 0.789398.\n",
      "episode: 1346   score: 105.0  epsilon: 1.0    steps: 842  evaluation reward: 193.1\n",
      "episode: 1347   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 193.85\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3220: Policy loss: -1.424415. Value loss: 131.484787. Entropy: 0.775083.\n",
      "Iteration 3221: Policy loss: -1.119941. Value loss: 33.980198. Entropy: 0.808110.\n",
      "Iteration 3222: Policy loss: -1.575700. Value loss: 22.111248. Entropy: 0.793661.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3223: Policy loss: 0.294900. Value loss: 22.365568. Entropy: 0.917146.\n",
      "Iteration 3224: Policy loss: 0.295074. Value loss: 12.097053. Entropy: 0.939261.\n",
      "Iteration 3225: Policy loss: 0.130933. Value loss: 9.357572. Entropy: 0.939091.\n",
      "episode: 1348   score: 130.0  epsilon: 1.0    steps: 387  evaluation reward: 191.6\n",
      "episode: 1349   score: 90.0  epsilon: 1.0    steps: 605  evaluation reward: 191.45\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3226: Policy loss: -1.179242. Value loss: 23.284988. Entropy: 0.881720.\n",
      "Iteration 3227: Policy loss: -1.195684. Value loss: 12.592655. Entropy: 0.892877.\n",
      "Iteration 3228: Policy loss: -1.336314. Value loss: 10.898036. Entropy: 0.907050.\n",
      "episode: 1350   score: 340.0  epsilon: 1.0    steps: 117  evaluation reward: 189.85\n",
      "now time :  2019-02-25 19:40:12.387277\n",
      "episode: 1351   score: 435.0  epsilon: 1.0    steps: 310  evaluation reward: 193.45\n",
      "episode: 1352   score: 170.0  epsilon: 1.0    steps: 679  evaluation reward: 192.05\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3229: Policy loss: -3.093148. Value loss: 181.455688. Entropy: 0.844424.\n",
      "Iteration 3230: Policy loss: -3.080722. Value loss: 84.404694. Entropy: 0.835337.\n",
      "Iteration 3231: Policy loss: -3.102366. Value loss: 39.879478. Entropy: 0.840346.\n",
      "episode: 1353   score: 75.0  epsilon: 1.0    steps: 810  evaluation reward: 191.55\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3232: Policy loss: 1.452959. Value loss: 31.593420. Entropy: 0.716682.\n",
      "Iteration 3233: Policy loss: 1.528407. Value loss: 14.180935. Entropy: 0.732580.\n",
      "Iteration 3234: Policy loss: 1.880521. Value loss: 9.881207. Entropy: 0.727660.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3235: Policy loss: -0.794539. Value loss: 25.226093. Entropy: 0.781339.\n",
      "Iteration 3236: Policy loss: -0.514805. Value loss: 14.691655. Entropy: 0.771093.\n",
      "Iteration 3237: Policy loss: -0.625512. Value loss: 12.301747. Entropy: 0.773079.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3238: Policy loss: 0.429278. Value loss: 26.660444. Entropy: 0.914631.\n",
      "Iteration 3239: Policy loss: 0.535695. Value loss: 10.158947. Entropy: 0.904039.\n",
      "Iteration 3240: Policy loss: 0.474257. Value loss: 7.861587. Entropy: 0.918220.\n",
      "episode: 1354   score: 240.0  epsilon: 1.0    steps: 216  evaluation reward: 192.75\n",
      "episode: 1355   score: 125.0  epsilon: 1.0    steps: 496  evaluation reward: 189.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3241: Policy loss: 1.704706. Value loss: 18.751287. Entropy: 0.828530.\n",
      "Iteration 3242: Policy loss: 1.534495. Value loss: 9.840940. Entropy: 0.858076.\n",
      "Iteration 3243: Policy loss: 1.649081. Value loss: 7.053418. Entropy: 0.850378.\n",
      "episode: 1356   score: 110.0  epsilon: 1.0    steps: 81  evaluation reward: 189.95\n",
      "episode: 1357   score: 155.0  epsilon: 1.0    steps: 726  evaluation reward: 188.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3244: Policy loss: 0.836768. Value loss: 25.097254. Entropy: 0.800768.\n",
      "Iteration 3245: Policy loss: 1.207740. Value loss: 12.916876. Entropy: 0.808352.\n",
      "Iteration 3246: Policy loss: 1.138093. Value loss: 10.089384. Entropy: 0.822039.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3247: Policy loss: 2.409650. Value loss: 25.296333. Entropy: 0.704924.\n",
      "Iteration 3248: Policy loss: 2.925929. Value loss: 13.963114. Entropy: 0.746630.\n",
      "Iteration 3249: Policy loss: 2.685112. Value loss: 11.035100. Entropy: 0.754608.\n",
      "episode: 1358   score: 260.0  epsilon: 1.0    steps: 298  evaluation reward: 190.25\n",
      "episode: 1359   score: 220.0  epsilon: 1.0    steps: 584  evaluation reward: 190.65\n",
      "episode: 1360   score: 275.0  epsilon: 1.0    steps: 973  evaluation reward: 191.3\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3250: Policy loss: 0.562543. Value loss: 13.570396. Entropy: 0.742496.\n",
      "Iteration 3251: Policy loss: 0.767198. Value loss: 8.715211. Entropy: 0.757745.\n",
      "Iteration 3252: Policy loss: 0.588957. Value loss: 6.463786. Entropy: 0.761593.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3253: Policy loss: 1.425778. Value loss: 23.487370. Entropy: 0.712706.\n",
      "Iteration 3254: Policy loss: 1.359885. Value loss: 14.682141. Entropy: 0.693468.\n",
      "Iteration 3255: Policy loss: 1.200894. Value loss: 12.683733. Entropy: 0.726826.\n",
      "episode: 1361   score: 120.0  epsilon: 1.0    steps: 195  evaluation reward: 189.6\n",
      "episode: 1362   score: 110.0  epsilon: 1.0    steps: 470  evaluation reward: 186.6\n",
      "episode: 1363   score: 175.0  epsilon: 1.0    steps: 893  evaluation reward: 184.25\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3256: Policy loss: 2.451545. Value loss: 19.789833. Entropy: 1.015840.\n",
      "Iteration 3257: Policy loss: 2.471678. Value loss: 12.538779. Entropy: 1.008142.\n",
      "Iteration 3258: Policy loss: 2.363152. Value loss: 12.339740. Entropy: 1.009101.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3259: Policy loss: 2.273467. Value loss: 23.290602. Entropy: 0.924073.\n",
      "Iteration 3260: Policy loss: 1.965103. Value loss: 11.892009. Entropy: 0.920309.\n",
      "Iteration 3261: Policy loss: 2.415603. Value loss: 9.137600. Entropy: 0.944997.\n",
      "episode: 1364   score: 195.0  epsilon: 1.0    steps: 725  evaluation reward: 182.15\n",
      "episode: 1365   score: 180.0  epsilon: 1.0    steps: 996  evaluation reward: 182.7\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3262: Policy loss: 0.699352. Value loss: 30.604046. Entropy: 0.915388.\n",
      "Iteration 3263: Policy loss: 0.663925. Value loss: 21.118582. Entropy: 0.942087.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3264: Policy loss: 0.562550. Value loss: 18.334076. Entropy: 0.931836.\n",
      "episode: 1366   score: 110.0  epsilon: 1.0    steps: 16  evaluation reward: 182.75\n",
      "episode: 1367   score: 135.0  epsilon: 1.0    steps: 205  evaluation reward: 180.85\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3265: Policy loss: 0.651147. Value loss: 25.254501. Entropy: 0.932665.\n",
      "Iteration 3266: Policy loss: 0.549493. Value loss: 15.894929. Entropy: 0.919865.\n",
      "Iteration 3267: Policy loss: 0.474678. Value loss: 12.498925. Entropy: 0.922812.\n",
      "episode: 1368   score: 235.0  epsilon: 1.0    steps: 291  evaluation reward: 181.1\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3268: Policy loss: 0.199403. Value loss: 26.166117. Entropy: 0.938919.\n",
      "Iteration 3269: Policy loss: 0.237971. Value loss: 13.622351. Entropy: 0.943973.\n",
      "Iteration 3270: Policy loss: 0.185539. Value loss: 10.503524. Entropy: 0.953202.\n",
      "episode: 1369   score: 305.0  epsilon: 1.0    steps: 597  evaluation reward: 183.05\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3271: Policy loss: -1.529321. Value loss: 29.038265. Entropy: 0.973642.\n",
      "Iteration 3272: Policy loss: -1.653652. Value loss: 18.975632. Entropy: 0.952740.\n",
      "Iteration 3273: Policy loss: -1.693527. Value loss: 16.286884. Entropy: 0.936091.\n",
      "episode: 1370   score: 185.0  epsilon: 1.0    steps: 804  evaluation reward: 183.85\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3274: Policy loss: -0.582586. Value loss: 24.129757. Entropy: 0.726178.\n",
      "Iteration 3275: Policy loss: -0.553499. Value loss: 16.050709. Entropy: 0.690395.\n",
      "Iteration 3276: Policy loss: -0.934017. Value loss: 12.310104. Entropy: 0.709063.\n",
      "episode: 1371   score: 115.0  epsilon: 1.0    steps: 72  evaluation reward: 183.2\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3277: Policy loss: 0.767930. Value loss: 20.171146. Entropy: 0.881635.\n",
      "Iteration 3278: Policy loss: 1.261778. Value loss: 9.194451. Entropy: 0.898492.\n",
      "Iteration 3279: Policy loss: 0.992825. Value loss: 7.840649. Entropy: 0.920443.\n",
      "episode: 1372   score: 120.0  epsilon: 1.0    steps: 374  evaluation reward: 182.3\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3280: Policy loss: 0.259898. Value loss: 20.998446. Entropy: 0.875924.\n",
      "Iteration 3281: Policy loss: 0.421326. Value loss: 9.804522. Entropy: 0.845037.\n",
      "Iteration 3282: Policy loss: -0.028051. Value loss: 8.452864. Entropy: 0.863405.\n",
      "episode: 1373   score: 325.0  epsilon: 1.0    steps: 477  evaluation reward: 183.45\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3283: Policy loss: -1.847480. Value loss: 43.560055. Entropy: 1.096012.\n",
      "Iteration 3284: Policy loss: -1.818315. Value loss: 19.982513. Entropy: 1.079689.\n",
      "Iteration 3285: Policy loss: -1.982698. Value loss: 15.875752. Entropy: 1.085485.\n",
      "episode: 1374   score: 160.0  epsilon: 1.0    steps: 627  evaluation reward: 183.25\n",
      "episode: 1375   score: 375.0  epsilon: 1.0    steps: 734  evaluation reward: 182.9\n",
      "episode: 1376   score: 150.0  epsilon: 1.0    steps: 875  evaluation reward: 182.85\n",
      "episode: 1377   score: 200.0  epsilon: 1.0    steps: 899  evaluation reward: 183.3\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3286: Policy loss: 0.328052. Value loss: 33.175316. Entropy: 0.900082.\n",
      "Iteration 3287: Policy loss: 0.543920. Value loss: 14.007171. Entropy: 0.873246.\n",
      "Iteration 3288: Policy loss: 0.293604. Value loss: 11.970203. Entropy: 0.883571.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3289: Policy loss: 1.528946. Value loss: 33.449181. Entropy: 0.923988.\n",
      "Iteration 3290: Policy loss: 1.612479. Value loss: 15.474913. Entropy: 0.898801.\n",
      "Iteration 3291: Policy loss: 1.722829. Value loss: 13.443402. Entropy: 0.897529.\n",
      "episode: 1378   score: 400.0  epsilon: 1.0    steps: 188  evaluation reward: 185.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3292: Policy loss: 1.966706. Value loss: 25.180517. Entropy: 0.868117.\n",
      "Iteration 3293: Policy loss: 1.877367. Value loss: 13.749829. Entropy: 0.878428.\n",
      "Iteration 3294: Policy loss: 1.889490. Value loss: 10.931806. Entropy: 0.868302.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3295: Policy loss: 0.576077. Value loss: 18.506338. Entropy: 0.868726.\n",
      "Iteration 3296: Policy loss: 0.814415. Value loss: 12.091747. Entropy: 0.870930.\n",
      "Iteration 3297: Policy loss: 0.517328. Value loss: 10.462582. Entropy: 0.872102.\n",
      "episode: 1379   score: 185.0  epsilon: 1.0    steps: 258  evaluation reward: 185.25\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3298: Policy loss: 1.908320. Value loss: 20.895355. Entropy: 0.855231.\n",
      "Iteration 3299: Policy loss: 2.041552. Value loss: 10.615283. Entropy: 0.856352.\n",
      "Iteration 3300: Policy loss: 1.973125. Value loss: 9.099528. Entropy: 0.867368.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3301: Policy loss: 1.086221. Value loss: 29.428825. Entropy: 0.721260.\n",
      "Iteration 3302: Policy loss: 1.445133. Value loss: 16.923111. Entropy: 0.709303.\n",
      "Iteration 3303: Policy loss: 0.982230. Value loss: 14.052497. Entropy: 0.731565.\n",
      "episode: 1380   score: 110.0  epsilon: 1.0    steps: 239  evaluation reward: 184.25\n",
      "episode: 1381   score: 140.0  epsilon: 1.0    steps: 650  evaluation reward: 183.55\n",
      "episode: 1382   score: 235.0  epsilon: 1.0    steps: 939  evaluation reward: 183.8\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3304: Policy loss: 1.118818. Value loss: 14.871935. Entropy: 0.968073.\n",
      "Iteration 3305: Policy loss: 1.129195. Value loss: 9.093447. Entropy: 0.978730.\n",
      "Iteration 3306: Policy loss: 1.055733. Value loss: 7.033497. Entropy: 0.980954.\n",
      "episode: 1383   score: 290.0  epsilon: 1.0    steps: 73  evaluation reward: 184.6\n",
      "episode: 1384   score: 160.0  epsilon: 1.0    steps: 525  evaluation reward: 184.1\n",
      "episode: 1385   score: 265.0  epsilon: 1.0    steps: 855  evaluation reward: 184.65\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3307: Policy loss: 0.555472. Value loss: 48.315132. Entropy: 0.958318.\n",
      "Iteration 3308: Policy loss: 0.936154. Value loss: 28.760998. Entropy: 0.934407.\n",
      "Iteration 3309: Policy loss: 0.389304. Value loss: 18.623127. Entropy: 0.924494.\n",
      "episode: 1386   score: 255.0  epsilon: 1.0    steps: 394  evaluation reward: 185.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3310: Policy loss: 0.513677. Value loss: 19.194813. Entropy: 0.868447.\n",
      "Iteration 3311: Policy loss: 0.394534. Value loss: 11.773130. Entropy: 0.869397.\n",
      "Iteration 3312: Policy loss: 0.252266. Value loss: 9.752720. Entropy: 0.864628.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3313: Policy loss: -1.474117. Value loss: 32.999352. Entropy: 1.040977.\n",
      "Iteration 3314: Policy loss: -1.269931. Value loss: 16.731339. Entropy: 1.023181.\n",
      "Iteration 3315: Policy loss: -1.448097. Value loss: 15.017483. Entropy: 1.030093.\n",
      "episode: 1387   score: 150.0  epsilon: 1.0    steps: 753  evaluation reward: 185.8\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3316: Policy loss: 0.345142. Value loss: 20.183739. Entropy: 0.900853.\n",
      "Iteration 3317: Policy loss: 0.231025. Value loss: 12.961477. Entropy: 0.899360.\n",
      "Iteration 3318: Policy loss: 0.521805. Value loss: 8.979317. Entropy: 0.909510.\n",
      "episode: 1388   score: 110.0  epsilon: 1.0    steps: 122  evaluation reward: 186.4\n",
      "episode: 1389   score: 235.0  epsilon: 1.0    steps: 367  evaluation reward: 187.3\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3319: Policy loss: -3.479913. Value loss: 245.496994. Entropy: 0.797345.\n",
      "Iteration 3320: Policy loss: -3.591043. Value loss: 176.721527. Entropy: 0.741955.\n",
      "Iteration 3321: Policy loss: -4.333574. Value loss: 129.358810. Entropy: 0.674320.\n",
      "episode: 1390   score: 195.0  epsilon: 1.0    steps: 969  evaluation reward: 187.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3322: Policy loss: 1.039487. Value loss: 33.081768. Entropy: 0.665471.\n",
      "Iteration 3323: Policy loss: 0.697030. Value loss: 13.072916. Entropy: 0.662088.\n",
      "Iteration 3324: Policy loss: 0.839117. Value loss: 10.707236. Entropy: 0.663341.\n",
      "episode: 1391   score: 105.0  epsilon: 1.0    steps: 160  evaluation reward: 187.6\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3325: Policy loss: 1.119591. Value loss: 35.370167. Entropy: 0.659651.\n",
      "Iteration 3326: Policy loss: 0.548659. Value loss: 19.649475. Entropy: 0.680996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3327: Policy loss: 0.575702. Value loss: 17.065285. Entropy: 0.686544.\n",
      "episode: 1392   score: 390.0  epsilon: 1.0    steps: 523  evaluation reward: 189.2\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3328: Policy loss: -1.632879. Value loss: 55.998138. Entropy: 0.781295.\n",
      "Iteration 3329: Policy loss: -1.564750. Value loss: 32.271397. Entropy: 0.808881.\n",
      "Iteration 3330: Policy loss: -1.733234. Value loss: 22.843565. Entropy: 0.809656.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3331: Policy loss: 1.119585. Value loss: 23.952715. Entropy: 0.738428.\n",
      "Iteration 3332: Policy loss: 0.826601. Value loss: 13.290872. Entropy: 0.722092.\n",
      "Iteration 3333: Policy loss: 1.122491. Value loss: 7.897899. Entropy: 0.756112.\n",
      "episode: 1393   score: 225.0  epsilon: 1.0    steps: 865  evaluation reward: 190.4\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3334: Policy loss: 4.507875. Value loss: 28.743958. Entropy: 0.826611.\n",
      "Iteration 3335: Policy loss: 3.950831. Value loss: 16.530024. Entropy: 0.850171.\n",
      "Iteration 3336: Policy loss: 4.202403. Value loss: 12.566606. Entropy: 0.864418.\n",
      "episode: 1394   score: 175.0  epsilon: 1.0    steps: 30  evaluation reward: 189.85\n",
      "episode: 1395   score: 660.0  epsilon: 1.0    steps: 453  evaluation reward: 195.4\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3337: Policy loss: -0.781235. Value loss: 271.440369. Entropy: 0.742212.\n",
      "Iteration 3338: Policy loss: -0.040498. Value loss: 177.813736. Entropy: 0.590360.\n",
      "Iteration 3339: Policy loss: 0.095172. Value loss: 143.998718. Entropy: 0.664764.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3340: Policy loss: -0.644972. Value loss: 38.677471. Entropy: 0.572599.\n",
      "Iteration 3341: Policy loss: -0.587901. Value loss: 18.814514. Entropy: 0.556314.\n",
      "Iteration 3342: Policy loss: -0.905335. Value loss: 16.009892. Entropy: 0.587179.\n",
      "episode: 1396   score: 155.0  epsilon: 1.0    steps: 316  evaluation reward: 194.2\n",
      "episode: 1397   score: 160.0  epsilon: 1.0    steps: 585  evaluation reward: 192.75\n",
      "episode: 1398   score: 405.0  epsilon: 1.0    steps: 745  evaluation reward: 194.5\n",
      "episode: 1399   score: 290.0  epsilon: 1.0    steps: 998  evaluation reward: 196.3\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3343: Policy loss: 0.558367. Value loss: 18.268181. Entropy: 0.737563.\n",
      "Iteration 3344: Policy loss: 0.605056. Value loss: 11.925523. Entropy: 0.729743.\n",
      "Iteration 3345: Policy loss: 0.554763. Value loss: 9.095323. Entropy: 0.730852.\n",
      "episode: 1400   score: 105.0  epsilon: 1.0    steps: 888  evaluation reward: 196.2\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3346: Policy loss: 1.144105. Value loss: 31.482281. Entropy: 0.691601.\n",
      "Iteration 3347: Policy loss: 1.056870. Value loss: 20.162098. Entropy: 0.721602.\n",
      "Iteration 3348: Policy loss: 1.085498. Value loss: 15.392119. Entropy: 0.708478.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3349: Policy loss: -2.704324. Value loss: 34.091431. Entropy: 0.790150.\n",
      "Iteration 3350: Policy loss: -2.268175. Value loss: 19.342188. Entropy: 0.780947.\n",
      "Iteration 3351: Policy loss: -2.627315. Value loss: 16.751217. Entropy: 0.762025.\n",
      "now time :  2019-02-25 19:42:30.445361\n",
      "episode: 1401   score: 135.0  epsilon: 1.0    steps: 414  evaluation reward: 194.4\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3352: Policy loss: 1.400832. Value loss: 31.350225. Entropy: 0.790303.\n",
      "Iteration 3353: Policy loss: 1.142925. Value loss: 15.909397. Entropy: 0.772667.\n",
      "Iteration 3354: Policy loss: 1.187242. Value loss: 13.829768. Entropy: 0.801810.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3355: Policy loss: -0.445376. Value loss: 33.394985. Entropy: 0.790322.\n",
      "Iteration 3356: Policy loss: -0.200651. Value loss: 22.607706. Entropy: 0.852572.\n",
      "Iteration 3357: Policy loss: -0.226606. Value loss: 17.288876. Entropy: 0.849235.\n",
      "episode: 1402   score: 385.0  epsilon: 1.0    steps: 126  evaluation reward: 197.2\n",
      "episode: 1403   score: 445.0  epsilon: 1.0    steps: 161  evaluation reward: 199.85\n",
      "episode: 1404   score: 110.0  epsilon: 1.0    steps: 288  evaluation reward: 199.2\n",
      "episode: 1405   score: 170.0  epsilon: 1.0    steps: 551  evaluation reward: 196.05\n",
      "episode: 1406   score: 110.0  epsilon: 1.0    steps: 680  evaluation reward: 196.1\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3358: Policy loss: 1.091994. Value loss: 18.933214. Entropy: 0.860003.\n",
      "Iteration 3359: Policy loss: 0.969679. Value loss: 10.957986. Entropy: 0.894815.\n",
      "Iteration 3360: Policy loss: 0.959324. Value loss: 9.779557. Entropy: 0.876823.\n",
      "episode: 1407   score: 195.0  epsilon: 1.0    steps: 941  evaluation reward: 196.95\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3361: Policy loss: 0.823703. Value loss: 23.760345. Entropy: 0.776215.\n",
      "Iteration 3362: Policy loss: 0.786553. Value loss: 15.168273. Entropy: 0.800122.\n",
      "Iteration 3363: Policy loss: 0.780642. Value loss: 12.026063. Entropy: 0.840925.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3364: Policy loss: -2.200348. Value loss: 34.408363. Entropy: 0.878369.\n",
      "Iteration 3365: Policy loss: -2.109514. Value loss: 18.803432. Entropy: 0.896894.\n",
      "Iteration 3366: Policy loss: -2.054083. Value loss: 15.275873. Entropy: 0.862508.\n",
      "episode: 1408   score: 160.0  epsilon: 1.0    steps: 494  evaluation reward: 196.0\n",
      "episode: 1409   score: 70.0  epsilon: 1.0    steps: 823  evaluation reward: 194.9\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3367: Policy loss: 1.166299. Value loss: 31.420958. Entropy: 0.737925.\n",
      "Iteration 3368: Policy loss: 0.902793. Value loss: 18.547770. Entropy: 0.741020.\n",
      "Iteration 3369: Policy loss: 0.729709. Value loss: 17.156885. Entropy: 0.744945.\n",
      "episode: 1410   score: 155.0  epsilon: 1.0    steps: 613  evaluation reward: 194.35\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3370: Policy loss: 0.932394. Value loss: 30.526459. Entropy: 0.728697.\n",
      "Iteration 3371: Policy loss: 0.920102. Value loss: 18.081537. Entropy: 0.713644.\n",
      "Iteration 3372: Policy loss: 0.880970. Value loss: 13.167696. Entropy: 0.730136.\n",
      "episode: 1411   score: 220.0  epsilon: 1.0    steps: 359  evaluation reward: 194.45\n",
      "episode: 1412   score: 255.0  epsilon: 1.0    steps: 767  evaluation reward: 195.15\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3373: Policy loss: 2.174773. Value loss: 20.895834. Entropy: 0.788282.\n",
      "Iteration 3374: Policy loss: 2.192866. Value loss: 10.981733. Entropy: 0.809874.\n",
      "Iteration 3375: Policy loss: 2.033709. Value loss: 8.561753. Entropy: 0.829991.\n",
      "episode: 1413   score: 125.0  epsilon: 1.0    steps: 17  evaluation reward: 194.3\n",
      "episode: 1414   score: 95.0  epsilon: 1.0    steps: 813  evaluation reward: 191.45\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3376: Policy loss: -1.110990. Value loss: 26.327406. Entropy: 0.884413.\n",
      "Iteration 3377: Policy loss: -0.995713. Value loss: 17.817875. Entropy: 0.887076.\n",
      "Iteration 3378: Policy loss: -1.136515. Value loss: 15.927956. Entropy: 0.861589.\n",
      "episode: 1415   score: 200.0  epsilon: 1.0    steps: 1007  evaluation reward: 191.65\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3379: Policy loss: 1.862989. Value loss: 26.833752. Entropy: 0.867097.\n",
      "Iteration 3380: Policy loss: 2.135261. Value loss: 15.611289. Entropy: 0.893560.\n",
      "Iteration 3381: Policy loss: 1.931861. Value loss: 13.207757. Entropy: 0.871772.\n",
      "episode: 1416   score: 200.0  epsilon: 1.0    steps: 135  evaluation reward: 191.85\n",
      "episode: 1417   score: 155.0  epsilon: 1.0    steps: 494  evaluation reward: 192.35\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3382: Policy loss: -0.736505. Value loss: 29.695446. Entropy: 0.914710.\n",
      "Iteration 3383: Policy loss: -1.033367. Value loss: 17.504566. Entropy: 0.960252.\n",
      "Iteration 3384: Policy loss: -0.603293. Value loss: 13.207139. Entropy: 0.918369.\n",
      "episode: 1418   score: 115.0  epsilon: 1.0    steps: 602  evaluation reward: 192.4\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3385: Policy loss: 0.431117. Value loss: 23.265160. Entropy: 0.709541.\n",
      "Iteration 3386: Policy loss: 0.448900. Value loss: 11.607702. Entropy: 0.683871.\n",
      "Iteration 3387: Policy loss: 0.442309. Value loss: 10.337549. Entropy: 0.672312.\n",
      "episode: 1419   score: 155.0  epsilon: 1.0    steps: 692  evaluation reward: 190.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1420   score: 110.0  epsilon: 1.0    steps: 896  evaluation reward: 190.75\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3388: Policy loss: -2.242440. Value loss: 33.323666. Entropy: 0.962963.\n",
      "Iteration 3389: Policy loss: -2.218866. Value loss: 20.005610. Entropy: 0.939614.\n",
      "Iteration 3390: Policy loss: -2.187312. Value loss: 15.684772. Entropy: 0.956152.\n",
      "episode: 1421   score: 175.0  epsilon: 1.0    steps: 22  evaluation reward: 190.9\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3391: Policy loss: 0.848501. Value loss: 23.482792. Entropy: 0.835108.\n",
      "Iteration 3392: Policy loss: 0.674023. Value loss: 14.148493. Entropy: 0.832287.\n",
      "Iteration 3393: Policy loss: 0.814502. Value loss: 12.008693. Entropy: 0.871826.\n",
      "episode: 1422   score: 80.0  epsilon: 1.0    steps: 905  evaluation reward: 190.65\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3394: Policy loss: 0.603395. Value loss: 23.205849. Entropy: 0.896663.\n",
      "Iteration 3395: Policy loss: 0.508575. Value loss: 14.863689. Entropy: 0.869812.\n",
      "Iteration 3396: Policy loss: 0.631839. Value loss: 10.533062. Entropy: 0.884682.\n",
      "episode: 1423   score: 215.0  epsilon: 1.0    steps: 174  evaluation reward: 190.7\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3397: Policy loss: -1.922490. Value loss: 46.209431. Entropy: 0.894568.\n",
      "Iteration 3398: Policy loss: -1.970642. Value loss: 21.469864. Entropy: 0.850477.\n",
      "Iteration 3399: Policy loss: -1.780688. Value loss: 18.390802. Entropy: 0.837951.\n",
      "episode: 1424   score: 170.0  epsilon: 1.0    steps: 605  evaluation reward: 190.2\n",
      "episode: 1425   score: 110.0  epsilon: 1.0    steps: 733  evaluation reward: 190.25\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3400: Policy loss: -0.624954. Value loss: 38.408524. Entropy: 0.809478.\n",
      "Iteration 3401: Policy loss: -0.385076. Value loss: 23.503061. Entropy: 0.790231.\n",
      "Iteration 3402: Policy loss: -0.887598. Value loss: 19.715677. Entropy: 0.804661.\n",
      "episode: 1426   score: 150.0  epsilon: 1.0    steps: 121  evaluation reward: 189.7\n",
      "episode: 1427   score: 455.0  epsilon: 1.0    steps: 272  evaluation reward: 192.6\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3403: Policy loss: 0.047954. Value loss: 42.721550. Entropy: 0.785146.\n",
      "Iteration 3404: Policy loss: 0.378239. Value loss: 27.359426. Entropy: 0.752221.\n",
      "Iteration 3405: Policy loss: 0.106440. Value loss: 22.558456. Entropy: 0.769964.\n",
      "episode: 1428   score: 375.0  epsilon: 1.0    steps: 511  evaluation reward: 194.25\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3406: Policy loss: 0.569502. Value loss: 23.040907. Entropy: 0.828668.\n",
      "Iteration 3407: Policy loss: 0.686318. Value loss: 15.204548. Entropy: 0.854786.\n",
      "Iteration 3408: Policy loss: 0.578510. Value loss: 13.548650. Entropy: 0.871995.\n",
      "episode: 1429   score: 175.0  epsilon: 1.0    steps: 962  evaluation reward: 194.15\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3409: Policy loss: 0.342050. Value loss: 22.608152. Entropy: 0.837123.\n",
      "Iteration 3410: Policy loss: 0.274637. Value loss: 13.309057. Entropy: 0.848292.\n",
      "Iteration 3411: Policy loss: 0.375138. Value loss: 12.465377. Entropy: 0.846319.\n",
      "episode: 1430   score: 170.0  epsilon: 1.0    steps: 179  evaluation reward: 194.25\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3412: Policy loss: -1.288463. Value loss: 37.634098. Entropy: 0.642041.\n",
      "Iteration 3413: Policy loss: -1.169867. Value loss: 20.904339. Entropy: 0.641406.\n",
      "Iteration 3414: Policy loss: -1.231879. Value loss: 17.888866. Entropy: 0.635171.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3415: Policy loss: -0.435598. Value loss: 34.564365. Entropy: 0.610451.\n",
      "Iteration 3416: Policy loss: -0.527317. Value loss: 21.736650. Entropy: 0.654305.\n",
      "Iteration 3417: Policy loss: -0.539214. Value loss: 15.075855. Entropy: 0.630781.\n",
      "episode: 1431   score: 135.0  epsilon: 1.0    steps: 38  evaluation reward: 194.15\n",
      "episode: 1432   score: 235.0  epsilon: 1.0    steps: 275  evaluation reward: 195.45\n",
      "episode: 1433   score: 155.0  epsilon: 1.0    steps: 518  evaluation reward: 195.95\n",
      "episode: 1434   score: 330.0  epsilon: 1.0    steps: 746  evaluation reward: 198.2\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3418: Policy loss: -0.195204. Value loss: 27.060772. Entropy: 0.627332.\n",
      "Iteration 3419: Policy loss: 0.129242. Value loss: 20.294498. Entropy: 0.636537.\n",
      "Iteration 3420: Policy loss: 0.134593. Value loss: 15.006546. Entropy: 0.641418.\n",
      "episode: 1435   score: 125.0  epsilon: 1.0    steps: 448  evaluation reward: 198.4\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3421: Policy loss: 1.094456. Value loss: 23.033539. Entropy: 0.669541.\n",
      "Iteration 3422: Policy loss: 1.050118. Value loss: 12.441505. Entropy: 0.666400.\n",
      "Iteration 3423: Policy loss: 1.007196. Value loss: 8.957382. Entropy: 0.682542.\n",
      "episode: 1436   score: 460.0  epsilon: 1.0    steps: 820  evaluation reward: 199.4\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3424: Policy loss: 3.640828. Value loss: 35.180511. Entropy: 0.869780.\n",
      "Iteration 3425: Policy loss: 3.934504. Value loss: 16.179052. Entropy: 0.873183.\n",
      "Iteration 3426: Policy loss: 3.549972. Value loss: 12.406026. Entropy: 0.866188.\n",
      "episode: 1437   score: 110.0  epsilon: 1.0    steps: 366  evaluation reward: 198.15\n",
      "episode: 1438   score: 120.0  epsilon: 1.0    steps: 981  evaluation reward: 198.3\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3427: Policy loss: 2.451888. Value loss: 21.881676. Entropy: 0.906261.\n",
      "Iteration 3428: Policy loss: 2.921505. Value loss: 13.731872. Entropy: 0.906908.\n",
      "Iteration 3429: Policy loss: 2.098941. Value loss: 11.114765. Entropy: 0.929895.\n",
      "episode: 1439   score: 125.0  epsilon: 1.0    steps: 167  evaluation reward: 197.7\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3430: Policy loss: 0.871093. Value loss: 26.082462. Entropy: 0.928819.\n",
      "Iteration 3431: Policy loss: 0.894788. Value loss: 15.012793. Entropy: 0.937702.\n",
      "Iteration 3432: Policy loss: 0.535270. Value loss: 11.168178. Entropy: 0.951812.\n",
      "episode: 1440   score: 65.0  epsilon: 1.0    steps: 659  evaluation reward: 196.4\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3433: Policy loss: 2.429195. Value loss: 31.148031. Entropy: 0.724601.\n",
      "Iteration 3434: Policy loss: 2.709675. Value loss: 15.706885. Entropy: 0.713167.\n",
      "Iteration 3435: Policy loss: 2.425281. Value loss: 13.854093. Entropy: 0.733518.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3436: Policy loss: 0.112621. Value loss: 23.664942. Entropy: 0.729638.\n",
      "Iteration 3437: Policy loss: 0.103871. Value loss: 16.894651. Entropy: 0.746449.\n",
      "Iteration 3438: Policy loss: 0.120650. Value loss: 11.666494. Entropy: 0.759357.\n",
      "episode: 1441   score: 175.0  epsilon: 1.0    steps: 61  evaluation reward: 197.1\n",
      "episode: 1442   score: 245.0  epsilon: 1.0    steps: 531  evaluation reward: 198.5\n",
      "episode: 1443   score: 190.0  epsilon: 1.0    steps: 847  evaluation reward: 198.45\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3439: Policy loss: -0.620800. Value loss: 25.352057. Entropy: 0.761213.\n",
      "Iteration 3440: Policy loss: -0.486336. Value loss: 14.443405. Entropy: 0.726956.\n",
      "Iteration 3441: Policy loss: -0.814068. Value loss: 12.393377. Entropy: 0.730678.\n",
      "episode: 1444   score: 110.0  epsilon: 1.0    steps: 257  evaluation reward: 198.8\n",
      "episode: 1445   score: 155.0  epsilon: 1.0    steps: 940  evaluation reward: 198.8\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3442: Policy loss: -0.917910. Value loss: 37.709156. Entropy: 0.658544.\n",
      "Iteration 3443: Policy loss: -0.666386. Value loss: 20.691044. Entropy: 0.672100.\n",
      "Iteration 3444: Policy loss: -0.935826. Value loss: 20.545578. Entropy: 0.661225.\n",
      "episode: 1446   score: 395.0  epsilon: 1.0    steps: 430  evaluation reward: 201.7\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3445: Policy loss: 2.295907. Value loss: 24.467644. Entropy: 0.735293.\n",
      "Iteration 3446: Policy loss: 2.319959. Value loss: 15.379578. Entropy: 0.793252.\n",
      "Iteration 3447: Policy loss: 2.399827. Value loss: 13.565735. Entropy: 0.806759.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3448: Policy loss: -4.189172. Value loss: 298.800659. Entropy: 0.877606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3449: Policy loss: -3.317024. Value loss: 138.909973. Entropy: 0.885149.\n",
      "Iteration 3450: Policy loss: -4.418789. Value loss: 126.707222. Entropy: 0.848173.\n",
      "episode: 1447   score: 185.0  epsilon: 1.0    steps: 197  evaluation reward: 201.75\n",
      "episode: 1448   score: 230.0  epsilon: 1.0    steps: 686  evaluation reward: 202.75\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3451: Policy loss: -0.771842. Value loss: 55.904716. Entropy: 0.906309.\n",
      "Iteration 3452: Policy loss: -0.730284. Value loss: 26.572807. Entropy: 0.905456.\n",
      "Iteration 3453: Policy loss: -0.527592. Value loss: 20.127745. Entropy: 0.869143.\n",
      "episode: 1449   score: 135.0  epsilon: 1.0    steps: 45  evaluation reward: 203.2\n",
      "episode: 1450   score: 180.0  epsilon: 1.0    steps: 562  evaluation reward: 201.6\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3454: Policy loss: 2.092057. Value loss: 44.034809. Entropy: 0.773828.\n",
      "Iteration 3455: Policy loss: 2.310785. Value loss: 23.084213. Entropy: 0.782870.\n",
      "Iteration 3456: Policy loss: 2.228538. Value loss: 19.647554. Entropy: 0.793807.\n",
      "now time :  2019-02-25 19:44:29.561007\n",
      "episode: 1451   score: 160.0  epsilon: 1.0    steps: 963  evaluation reward: 198.85\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3457: Policy loss: 1.415819. Value loss: 26.457542. Entropy: 0.828512.\n",
      "Iteration 3458: Policy loss: 1.532342. Value loss: 17.356539. Entropy: 0.846239.\n",
      "Iteration 3459: Policy loss: 1.358745. Value loss: 15.015148. Entropy: 0.825372.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3460: Policy loss: 1.094338. Value loss: 28.105789. Entropy: 0.768076.\n",
      "Iteration 3461: Policy loss: 1.039828. Value loss: 13.501611. Entropy: 0.782841.\n",
      "Iteration 3462: Policy loss: 1.285532. Value loss: 11.241879. Entropy: 0.784152.\n",
      "episode: 1452   score: 245.0  epsilon: 1.0    steps: 294  evaluation reward: 199.6\n",
      "episode: 1453   score: 315.0  epsilon: 1.0    steps: 435  evaluation reward: 202.0\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3463: Policy loss: -0.791864. Value loss: 49.248585. Entropy: 0.907323.\n",
      "Iteration 3464: Policy loss: -0.642060. Value loss: 18.825548. Entropy: 0.895621.\n",
      "Iteration 3465: Policy loss: -0.745992. Value loss: 14.779746. Entropy: 0.872078.\n",
      "episode: 1454   score: 170.0  epsilon: 1.0    steps: 226  evaluation reward: 201.3\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3466: Policy loss: 1.790235. Value loss: 39.165493. Entropy: 0.797094.\n",
      "Iteration 3467: Policy loss: 1.773877. Value loss: 17.893219. Entropy: 0.789089.\n",
      "Iteration 3468: Policy loss: 2.019314. Value loss: 13.473990. Entropy: 0.785942.\n",
      "episode: 1455   score: 130.0  epsilon: 1.0    steps: 123  evaluation reward: 201.35\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3469: Policy loss: 0.486179. Value loss: 28.750675. Entropy: 0.838372.\n",
      "Iteration 3470: Policy loss: 0.590652. Value loss: 16.237036. Entropy: 0.834210.\n",
      "Iteration 3471: Policy loss: 0.356907. Value loss: 14.665895. Entropy: 0.857331.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3472: Policy loss: 1.220756. Value loss: 38.665379. Entropy: 0.643562.\n",
      "Iteration 3473: Policy loss: 1.143554. Value loss: 17.878466. Entropy: 0.656384.\n",
      "Iteration 3474: Policy loss: 1.120677. Value loss: 12.379459. Entropy: 0.660493.\n",
      "episode: 1456   score: 215.0  epsilon: 1.0    steps: 679  evaluation reward: 202.4\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3475: Policy loss: 4.232888. Value loss: 31.616055. Entropy: 0.826513.\n",
      "Iteration 3476: Policy loss: 4.201735. Value loss: 17.100666. Entropy: 0.842150.\n",
      "Iteration 3477: Policy loss: 4.064585. Value loss: 13.387587. Entropy: 0.873799.\n",
      "episode: 1457   score: 800.0  epsilon: 1.0    steps: 820  evaluation reward: 208.85\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3478: Policy loss: 0.771966. Value loss: 25.848804. Entropy: 0.883687.\n",
      "Iteration 3479: Policy loss: 0.953697. Value loss: 10.795610. Entropy: 0.884995.\n",
      "Iteration 3480: Policy loss: 0.820894. Value loss: 8.779978. Entropy: 0.871855.\n",
      "episode: 1458   score: 110.0  epsilon: 1.0    steps: 318  evaluation reward: 207.35\n",
      "episode: 1459   score: 340.0  epsilon: 1.0    steps: 948  evaluation reward: 208.55\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3481: Policy loss: 1.740492. Value loss: 14.969496. Entropy: 0.885595.\n",
      "Iteration 3482: Policy loss: 1.582866. Value loss: 9.294650. Entropy: 0.887435.\n",
      "Iteration 3483: Policy loss: 1.574701. Value loss: 7.947433. Entropy: 0.887530.\n",
      "episode: 1460   score: 200.0  epsilon: 1.0    steps: 205  evaluation reward: 207.8\n",
      "episode: 1461   score: 235.0  epsilon: 1.0    steps: 577  evaluation reward: 208.95\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3484: Policy loss: 0.979917. Value loss: 266.332764. Entropy: 0.814891.\n",
      "Iteration 3485: Policy loss: 0.748377. Value loss: 150.499619. Entropy: 0.761843.\n",
      "Iteration 3486: Policy loss: -0.259763. Value loss: 164.411743. Entropy: 0.714289.\n",
      "episode: 1462   score: 450.0  epsilon: 1.0    steps: 471  evaluation reward: 212.35\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3487: Policy loss: 2.209461. Value loss: 42.551189. Entropy: 0.789298.\n",
      "Iteration 3488: Policy loss: 2.503656. Value loss: 26.400686. Entropy: 0.793685.\n",
      "Iteration 3489: Policy loss: 2.216708. Value loss: 22.834911. Entropy: 0.803668.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3490: Policy loss: -1.185353. Value loss: 31.018656. Entropy: 0.845913.\n",
      "Iteration 3491: Policy loss: -1.255393. Value loss: 17.677288. Entropy: 0.857561.\n",
      "Iteration 3492: Policy loss: -1.412323. Value loss: 16.072115. Entropy: 0.839000.\n",
      "episode: 1463   score: 180.0  epsilon: 1.0    steps: 68  evaluation reward: 212.4\n",
      "episode: 1464   score: 220.0  epsilon: 1.0    steps: 740  evaluation reward: 212.65\n",
      "episode: 1465   score: 70.0  epsilon: 1.0    steps: 825  evaluation reward: 211.55\n",
      "episode: 1466   score: 180.0  epsilon: 1.0    steps: 1019  evaluation reward: 212.25\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3493: Policy loss: 2.831191. Value loss: 33.255550. Entropy: 0.875569.\n",
      "Iteration 3494: Policy loss: 2.815145. Value loss: 18.200966. Entropy: 0.885187.\n",
      "Iteration 3495: Policy loss: 2.893447. Value loss: 11.959643. Entropy: 0.883238.\n",
      "episode: 1467   score: 160.0  epsilon: 1.0    steps: 333  evaluation reward: 212.5\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3496: Policy loss: 3.583281. Value loss: 24.453945. Entropy: 0.801368.\n",
      "Iteration 3497: Policy loss: 3.722222. Value loss: 13.838293. Entropy: 0.819540.\n",
      "Iteration 3498: Policy loss: 3.826318. Value loss: 9.727763. Entropy: 0.797795.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3499: Policy loss: 0.484649. Value loss: 28.574787. Entropy: 0.979411.\n",
      "Iteration 3500: Policy loss: 0.417234. Value loss: 17.007950. Entropy: 0.983268.\n",
      "Iteration 3501: Policy loss: 0.383547. Value loss: 13.552874. Entropy: 0.964677.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3502: Policy loss: 0.827444. Value loss: 26.546669. Entropy: 0.759100.\n",
      "Iteration 3503: Policy loss: 1.121740. Value loss: 18.492346. Entropy: 0.772497.\n",
      "Iteration 3504: Policy loss: 0.859119. Value loss: 16.308722. Entropy: 0.776824.\n",
      "episode: 1468   score: 165.0  epsilon: 1.0    steps: 182  evaluation reward: 211.8\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3505: Policy loss: 0.860909. Value loss: 20.082380. Entropy: 0.703735.\n",
      "Iteration 3506: Policy loss: 1.116666. Value loss: 14.708115. Entropy: 0.721374.\n",
      "Iteration 3507: Policy loss: 0.672458. Value loss: 12.993223. Entropy: 0.733488.\n",
      "episode: 1469   score: 260.0  epsilon: 1.0    steps: 623  evaluation reward: 211.35\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3508: Policy loss: 2.182828. Value loss: 17.960737. Entropy: 0.812478.\n",
      "Iteration 3509: Policy loss: 2.318455. Value loss: 12.919535. Entropy: 0.821658.\n",
      "Iteration 3510: Policy loss: 2.146842. Value loss: 9.374352. Entropy: 0.815237.\n",
      "episode: 1470   score: 160.0  epsilon: 1.0    steps: 127  evaluation reward: 211.1\n",
      "episode: 1471   score: 110.0  epsilon: 1.0    steps: 378  evaluation reward: 211.05\n",
      "episode: 1472   score: 135.0  epsilon: 1.0    steps: 677  evaluation reward: 211.2\n",
      "episode: 1473   score: 135.0  epsilon: 1.0    steps: 780  evaluation reward: 209.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3511: Policy loss: 2.660785. Value loss: 16.652563. Entropy: 0.916543.\n",
      "Iteration 3512: Policy loss: 2.772571. Value loss: 10.195044. Entropy: 0.943692.\n",
      "Iteration 3513: Policy loss: 2.649359. Value loss: 8.881975. Entropy: 0.919704.\n",
      "episode: 1474   score: 45.0  epsilon: 1.0    steps: 141  evaluation reward: 208.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3514: Policy loss: 1.168110. Value loss: 14.466429. Entropy: 0.979306.\n",
      "Iteration 3515: Policy loss: 1.220703. Value loss: 11.047109. Entropy: 0.977770.\n",
      "Iteration 3516: Policy loss: 1.048719. Value loss: 9.907266. Entropy: 0.987161.\n",
      "episode: 1475   score: 215.0  epsilon: 1.0    steps: 466  evaluation reward: 206.55\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3517: Policy loss: 1.953385. Value loss: 22.745163. Entropy: 0.952823.\n",
      "Iteration 3518: Policy loss: 2.005641. Value loss: 12.805627. Entropy: 0.961023.\n",
      "Iteration 3519: Policy loss: 1.968505. Value loss: 10.170527. Entropy: 0.981891.\n",
      "episode: 1476   score: 90.0  epsilon: 1.0    steps: 618  evaluation reward: 205.95\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3520: Policy loss: 2.367157. Value loss: 13.537159. Entropy: 0.938827.\n",
      "Iteration 3521: Policy loss: 2.050434. Value loss: 7.801264. Entropy: 0.963252.\n",
      "Iteration 3522: Policy loss: 2.209280. Value loss: 5.933868. Entropy: 0.966376.\n",
      "episode: 1477   score: 45.0  epsilon: 1.0    steps: 150  evaluation reward: 204.4\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3523: Policy loss: -1.463002. Value loss: 30.627350. Entropy: 1.172560.\n",
      "Iteration 3524: Policy loss: -1.398201. Value loss: 17.061762. Entropy: 1.153295.\n",
      "Iteration 3525: Policy loss: -1.503692. Value loss: 13.883865. Entropy: 1.153556.\n",
      "episode: 1478   score: 240.0  epsilon: 1.0    steps: 978  evaluation reward: 202.8\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3526: Policy loss: -4.330944. Value loss: 280.888550. Entropy: 1.071693.\n",
      "Iteration 3527: Policy loss: -4.827031. Value loss: 214.960938. Entropy: 1.001003.\n",
      "Iteration 3528: Policy loss: -4.715537. Value loss: 180.294785. Entropy: 1.019235.\n",
      "episode: 1479   score: 60.0  epsilon: 1.0    steps: 255  evaluation reward: 201.55\n",
      "episode: 1480   score: 495.0  epsilon: 1.0    steps: 363  evaluation reward: 205.4\n",
      "episode: 1481   score: 155.0  epsilon: 1.0    steps: 808  evaluation reward: 205.55\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3529: Policy loss: 0.399046. Value loss: 32.021553. Entropy: 0.907079.\n",
      "Iteration 3530: Policy loss: 0.499132. Value loss: 19.412369. Entropy: 0.923415.\n",
      "Iteration 3531: Policy loss: 0.541222. Value loss: 16.270699. Entropy: 0.929504.\n",
      "episode: 1482   score: 155.0  epsilon: 1.0    steps: 724  evaluation reward: 204.75\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3532: Policy loss: 2.502434. Value loss: 30.940689. Entropy: 0.885023.\n",
      "Iteration 3533: Policy loss: 2.172550. Value loss: 15.402609. Entropy: 0.903071.\n",
      "Iteration 3534: Policy loss: 2.513137. Value loss: 13.409717. Entropy: 0.915857.\n",
      "episode: 1483   score: 245.0  epsilon: 1.0    steps: 59  evaluation reward: 204.3\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3535: Policy loss: 2.345975. Value loss: 22.507547. Entropy: 0.957230.\n",
      "Iteration 3536: Policy loss: 2.315510. Value loss: 12.899391. Entropy: 0.974858.\n",
      "Iteration 3537: Policy loss: 2.182128. Value loss: 12.619074. Entropy: 0.954090.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3538: Policy loss: -0.542196. Value loss: 42.883183. Entropy: 1.109570.\n",
      "Iteration 3539: Policy loss: -0.725831. Value loss: 26.594944. Entropy: 1.110951.\n",
      "Iteration 3540: Policy loss: -0.699572. Value loss: 20.861803. Entropy: 1.094601.\n",
      "episode: 1484   score: 65.0  epsilon: 1.0    steps: 284  evaluation reward: 203.35\n",
      "episode: 1485   score: 310.0  epsilon: 1.0    steps: 407  evaluation reward: 203.8\n",
      "episode: 1486   score: 45.0  epsilon: 1.0    steps: 955  evaluation reward: 201.7\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3541: Policy loss: -1.682805. Value loss: 221.848679. Entropy: 1.053762.\n",
      "Iteration 3542: Policy loss: -1.633307. Value loss: 173.408722. Entropy: 1.032026.\n",
      "Iteration 3543: Policy loss: -2.089324. Value loss: 157.692963. Entropy: 0.986243.\n",
      "episode: 1487   score: 340.0  epsilon: 1.0    steps: 636  evaluation reward: 203.6\n",
      "episode: 1488   score: 190.0  epsilon: 1.0    steps: 845  evaluation reward: 204.4\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3544: Policy loss: 0.754470. Value loss: 60.931923. Entropy: 0.855783.\n",
      "Iteration 3545: Policy loss: 0.938904. Value loss: 31.313303. Entropy: 0.879923.\n",
      "Iteration 3546: Policy loss: 0.699164. Value loss: 25.368580. Entropy: 0.857584.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3547: Policy loss: -0.552424. Value loss: 228.728790. Entropy: 1.046086.\n",
      "Iteration 3548: Policy loss: -0.635647. Value loss: 199.685379. Entropy: 1.024430.\n",
      "Iteration 3549: Policy loss: -0.930748. Value loss: 121.773209. Entropy: 0.925970.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3550: Policy loss: 2.376374. Value loss: 47.312946. Entropy: 0.710794.\n",
      "Iteration 3551: Policy loss: 2.764639. Value loss: 21.155413. Entropy: 0.679958.\n",
      "Iteration 3552: Policy loss: 2.625243. Value loss: 15.508173. Entropy: 0.679462.\n",
      "episode: 1489   score: 435.0  epsilon: 1.0    steps: 188  evaluation reward: 206.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3553: Policy loss: -0.150898. Value loss: 296.161499. Entropy: 0.844270.\n",
      "Iteration 3554: Policy loss: 0.819574. Value loss: 164.058594. Entropy: 0.741616.\n",
      "Iteration 3555: Policy loss: 0.340853. Value loss: 110.467323. Entropy: 0.790038.\n",
      "episode: 1490   score: 160.0  epsilon: 1.0    steps: 436  evaluation reward: 206.05\n",
      "episode: 1491   score: 720.0  epsilon: 1.0    steps: 759  evaluation reward: 212.2\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3556: Policy loss: -0.830779. Value loss: 233.466507. Entropy: 0.851188.\n",
      "Iteration 3557: Policy loss: -0.770471. Value loss: 134.178452. Entropy: 0.798837.\n",
      "Iteration 3558: Policy loss: -0.857810. Value loss: 103.050667. Entropy: 0.807927.\n",
      "episode: 1492   score: 320.0  epsilon: 1.0    steps: 109  evaluation reward: 211.5\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3559: Policy loss: 0.497616. Value loss: 52.863373. Entropy: 0.823497.\n",
      "Iteration 3560: Policy loss: 0.604779. Value loss: 34.807911. Entropy: 0.858610.\n",
      "Iteration 3561: Policy loss: 0.613430. Value loss: 28.173071. Entropy: 0.857265.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3562: Policy loss: 4.543953. Value loss: 80.333122. Entropy: 0.742273.\n",
      "Iteration 3563: Policy loss: 4.647146. Value loss: 39.403446. Entropy: 0.774795.\n",
      "Iteration 3564: Policy loss: 4.502617. Value loss: 28.372679. Entropy: 0.748536.\n",
      "episode: 1493   score: 125.0  epsilon: 1.0    steps: 845  evaluation reward: 210.5\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3565: Policy loss: 3.017982. Value loss: 72.214203. Entropy: 0.730969.\n",
      "Iteration 3566: Policy loss: 3.065550. Value loss: 37.130257. Entropy: 0.733659.\n",
      "Iteration 3567: Policy loss: 3.144295. Value loss: 25.970966. Entropy: 0.747763.\n",
      "episode: 1494   score: 145.0  epsilon: 1.0    steps: 231  evaluation reward: 210.2\n",
      "episode: 1495   score: 290.0  epsilon: 1.0    steps: 572  evaluation reward: 206.5\n",
      "episode: 1496   score: 235.0  epsilon: 1.0    steps: 932  evaluation reward: 207.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3568: Policy loss: 4.092143. Value loss: 38.676937. Entropy: 0.854456.\n",
      "Iteration 3569: Policy loss: 4.266491. Value loss: 21.274792. Entropy: 0.872922.\n",
      "Iteration 3570: Policy loss: 4.176246. Value loss: 16.117832. Entropy: 0.874556.\n",
      "episode: 1497   score: 560.0  epsilon: 1.0    steps: 269  evaluation reward: 211.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3571: Policy loss: 0.631068. Value loss: 35.622650. Entropy: 0.774409.\n",
      "Iteration 3572: Policy loss: 0.385946. Value loss: 22.490881. Entropy: 0.771993.\n",
      "Iteration 3573: Policy loss: 0.263395. Value loss: 16.223959. Entropy: 0.766601.\n",
      "episode: 1498   score: 170.0  epsilon: 1.0    steps: 505  evaluation reward: 208.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1499   score: 165.0  epsilon: 1.0    steps: 643  evaluation reward: 207.7\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3574: Policy loss: 0.764462. Value loss: 38.377995. Entropy: 0.977512.\n",
      "Iteration 3575: Policy loss: 0.468942. Value loss: 23.501379. Entropy: 0.960963.\n",
      "Iteration 3576: Policy loss: 0.563487. Value loss: 17.304289. Entropy: 0.970583.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3577: Policy loss: -0.478043. Value loss: 28.697683. Entropy: 0.783407.\n",
      "Iteration 3578: Policy loss: -0.411476. Value loss: 19.396179. Entropy: 0.788610.\n",
      "Iteration 3579: Policy loss: -0.405480. Value loss: 14.669308. Entropy: 0.802209.\n",
      "episode: 1500   score: 200.0  epsilon: 1.0    steps: 50  evaluation reward: 208.65\n",
      "now time :  2019-02-25 19:46:47.936441\n",
      "episode: 1501   score: 105.0  epsilon: 1.0    steps: 877  evaluation reward: 208.35\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3580: Policy loss: 0.065368. Value loss: 21.384752. Entropy: 0.765654.\n",
      "Iteration 3581: Policy loss: -0.441049. Value loss: 15.127535. Entropy: 0.738036.\n",
      "Iteration 3582: Policy loss: -0.222424. Value loss: 13.435800. Entropy: 0.754242.\n",
      "episode: 1502   score: 175.0  epsilon: 1.0    steps: 967  evaluation reward: 206.25\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3583: Policy loss: 0.440315. Value loss: 29.050205. Entropy: 0.822858.\n",
      "Iteration 3584: Policy loss: 0.736930. Value loss: 15.620356. Entropy: 0.812091.\n",
      "Iteration 3585: Policy loss: 0.564756. Value loss: 13.783444. Entropy: 0.843085.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3586: Policy loss: -4.278852. Value loss: 409.092834. Entropy: 0.719120.\n",
      "Iteration 3587: Policy loss: -4.804626. Value loss: 201.226654. Entropy: 0.700037.\n",
      "Iteration 3588: Policy loss: -3.896836. Value loss: 145.815491. Entropy: 0.749392.\n",
      "episode: 1503   score: 225.0  epsilon: 1.0    steps: 146  evaluation reward: 204.05\n",
      "episode: 1504   score: 235.0  epsilon: 1.0    steps: 686  evaluation reward: 205.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3589: Policy loss: -1.447439. Value loss: 49.196999. Entropy: 0.830562.\n",
      "Iteration 3590: Policy loss: -1.364176. Value loss: 28.534704. Entropy: 0.867284.\n",
      "Iteration 3591: Policy loss: -1.201580. Value loss: 20.947792. Entropy: 0.858138.\n",
      "episode: 1505   score: 90.0  epsilon: 1.0    steps: 439  evaluation reward: 204.5\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3592: Policy loss: 0.382178. Value loss: 36.770222. Entropy: 0.945241.\n",
      "Iteration 3593: Policy loss: 0.469460. Value loss: 19.641586. Entropy: 0.967894.\n",
      "Iteration 3594: Policy loss: 0.382946. Value loss: 14.447771. Entropy: 0.960830.\n",
      "episode: 1506   score: 155.0  epsilon: 1.0    steps: 896  evaluation reward: 204.95\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3595: Policy loss: 2.749003. Value loss: 30.712124. Entropy: 0.846330.\n",
      "Iteration 3596: Policy loss: 2.210159. Value loss: 14.122372. Entropy: 0.879384.\n",
      "Iteration 3597: Policy loss: 2.296648. Value loss: 10.405000. Entropy: 0.870837.\n",
      "episode: 1507   score: 555.0  epsilon: 1.0    steps: 282  evaluation reward: 208.55\n",
      "episode: 1508   score: 485.0  epsilon: 1.0    steps: 583  evaluation reward: 211.8\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3598: Policy loss: -1.062808. Value loss: 208.728378. Entropy: 0.883787.\n",
      "Iteration 3599: Policy loss: -1.019210. Value loss: 142.625443. Entropy: 0.845174.\n",
      "Iteration 3600: Policy loss: -1.125009. Value loss: 122.949272. Entropy: 0.832145.\n",
      "episode: 1509   score: 405.0  epsilon: 1.0    steps: 109  evaluation reward: 215.15\n",
      "episode: 1510   score: 80.0  epsilon: 1.0    steps: 428  evaluation reward: 214.4\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3601: Policy loss: 0.977615. Value loss: 47.354065. Entropy: 0.899372.\n",
      "Iteration 3602: Policy loss: 0.916425. Value loss: 24.509335. Entropy: 0.928076.\n",
      "Iteration 3603: Policy loss: 0.730045. Value loss: 18.887354. Entropy: 0.938857.\n",
      "episode: 1511   score: 95.0  epsilon: 1.0    steps: 723  evaluation reward: 213.15\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3604: Policy loss: -1.026315. Value loss: 22.170822. Entropy: 0.866820.\n",
      "Iteration 3605: Policy loss: -0.969932. Value loss: 13.797312. Entropy: 0.830455.\n",
      "Iteration 3606: Policy loss: -0.979762. Value loss: 10.400427. Entropy: 0.865479.\n",
      "episode: 1512   score: 130.0  epsilon: 1.0    steps: 933  evaluation reward: 211.9\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3607: Policy loss: 0.354249. Value loss: 21.190691. Entropy: 0.999818.\n",
      "Iteration 3608: Policy loss: 0.009741. Value loss: 9.177395. Entropy: 0.989553.\n",
      "Iteration 3609: Policy loss: 0.276338. Value loss: 8.802262. Entropy: 0.994714.\n",
      "episode: 1513   score: 115.0  epsilon: 1.0    steps: 379  evaluation reward: 211.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3610: Policy loss: 1.260403. Value loss: 27.751802. Entropy: 0.906821.\n",
      "Iteration 3611: Policy loss: 1.016221. Value loss: 14.883157. Entropy: 0.934130.\n",
      "Iteration 3612: Policy loss: 1.129354. Value loss: 12.014628. Entropy: 0.904574.\n",
      "episode: 1514   score: 220.0  epsilon: 1.0    steps: 233  evaluation reward: 213.05\n",
      "episode: 1515   score: 130.0  epsilon: 1.0    steps: 597  evaluation reward: 212.35\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3613: Policy loss: 2.197557. Value loss: 23.727837. Entropy: 0.890580.\n",
      "Iteration 3614: Policy loss: 2.110855. Value loss: 14.667282. Entropy: 0.903377.\n",
      "Iteration 3615: Policy loss: 2.169298. Value loss: 11.738961. Entropy: 0.915623.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3616: Policy loss: -0.495693. Value loss: 23.265816. Entropy: 1.001639.\n",
      "Iteration 3617: Policy loss: -0.337230. Value loss: 13.627586. Entropy: 1.015672.\n",
      "Iteration 3618: Policy loss: -0.475536. Value loss: 11.083760. Entropy: 1.005092.\n",
      "episode: 1516   score: 135.0  epsilon: 1.0    steps: 6  evaluation reward: 211.7\n",
      "episode: 1517   score: 170.0  epsilon: 1.0    steps: 469  evaluation reward: 211.85\n",
      "episode: 1518   score: 320.0  epsilon: 1.0    steps: 851  evaluation reward: 213.9\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3619: Policy loss: 2.252479. Value loss: 17.786852. Entropy: 1.019187.\n",
      "Iteration 3620: Policy loss: 2.180503. Value loss: 11.555337. Entropy: 1.032184.\n",
      "Iteration 3621: Policy loss: 2.102966. Value loss: 7.802505. Entropy: 1.017103.\n",
      "episode: 1519   score: 80.0  epsilon: 1.0    steps: 655  evaluation reward: 213.15\n",
      "episode: 1520   score: 105.0  epsilon: 1.0    steps: 961  evaluation reward: 213.1\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3622: Policy loss: 0.546544. Value loss: 14.869720. Entropy: 0.948000.\n",
      "Iteration 3623: Policy loss: 0.253686. Value loss: 9.915448. Entropy: 0.948820.\n",
      "Iteration 3624: Policy loss: 0.469801. Value loss: 7.847096. Entropy: 0.959830.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3625: Policy loss: 0.812369. Value loss: 19.172535. Entropy: 0.934715.\n",
      "Iteration 3626: Policy loss: 0.645794. Value loss: 13.622821. Entropy: 0.926909.\n",
      "Iteration 3627: Policy loss: 0.883905. Value loss: 10.317039. Entropy: 0.913933.\n",
      "episode: 1521   score: 95.0  epsilon: 1.0    steps: 318  evaluation reward: 212.3\n",
      "episode: 1522   score: 95.0  epsilon: 1.0    steps: 591  evaluation reward: 212.45\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3628: Policy loss: 2.423448. Value loss: 22.142424. Entropy: 1.056089.\n",
      "Iteration 3629: Policy loss: 2.341140. Value loss: 12.418760. Entropy: 1.012866.\n",
      "Iteration 3630: Policy loss: 2.427978. Value loss: 10.871259. Entropy: 1.019111.\n",
      "episode: 1523   score: 120.0  epsilon: 1.0    steps: 121  evaluation reward: 211.5\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3631: Policy loss: 0.946047. Value loss: 24.331461. Entropy: 0.934780.\n",
      "Iteration 3632: Policy loss: 1.170804. Value loss: 14.054648. Entropy: 0.928356.\n",
      "Iteration 3633: Policy loss: 0.978903. Value loss: 11.296371. Entropy: 0.926829.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3634: Policy loss: -2.313374. Value loss: 34.889591. Entropy: 0.980185.\n",
      "Iteration 3635: Policy loss: -1.931997. Value loss: 17.445784. Entropy: 0.971353.\n",
      "Iteration 3636: Policy loss: -2.432507. Value loss: 14.402529. Entropy: 0.944617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3637: Policy loss: -2.699434. Value loss: 32.799004. Entropy: 0.900461.\n",
      "Iteration 3638: Policy loss: -2.910495. Value loss: 19.586990. Entropy: 0.927161.\n",
      "Iteration 3639: Policy loss: -2.960784. Value loss: 12.630003. Entropy: 0.911010.\n",
      "episode: 1524   score: 110.0  epsilon: 1.0    steps: 457  evaluation reward: 210.9\n",
      "episode: 1525   score: 175.0  epsilon: 1.0    steps: 877  evaluation reward: 211.55\n",
      "episode: 1526   score: 205.0  epsilon: 1.0    steps: 954  evaluation reward: 212.1\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3640: Policy loss: 0.984288. Value loss: 48.204117. Entropy: 1.022933.\n",
      "Iteration 3641: Policy loss: 0.944600. Value loss: 26.478954. Entropy: 0.998242.\n",
      "Iteration 3642: Policy loss: 0.979317. Value loss: 18.981312. Entropy: 1.007538.\n",
      "episode: 1527   score: 515.0  epsilon: 1.0    steps: 144  evaluation reward: 212.7\n",
      "episode: 1528   score: 115.0  epsilon: 1.0    steps: 315  evaluation reward: 210.1\n",
      "episode: 1529   score: 200.0  epsilon: 1.0    steps: 704  evaluation reward: 210.35\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3643: Policy loss: -2.305712. Value loss: 197.068039. Entropy: 1.064420.\n",
      "Iteration 3644: Policy loss: -2.022239. Value loss: 141.898468. Entropy: 1.041071.\n",
      "Iteration 3645: Policy loss: -2.318262. Value loss: 140.859421. Entropy: 0.972144.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3646: Policy loss: 0.431592. Value loss: 40.614399. Entropy: 0.855163.\n",
      "Iteration 3647: Policy loss: 0.511547. Value loss: 25.131704. Entropy: 0.845006.\n",
      "Iteration 3648: Policy loss: 0.666125. Value loss: 21.797504. Entropy: 0.847202.\n",
      "episode: 1530   score: 400.0  epsilon: 1.0    steps: 620  evaluation reward: 212.65\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3649: Policy loss: 4.583891. Value loss: 23.746143. Entropy: 0.709276.\n",
      "Iteration 3650: Policy loss: 4.389522. Value loss: 13.357492. Entropy: 0.740525.\n",
      "Iteration 3651: Policy loss: 4.459016. Value loss: 9.550244. Entropy: 0.747674.\n",
      "episode: 1531   score: 290.0  epsilon: 1.0    steps: 58  evaluation reward: 214.2\n",
      "episode: 1532   score: 75.0  epsilon: 1.0    steps: 189  evaluation reward: 212.6\n",
      "episode: 1533   score: 15.0  epsilon: 1.0    steps: 878  evaluation reward: 211.2\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3652: Policy loss: 1.283159. Value loss: 27.751308. Entropy: 1.022421.\n",
      "Iteration 3653: Policy loss: 1.341554. Value loss: 16.696714. Entropy: 1.036982.\n",
      "Iteration 3654: Policy loss: 1.538650. Value loss: 13.767813. Entropy: 0.999657.\n",
      "episode: 1534   score: 90.0  epsilon: 1.0    steps: 445  evaluation reward: 208.8\n",
      "episode: 1535   score: 135.0  epsilon: 1.0    steps: 938  evaluation reward: 208.9\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3655: Policy loss: 1.955571. Value loss: 21.843868. Entropy: 0.890812.\n",
      "Iteration 3656: Policy loss: 2.135119. Value loss: 12.137706. Entropy: 0.900405.\n",
      "Iteration 3657: Policy loss: 2.291589. Value loss: 10.265651. Entropy: 0.898191.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3658: Policy loss: 3.304319. Value loss: 27.401207. Entropy: 0.772470.\n",
      "Iteration 3659: Policy loss: 2.998715. Value loss: 11.539293. Entropy: 0.797889.\n",
      "Iteration 3660: Policy loss: 3.131283. Value loss: 9.541156. Entropy: 0.776538.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3661: Policy loss: 0.908520. Value loss: 15.364881. Entropy: 0.869998.\n",
      "Iteration 3662: Policy loss: 0.762432. Value loss: 9.265851. Entropy: 0.852372.\n",
      "Iteration 3663: Policy loss: 0.899459. Value loss: 7.915530. Entropy: 0.836328.\n",
      "episode: 1536   score: 155.0  epsilon: 1.0    steps: 362  evaluation reward: 205.85\n",
      "episode: 1537   score: 205.0  epsilon: 1.0    steps: 678  evaluation reward: 206.8\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3664: Policy loss: 1.802261. Value loss: 28.189482. Entropy: 0.917708.\n",
      "Iteration 3665: Policy loss: 1.909390. Value loss: 19.116486. Entropy: 0.871743.\n",
      "Iteration 3666: Policy loss: 1.742441. Value loss: 17.833105. Entropy: 0.899343.\n",
      "episode: 1538   score: 165.0  epsilon: 1.0    steps: 79  evaluation reward: 207.25\n",
      "episode: 1539   score: 25.0  epsilon: 1.0    steps: 172  evaluation reward: 206.25\n",
      "episode: 1540   score: 40.0  epsilon: 1.0    steps: 504  evaluation reward: 206.0\n",
      "episode: 1541   score: 145.0  epsilon: 1.0    steps: 613  evaluation reward: 205.7\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3667: Policy loss: 1.675609. Value loss: 22.840214. Entropy: 0.834897.\n",
      "Iteration 3668: Policy loss: 1.957233. Value loss: 14.294708. Entropy: 0.843369.\n",
      "Iteration 3669: Policy loss: 1.857627. Value loss: 12.918538. Entropy: 0.874365.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3670: Policy loss: -0.504880. Value loss: 18.495350. Entropy: 0.965272.\n",
      "Iteration 3671: Policy loss: -0.431105. Value loss: 12.232017. Entropy: 0.958739.\n",
      "Iteration 3672: Policy loss: -0.330487. Value loss: 9.451530. Entropy: 0.981175.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3673: Policy loss: -1.365891. Value loss: 22.693073. Entropy: 0.899236.\n",
      "Iteration 3674: Policy loss: -1.374919. Value loss: 14.659650. Entropy: 0.874630.\n",
      "Iteration 3675: Policy loss: -1.475085. Value loss: 13.700814. Entropy: 0.858856.\n",
      "episode: 1542   score: 75.0  epsilon: 1.0    steps: 688  evaluation reward: 204.0\n",
      "episode: 1543   score: 155.0  epsilon: 1.0    steps: 803  evaluation reward: 203.65\n",
      "episode: 1544   score: 145.0  epsilon: 1.0    steps: 923  evaluation reward: 204.0\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3676: Policy loss: 2.468711. Value loss: 16.085052. Entropy: 0.910269.\n",
      "Iteration 3677: Policy loss: 2.537684. Value loss: 8.120650. Entropy: 0.899136.\n",
      "Iteration 3678: Policy loss: 2.554049. Value loss: 6.647344. Entropy: 0.928881.\n",
      "episode: 1545   score: 80.0  epsilon: 1.0    steps: 22  evaluation reward: 203.25\n",
      "episode: 1546   score: 95.0  epsilon: 1.0    steps: 337  evaluation reward: 200.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3679: Policy loss: 0.192498. Value loss: 35.573902. Entropy: 0.866200.\n",
      "Iteration 3680: Policy loss: 0.213665. Value loss: 20.971434. Entropy: 0.876263.\n",
      "Iteration 3681: Policy loss: 0.239314. Value loss: 15.833775. Entropy: 0.872582.\n",
      "episode: 1547   score: 55.0  epsilon: 1.0    steps: 401  evaluation reward: 198.95\n",
      "episode: 1548   score: 130.0  epsilon: 1.0    steps: 630  evaluation reward: 197.95\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3682: Policy loss: -1.589611. Value loss: 21.288378. Entropy: 0.901935.\n",
      "Iteration 3683: Policy loss: -1.355067. Value loss: 10.885777. Entropy: 0.921662.\n",
      "Iteration 3684: Policy loss: -1.312546. Value loss: 9.738713. Entropy: 0.901511.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3685: Policy loss: -0.131686. Value loss: 23.103615. Entropy: 0.841983.\n",
      "Iteration 3686: Policy loss: 0.008222. Value loss: 14.374502. Entropy: 0.867851.\n",
      "Iteration 3687: Policy loss: 0.223083. Value loss: 11.999615. Entropy: 0.846897.\n",
      "episode: 1549   score: 205.0  epsilon: 1.0    steps: 147  evaluation reward: 198.65\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3688: Policy loss: -0.349019. Value loss: 19.397995. Entropy: 0.901228.\n",
      "Iteration 3689: Policy loss: -0.358765. Value loss: 12.904984. Entropy: 0.898849.\n",
      "Iteration 3690: Policy loss: -0.291861. Value loss: 9.280176. Entropy: 0.889070.\n",
      "episode: 1550   score: 85.0  epsilon: 1.0    steps: 310  evaluation reward: 197.7\n",
      "now time :  2019-02-25 19:48:52.530384\n",
      "episode: 1551   score: 145.0  epsilon: 1.0    steps: 835  evaluation reward: 197.55\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3691: Policy loss: -0.015985. Value loss: 20.070591. Entropy: 1.079225.\n",
      "Iteration 3692: Policy loss: -0.174375. Value loss: 14.462512. Entropy: 1.087750.\n",
      "Iteration 3693: Policy loss: -0.152999. Value loss: 14.281702. Entropy: 1.083409.\n",
      "episode: 1552   score: 160.0  epsilon: 1.0    steps: 25  evaluation reward: 196.7\n",
      "episode: 1553   score: 35.0  epsilon: 1.0    steps: 512  evaluation reward: 193.9\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3694: Policy loss: 2.387307. Value loss: 12.139969. Entropy: 0.815642.\n",
      "Iteration 3695: Policy loss: 2.034866. Value loss: 6.125576. Entropy: 0.829094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3696: Policy loss: 2.251923. Value loss: 4.433064. Entropy: 0.862905.\n",
      "episode: 1554   score: 120.0  epsilon: 1.0    steps: 741  evaluation reward: 193.4\n",
      "episode: 1555   score: 200.0  epsilon: 1.0    steps: 899  evaluation reward: 194.1\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3697: Policy loss: 1.662888. Value loss: 17.837080. Entropy: 0.818770.\n",
      "Iteration 3698: Policy loss: 1.621379. Value loss: 14.856506. Entropy: 0.830068.\n",
      "Iteration 3699: Policy loss: 1.611957. Value loss: 10.336171. Entropy: 0.826385.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3700: Policy loss: 0.169303. Value loss: 27.434158. Entropy: 0.966918.\n",
      "Iteration 3701: Policy loss: 0.269081. Value loss: 16.782471. Entropy: 0.956865.\n",
      "Iteration 3702: Policy loss: -0.036895. Value loss: 17.066956. Entropy: 0.974923.\n",
      "episode: 1556   score: 80.0  epsilon: 1.0    steps: 317  evaluation reward: 192.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3703: Policy loss: -3.491068. Value loss: 281.436035. Entropy: 1.012553.\n",
      "Iteration 3704: Policy loss: -2.850829. Value loss: 153.952744. Entropy: 0.951556.\n",
      "Iteration 3705: Policy loss: -2.694825. Value loss: 118.902939. Entropy: 0.930550.\n",
      "episode: 1557   score: 315.0  epsilon: 1.0    steps: 236  evaluation reward: 187.9\n",
      "episode: 1558   score: 65.0  epsilon: 1.0    steps: 827  evaluation reward: 187.45\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3706: Policy loss: 1.319999. Value loss: 15.948678. Entropy: 0.872145.\n",
      "Iteration 3707: Policy loss: 1.286402. Value loss: 11.160949. Entropy: 0.872750.\n",
      "Iteration 3708: Policy loss: 1.246195. Value loss: 7.633532. Entropy: 0.897905.\n",
      "episode: 1559   score: 150.0  epsilon: 1.0    steps: 32  evaluation reward: 185.55\n",
      "episode: 1560   score: 185.0  epsilon: 1.0    steps: 577  evaluation reward: 185.4\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3709: Policy loss: 0.453894. Value loss: 18.241741. Entropy: 0.884002.\n",
      "Iteration 3710: Policy loss: 0.587917. Value loss: 12.311000. Entropy: 0.869998.\n",
      "Iteration 3711: Policy loss: 0.345084. Value loss: 9.372926. Entropy: 0.877465.\n",
      "episode: 1561   score: 135.0  epsilon: 1.0    steps: 736  evaluation reward: 184.4\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3712: Policy loss: -0.664283. Value loss: 23.562147. Entropy: 0.884955.\n",
      "Iteration 3713: Policy loss: -0.781066. Value loss: 15.480861. Entropy: 0.878378.\n",
      "Iteration 3714: Policy loss: -0.642840. Value loss: 13.137928. Entropy: 0.869625.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3715: Policy loss: -1.014888. Value loss: 23.962656. Entropy: 1.024995.\n",
      "Iteration 3716: Policy loss: -1.202474. Value loss: 16.965670. Entropy: 0.999986.\n",
      "Iteration 3717: Policy loss: -0.936780. Value loss: 14.328628. Entropy: 1.006716.\n",
      "episode: 1562   score: 235.0  epsilon: 1.0    steps: 996  evaluation reward: 182.25\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3718: Policy loss: -0.646337. Value loss: 25.809877. Entropy: 0.912833.\n",
      "Iteration 3719: Policy loss: -0.709521. Value loss: 14.537603. Entropy: 0.932264.\n",
      "Iteration 3720: Policy loss: -0.768369. Value loss: 10.092725. Entropy: 0.910565.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3721: Policy loss: -0.944057. Value loss: 35.576717. Entropy: 0.943860.\n",
      "Iteration 3722: Policy loss: -0.902622. Value loss: 18.761595. Entropy: 0.960377.\n",
      "Iteration 3723: Policy loss: -0.992494. Value loss: 14.738586. Entropy: 0.958214.\n",
      "episode: 1563   score: 125.0  epsilon: 1.0    steps: 4  evaluation reward: 181.7\n",
      "episode: 1564   score: 165.0  epsilon: 1.0    steps: 221  evaluation reward: 181.15\n",
      "episode: 1565   score: 155.0  epsilon: 1.0    steps: 290  evaluation reward: 182.0\n",
      "episode: 1566   score: 135.0  epsilon: 1.0    steps: 604  evaluation reward: 181.55\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3724: Policy loss: 0.663901. Value loss: 33.406467. Entropy: 0.898102.\n",
      "Iteration 3725: Policy loss: 0.556890. Value loss: 14.119555. Entropy: 0.902098.\n",
      "Iteration 3726: Policy loss: 0.397645. Value loss: 11.291290. Entropy: 0.911236.\n",
      "episode: 1567   score: 300.0  epsilon: 1.0    steps: 428  evaluation reward: 182.95\n",
      "episode: 1568   score: 35.0  epsilon: 1.0    steps: 663  evaluation reward: 181.65\n",
      "episode: 1569   score: 290.0  epsilon: 1.0    steps: 860  evaluation reward: 181.95\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3727: Policy loss: 1.163006. Value loss: 9.518691. Entropy: 0.696121.\n",
      "Iteration 3728: Policy loss: 1.051363. Value loss: 6.347326. Entropy: 0.715430.\n",
      "Iteration 3729: Policy loss: 1.395646. Value loss: 5.778794. Entropy: 0.731852.\n",
      "episode: 1570   score: 20.0  epsilon: 1.0    steps: 977  evaluation reward: 180.55\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3730: Policy loss: -1.708381. Value loss: 21.342682. Entropy: 0.504892.\n",
      "Iteration 3731: Policy loss: -1.870444. Value loss: 18.318199. Entropy: 0.514336.\n",
      "Iteration 3732: Policy loss: -1.760521. Value loss: 15.307672. Entropy: 0.490342.\n",
      "episode: 1571   score: 55.0  epsilon: 1.0    steps: 113  evaluation reward: 180.0\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3733: Policy loss: -0.191127. Value loss: 12.133436. Entropy: 0.568952.\n",
      "Iteration 3734: Policy loss: -0.149813. Value loss: 9.891516. Entropy: 0.598346.\n",
      "Iteration 3735: Policy loss: -0.104434. Value loss: 7.243252. Entropy: 0.569137.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3736: Policy loss: -0.567305. Value loss: 16.641163. Entropy: 0.749421.\n",
      "Iteration 3737: Policy loss: -0.588642. Value loss: 9.766470. Entropy: 0.725241.\n",
      "Iteration 3738: Policy loss: -0.770046. Value loss: 7.952863. Entropy: 0.700933.\n",
      "episode: 1572   score: 135.0  epsilon: 1.0    steps: 237  evaluation reward: 180.0\n",
      "episode: 1573   score: 70.0  epsilon: 1.0    steps: 267  evaluation reward: 179.35\n",
      "episode: 1574   score: 65.0  epsilon: 1.0    steps: 604  evaluation reward: 179.55\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3739: Policy loss: 0.435743. Value loss: 23.343821. Entropy: 0.851631.\n",
      "Iteration 3740: Policy loss: 0.786149. Value loss: 17.730671. Entropy: 0.851524.\n",
      "Iteration 3741: Policy loss: 0.636306. Value loss: 14.503972. Entropy: 0.849452.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3742: Policy loss: 0.173123. Value loss: 24.007505. Entropy: 0.806113.\n",
      "Iteration 3743: Policy loss: 0.269270. Value loss: 16.991325. Entropy: 0.778200.\n",
      "Iteration 3744: Policy loss: 0.113978. Value loss: 13.363346. Entropy: 0.774811.\n",
      "episode: 1575   score: 210.0  epsilon: 1.0    steps: 425  evaluation reward: 179.5\n",
      "episode: 1576   score: 130.0  epsilon: 1.0    steps: 971  evaluation reward: 179.9\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3745: Policy loss: -1.812238. Value loss: 24.463211. Entropy: 0.837344.\n",
      "Iteration 3746: Policy loss: -1.994462. Value loss: 13.719025. Entropy: 0.832516.\n",
      "Iteration 3747: Policy loss: -1.662499. Value loss: 12.412639. Entropy: 0.827135.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3748: Policy loss: 0.038661. Value loss: 18.223618. Entropy: 0.834112.\n",
      "Iteration 3749: Policy loss: 0.023513. Value loss: 12.448957. Entropy: 0.830619.\n",
      "Iteration 3750: Policy loss: 0.109215. Value loss: 10.527567. Entropy: 0.831189.\n",
      "episode: 1577   score: 115.0  epsilon: 1.0    steps: 14  evaluation reward: 180.6\n",
      "episode: 1578   score: 235.0  epsilon: 1.0    steps: 756  evaluation reward: 180.55\n",
      "episode: 1579   score: 180.0  epsilon: 1.0    steps: 770  evaluation reward: 181.75\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3751: Policy loss: 1.602650. Value loss: 23.479010. Entropy: 0.561502.\n",
      "Iteration 3752: Policy loss: 1.761805. Value loss: 11.312103. Entropy: 0.546457.\n",
      "Iteration 3753: Policy loss: 1.815988. Value loss: 11.001423. Entropy: 0.569978.\n",
      "episode: 1580   score: 55.0  epsilon: 1.0    steps: 330  evaluation reward: 177.35\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3754: Policy loss: 0.421187. Value loss: 17.986809. Entropy: 0.925617.\n",
      "Iteration 3755: Policy loss: 0.600756. Value loss: 11.036923. Entropy: 0.907600.\n",
      "Iteration 3756: Policy loss: 0.518573. Value loss: 7.815747. Entropy: 0.907146.\n",
      "episode: 1581   score: 75.0  epsilon: 1.0    steps: 393  evaluation reward: 176.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3757: Policy loss: -0.990824. Value loss: 27.975145. Entropy: 0.834651.\n",
      "Iteration 3758: Policy loss: -0.807316. Value loss: 14.909947. Entropy: 0.871630.\n",
      "Iteration 3759: Policy loss: -0.734032. Value loss: 13.361601. Entropy: 0.831049.\n",
      "episode: 1582   score: 85.0  epsilon: 1.0    steps: 871  evaluation reward: 175.85\n",
      "episode: 1583   score: 140.0  epsilon: 1.0    steps: 978  evaluation reward: 174.8\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3760: Policy loss: -1.833234. Value loss: 30.747025. Entropy: 0.757478.\n",
      "Iteration 3761: Policy loss: -1.766570. Value loss: 19.388432. Entropy: 0.713523.\n",
      "Iteration 3762: Policy loss: -1.862978. Value loss: 16.032007. Entropy: 0.730162.\n",
      "episode: 1584   score: 235.0  epsilon: 1.0    steps: 134  evaluation reward: 176.5\n",
      "episode: 1585   score: 170.0  epsilon: 1.0    steps: 521  evaluation reward: 175.1\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3763: Policy loss: -0.523869. Value loss: 23.888737. Entropy: 0.765890.\n",
      "Iteration 3764: Policy loss: -0.566998. Value loss: 15.319512. Entropy: 0.721262.\n",
      "Iteration 3765: Policy loss: -0.489967. Value loss: 11.636781. Entropy: 0.734482.\n",
      "episode: 1586   score: 150.0  epsilon: 1.0    steps: 13  evaluation reward: 176.15\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3766: Policy loss: 0.175847. Value loss: 17.362478. Entropy: 0.761689.\n",
      "Iteration 3767: Policy loss: 0.241633. Value loss: 11.172336. Entropy: 0.777220.\n",
      "Iteration 3768: Policy loss: 0.140947. Value loss: 9.874558. Entropy: 0.759095.\n",
      "episode: 1587   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 174.85\n",
      "episode: 1588   score: 135.0  epsilon: 1.0    steps: 512  evaluation reward: 174.3\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3769: Policy loss: 0.055238. Value loss: 15.908510. Entropy: 0.800322.\n",
      "Iteration 3770: Policy loss: 0.303835. Value loss: 9.833899. Entropy: 0.779263.\n",
      "Iteration 3771: Policy loss: 0.167219. Value loss: 8.041371. Entropy: 0.819855.\n",
      "episode: 1589   score: 65.0  epsilon: 1.0    steps: 964  evaluation reward: 170.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3772: Policy loss: 2.363192. Value loss: 20.441645. Entropy: 0.807821.\n",
      "Iteration 3773: Policy loss: 2.496277. Value loss: 12.467370. Entropy: 0.808890.\n",
      "Iteration 3774: Policy loss: 2.465688. Value loss: 10.683701. Entropy: 0.781674.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3775: Policy loss: -0.073485. Value loss: 16.183065. Entropy: 0.865009.\n",
      "Iteration 3776: Policy loss: 0.097670. Value loss: 9.875361. Entropy: 0.878422.\n",
      "Iteration 3777: Policy loss: 0.061144. Value loss: 8.668719. Entropy: 0.835602.\n",
      "episode: 1590   score: 230.0  epsilon: 1.0    steps: 724  evaluation reward: 171.3\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3778: Policy loss: -1.405364. Value loss: 27.163927. Entropy: 0.876826.\n",
      "Iteration 3779: Policy loss: -1.414291. Value loss: 14.246454. Entropy: 0.888044.\n",
      "Iteration 3780: Policy loss: -1.479455. Value loss: 11.643378. Entropy: 0.883609.\n",
      "episode: 1591   score: 145.0  epsilon: 1.0    steps: 20  evaluation reward: 165.55\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3781: Policy loss: -0.520798. Value loss: 26.017134. Entropy: 0.801286.\n",
      "Iteration 3782: Policy loss: -0.777936. Value loss: 14.182686. Entropy: 0.800535.\n",
      "Iteration 3783: Policy loss: -0.741768. Value loss: 13.355509. Entropy: 0.805748.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3784: Policy loss: -3.674761. Value loss: 266.141235. Entropy: 0.822136.\n",
      "Iteration 3785: Policy loss: -3.336310. Value loss: 128.848862. Entropy: 0.784407.\n",
      "Iteration 3786: Policy loss: -2.935757. Value loss: 73.375847. Entropy: 0.777472.\n",
      "episode: 1592   score: 250.0  epsilon: 1.0    steps: 209  evaluation reward: 164.85\n",
      "episode: 1593   score: 165.0  epsilon: 1.0    steps: 293  evaluation reward: 165.25\n",
      "episode: 1594   score: 155.0  epsilon: 1.0    steps: 415  evaluation reward: 165.35\n",
      "episode: 1595   score: 560.0  epsilon: 1.0    steps: 546  evaluation reward: 168.05\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3787: Policy loss: 1.727220. Value loss: 21.098701. Entropy: 0.831096.\n",
      "Iteration 3788: Policy loss: 1.167113. Value loss: 13.179849. Entropy: 0.794792.\n",
      "Iteration 3789: Policy loss: 1.290335. Value loss: 10.451786. Entropy: 0.785848.\n",
      "episode: 1596   score: 200.0  epsilon: 1.0    steps: 824  evaluation reward: 167.7\n",
      "episode: 1597   score: 175.0  epsilon: 1.0    steps: 992  evaluation reward: 163.85\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3790: Policy loss: 3.314902. Value loss: 25.275068. Entropy: 0.804881.\n",
      "Iteration 3791: Policy loss: 3.476334. Value loss: 16.293938. Entropy: 0.824301.\n",
      "Iteration 3792: Policy loss: 3.386518. Value loss: 13.446301. Entropy: 0.821134.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3793: Policy loss: 3.898501. Value loss: 14.443712. Entropy: 0.753913.\n",
      "Iteration 3794: Policy loss: 3.735173. Value loss: 6.750707. Entropy: 0.790800.\n",
      "Iteration 3795: Policy loss: 3.721400. Value loss: 5.296805. Entropy: 0.795934.\n",
      "episode: 1598   score: 80.0  epsilon: 1.0    steps: 19  evaluation reward: 162.95\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3796: Policy loss: 1.090898. Value loss: 9.888583. Entropy: 0.692757.\n",
      "Iteration 3797: Policy loss: 1.094337. Value loss: 5.699481. Entropy: 0.731034.\n",
      "Iteration 3798: Policy loss: 1.142761. Value loss: 4.693959. Entropy: 0.726693.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3799: Policy loss: -2.330673. Value loss: 29.650354. Entropy: 0.691384.\n",
      "Iteration 3800: Policy loss: -2.173385. Value loss: 20.026785. Entropy: 0.649203.\n",
      "Iteration 3801: Policy loss: -2.546966. Value loss: 16.735865. Entropy: 0.660768.\n",
      "episode: 1599   score: 125.0  epsilon: 1.0    steps: 227  evaluation reward: 162.55\n",
      "episode: 1600   score: 115.0  epsilon: 1.0    steps: 403  evaluation reward: 161.7\n",
      "now time :  2019-02-25 19:50:57.058555\n",
      "episode: 1601   score: 160.0  epsilon: 1.0    steps: 557  evaluation reward: 162.25\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3802: Policy loss: -1.447205. Value loss: 29.658527. Entropy: 0.748014.\n",
      "Iteration 3803: Policy loss: -1.149785. Value loss: 16.606369. Entropy: 0.756949.\n",
      "Iteration 3804: Policy loss: -1.474172. Value loss: 15.660563. Entropy: 0.739298.\n",
      "episode: 1602   score: 230.0  epsilon: 1.0    steps: 703  evaluation reward: 162.8\n",
      "episode: 1603   score: 225.0  epsilon: 1.0    steps: 867  evaluation reward: 162.8\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3805: Policy loss: -2.141939. Value loss: 28.070539. Entropy: 0.729761.\n",
      "Iteration 3806: Policy loss: -2.140708. Value loss: 17.702559. Entropy: 0.784037.\n",
      "Iteration 3807: Policy loss: -2.276739. Value loss: 15.851267. Entropy: 0.742529.\n",
      "episode: 1604   score: 140.0  epsilon: 1.0    steps: 903  evaluation reward: 161.85\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3808: Policy loss: 1.616744. Value loss: 19.117468. Entropy: 0.561973.\n",
      "Iteration 3809: Policy loss: 1.884283. Value loss: 8.533882. Entropy: 0.585953.\n",
      "Iteration 3810: Policy loss: 1.860548. Value loss: 6.531301. Entropy: 0.567450.\n",
      "episode: 1605   score: 150.0  epsilon: 1.0    steps: 86  evaluation reward: 162.45\n",
      "episode: 1606   score: 205.0  epsilon: 1.0    steps: 302  evaluation reward: 162.95\n",
      "episode: 1607   score: 35.0  epsilon: 1.0    steps: 592  evaluation reward: 157.75\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3811: Policy loss: 1.155349. Value loss: 24.674044. Entropy: 0.592390.\n",
      "Iteration 3812: Policy loss: 1.200581. Value loss: 14.327293. Entropy: 0.645997.\n",
      "Iteration 3813: Policy loss: 1.042430. Value loss: 10.965528. Entropy: 0.601906.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3814: Policy loss: -0.887382. Value loss: 16.792255. Entropy: 0.836155.\n",
      "Iteration 3815: Policy loss: -0.811744. Value loss: 12.040401. Entropy: 0.790043.\n",
      "Iteration 3816: Policy loss: -1.011219. Value loss: 9.470098. Entropy: 0.825975.\n",
      "episode: 1608   score: 75.0  epsilon: 1.0    steps: 249  evaluation reward: 153.65\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3817: Policy loss: -0.867978. Value loss: 29.438631. Entropy: 0.767398.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3818: Policy loss: -0.725947. Value loss: 22.662310. Entropy: 0.800666.\n",
      "Iteration 3819: Policy loss: -0.972820. Value loss: 17.273247. Entropy: 0.785527.\n",
      "episode: 1609   score: 160.0  epsilon: 1.0    steps: 723  evaluation reward: 151.2\n",
      "episode: 1610   score: 55.0  epsilon: 1.0    steps: 964  evaluation reward: 150.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3820: Policy loss: -1.082112. Value loss: 19.939062. Entropy: 0.896103.\n",
      "Iteration 3821: Policy loss: -1.103699. Value loss: 11.931809. Entropy: 0.958271.\n",
      "Iteration 3822: Policy loss: -1.321370. Value loss: 9.251760. Entropy: 0.955404.\n",
      "episode: 1611   score: 140.0  epsilon: 1.0    steps: 113  evaluation reward: 151.4\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3823: Policy loss: -0.012587. Value loss: 26.853262. Entropy: 0.832468.\n",
      "Iteration 3824: Policy loss: 0.010993. Value loss: 15.738640. Entropy: 0.799419.\n",
      "Iteration 3825: Policy loss: 0.010936. Value loss: 12.942081. Entropy: 0.835199.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3826: Policy loss: 1.186627. Value loss: 20.450384. Entropy: 0.649146.\n",
      "Iteration 3827: Policy loss: 1.388873. Value loss: 8.973187. Entropy: 0.676137.\n",
      "Iteration 3828: Policy loss: 0.938002. Value loss: 7.457014. Entropy: 0.691662.\n",
      "episode: 1612   score: 275.0  epsilon: 1.0    steps: 486  evaluation reward: 152.85\n",
      "episode: 1613   score: 270.0  epsilon: 1.0    steps: 853  evaluation reward: 154.4\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3829: Policy loss: -0.103896. Value loss: 19.975723. Entropy: 0.785146.\n",
      "Iteration 3830: Policy loss: -0.021746. Value loss: 10.619536. Entropy: 0.762891.\n",
      "Iteration 3831: Policy loss: -0.039601. Value loss: 9.327938. Entropy: 0.781422.\n",
      "episode: 1614   score: 240.0  epsilon: 1.0    steps: 605  evaluation reward: 154.6\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3832: Policy loss: 1.323481. Value loss: 21.087305. Entropy: 0.735588.\n",
      "Iteration 3833: Policy loss: 1.570104. Value loss: 14.410709. Entropy: 0.745824.\n",
      "Iteration 3834: Policy loss: 1.534339. Value loss: 10.926757. Entropy: 0.744301.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3835: Policy loss: 0.917563. Value loss: 23.047235. Entropy: 0.863018.\n",
      "Iteration 3836: Policy loss: 1.058596. Value loss: 15.345510. Entropy: 0.846681.\n",
      "Iteration 3837: Policy loss: 0.976024. Value loss: 12.644193. Entropy: 0.786507.\n",
      "episode: 1615   score: 150.0  epsilon: 1.0    steps: 756  evaluation reward: 154.8\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3838: Policy loss: 0.088525. Value loss: 31.082088. Entropy: 0.727455.\n",
      "Iteration 3839: Policy loss: 0.253954. Value loss: 18.569500. Entropy: 0.742476.\n",
      "Iteration 3840: Policy loss: 0.370622. Value loss: 13.011412. Entropy: 0.727737.\n",
      "episode: 1616   score: 290.0  epsilon: 1.0    steps: 291  evaluation reward: 156.35\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3841: Policy loss: -1.657722. Value loss: 195.914627. Entropy: 0.755338.\n",
      "Iteration 3842: Policy loss: -0.813657. Value loss: 86.323662. Entropy: 0.480288.\n",
      "Iteration 3843: Policy loss: -0.986914. Value loss: 54.270901. Entropy: 0.523834.\n",
      "episode: 1617   score: 165.0  epsilon: 1.0    steps: 4  evaluation reward: 156.3\n",
      "episode: 1618   score: 495.0  epsilon: 1.0    steps: 165  evaluation reward: 158.05\n",
      "episode: 1619   score: 115.0  epsilon: 1.0    steps: 860  evaluation reward: 158.4\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3844: Policy loss: -3.378090. Value loss: 152.931839. Entropy: 0.495561.\n",
      "Iteration 3845: Policy loss: -3.611030. Value loss: 44.704220. Entropy: 0.492991.\n",
      "Iteration 3846: Policy loss: -3.395799. Value loss: 25.129726. Entropy: 0.464756.\n",
      "episode: 1620   score: 125.0  epsilon: 1.0    steps: 614  evaluation reward: 158.6\n",
      "episode: 1621   score: 265.0  epsilon: 1.0    steps: 941  evaluation reward: 160.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3847: Policy loss: 1.302846. Value loss: 38.586945. Entropy: 0.548629.\n",
      "Iteration 3848: Policy loss: 1.127497. Value loss: 14.231504. Entropy: 0.557128.\n",
      "Iteration 3849: Policy loss: 1.350021. Value loss: 11.025216. Entropy: 0.549821.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3850: Policy loss: 0.168663. Value loss: 11.708791. Entropy: 0.562427.\n",
      "Iteration 3851: Policy loss: 0.167248. Value loss: 8.338347. Entropy: 0.564884.\n",
      "Iteration 3852: Policy loss: 0.257760. Value loss: 6.500677. Entropy: 0.597784.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3853: Policy loss: 1.629894. Value loss: 27.643248. Entropy: 0.498792.\n",
      "Iteration 3854: Policy loss: 1.572243. Value loss: 16.689959. Entropy: 0.538101.\n",
      "Iteration 3855: Policy loss: 1.431744. Value loss: 13.565146. Entropy: 0.559807.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3856: Policy loss: -3.795169. Value loss: 465.754181. Entropy: 0.764811.\n",
      "Iteration 3857: Policy loss: -2.933885. Value loss: 331.964539. Entropy: 0.647590.\n",
      "Iteration 3858: Policy loss: -4.278259. Value loss: 271.474487. Entropy: 0.658314.\n",
      "episode: 1622   score: 155.0  epsilon: 1.0    steps: 675  evaluation reward: 160.9\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3859: Policy loss: 1.248592. Value loss: 32.674835. Entropy: 0.539538.\n",
      "Iteration 3860: Policy loss: 1.534727. Value loss: 17.489746. Entropy: 0.512976.\n",
      "Iteration 3861: Policy loss: 1.247170. Value loss: 12.428912. Entropy: 0.499611.\n",
      "episode: 1623   score: 355.0  epsilon: 1.0    steps: 41  evaluation reward: 163.25\n",
      "episode: 1624   score: 450.0  epsilon: 1.0    steps: 459  evaluation reward: 166.65\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3862: Policy loss: -1.407767. Value loss: 44.620235. Entropy: 0.422449.\n",
      "Iteration 3863: Policy loss: -1.241200. Value loss: 20.429201. Entropy: 0.420466.\n",
      "Iteration 3864: Policy loss: -1.367697. Value loss: 17.304401. Entropy: 0.430270.\n",
      "episode: 1625   score: 225.0  epsilon: 1.0    steps: 376  evaluation reward: 167.15\n",
      "episode: 1626   score: 310.0  epsilon: 1.0    steps: 871  evaluation reward: 168.2\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3865: Policy loss: 0.457781. Value loss: 58.456478. Entropy: 0.640597.\n",
      "Iteration 3866: Policy loss: 0.480044. Value loss: 37.265495. Entropy: 0.650146.\n",
      "Iteration 3867: Policy loss: 0.493582. Value loss: 26.985331. Entropy: 0.650086.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3868: Policy loss: -0.487628. Value loss: 27.475517. Entropy: 0.559850.\n",
      "Iteration 3869: Policy loss: -0.385677. Value loss: 17.146818. Entropy: 0.560976.\n",
      "Iteration 3870: Policy loss: -0.575249. Value loss: 13.842108. Entropy: 0.569778.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3871: Policy loss: -0.959160. Value loss: 46.821381. Entropy: 0.624402.\n",
      "Iteration 3872: Policy loss: -0.846969. Value loss: 26.137320. Entropy: 0.606591.\n",
      "Iteration 3873: Policy loss: -0.723001. Value loss: 21.152983. Entropy: 0.603589.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3874: Policy loss: 1.436117. Value loss: 49.007847. Entropy: 0.465758.\n",
      "Iteration 3875: Policy loss: 1.338519. Value loss: 26.971594. Entropy: 0.463308.\n",
      "Iteration 3876: Policy loss: 1.371474. Value loss: 22.680796. Entropy: 0.492687.\n",
      "episode: 1627   score: 150.0  epsilon: 1.0    steps: 472  evaluation reward: 164.55\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3877: Policy loss: 4.599454. Value loss: 31.156134. Entropy: 0.588155.\n",
      "Iteration 3878: Policy loss: 4.679482. Value loss: 19.255987. Entropy: 0.619054.\n",
      "Iteration 3879: Policy loss: 4.207891. Value loss: 13.056346. Entropy: 0.652453.\n",
      "episode: 1628   score: 265.0  epsilon: 1.0    steps: 633  evaluation reward: 166.05\n",
      "episode: 1629   score: 105.0  epsilon: 1.0    steps: 864  evaluation reward: 165.1\n",
      "episode: 1630   score: 515.0  epsilon: 1.0    steps: 936  evaluation reward: 166.25\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3880: Policy loss: 0.123534. Value loss: 249.061127. Entropy: 0.696838.\n",
      "Iteration 3881: Policy loss: 0.287461. Value loss: 112.083450. Entropy: 0.700504.\n",
      "Iteration 3882: Policy loss: 0.271402. Value loss: 131.591431. Entropy: 0.674996.\n",
      "episode: 1631   score: 575.0  epsilon: 1.0    steps: 192  evaluation reward: 169.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3883: Policy loss: 1.768116. Value loss: 25.668589. Entropy: 0.744692.\n",
      "Iteration 3884: Policy loss: 1.938274. Value loss: 17.211302. Entropy: 0.748281.\n",
      "Iteration 3885: Policy loss: 1.805433. Value loss: 14.154957. Entropy: 0.737697.\n",
      "episode: 1632   score: 380.0  epsilon: 1.0    steps: 728  evaluation reward: 172.15\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3886: Policy loss: 0.042832. Value loss: 30.538105. Entropy: 0.823056.\n",
      "Iteration 3887: Policy loss: 0.187959. Value loss: 20.343138. Entropy: 0.827826.\n",
      "Iteration 3888: Policy loss: 0.127548. Value loss: 17.184740. Entropy: 0.812576.\n",
      "episode: 1633   score: 230.0  epsilon: 1.0    steps: 118  evaluation reward: 174.3\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3889: Policy loss: 3.164565. Value loss: 37.313778. Entropy: 0.843112.\n",
      "Iteration 3890: Policy loss: 3.057849. Value loss: 16.425806. Entropy: 0.850409.\n",
      "Iteration 3891: Policy loss: 2.946158. Value loss: 12.865721. Entropy: 0.867870.\n",
      "episode: 1634   score: 295.0  epsilon: 1.0    steps: 330  evaluation reward: 176.35\n",
      "episode: 1635   score: 200.0  epsilon: 1.0    steps: 467  evaluation reward: 177.0\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3892: Policy loss: -3.060574. Value loss: 181.400604. Entropy: 0.627118.\n",
      "Iteration 3893: Policy loss: -4.413174. Value loss: 183.800064. Entropy: 0.565550.\n",
      "Iteration 3894: Policy loss: -3.719028. Value loss: 142.308624. Entropy: 0.561091.\n",
      "episode: 1636   score: 160.0  epsilon: 1.0    steps: 873  evaluation reward: 177.05\n",
      "episode: 1637   score: 130.0  epsilon: 1.0    steps: 965  evaluation reward: 176.3\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3895: Policy loss: 0.708650. Value loss: 220.195663. Entropy: 0.771327.\n",
      "Iteration 3896: Policy loss: 0.333420. Value loss: 158.573074. Entropy: 0.797146.\n",
      "Iteration 3897: Policy loss: 0.488641. Value loss: 139.026276. Entropy: 0.774126.\n",
      "episode: 1638   score: 205.0  epsilon: 1.0    steps: 620  evaluation reward: 176.7\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3898: Policy loss: 3.647544. Value loss: 81.204811. Entropy: 0.838460.\n",
      "Iteration 3899: Policy loss: 3.569599. Value loss: 38.763668. Entropy: 0.836136.\n",
      "Iteration 3900: Policy loss: 3.536808. Value loss: 34.974133. Entropy: 0.845650.\n",
      "episode: 1639   score: 185.0  epsilon: 1.0    steps: 113  evaluation reward: 178.3\n",
      "episode: 1640   score: 65.0  epsilon: 1.0    steps: 423  evaluation reward: 178.55\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3901: Policy loss: 2.006298. Value loss: 29.138109. Entropy: 0.759564.\n",
      "Iteration 3902: Policy loss: 1.957772. Value loss: 17.180153. Entropy: 0.779289.\n",
      "Iteration 3903: Policy loss: 1.919894. Value loss: 13.250432. Entropy: 0.767257.\n",
      "episode: 1641   score: 400.0  epsilon: 1.0    steps: 143  evaluation reward: 181.1\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3904: Policy loss: 3.913487. Value loss: 103.900230. Entropy: 0.704637.\n",
      "Iteration 3905: Policy loss: 4.080754. Value loss: 47.245880. Entropy: 0.703991.\n",
      "Iteration 3906: Policy loss: 3.850743. Value loss: 33.855042. Entropy: 0.742466.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3907: Policy loss: 5.055170. Value loss: 51.139015. Entropy: 0.796322.\n",
      "Iteration 3908: Policy loss: 4.217959. Value loss: 22.553226. Entropy: 0.809181.\n",
      "Iteration 3909: Policy loss: 4.382005. Value loss: 16.135460. Entropy: 0.805996.\n",
      "episode: 1642   score: 140.0  epsilon: 1.0    steps: 338  evaluation reward: 181.75\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3910: Policy loss: -0.176657. Value loss: 48.902004. Entropy: 0.663583.\n",
      "Iteration 3911: Policy loss: 0.136986. Value loss: 27.289408. Entropy: 0.652260.\n",
      "Iteration 3912: Policy loss: 0.032613. Value loss: 18.531199. Entropy: 0.667659.\n",
      "episode: 1643   score: 105.0  epsilon: 1.0    steps: 560  evaluation reward: 181.25\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3913: Policy loss: 1.317901. Value loss: 59.453419. Entropy: 0.751701.\n",
      "Iteration 3914: Policy loss: 1.491322. Value loss: 26.156483. Entropy: 0.763367.\n",
      "Iteration 3915: Policy loss: 1.444456. Value loss: 19.419901. Entropy: 0.779596.\n",
      "episode: 1644   score: 85.0  epsilon: 1.0    steps: 126  evaluation reward: 180.65\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3916: Policy loss: -1.754157. Value loss: 293.699341. Entropy: 0.873673.\n",
      "Iteration 3917: Policy loss: -1.271812. Value loss: 168.091583. Entropy: 0.781155.\n",
      "Iteration 3918: Policy loss: -1.410103. Value loss: 103.391136. Entropy: 0.760715.\n",
      "episode: 1645   score: 180.0  epsilon: 1.0    steps: 178  evaluation reward: 181.65\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3919: Policy loss: 1.866771. Value loss: 37.677303. Entropy: 0.561329.\n",
      "Iteration 3920: Policy loss: 2.196576. Value loss: 19.923317. Entropy: 0.592486.\n",
      "Iteration 3921: Policy loss: 1.893421. Value loss: 15.167665. Entropy: 0.561959.\n",
      "episode: 1646   score: 900.0  epsilon: 1.0    steps: 657  evaluation reward: 189.7\n",
      "episode: 1647   score: 355.0  epsilon: 1.0    steps: 961  evaluation reward: 192.7\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3922: Policy loss: 3.261243. Value loss: 48.201851. Entropy: 0.634549.\n",
      "Iteration 3923: Policy loss: 3.315677. Value loss: 25.504150. Entropy: 0.648073.\n",
      "Iteration 3924: Policy loss: 3.305613. Value loss: 17.089460. Entropy: 0.645886.\n",
      "episode: 1648   score: 140.0  epsilon: 1.0    steps: 310  evaluation reward: 192.8\n",
      "episode: 1649   score: 345.0  epsilon: 1.0    steps: 425  evaluation reward: 194.2\n",
      "episode: 1650   score: 65.0  epsilon: 1.0    steps: 582  evaluation reward: 194.0\n",
      "now time :  2019-02-25 19:53:15.775104\n",
      "episode: 1651   score: 205.0  epsilon: 1.0    steps: 785  evaluation reward: 194.6\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3925: Policy loss: 0.622507. Value loss: 28.760254. Entropy: 0.550246.\n",
      "Iteration 3926: Policy loss: 0.561126. Value loss: 13.961058. Entropy: 0.523904.\n",
      "Iteration 3927: Policy loss: 0.626090. Value loss: 13.011758. Entropy: 0.510668.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3928: Policy loss: 0.449013. Value loss: 23.953104. Entropy: 0.790800.\n",
      "Iteration 3929: Policy loss: 0.241117. Value loss: 16.783289. Entropy: 0.753289.\n",
      "Iteration 3930: Policy loss: 0.326716. Value loss: 12.821317. Entropy: 0.766294.\n",
      "episode: 1652   score: 70.0  epsilon: 1.0    steps: 26  evaluation reward: 193.7\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3931: Policy loss: 2.326037. Value loss: 27.275745. Entropy: 0.900078.\n",
      "Iteration 3932: Policy loss: 2.375082. Value loss: 18.508587. Entropy: 0.874496.\n",
      "Iteration 3933: Policy loss: 2.359877. Value loss: 16.294611. Entropy: 0.896065.\n",
      "episode: 1653   score: 160.0  epsilon: 1.0    steps: 179  evaluation reward: 194.95\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3934: Policy loss: 2.461376. Value loss: 10.417789. Entropy: 0.788972.\n",
      "Iteration 3935: Policy loss: 2.311259. Value loss: 5.905224. Entropy: 0.803429.\n",
      "Iteration 3936: Policy loss: 2.491246. Value loss: 5.047234. Entropy: 0.804190.\n",
      "episode: 1654   score: 105.0  epsilon: 1.0    steps: 728  evaluation reward: 194.8\n",
      "episode: 1655   score: 80.0  epsilon: 1.0    steps: 926  evaluation reward: 193.6\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3937: Policy loss: 2.349411. Value loss: 16.022583. Entropy: 0.771959.\n",
      "Iteration 3938: Policy loss: 2.631777. Value loss: 8.578366. Entropy: 0.794404.\n",
      "Iteration 3939: Policy loss: 2.337279. Value loss: 7.335492. Entropy: 0.785268.\n",
      "episode: 1656   score: 75.0  epsilon: 1.0    steps: 52  evaluation reward: 193.55\n",
      "episode: 1657   score: 75.0  epsilon: 1.0    steps: 557  evaluation reward: 191.15\n",
      "episode: 1658   score: 150.0  epsilon: 1.0    steps: 866  evaluation reward: 192.0\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3940: Policy loss: 1.246667. Value loss: 18.348602. Entropy: 0.676258.\n",
      "Iteration 3941: Policy loss: 1.226342. Value loss: 12.033164. Entropy: 0.685326.\n",
      "Iteration 3942: Policy loss: 1.220014. Value loss: 9.026778. Entropy: 0.678110.\n",
      "episode: 1659   score: 205.0  epsilon: 1.0    steps: 376  evaluation reward: 192.55\n",
      "episode: 1660   score: 170.0  epsilon: 1.0    steps: 512  evaluation reward: 192.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3943: Policy loss: -1.359923. Value loss: 30.184959. Entropy: 0.903534.\n",
      "Iteration 3944: Policy loss: -1.461080. Value loss: 19.421486. Entropy: 0.887322.\n",
      "Iteration 3945: Policy loss: -1.242838. Value loss: 18.694124. Entropy: 0.887856.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3946: Policy loss: 0.088936. Value loss: 10.906717. Entropy: 0.875661.\n",
      "Iteration 3947: Policy loss: 0.134971. Value loss: 7.300690. Entropy: 0.876021.\n",
      "Iteration 3948: Policy loss: 0.019350. Value loss: 7.638121. Entropy: 0.886967.\n",
      "episode: 1661   score: 75.0  epsilon: 1.0    steps: 952  evaluation reward: 191.8\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3949: Policy loss: 1.375321. Value loss: 20.641426. Entropy: 0.908200.\n",
      "Iteration 3950: Policy loss: 1.359989. Value loss: 12.902821. Entropy: 0.896101.\n",
      "Iteration 3951: Policy loss: 1.252145. Value loss: 11.367594. Entropy: 0.905376.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3952: Policy loss: 0.152733. Value loss: 28.628860. Entropy: 0.928642.\n",
      "Iteration 3953: Policy loss: -0.112932. Value loss: 16.594503. Entropy: 0.915771.\n",
      "Iteration 3954: Policy loss: -0.019007. Value loss: 13.395580. Entropy: 0.907719.\n",
      "episode: 1662   score: 120.0  epsilon: 1.0    steps: 67  evaluation reward: 190.65\n",
      "episode: 1663   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 190.95\n",
      "episode: 1664   score: 125.0  epsilon: 1.0    steps: 554  evaluation reward: 190.55\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3955: Policy loss: 0.923905. Value loss: 16.440945. Entropy: 0.764911.\n",
      "Iteration 3956: Policy loss: 0.982295. Value loss: 7.218238. Entropy: 0.757210.\n",
      "Iteration 3957: Policy loss: 0.971577. Value loss: 6.814905. Entropy: 0.786423.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3958: Policy loss: -0.106423. Value loss: 24.362463. Entropy: 0.943571.\n",
      "Iteration 3959: Policy loss: -0.134584. Value loss: 14.016745. Entropy: 0.937278.\n",
      "Iteration 3960: Policy loss: -0.116260. Value loss: 10.589505. Entropy: 0.937751.\n",
      "episode: 1665   score: 115.0  epsilon: 1.0    steps: 264  evaluation reward: 190.15\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3961: Policy loss: -0.572323. Value loss: 35.254436. Entropy: 0.807003.\n",
      "Iteration 3962: Policy loss: -0.627171. Value loss: 19.715063. Entropy: 0.803549.\n",
      "Iteration 3963: Policy loss: -0.535893. Value loss: 12.945395. Entropy: 0.798202.\n",
      "episode: 1666   score: 150.0  epsilon: 1.0    steps: 968  evaluation reward: 190.3\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3964: Policy loss: -1.063077. Value loss: 29.913933. Entropy: 0.950824.\n",
      "Iteration 3965: Policy loss: -0.980336. Value loss: 16.590927. Entropy: 0.942832.\n",
      "Iteration 3966: Policy loss: -0.933773. Value loss: 14.214768. Entropy: 0.962013.\n",
      "episode: 1667   score: 340.0  epsilon: 1.0    steps: 444  evaluation reward: 190.7\n",
      "episode: 1668   score: 85.0  epsilon: 1.0    steps: 547  evaluation reward: 191.2\n",
      "episode: 1669   score: 420.0  epsilon: 1.0    steps: 766  evaluation reward: 192.5\n",
      "episode: 1670   score: 225.0  epsilon: 1.0    steps: 823  evaluation reward: 194.55\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3967: Policy loss: -1.260282. Value loss: 24.516624. Entropy: 0.962598.\n",
      "Iteration 3968: Policy loss: -1.635193. Value loss: 16.374445. Entropy: 0.942084.\n",
      "Iteration 3969: Policy loss: -1.500209. Value loss: 13.209600. Entropy: 0.940792.\n",
      "episode: 1671   score: 185.0  epsilon: 1.0    steps: 123  evaluation reward: 195.85\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3970: Policy loss: -2.646448. Value loss: 26.841778. Entropy: 0.971512.\n",
      "Iteration 3971: Policy loss: -2.633597. Value loss: 16.084036. Entropy: 0.976642.\n",
      "Iteration 3972: Policy loss: -2.498940. Value loss: 11.907911. Entropy: 0.970343.\n",
      "episode: 1672   score: 125.0  epsilon: 1.0    steps: 283  evaluation reward: 195.75\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3973: Policy loss: -1.509678. Value loss: 23.162258. Entropy: 1.110357.\n",
      "Iteration 3974: Policy loss: -1.433448. Value loss: 13.006215. Entropy: 1.121844.\n",
      "Iteration 3975: Policy loss: -1.479452. Value loss: 10.281493. Entropy: 1.119327.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3976: Policy loss: 1.055329. Value loss: 31.221104. Entropy: 0.861193.\n",
      "Iteration 3977: Policy loss: 1.009184. Value loss: 18.173641. Entropy: 0.858860.\n",
      "Iteration 3978: Policy loss: 0.755501. Value loss: 15.556527. Entropy: 0.842465.\n",
      "episode: 1673   score: 370.0  epsilon: 1.0    steps: 189  evaluation reward: 198.75\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3979: Policy loss: 0.316644. Value loss: 30.721487. Entropy: 0.723483.\n",
      "Iteration 3980: Policy loss: -0.082786. Value loss: 16.946190. Entropy: 0.700035.\n",
      "Iteration 3981: Policy loss: -0.151523. Value loss: 14.082750. Entropy: 0.676883.\n",
      "episode: 1674   score: 105.0  epsilon: 1.0    steps: 698  evaluation reward: 199.15\n",
      "episode: 1675   score: 155.0  epsilon: 1.0    steps: 898  evaluation reward: 198.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3982: Policy loss: 0.360894. Value loss: 22.489790. Entropy: 0.946857.\n",
      "Iteration 3983: Policy loss: -0.166931. Value loss: 11.807831. Entropy: 0.937432.\n",
      "Iteration 3984: Policy loss: 0.046331. Value loss: 8.448361. Entropy: 0.950963.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3985: Policy loss: -0.166445. Value loss: 42.003178. Entropy: 0.868454.\n",
      "Iteration 3986: Policy loss: -0.321468. Value loss: 23.454683. Entropy: 0.872189.\n",
      "Iteration 3987: Policy loss: -0.438745. Value loss: 19.821127. Entropy: 0.851984.\n",
      "episode: 1676   score: 125.0  epsilon: 1.0    steps: 16  evaluation reward: 198.55\n",
      "episode: 1677   score: 30.0  epsilon: 1.0    steps: 226  evaluation reward: 197.7\n",
      "episode: 1678   score: 175.0  epsilon: 1.0    steps: 266  evaluation reward: 197.1\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3988: Policy loss: 1.295436. Value loss: 38.036293. Entropy: 0.945115.\n",
      "Iteration 3989: Policy loss: 1.245231. Value loss: 20.586439. Entropy: 0.950213.\n",
      "Iteration 3990: Policy loss: 1.187985. Value loss: 15.897985. Entropy: 0.950513.\n",
      "episode: 1679   score: 400.0  epsilon: 1.0    steps: 604  evaluation reward: 199.3\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3991: Policy loss: -0.143341. Value loss: 34.703785. Entropy: 0.922516.\n",
      "Iteration 3992: Policy loss: 0.076698. Value loss: 17.232819. Entropy: 0.918137.\n",
      "Iteration 3993: Policy loss: -0.414179. Value loss: 13.510647. Entropy: 0.922716.\n",
      "episode: 1680   score: 470.0  epsilon: 1.0    steps: 425  evaluation reward: 203.45\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3994: Policy loss: -0.619921. Value loss: 38.529991. Entropy: 0.798820.\n",
      "Iteration 3995: Policy loss: -0.728098. Value loss: 20.235765. Entropy: 0.809122.\n",
      "Iteration 3996: Policy loss: -0.248948. Value loss: 16.933014. Entropy: 0.802591.\n",
      "episode: 1681   score: 445.0  epsilon: 1.0    steps: 773  evaluation reward: 207.15\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3997: Policy loss: 1.204314. Value loss: 22.803068. Entropy: 0.782619.\n",
      "Iteration 3998: Policy loss: 1.449508. Value loss: 11.187347. Entropy: 0.783876.\n",
      "Iteration 3999: Policy loss: 1.216942. Value loss: 7.938264. Entropy: 0.797735.\n",
      "episode: 1682   score: 165.0  epsilon: 1.0    steps: 240  evaluation reward: 207.95\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 4000: Policy loss: -0.102482. Value loss: 45.158260. Entropy: 0.772843.\n",
      "Iteration 4001: Policy loss: -0.120854. Value loss: 29.549250. Entropy: 0.768381.\n",
      "Iteration 4002: Policy loss: -0.128037. Value loss: 21.314753. Entropy: 0.766474.\n",
      "episode: 1683   score: 170.0  epsilon: 1.0    steps: 917  evaluation reward: 208.25\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4003: Policy loss: -2.076290. Value loss: 353.728058. Entropy: 0.742116.\n",
      "Iteration 4004: Policy loss: -1.970530. Value loss: 162.686005. Entropy: 0.763295.\n",
      "Iteration 4005: Policy loss: -2.138710. Value loss: 130.644577. Entropy: 0.721133.\n",
      "episode: 1684   score: 215.0  epsilon: 1.0    steps: 103  evaluation reward: 208.05\n",
      "episode: 1685   score: 185.0  epsilon: 1.0    steps: 723  evaluation reward: 208.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4006: Policy loss: 2.524961. Value loss: 37.957176. Entropy: 0.592871.\n",
      "Iteration 4007: Policy loss: 2.340931. Value loss: 26.980177. Entropy: 0.596573.\n",
      "Iteration 4008: Policy loss: 2.350275. Value loss: 21.611694. Entropy: 0.590795.\n",
      "episode: 1686   score: 330.0  epsilon: 1.0    steps: 292  evaluation reward: 210.0\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4009: Policy loss: 0.332290. Value loss: 54.177879. Entropy: 0.837640.\n",
      "Iteration 4010: Policy loss: -0.403712. Value loss: 25.840693. Entropy: 0.811013.\n",
      "Iteration 4011: Policy loss: 0.148383. Value loss: 18.494020. Entropy: 0.804038.\n",
      "episode: 1687   score: 210.0  epsilon: 1.0    steps: 826  evaluation reward: 210.0\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4012: Policy loss: 2.125710. Value loss: 37.076645. Entropy: 0.685762.\n",
      "Iteration 4013: Policy loss: 2.079665. Value loss: 21.796688. Entropy: 0.695843.\n",
      "Iteration 4014: Policy loss: 1.709199. Value loss: 18.313179. Entropy: 0.689195.\n",
      "episode: 1688   score: 270.0  epsilon: 1.0    steps: 585  evaluation reward: 211.35\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4015: Policy loss: 2.791308. Value loss: 34.379387. Entropy: 0.683388.\n",
      "Iteration 4016: Policy loss: 3.176396. Value loss: 20.746412. Entropy: 0.729784.\n",
      "Iteration 4017: Policy loss: 3.000346. Value loss: 14.907626. Entropy: 0.728604.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4018: Policy loss: 0.136001. Value loss: 47.003010. Entropy: 0.771967.\n",
      "Iteration 4019: Policy loss: 0.275163. Value loss: 22.276880. Entropy: 0.761999.\n",
      "Iteration 4020: Policy loss: 0.197011. Value loss: 15.131783. Entropy: 0.779303.\n",
      "episode: 1689   score: 330.0  epsilon: 1.0    steps: 222  evaluation reward: 214.0\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4021: Policy loss: -0.128996. Value loss: 245.931519. Entropy: 0.760199.\n",
      "Iteration 4022: Policy loss: 0.301873. Value loss: 129.638885. Entropy: 0.769804.\n",
      "Iteration 4023: Policy loss: -0.490735. Value loss: 129.338776. Entropy: 0.733780.\n",
      "episode: 1690   score: 640.0  epsilon: 1.0    steps: 449  evaluation reward: 218.1\n",
      "episode: 1691   score: 70.0  epsilon: 1.0    steps: 642  evaluation reward: 217.35\n",
      "episode: 1692   score: 285.0  epsilon: 1.0    steps: 948  evaluation reward: 217.7\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4024: Policy loss: 2.302786. Value loss: 36.601494. Entropy: 0.733691.\n",
      "Iteration 4025: Policy loss: 2.310795. Value loss: 17.859089. Entropy: 0.760166.\n",
      "Iteration 4026: Policy loss: 2.305161. Value loss: 13.045868. Entropy: 0.768452.\n",
      "episode: 1693   score: 165.0  epsilon: 1.0    steps: 374  evaluation reward: 217.7\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4027: Policy loss: 2.160027. Value loss: 55.966404. Entropy: 0.786481.\n",
      "Iteration 4028: Policy loss: 1.829432. Value loss: 30.543142. Entropy: 0.797801.\n",
      "Iteration 4029: Policy loss: 2.139875. Value loss: 21.402620. Entropy: 0.752291.\n",
      "episode: 1694   score: 335.0  epsilon: 1.0    steps: 110  evaluation reward: 219.5\n",
      "episode: 1695   score: 175.0  epsilon: 1.0    steps: 634  evaluation reward: 215.65\n",
      "episode: 1696   score: 70.0  epsilon: 1.0    steps: 745  evaluation reward: 214.35\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4030: Policy loss: -0.091378. Value loss: 193.369614. Entropy: 0.765072.\n",
      "Iteration 4031: Policy loss: -0.046770. Value loss: 126.512733. Entropy: 0.724452.\n",
      "Iteration 4032: Policy loss: 0.247150. Value loss: 92.054176. Entropy: 0.709267.\n",
      "episode: 1697   score: 450.0  epsilon: 1.0    steps: 770  evaluation reward: 217.1\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4033: Policy loss: 2.364265. Value loss: 41.037865. Entropy: 0.929759.\n",
      "Iteration 4034: Policy loss: 2.416649. Value loss: 26.739655. Entropy: 0.929843.\n",
      "Iteration 4035: Policy loss: 2.247641. Value loss: 20.389288. Entropy: 0.956302.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4036: Policy loss: 2.187346. Value loss: 49.745701. Entropy: 0.741298.\n",
      "Iteration 4037: Policy loss: 2.020255. Value loss: 29.950575. Entropy: 0.745584.\n",
      "Iteration 4038: Policy loss: 2.406512. Value loss: 21.815195. Entropy: 0.754391.\n",
      "episode: 1698   score: 90.0  epsilon: 1.0    steps: 268  evaluation reward: 217.2\n",
      "episode: 1699   score: 160.0  epsilon: 1.0    steps: 512  evaluation reward: 217.55\n",
      "episode: 1700   score: 135.0  epsilon: 1.0    steps: 993  evaluation reward: 217.75\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4039: Policy loss: 2.366558. Value loss: 64.498878. Entropy: 0.753897.\n",
      "Iteration 4040: Policy loss: 2.831253. Value loss: 34.530865. Entropy: 0.711072.\n",
      "Iteration 4041: Policy loss: 2.257878. Value loss: 24.768494. Entropy: 0.738741.\n",
      "now time :  2019-02-25 19:55:25.376409\n",
      "episode: 1701   score: 520.0  epsilon: 1.0    steps: 180  evaluation reward: 221.35\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4042: Policy loss: 3.876117. Value loss: 20.616692. Entropy: 0.745459.\n",
      "Iteration 4043: Policy loss: 3.438106. Value loss: 11.820231. Entropy: 0.705646.\n",
      "Iteration 4044: Policy loss: 3.554352. Value loss: 9.336132. Entropy: 0.750609.\n",
      "episode: 1702   score: 70.0  epsilon: 1.0    steps: 375  evaluation reward: 219.75\n",
      "episode: 1703   score: 100.0  epsilon: 1.0    steps: 660  evaluation reward: 218.5\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4045: Policy loss: 1.331212. Value loss: 35.244057. Entropy: 0.840210.\n",
      "Iteration 4046: Policy loss: 0.979244. Value loss: 21.325693. Entropy: 0.840971.\n",
      "Iteration 4047: Policy loss: 1.275438. Value loss: 16.740665. Entropy: 0.822624.\n",
      "episode: 1704   score: 145.0  epsilon: 1.0    steps: 530  evaluation reward: 218.55\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4048: Policy loss: 1.868767. Value loss: 27.636423. Entropy: 0.827916.\n",
      "Iteration 4049: Policy loss: 1.622248. Value loss: 18.629292. Entropy: 0.806467.\n",
      "Iteration 4050: Policy loss: 1.850541. Value loss: 15.544121. Entropy: 0.824721.\n",
      "episode: 1705   score: 75.0  epsilon: 1.0    steps: 157  evaluation reward: 217.8\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4051: Policy loss: 0.809009. Value loss: 32.865463. Entropy: 0.638471.\n",
      "Iteration 4052: Policy loss: 0.652683. Value loss: 20.871429. Entropy: 0.623425.\n",
      "Iteration 4053: Policy loss: 0.717273. Value loss: 16.313093. Entropy: 0.607000.\n",
      "episode: 1706   score: 310.0  epsilon: 1.0    steps: 893  evaluation reward: 218.85\n",
      "episode: 1707   score: 100.0  epsilon: 1.0    steps: 944  evaluation reward: 219.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4054: Policy loss: -0.375276. Value loss: 36.866718. Entropy: 0.756545.\n",
      "Iteration 4055: Policy loss: 0.100286. Value loss: 16.981678. Entropy: 0.770011.\n",
      "Iteration 4056: Policy loss: -0.472187. Value loss: 14.354343. Entropy: 0.769247.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4057: Policy loss: -1.182911. Value loss: 39.293934. Entropy: 0.590580.\n",
      "Iteration 4058: Policy loss: -0.922288. Value loss: 24.456717. Entropy: 0.631738.\n",
      "Iteration 4059: Policy loss: -0.804492. Value loss: 19.133423. Entropy: 0.606311.\n",
      "episode: 1708   score: 135.0  epsilon: 1.0    steps: 354  evaluation reward: 220.1\n",
      "episode: 1709   score: 190.0  epsilon: 1.0    steps: 693  evaluation reward: 220.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4060: Policy loss: 1.574885. Value loss: 33.224663. Entropy: 0.707971.\n",
      "Iteration 4061: Policy loss: 1.746016. Value loss: 17.953329. Entropy: 0.688505.\n",
      "Iteration 4062: Policy loss: 1.452205. Value loss: 11.858515. Entropy: 0.671426.\n",
      "episode: 1710   score: 320.0  epsilon: 1.0    steps: 55  evaluation reward: 223.05\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4063: Policy loss: -2.072632. Value loss: 446.628662. Entropy: 0.834773.\n",
      "Iteration 4064: Policy loss: -2.068572. Value loss: 274.386414. Entropy: 0.722979.\n",
      "Iteration 4065: Policy loss: -2.109508. Value loss: 164.649994. Entropy: 0.725438.\n",
      "episode: 1711   score: 175.0  epsilon: 1.0    steps: 240  evaluation reward: 223.4\n",
      "episode: 1712   score: 430.0  epsilon: 1.0    steps: 390  evaluation reward: 224.95\n",
      "episode: 1713   score: 35.0  epsilon: 1.0    steps: 943  evaluation reward: 222.6\n",
      "Training network. lr: 0.000219. clip: 0.087568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4066: Policy loss: 1.241672. Value loss: 34.241470. Entropy: 0.735828.\n",
      "Iteration 4067: Policy loss: 1.243848. Value loss: 20.936234. Entropy: 0.718068.\n",
      "Iteration 4068: Policy loss: 1.107583. Value loss: 17.640062. Entropy: 0.729643.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4069: Policy loss: -1.294599. Value loss: 41.482677. Entropy: 0.811691.\n",
      "Iteration 4070: Policy loss: -1.205549. Value loss: 23.536423. Entropy: 0.808578.\n",
      "Iteration 4071: Policy loss: -1.576101. Value loss: 19.116674. Entropy: 0.808556.\n",
      "episode: 1714   score: 65.0  epsilon: 1.0    steps: 319  evaluation reward: 220.85\n",
      "episode: 1715   score: 525.0  epsilon: 1.0    steps: 604  evaluation reward: 224.6\n",
      "episode: 1716   score: 185.0  epsilon: 1.0    steps: 780  evaluation reward: 223.55\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4072: Policy loss: 0.683704. Value loss: 49.206837. Entropy: 0.789389.\n",
      "Iteration 4073: Policy loss: 0.890310. Value loss: 25.873915. Entropy: 0.761750.\n",
      "Iteration 4074: Policy loss: 0.922128. Value loss: 20.330315. Entropy: 0.772879.\n",
      "episode: 1717   score: 160.0  epsilon: 1.0    steps: 747  evaluation reward: 223.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4075: Policy loss: 2.161282. Value loss: 36.348442. Entropy: 0.698684.\n",
      "Iteration 4076: Policy loss: 1.960449. Value loss: 20.177788. Entropy: 0.633875.\n",
      "Iteration 4077: Policy loss: 2.454738. Value loss: 17.017628. Entropy: 0.656764.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4078: Policy loss: -0.310029. Value loss: 41.827644. Entropy: 0.616735.\n",
      "Iteration 4079: Policy loss: -0.349289. Value loss: 19.643766. Entropy: 0.634264.\n",
      "Iteration 4080: Policy loss: -0.325205. Value loss: 15.608920. Entropy: 0.612956.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4081: Policy loss: 3.792126. Value loss: 40.386147. Entropy: 0.531861.\n",
      "Iteration 4082: Policy loss: 4.111128. Value loss: 18.759598. Entropy: 0.557678.\n",
      "Iteration 4083: Policy loss: 3.634568. Value loss: 12.600041. Entropy: 0.565266.\n",
      "episode: 1718   score: 45.0  epsilon: 1.0    steps: 555  evaluation reward: 219.0\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4084: Policy loss: -0.704175. Value loss: 35.578060. Entropy: 0.278587.\n",
      "Iteration 4085: Policy loss: -0.888584. Value loss: 22.271580. Entropy: 0.267837.\n",
      "Iteration 4086: Policy loss: -0.559983. Value loss: 19.139690. Entropy: 0.267813.\n",
      "episode: 1719   score: 240.0  epsilon: 1.0    steps: 156  evaluation reward: 220.25\n",
      "episode: 1720   score: 300.0  epsilon: 1.0    steps: 428  evaluation reward: 222.0\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4087: Policy loss: 1.107800. Value loss: 27.442743. Entropy: 0.348672.\n",
      "Iteration 4088: Policy loss: 1.223803. Value loss: 11.688593. Entropy: 0.346975.\n",
      "Iteration 4089: Policy loss: 0.981857. Value loss: 9.408444. Entropy: 0.350958.\n",
      "episode: 1721   score: 320.0  epsilon: 1.0    steps: 82  evaluation reward: 222.55\n",
      "episode: 1722   score: 260.0  epsilon: 1.0    steps: 379  evaluation reward: 223.6\n",
      "episode: 1723   score: 115.0  epsilon: 1.0    steps: 738  evaluation reward: 221.2\n",
      "episode: 1724   score: 245.0  epsilon: 1.0    steps: 834  evaluation reward: 219.15\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4090: Policy loss: -0.917520. Value loss: 39.307407. Entropy: 0.409639.\n",
      "Iteration 4091: Policy loss: -1.138319. Value loss: 23.928389. Entropy: 0.399997.\n",
      "Iteration 4092: Policy loss: -0.781876. Value loss: 19.116232. Entropy: 0.411266.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4093: Policy loss: -1.532905. Value loss: 18.803747. Entropy: 0.812299.\n",
      "Iteration 4094: Policy loss: -1.452447. Value loss: 10.686622. Entropy: 0.798501.\n",
      "Iteration 4095: Policy loss: -1.508679. Value loss: 8.873116. Entropy: 0.789888.\n",
      "episode: 1725   score: 360.0  epsilon: 1.0    steps: 928  evaluation reward: 220.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4096: Policy loss: -0.202966. Value loss: 12.507416. Entropy: 0.563962.\n",
      "Iteration 4097: Policy loss: -0.119069. Value loss: 9.406877. Entropy: 0.594737.\n",
      "Iteration 4098: Policy loss: -0.137088. Value loss: 8.533385. Entropy: 0.571136.\n",
      "episode: 1726   score: 105.0  epsilon: 1.0    steps: 591  evaluation reward: 218.45\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4099: Policy loss: 1.480740. Value loss: 17.583111. Entropy: 0.694705.\n",
      "Iteration 4100: Policy loss: 0.739754. Value loss: 10.375063. Entropy: 0.773495.\n",
      "Iteration 4101: Policy loss: 0.985594. Value loss: 7.306297. Entropy: 0.824652.\n",
      "episode: 1727   score: 105.0  epsilon: 1.0    steps: 459  evaluation reward: 218.0\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4102: Policy loss: -1.595985. Value loss: 182.392303. Entropy: 0.764279.\n",
      "Iteration 4103: Policy loss: -2.856138. Value loss: 168.762253. Entropy: 0.669827.\n",
      "Iteration 4104: Policy loss: -2.200469. Value loss: 64.063728. Entropy: 0.615591.\n",
      "episode: 1728   score: 135.0  epsilon: 1.0    steps: 734  evaluation reward: 216.7\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4105: Policy loss: -0.722281. Value loss: 21.371639. Entropy: 0.389587.\n",
      "Iteration 4106: Policy loss: -0.527659. Value loss: 12.633982. Entropy: 0.385933.\n",
      "Iteration 4107: Policy loss: -0.799008. Value loss: 10.518665. Entropy: 0.365667.\n",
      "episode: 1729   score: 135.0  epsilon: 1.0    steps: 72  evaluation reward: 217.0\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4108: Policy loss: 0.167738. Value loss: 30.627687. Entropy: 0.487149.\n",
      "Iteration 4109: Policy loss: 0.156747. Value loss: 17.659506. Entropy: 0.472096.\n",
      "Iteration 4110: Policy loss: 0.230039. Value loss: 13.751922. Entropy: 0.477988.\n",
      "episode: 1730   score: 505.0  epsilon: 1.0    steps: 235  evaluation reward: 216.9\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4111: Policy loss: -0.016110. Value loss: 39.007626. Entropy: 0.369093.\n",
      "Iteration 4112: Policy loss: -0.365828. Value loss: 14.632327. Entropy: 0.383240.\n",
      "Iteration 4113: Policy loss: 0.074748. Value loss: 12.306820. Entropy: 0.421448.\n",
      "episode: 1731   score: 95.0  epsilon: 1.0    steps: 600  evaluation reward: 212.1\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4114: Policy loss: -0.733508. Value loss: 25.009594. Entropy: 0.473589.\n",
      "Iteration 4115: Policy loss: -0.752433. Value loss: 11.336976. Entropy: 0.457098.\n",
      "Iteration 4116: Policy loss: -0.527389. Value loss: 10.201313. Entropy: 0.483493.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4117: Policy loss: -1.310783. Value loss: 26.937819. Entropy: 0.361262.\n",
      "Iteration 4118: Policy loss: -1.518256. Value loss: 14.248619. Entropy: 0.399513.\n",
      "Iteration 4119: Policy loss: -1.180897. Value loss: 11.165972. Entropy: 0.390344.\n",
      "episode: 1732   score: 285.0  epsilon: 1.0    steps: 285  evaluation reward: 211.15\n",
      "episode: 1733   score: 105.0  epsilon: 1.0    steps: 661  evaluation reward: 209.9\n",
      "episode: 1734   score: 370.0  epsilon: 1.0    steps: 769  evaluation reward: 210.65\n",
      "episode: 1735   score: 225.0  epsilon: 1.0    steps: 990  evaluation reward: 210.9\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4120: Policy loss: 1.003803. Value loss: 22.126949. Entropy: 0.532108.\n",
      "Iteration 4121: Policy loss: 0.689602. Value loss: 9.170747. Entropy: 0.568737.\n",
      "Iteration 4122: Policy loss: 0.694262. Value loss: 7.094875. Entropy: 0.560733.\n",
      "episode: 1736   score: 155.0  epsilon: 1.0    steps: 200  evaluation reward: 210.85\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4123: Policy loss: -0.720726. Value loss: 28.332407. Entropy: 0.463334.\n",
      "Iteration 4124: Policy loss: -0.762886. Value loss: 17.727997. Entropy: 0.454736.\n",
      "Iteration 4125: Policy loss: -0.575584. Value loss: 14.952830. Entropy: 0.465839.\n",
      "episode: 1737   score: 160.0  epsilon: 1.0    steps: 100  evaluation reward: 211.15\n",
      "episode: 1738   score: 250.0  epsilon: 1.0    steps: 496  evaluation reward: 211.6\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4126: Policy loss: -0.315431. Value loss: 15.996152. Entropy: 0.562110.\n",
      "Iteration 4127: Policy loss: -0.311813. Value loss: 10.444864. Entropy: 0.526883.\n",
      "Iteration 4128: Policy loss: -0.346417. Value loss: 9.283891. Entropy: 0.535467.\n",
      "episode: 1739   score: 75.0  epsilon: 1.0    steps: 344  evaluation reward: 210.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1740   score: 180.0  epsilon: 1.0    steps: 577  evaluation reward: 211.65\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4129: Policy loss: 0.192784. Value loss: 14.072398. Entropy: 0.460462.\n",
      "Iteration 4130: Policy loss: 0.493532. Value loss: 8.674212. Entropy: 0.499142.\n",
      "Iteration 4131: Policy loss: 0.214345. Value loss: 9.702655. Entropy: 0.479446.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4132: Policy loss: -1.889883. Value loss: 21.478243. Entropy: 0.514699.\n",
      "Iteration 4133: Policy loss: -1.987000. Value loss: 12.877275. Entropy: 0.517450.\n",
      "Iteration 4134: Policy loss: -1.954674. Value loss: 11.005254. Entropy: 0.511522.\n",
      "episode: 1741   score: 160.0  epsilon: 1.0    steps: 674  evaluation reward: 209.25\n",
      "episode: 1742   score: 120.0  epsilon: 1.0    steps: 903  evaluation reward: 209.05\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4135: Policy loss: -4.332361. Value loss: 281.030060. Entropy: 0.601810.\n",
      "Iteration 4136: Policy loss: -4.377000. Value loss: 170.798355. Entropy: 0.600254.\n",
      "Iteration 4137: Policy loss: -4.122547. Value loss: 124.634117. Entropy: 0.586104.\n",
      "episode: 1743   score: 105.0  epsilon: 1.0    steps: 440  evaluation reward: 209.05\n",
      "episode: 1744   score: 105.0  epsilon: 1.0    steps: 626  evaluation reward: 209.25\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4138: Policy loss: -0.195855. Value loss: 27.054195. Entropy: 0.519683.\n",
      "Iteration 4139: Policy loss: 0.046689. Value loss: 15.341025. Entropy: 0.502363.\n",
      "Iteration 4140: Policy loss: -0.049529. Value loss: 11.740241. Entropy: 0.507670.\n",
      "episode: 1745   score: 235.0  epsilon: 1.0    steps: 245  evaluation reward: 209.8\n",
      "episode: 1746   score: 105.0  epsilon: 1.0    steps: 341  evaluation reward: 201.85\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4141: Policy loss: 1.782814. Value loss: 37.373878. Entropy: 0.463824.\n",
      "Iteration 4142: Policy loss: 1.839720. Value loss: 18.567486. Entropy: 0.458253.\n",
      "Iteration 4143: Policy loss: 1.748064. Value loss: 16.246374. Entropy: 0.469747.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4144: Policy loss: -0.060409. Value loss: 18.510481. Entropy: 0.471049.\n",
      "Iteration 4145: Policy loss: -0.157880. Value loss: 12.534245. Entropy: 0.463506.\n",
      "Iteration 4146: Policy loss: 0.027599. Value loss: 8.819519. Entropy: 0.465832.\n",
      "episode: 1747   score: 105.0  epsilon: 1.0    steps: 686  evaluation reward: 199.35\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4147: Policy loss: -0.201030. Value loss: 27.771687. Entropy: 0.526961.\n",
      "Iteration 4148: Policy loss: 0.297918. Value loss: 15.914658. Entropy: 0.535445.\n",
      "Iteration 4149: Policy loss: -0.072296. Value loss: 13.487225. Entropy: 0.550572.\n",
      "episode: 1748   score: 105.0  epsilon: 1.0    steps: 447  evaluation reward: 199.0\n",
      "episode: 1749   score: 605.0  epsilon: 1.0    steps: 781  evaluation reward: 201.6\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4150: Policy loss: -1.045237. Value loss: 21.478899. Entropy: 0.568554.\n",
      "Iteration 4151: Policy loss: -1.126284. Value loss: 12.783946. Entropy: 0.594197.\n",
      "Iteration 4152: Policy loss: -1.129645. Value loss: 11.093326. Entropy: 0.574046.\n",
      "episode: 1750   score: 360.0  epsilon: 1.0    steps: 28  evaluation reward: 204.55\n",
      "now time :  2019-02-25 19:57:30.252009\n",
      "episode: 1751   score: 135.0  epsilon: 1.0    steps: 320  evaluation reward: 203.85\n",
      "episode: 1752   score: 120.0  epsilon: 1.0    steps: 606  evaluation reward: 204.35\n",
      "episode: 1753   score: 200.0  epsilon: 1.0    steps: 956  evaluation reward: 204.75\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4153: Policy loss: 1.298809. Value loss: 25.347488. Entropy: 0.536045.\n",
      "Iteration 4154: Policy loss: 1.295504. Value loss: 15.339868. Entropy: 0.528511.\n",
      "Iteration 4155: Policy loss: 1.224597. Value loss: 13.475696. Entropy: 0.554443.\n",
      "episode: 1754   score: 190.0  epsilon: 1.0    steps: 234  evaluation reward: 205.6\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4156: Policy loss: 0.834974. Value loss: 21.197277. Entropy: 0.602390.\n",
      "Iteration 4157: Policy loss: 0.775838. Value loss: 13.795658. Entropy: 0.584466.\n",
      "Iteration 4158: Policy loss: 0.742772. Value loss: 12.058568. Entropy: 0.607460.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4159: Policy loss: -0.685271. Value loss: 11.569565. Entropy: 0.513940.\n",
      "Iteration 4160: Policy loss: -0.422044. Value loss: 6.533199. Entropy: 0.519025.\n",
      "Iteration 4161: Policy loss: -0.658727. Value loss: 5.397736. Entropy: 0.553223.\n",
      "episode: 1755   score: 75.0  epsilon: 1.0    steps: 444  evaluation reward: 205.55\n",
      "episode: 1756   score: 210.0  epsilon: 1.0    steps: 681  evaluation reward: 206.9\n",
      "episode: 1757   score: 105.0  epsilon: 1.0    steps: 830  evaluation reward: 207.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4162: Policy loss: 0.193755. Value loss: 20.695248. Entropy: 0.519440.\n",
      "Iteration 4163: Policy loss: 0.493563. Value loss: 14.288280. Entropy: 0.533811.\n",
      "Iteration 4164: Policy loss: 0.352874. Value loss: 10.943003. Entropy: 0.523161.\n",
      "episode: 1758   score: 105.0  epsilon: 1.0    steps: 355  evaluation reward: 206.75\n",
      "episode: 1759   score: 110.0  epsilon: 1.0    steps: 537  evaluation reward: 205.8\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4165: Policy loss: 0.476048. Value loss: 19.928139. Entropy: 0.495919.\n",
      "Iteration 4166: Policy loss: 0.359472. Value loss: 12.794459. Entropy: 0.422674.\n",
      "Iteration 4167: Policy loss: 0.470571. Value loss: 8.942782. Entropy: 0.448479.\n",
      "episode: 1760   score: 140.0  epsilon: 1.0    steps: 21  evaluation reward: 205.5\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4168: Policy loss: -0.228295. Value loss: 22.531778. Entropy: 0.577564.\n",
      "Iteration 4169: Policy loss: -0.263012. Value loss: 11.401192. Entropy: 0.589599.\n",
      "Iteration 4170: Policy loss: -0.187812. Value loss: 9.166116. Entropy: 0.603210.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4171: Policy loss: 1.772336. Value loss: 15.306533. Entropy: 0.376085.\n",
      "Iteration 4172: Policy loss: 1.740586. Value loss: 6.949696. Entropy: 0.405749.\n",
      "Iteration 4173: Policy loss: 1.806217. Value loss: 6.205211. Entropy: 0.413683.\n",
      "episode: 1761   score: 120.0  epsilon: 1.0    steps: 172  evaluation reward: 205.95\n",
      "episode: 1762   score: 105.0  epsilon: 1.0    steps: 469  evaluation reward: 205.8\n",
      "episode: 1763   score: 105.0  epsilon: 1.0    steps: 711  evaluation reward: 205.3\n",
      "episode: 1764   score: 215.0  epsilon: 1.0    steps: 943  evaluation reward: 206.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4174: Policy loss: 0.219091. Value loss: 16.488924. Entropy: 0.715749.\n",
      "Iteration 4175: Policy loss: 0.092136. Value loss: 10.188334. Entropy: 0.742763.\n",
      "Iteration 4176: Policy loss: 0.455840. Value loss: 8.172613. Entropy: 0.728410.\n",
      "episode: 1765   score: 150.0  epsilon: 1.0    steps: 336  evaluation reward: 206.55\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4177: Policy loss: 0.531725. Value loss: 18.832653. Entropy: 0.590214.\n",
      "Iteration 4178: Policy loss: 0.633361. Value loss: 10.170026. Entropy: 0.567762.\n",
      "Iteration 4179: Policy loss: 0.546612. Value loss: 9.539112. Entropy: 0.633628.\n",
      "episode: 1766   score: 135.0  epsilon: 1.0    steps: 23  evaluation reward: 206.4\n",
      "episode: 1767   score: 140.0  epsilon: 1.0    steps: 591  evaluation reward: 204.4\n",
      "episode: 1768   score: 245.0  epsilon: 1.0    steps: 837  evaluation reward: 206.0\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4180: Policy loss: 1.946966. Value loss: 10.145529. Entropy: 0.627954.\n",
      "Iteration 4181: Policy loss: 1.967907. Value loss: 5.664090. Entropy: 0.653177.\n",
      "Iteration 4182: Policy loss: 2.002529. Value loss: 6.086825. Entropy: 0.629786.\n",
      "episode: 1769   score: 110.0  epsilon: 1.0    steps: 231  evaluation reward: 202.9\n",
      "episode: 1770   score: 80.0  epsilon: 1.0    steps: 768  evaluation reward: 201.45\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4183: Policy loss: 0.335722. Value loss: 13.446732. Entropy: 0.755976.\n",
      "Iteration 4184: Policy loss: 0.302275. Value loss: 9.226204. Entropy: 0.718702.\n",
      "Iteration 4185: Policy loss: 0.367578. Value loss: 8.295083. Entropy: 0.731769.\n",
      "episode: 1771   score: 105.0  epsilon: 1.0    steps: 940  evaluation reward: 200.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4186: Policy loss: 0.090403. Value loss: 15.343248. Entropy: 0.824198.\n",
      "Iteration 4187: Policy loss: 0.139201. Value loss: 9.498746. Entropy: 0.831689.\n",
      "Iteration 4188: Policy loss: 0.006534. Value loss: 6.450415. Entropy: 0.831878.\n",
      "episode: 1772   score: 105.0  epsilon: 1.0    steps: 82  evaluation reward: 200.45\n",
      "episode: 1773   score: 125.0  epsilon: 1.0    steps: 307  evaluation reward: 198.0\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4189: Policy loss: -3.379395. Value loss: 31.851261. Entropy: 0.630134.\n",
      "Iteration 4190: Policy loss: -3.196429. Value loss: 17.071424. Entropy: 0.609938.\n",
      "Iteration 4191: Policy loss: -3.225611. Value loss: 16.053169. Entropy: 0.621853.\n",
      "episode: 1774   score: 120.0  epsilon: 1.0    steps: 562  evaluation reward: 198.15\n",
      "episode: 1775   score: 180.0  epsilon: 1.0    steps: 813  evaluation reward: 198.4\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4192: Policy loss: 0.473538. Value loss: 12.157663. Entropy: 0.787862.\n",
      "Iteration 4193: Policy loss: 0.600934. Value loss: 6.459387. Entropy: 0.818402.\n",
      "Iteration 4194: Policy loss: 0.588846. Value loss: 5.466206. Entropy: 0.787592.\n",
      "episode: 1776   score: 255.0  epsilon: 1.0    steps: 428  evaluation reward: 199.7\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4195: Policy loss: 0.712296. Value loss: 12.152525. Entropy: 0.538451.\n",
      "Iteration 4196: Policy loss: 0.819748. Value loss: 6.911127. Entropy: 0.545763.\n",
      "Iteration 4197: Policy loss: 0.704300. Value loss: 6.246453. Entropy: 0.536491.\n",
      "episode: 1777   score: 125.0  epsilon: 1.0    steps: 140  evaluation reward: 200.65\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4198: Policy loss: -0.908597. Value loss: 25.614939. Entropy: 0.695851.\n",
      "Iteration 4199: Policy loss: -0.793098. Value loss: 18.016542. Entropy: 0.696541.\n",
      "Iteration 4200: Policy loss: -0.855415. Value loss: 15.619151. Entropy: 0.700650.\n",
      "episode: 1778   score: 155.0  epsilon: 1.0    steps: 270  evaluation reward: 200.45\n",
      "episode: 1779   score: 175.0  epsilon: 1.0    steps: 711  evaluation reward: 198.2\n",
      "episode: 1780   score: 110.0  epsilon: 1.0    steps: 856  evaluation reward: 194.6\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4201: Policy loss: -0.315314. Value loss: 18.073141. Entropy: 0.692556.\n",
      "Iteration 4202: Policy loss: -0.241896. Value loss: 10.670996. Entropy: 0.698183.\n",
      "Iteration 4203: Policy loss: -0.357961. Value loss: 8.940450. Entropy: 0.690835.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4204: Policy loss: -0.656828. Value loss: 23.427893. Entropy: 0.456697.\n",
      "Iteration 4205: Policy loss: -0.992787. Value loss: 14.015869. Entropy: 0.473309.\n",
      "Iteration 4206: Policy loss: -0.939212. Value loss: 10.541837. Entropy: 0.489917.\n",
      "episode: 1781   score: 135.0  epsilon: 1.0    steps: 510  evaluation reward: 191.5\n",
      "episode: 1782   score: 290.0  epsilon: 1.0    steps: 937  evaluation reward: 192.75\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4207: Policy loss: 0.316656. Value loss: 17.490749. Entropy: 0.483182.\n",
      "Iteration 4208: Policy loss: 0.425450. Value loss: 8.527542. Entropy: 0.474935.\n",
      "Iteration 4209: Policy loss: 0.356470. Value loss: 7.937157. Entropy: 0.475355.\n",
      "episode: 1783   score: 230.0  epsilon: 1.0    steps: 589  evaluation reward: 193.35\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4210: Policy loss: 2.012829. Value loss: 17.689091. Entropy: 0.576867.\n",
      "Iteration 4211: Policy loss: 2.407459. Value loss: 8.872621. Entropy: 0.622648.\n",
      "Iteration 4212: Policy loss: 2.221674. Value loss: 6.691227. Entropy: 0.613277.\n",
      "episode: 1784   score: 340.0  epsilon: 1.0    steps: 127  evaluation reward: 194.6\n",
      "episode: 1785   score: 105.0  epsilon: 1.0    steps: 704  evaluation reward: 193.8\n",
      "episode: 1786   score: 90.0  epsilon: 1.0    steps: 866  evaluation reward: 191.4\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4213: Policy loss: 0.841371. Value loss: 17.115131. Entropy: 0.458343.\n",
      "Iteration 4214: Policy loss: 1.018349. Value loss: 8.871500. Entropy: 0.481193.\n",
      "Iteration 4215: Policy loss: 0.965802. Value loss: 6.366234. Entropy: 0.483017.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4216: Policy loss: 0.475728. Value loss: 8.472140. Entropy: 0.660897.\n",
      "Iteration 4217: Policy loss: 0.504345. Value loss: 5.613781. Entropy: 0.647768.\n",
      "Iteration 4218: Policy loss: 0.407202. Value loss: 4.506896. Entropy: 0.657290.\n",
      "episode: 1787   score: 240.0  epsilon: 1.0    steps: 229  evaluation reward: 191.7\n",
      "episode: 1788   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 190.8\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4219: Policy loss: -0.128818. Value loss: 19.528698. Entropy: 0.479661.\n",
      "Iteration 4220: Policy loss: 0.124332. Value loss: 10.971729. Entropy: 0.439607.\n",
      "Iteration 4221: Policy loss: 0.046138. Value loss: 9.999928. Entropy: 0.452963.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4222: Policy loss: -1.218894. Value loss: 18.109322. Entropy: 0.628551.\n",
      "Iteration 4223: Policy loss: -0.927570. Value loss: 8.793230. Entropy: 0.623250.\n",
      "Iteration 4224: Policy loss: -1.473259. Value loss: 11.117557. Entropy: 0.588900.\n",
      "episode: 1789   score: 155.0  epsilon: 1.0    steps: 91  evaluation reward: 189.05\n",
      "episode: 1790   score: 195.0  epsilon: 1.0    steps: 398  evaluation reward: 184.6\n",
      "episode: 1791   score: 135.0  epsilon: 1.0    steps: 523  evaluation reward: 185.25\n",
      "episode: 1792   score: 155.0  epsilon: 1.0    steps: 831  evaluation reward: 183.95\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4225: Policy loss: -0.379946. Value loss: 21.143417. Entropy: 0.566769.\n",
      "Iteration 4226: Policy loss: -0.336084. Value loss: 12.791648. Entropy: 0.557680.\n",
      "Iteration 4227: Policy loss: -0.336208. Value loss: 10.065472. Entropy: 0.570583.\n",
      "episode: 1793   score: 190.0  epsilon: 1.0    steps: 693  evaluation reward: 184.2\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4228: Policy loss: -0.091991. Value loss: 15.226959. Entropy: 0.616990.\n",
      "Iteration 4229: Policy loss: 0.062545. Value loss: 7.873905. Entropy: 0.588274.\n",
      "Iteration 4230: Policy loss: -0.100053. Value loss: 7.225685. Entropy: 0.619972.\n",
      "episode: 1794   score: 335.0  epsilon: 1.0    steps: 928  evaluation reward: 184.2\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4231: Policy loss: 0.267413. Value loss: 19.945929. Entropy: 0.589174.\n",
      "Iteration 4232: Policy loss: 0.336610. Value loss: 12.164932. Entropy: 0.583437.\n",
      "Iteration 4233: Policy loss: 0.347039. Value loss: 10.180817. Entropy: 0.585389.\n",
      "episode: 1795   score: 195.0  epsilon: 1.0    steps: 356  evaluation reward: 184.4\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4234: Policy loss: -1.723089. Value loss: 30.922302. Entropy: 0.670907.\n",
      "Iteration 4235: Policy loss: -1.572364. Value loss: 19.951050. Entropy: 0.682427.\n",
      "Iteration 4236: Policy loss: -1.491796. Value loss: 18.056894. Entropy: 0.668691.\n",
      "episode: 1796   score: 155.0  epsilon: 1.0    steps: 796  evaluation reward: 185.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4237: Policy loss: -1.831626. Value loss: 27.523251. Entropy: 0.553008.\n",
      "Iteration 4238: Policy loss: -1.643678. Value loss: 14.232409. Entropy: 0.510435.\n",
      "Iteration 4239: Policy loss: -1.503030. Value loss: 12.757118. Entropy: 0.568371.\n",
      "episode: 1797   score: 200.0  epsilon: 1.0    steps: 92  evaluation reward: 182.75\n",
      "episode: 1798   score: 230.0  epsilon: 1.0    steps: 518  evaluation reward: 184.15\n",
      "episode: 1799   score: 180.0  epsilon: 1.0    steps: 669  evaluation reward: 184.35\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4240: Policy loss: -2.835545. Value loss: 269.711975. Entropy: 0.545421.\n",
      "Iteration 4241: Policy loss: -2.371047. Value loss: 81.995255. Entropy: 0.573966.\n",
      "Iteration 4242: Policy loss: -4.303485. Value loss: 66.365768. Entropy: 0.548570.\n",
      "episode: 1800   score: 125.0  epsilon: 1.0    steps: 998  evaluation reward: 184.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4243: Policy loss: 1.232545. Value loss: 45.724358. Entropy: 0.561907.\n",
      "Iteration 4244: Policy loss: 1.446775. Value loss: 26.601254. Entropy: 0.573985.\n",
      "Iteration 4245: Policy loss: 1.112876. Value loss: 22.397552. Entropy: 0.587468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2019-02-25 19:59:14.185647\n",
      "episode: 1801   score: 320.0  epsilon: 1.0    steps: 141  evaluation reward: 182.25\n",
      "episode: 1802   score: 605.0  epsilon: 1.0    steps: 461  evaluation reward: 187.6\n",
      "episode: 1803   score: 155.0  epsilon: 1.0    steps: 826  evaluation reward: 188.15\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4246: Policy loss: -0.158757. Value loss: 33.863724. Entropy: 0.682465.\n",
      "Iteration 4247: Policy loss: -0.114445. Value loss: 21.045986. Entropy: 0.658000.\n",
      "Iteration 4248: Policy loss: -0.302321. Value loss: 16.770164. Entropy: 0.668341.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4249: Policy loss: 1.324045. Value loss: 29.975592. Entropy: 0.715228.\n",
      "Iteration 4250: Policy loss: 1.504781. Value loss: 20.475922. Entropy: 0.702852.\n",
      "Iteration 4251: Policy loss: 1.538741. Value loss: 14.975239. Entropy: 0.739799.\n",
      "episode: 1804   score: 115.0  epsilon: 1.0    steps: 119  evaluation reward: 187.85\n",
      "episode: 1805   score: 205.0  epsilon: 1.0    steps: 601  evaluation reward: 189.15\n",
      "episode: 1806   score: 125.0  epsilon: 1.0    steps: 666  evaluation reward: 187.3\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4252: Policy loss: 2.273498. Value loss: 26.527847. Entropy: 0.688053.\n",
      "Iteration 4253: Policy loss: 2.660204. Value loss: 14.659584. Entropy: 0.687459.\n",
      "Iteration 4254: Policy loss: 2.320744. Value loss: 10.810397. Entropy: 0.680671.\n",
      "episode: 1807   score: 185.0  epsilon: 1.0    steps: 346  evaluation reward: 188.15\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4255: Policy loss: 2.729936. Value loss: 27.641275. Entropy: 0.896769.\n",
      "Iteration 4256: Policy loss: 3.505923. Value loss: 16.266552. Entropy: 0.909025.\n",
      "Iteration 4257: Policy loss: 2.932606. Value loss: 13.161742. Entropy: 0.872074.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4258: Policy loss: 0.342292. Value loss: 42.346333. Entropy: 0.589500.\n",
      "Iteration 4259: Policy loss: 0.633169. Value loss: 19.956808. Entropy: 0.598286.\n",
      "Iteration 4260: Policy loss: 0.384119. Value loss: 17.346458. Entropy: 0.581177.\n",
      "episode: 1808   score: 165.0  epsilon: 1.0    steps: 466  evaluation reward: 188.45\n",
      "episode: 1809   score: 110.0  epsilon: 1.0    steps: 663  evaluation reward: 187.65\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4261: Policy loss: -0.833627. Value loss: 40.123737. Entropy: 0.610268.\n",
      "Iteration 4262: Policy loss: -0.712037. Value loss: 23.071909. Entropy: 0.624876.\n",
      "Iteration 4263: Policy loss: -0.724590. Value loss: 16.195145. Entropy: 0.629683.\n",
      "episode: 1810   score: 105.0  epsilon: 1.0    steps: 335  evaluation reward: 185.5\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4264: Policy loss: 0.495339. Value loss: 47.418041. Entropy: 0.665136.\n",
      "Iteration 4265: Policy loss: 0.816115. Value loss: 25.461311. Entropy: 0.664980.\n",
      "Iteration 4266: Policy loss: 0.620696. Value loss: 21.871046. Entropy: 0.648849.\n",
      "episode: 1811   score: 320.0  epsilon: 1.0    steps: 134  evaluation reward: 186.95\n",
      "episode: 1812   score: 390.0  epsilon: 1.0    steps: 978  evaluation reward: 186.55\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4267: Policy loss: 0.166155. Value loss: 17.037750. Entropy: 0.575091.\n",
      "Iteration 4268: Policy loss: -0.082540. Value loss: 13.076696. Entropy: 0.547242.\n",
      "Iteration 4269: Policy loss: -0.210477. Value loss: 10.720220. Entropy: 0.529285.\n",
      "episode: 1813   score: 75.0  epsilon: 1.0    steps: 677  evaluation reward: 186.95\n",
      "episode: 1814   score: 275.0  epsilon: 1.0    steps: 845  evaluation reward: 189.05\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4270: Policy loss: 2.017976. Value loss: 18.066473. Entropy: 0.591106.\n",
      "Iteration 4271: Policy loss: 1.926371. Value loss: 9.954202. Entropy: 0.611179.\n",
      "Iteration 4272: Policy loss: 1.842959. Value loss: 9.534061. Entropy: 0.595168.\n",
      "episode: 1815   score: 230.0  epsilon: 1.0    steps: 564  evaluation reward: 186.1\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4273: Policy loss: 0.594813. Value loss: 16.117393. Entropy: 0.906149.\n",
      "Iteration 4274: Policy loss: 0.591682. Value loss: 9.984409. Entropy: 0.905782.\n",
      "Iteration 4275: Policy loss: 0.613276. Value loss: 8.185535. Entropy: 0.918848.\n",
      "episode: 1816   score: 245.0  epsilon: 1.0    steps: 70  evaluation reward: 186.7\n",
      "episode: 1817   score: 120.0  epsilon: 1.0    steps: 225  evaluation reward: 186.3\n",
      "episode: 1818   score: 110.0  epsilon: 1.0    steps: 265  evaluation reward: 186.95\n",
      "episode: 1819   score: 105.0  epsilon: 1.0    steps: 490  evaluation reward: 185.6\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4276: Policy loss: 0.295408. Value loss: 23.545643. Entropy: 0.734126.\n",
      "Iteration 4277: Policy loss: 0.260628. Value loss: 14.389716. Entropy: 0.732035.\n",
      "Iteration 4278: Policy loss: 0.171612. Value loss: 12.771366. Entropy: 0.737680.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4279: Policy loss: 1.373170. Value loss: 20.610163. Entropy: 0.832048.\n",
      "Iteration 4280: Policy loss: 1.474870. Value loss: 13.220924. Entropy: 0.846059.\n",
      "Iteration 4281: Policy loss: 1.237861. Value loss: 12.437668. Entropy: 0.865102.\n",
      "episode: 1820   score: 80.0  epsilon: 1.0    steps: 727  evaluation reward: 183.4\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4282: Policy loss: 0.473268. Value loss: 16.447393. Entropy: 0.720658.\n",
      "Iteration 4283: Policy loss: 0.434067. Value loss: 9.405787. Entropy: 0.733468.\n",
      "Iteration 4284: Policy loss: 0.495067. Value loss: 7.874116. Entropy: 0.746086.\n",
      "episode: 1821   score: 175.0  epsilon: 1.0    steps: 557  evaluation reward: 181.95\n",
      "episode: 1822   score: 120.0  epsilon: 1.0    steps: 791  evaluation reward: 180.55\n",
      "episode: 1823   score: 105.0  epsilon: 1.0    steps: 921  evaluation reward: 180.45\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4285: Policy loss: 0.755185. Value loss: 15.526279. Entropy: 0.891285.\n",
      "Iteration 4286: Policy loss: 0.730235. Value loss: 9.425144. Entropy: 0.879795.\n",
      "Iteration 4287: Policy loss: 0.492398. Value loss: 8.475322. Entropy: 0.897236.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4288: Policy loss: -0.241841. Value loss: 39.763691. Entropy: 0.693161.\n",
      "Iteration 4289: Policy loss: -0.336479. Value loss: 26.197002. Entropy: 0.683415.\n",
      "Iteration 4290: Policy loss: -0.295209. Value loss: 18.964764. Entropy: 0.696957.\n",
      "episode: 1824   score: 145.0  epsilon: 1.0    steps: 11  evaluation reward: 179.45\n",
      "episode: 1825   score: 115.0  epsilon: 1.0    steps: 332  evaluation reward: 177.0\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4291: Policy loss: 4.003002. Value loss: 19.122871. Entropy: 0.904491.\n",
      "Iteration 4292: Policy loss: 3.812307. Value loss: 10.974849. Entropy: 0.892700.\n",
      "Iteration 4293: Policy loss: 3.681723. Value loss: 10.536200. Entropy: 0.895576.\n",
      "episode: 1826   score: 200.0  epsilon: 1.0    steps: 388  evaluation reward: 177.95\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4294: Policy loss: -1.405102. Value loss: 36.661201. Entropy: 0.765983.\n",
      "Iteration 4295: Policy loss: -1.618335. Value loss: 20.961029. Entropy: 0.757382.\n",
      "Iteration 4296: Policy loss: -1.454377. Value loss: 16.240128. Entropy: 0.784922.\n",
      "episode: 1827   score: 230.0  epsilon: 1.0    steps: 230  evaluation reward: 179.2\n",
      "episode: 1828   score: 75.0  epsilon: 1.0    steps: 986  evaluation reward: 178.6\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4297: Policy loss: -1.804927. Value loss: 56.644970. Entropy: 0.725807.\n",
      "Iteration 4298: Policy loss: -1.733292. Value loss: 31.954782. Entropy: 0.681535.\n",
      "Iteration 4299: Policy loss: -1.326194. Value loss: 25.331127. Entropy: 0.651111.\n",
      "episode: 1829   score: 290.0  epsilon: 1.0    steps: 814  evaluation reward: 180.15\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4300: Policy loss: 0.228955. Value loss: 35.090096. Entropy: 0.670036.\n",
      "Iteration 4301: Policy loss: 0.204456. Value loss: 25.662596. Entropy: 0.666366.\n",
      "Iteration 4302: Policy loss: 0.214268. Value loss: 20.326277. Entropy: 0.660937.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4303: Policy loss: 1.491653. Value loss: 36.861485. Entropy: 0.472211.\n",
      "Iteration 4304: Policy loss: 1.757670. Value loss: 22.196924. Entropy: 0.482484.\n",
      "Iteration 4305: Policy loss: 1.643848. Value loss: 16.526731. Entropy: 0.470376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1830   score: 80.0  epsilon: 1.0    steps: 211  evaluation reward: 175.9\n",
      "episode: 1831   score: 280.0  epsilon: 1.0    steps: 569  evaluation reward: 177.75\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4306: Policy loss: -0.600603. Value loss: 30.909904. Entropy: 0.650346.\n",
      "Iteration 4307: Policy loss: -0.410529. Value loss: 16.758585. Entropy: 0.675421.\n",
      "Iteration 4308: Policy loss: -0.601856. Value loss: 13.671443. Entropy: 0.649706.\n",
      "episode: 1832   score: 275.0  epsilon: 1.0    steps: 38  evaluation reward: 177.65\n",
      "episode: 1833   score: 370.0  epsilon: 1.0    steps: 765  evaluation reward: 180.3\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4309: Policy loss: 1.475874. Value loss: 53.863350. Entropy: 0.516652.\n",
      "Iteration 4310: Policy loss: 1.950478. Value loss: 23.515356. Entropy: 0.527542.\n",
      "Iteration 4311: Policy loss: 1.136459. Value loss: 19.726957. Entropy: 0.532444.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4312: Policy loss: -0.463841. Value loss: 39.300560. Entropy: 0.634501.\n",
      "Iteration 4313: Policy loss: 0.025679. Value loss: 22.478043. Entropy: 0.665591.\n",
      "Iteration 4314: Policy loss: -0.502550. Value loss: 22.893129. Entropy: 0.665529.\n",
      "episode: 1834   score: 255.0  epsilon: 1.0    steps: 484  evaluation reward: 179.15\n",
      "episode: 1835   score: 125.0  epsilon: 1.0    steps: 844  evaluation reward: 178.15\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4315: Policy loss: 2.372309. Value loss: 30.701241. Entropy: 0.515742.\n",
      "Iteration 4316: Policy loss: 2.538027. Value loss: 16.341000. Entropy: 0.553964.\n",
      "Iteration 4317: Policy loss: 2.289568. Value loss: 11.773520. Entropy: 0.538395.\n",
      "episode: 1836   score: 400.0  epsilon: 1.0    steps: 346  evaluation reward: 180.6\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4318: Policy loss: 2.360973. Value loss: 22.138792. Entropy: 0.822279.\n",
      "Iteration 4319: Policy loss: 2.535279. Value loss: 15.579331. Entropy: 0.820142.\n",
      "Iteration 4320: Policy loss: 2.394721. Value loss: 11.720521. Entropy: 0.815697.\n",
      "episode: 1837   score: 105.0  epsilon: 1.0    steps: 245  evaluation reward: 180.05\n",
      "episode: 1838   score: 115.0  epsilon: 1.0    steps: 564  evaluation reward: 178.7\n",
      "episode: 1839   score: 315.0  epsilon: 1.0    steps: 918  evaluation reward: 181.1\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4321: Policy loss: 1.132927. Value loss: 24.773161. Entropy: 0.758542.\n",
      "Iteration 4322: Policy loss: 1.016834. Value loss: 17.370388. Entropy: 0.792245.\n",
      "Iteration 4323: Policy loss: 1.117391. Value loss: 14.063994. Entropy: 0.769006.\n",
      "episode: 1840   score: 105.0  epsilon: 1.0    steps: 493  evaluation reward: 180.35\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4324: Policy loss: -0.077976. Value loss: 24.490515. Entropy: 0.722337.\n",
      "Iteration 4325: Policy loss: -0.035877. Value loss: 13.158958. Entropy: 0.741872.\n",
      "Iteration 4326: Policy loss: 0.066634. Value loss: 11.223544. Entropy: 0.722458.\n",
      "episode: 1841   score: 220.0  epsilon: 1.0    steps: 59  evaluation reward: 180.95\n",
      "episode: 1842   score: 340.0  epsilon: 1.0    steps: 708  evaluation reward: 183.15\n",
      "episode: 1843   score: 125.0  epsilon: 1.0    steps: 863  evaluation reward: 183.35\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4327: Policy loss: 0.017450. Value loss: 26.672611. Entropy: 0.940551.\n",
      "Iteration 4328: Policy loss: 0.317075. Value loss: 16.793711. Entropy: 0.961957.\n",
      "Iteration 4329: Policy loss: -0.049594. Value loss: 14.984938. Entropy: 0.939252.\n",
      "episode: 1844   score: 120.0  epsilon: 1.0    steps: 1023  evaluation reward: 183.5\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4330: Policy loss: 0.259080. Value loss: 23.642712. Entropy: 1.078610.\n",
      "Iteration 4331: Policy loss: 0.937634. Value loss: 12.672464. Entropy: 1.074354.\n",
      "Iteration 4332: Policy loss: 0.495380. Value loss: 12.342435. Entropy: 1.072963.\n",
      "episode: 1845   score: 125.0  epsilon: 1.0    steps: 255  evaluation reward: 182.4\n",
      "episode: 1846   score: 115.0  epsilon: 1.0    steps: 637  evaluation reward: 182.5\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4333: Policy loss: -1.918563. Value loss: 28.894091. Entropy: 0.811597.\n",
      "Iteration 4334: Policy loss: -1.923745. Value loss: 20.105600. Entropy: 0.815412.\n",
      "Iteration 4335: Policy loss: -1.932418. Value loss: 17.001516. Entropy: 0.806155.\n",
      "episode: 1847   score: 135.0  epsilon: 1.0    steps: 66  evaluation reward: 182.8\n",
      "episode: 1848   score: 195.0  epsilon: 1.0    steps: 261  evaluation reward: 183.7\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4336: Policy loss: -1.466102. Value loss: 21.015461. Entropy: 0.755953.\n",
      "Iteration 4337: Policy loss: -1.606790. Value loss: 15.044279. Entropy: 0.799749.\n",
      "Iteration 4338: Policy loss: -1.587167. Value loss: 11.051954. Entropy: 0.796073.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4339: Policy loss: -0.448837. Value loss: 16.610996. Entropy: 0.695653.\n",
      "Iteration 4340: Policy loss: -0.455444. Value loss: 9.547912. Entropy: 0.693370.\n",
      "Iteration 4341: Policy loss: -0.585491. Value loss: 7.530751. Entropy: 0.679415.\n",
      "episode: 1849   score: 255.0  epsilon: 1.0    steps: 739  evaluation reward: 180.2\n",
      "episode: 1850   score: 140.0  epsilon: 1.0    steps: 886  evaluation reward: 178.0\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4342: Policy loss: 1.037190. Value loss: 25.640072. Entropy: 0.807914.\n",
      "Iteration 4343: Policy loss: 0.839752. Value loss: 17.264202. Entropy: 0.790457.\n",
      "Iteration 4344: Policy loss: 1.037884. Value loss: 13.573788. Entropy: 0.781485.\n",
      "now time :  2019-02-25 20:01:06.453163\n",
      "episode: 1851   score: 210.0  epsilon: 1.0    steps: 410  evaluation reward: 178.75\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4345: Policy loss: -0.324138. Value loss: 8.580768. Entropy: 0.790676.\n",
      "Iteration 4346: Policy loss: -0.528965. Value loss: 5.009566. Entropy: 0.775200.\n",
      "Iteration 4347: Policy loss: -0.529965. Value loss: 5.423286. Entropy: 0.786675.\n",
      "episode: 1852   score: 135.0  epsilon: 1.0    steps: 243  evaluation reward: 178.9\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4348: Policy loss: 2.783629. Value loss: 22.430637. Entropy: 0.619082.\n",
      "Iteration 4349: Policy loss: 2.739819. Value loss: 12.179357. Entropy: 0.629722.\n",
      "Iteration 4350: Policy loss: 2.621538. Value loss: 12.192703. Entropy: 0.690144.\n",
      "episode: 1853   score: 140.0  epsilon: 1.0    steps: 550  evaluation reward: 178.3\n",
      "episode: 1854   score: 220.0  epsilon: 1.0    steps: 946  evaluation reward: 178.6\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4351: Policy loss: -3.031855. Value loss: 276.249023. Entropy: 0.815264.\n",
      "Iteration 4352: Policy loss: -2.592277. Value loss: 121.705200. Entropy: 0.713758.\n",
      "Iteration 4353: Policy loss: -2.355530. Value loss: 78.262505. Entropy: 0.670958.\n",
      "episode: 1855   score: 165.0  epsilon: 1.0    steps: 89  evaluation reward: 179.5\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4354: Policy loss: -1.258816. Value loss: 42.927574. Entropy: 0.726169.\n",
      "Iteration 4355: Policy loss: -1.526949. Value loss: 30.864614. Entropy: 0.711989.\n",
      "Iteration 4356: Policy loss: -1.589784. Value loss: 21.419422. Entropy: 0.725070.\n",
      "episode: 1856   score: 30.0  epsilon: 1.0    steps: 252  evaluation reward: 177.7\n",
      "episode: 1857   score: 485.0  epsilon: 1.0    steps: 328  evaluation reward: 181.5\n",
      "episode: 1858   score: 135.0  epsilon: 1.0    steps: 439  evaluation reward: 181.8\n",
      "episode: 1859   score: 145.0  epsilon: 1.0    steps: 692  evaluation reward: 182.15\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4357: Policy loss: 1.253978. Value loss: 27.433468. Entropy: 0.804565.\n",
      "Iteration 4358: Policy loss: 1.354893. Value loss: 16.734552. Entropy: 0.819148.\n",
      "Iteration 4359: Policy loss: 1.270734. Value loss: 16.646627. Entropy: 0.837430.\n",
      "episode: 1860   score: 155.0  epsilon: 1.0    steps: 775  evaluation reward: 182.3\n",
      "episode: 1861   score: 75.0  epsilon: 1.0    steps: 971  evaluation reward: 181.85\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4360: Policy loss: -2.290368. Value loss: 18.720999. Entropy: 0.772617.\n",
      "Iteration 4361: Policy loss: -2.307038. Value loss: 11.785280. Entropy: 0.773958.\n",
      "Iteration 4362: Policy loss: -2.189841. Value loss: 9.058334. Entropy: 0.732911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4363: Policy loss: -0.337633. Value loss: 15.115201. Entropy: 0.655489.\n",
      "Iteration 4364: Policy loss: -0.413407. Value loss: 9.746082. Entropy: 0.667004.\n",
      "Iteration 4365: Policy loss: -0.364224. Value loss: 8.135192. Entropy: 0.656717.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4366: Policy loss: -0.858330. Value loss: 6.462837. Entropy: 0.824394.\n",
      "Iteration 4367: Policy loss: -0.865341. Value loss: 5.072128. Entropy: 0.807055.\n",
      "Iteration 4368: Policy loss: -0.800346. Value loss: 4.003242. Entropy: 0.800901.\n",
      "episode: 1862   score: 75.0  epsilon: 1.0    steps: 885  evaluation reward: 181.55\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4369: Policy loss: 0.701454. Value loss: 9.133399. Entropy: 0.695183.\n",
      "Iteration 4370: Policy loss: 0.666232. Value loss: 4.983834. Entropy: 0.707576.\n",
      "Iteration 4371: Policy loss: 0.648337. Value loss: 4.849241. Entropy: 0.715540.\n",
      "episode: 1863   score: 120.0  epsilon: 1.0    steps: 179  evaluation reward: 181.7\n",
      "episode: 1864   score: 105.0  epsilon: 1.0    steps: 386  evaluation reward: 180.6\n",
      "episode: 1865   score: 210.0  epsilon: 1.0    steps: 593  evaluation reward: 181.2\n",
      "episode: 1866   score: 75.0  epsilon: 1.0    steps: 1004  evaluation reward: 180.6\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4372: Policy loss: 1.531918. Value loss: 10.731093. Entropy: 0.587219.\n",
      "Iteration 4373: Policy loss: 1.322812. Value loss: 4.189013. Entropy: 0.566968.\n",
      "Iteration 4374: Policy loss: 1.510091. Value loss: 3.993501. Entropy: 0.595320.\n",
      "episode: 1867   score: 210.0  epsilon: 1.0    steps: 372  evaluation reward: 181.3\n",
      "episode: 1868   score: 180.0  epsilon: 1.0    steps: 745  evaluation reward: 180.65\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4375: Policy loss: -0.110982. Value loss: 25.692421. Entropy: 0.670155.\n",
      "Iteration 4376: Policy loss: 0.201359. Value loss: 16.656124. Entropy: 0.685568.\n",
      "Iteration 4377: Policy loss: 0.156080. Value loss: 14.359706. Entropy: 0.659621.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4378: Policy loss: 0.754203. Value loss: 10.286278. Entropy: 0.736659.\n",
      "Iteration 4379: Policy loss: 0.655215. Value loss: 7.403594. Entropy: 0.729674.\n",
      "Iteration 4380: Policy loss: 0.712491. Value loss: 6.278486. Entropy: 0.709607.\n",
      "episode: 1869   score: 245.0  epsilon: 1.0    steps: 38  evaluation reward: 182.0\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4381: Policy loss: 0.196656. Value loss: 10.407612. Entropy: 0.799824.\n",
      "Iteration 4382: Policy loss: 0.176558. Value loss: 7.028966. Entropy: 0.829955.\n",
      "Iteration 4383: Policy loss: 0.163075. Value loss: 6.449906. Entropy: 0.816103.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4384: Policy loss: -1.453144. Value loss: 14.305884. Entropy: 0.855909.\n",
      "Iteration 4385: Policy loss: -1.475440. Value loss: 7.695578. Entropy: 0.862502.\n",
      "Iteration 4386: Policy loss: -1.590932. Value loss: 7.672090. Entropy: 0.855982.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4387: Policy loss: 1.028095. Value loss: 16.038471. Entropy: 0.692729.\n",
      "Iteration 4388: Policy loss: 0.522814. Value loss: 8.877728. Entropy: 0.675383.\n",
      "Iteration 4389: Policy loss: 0.885313. Value loss: 5.172630. Entropy: 0.672203.\n",
      "episode: 1870   score: 180.0  epsilon: 1.0    steps: 242  evaluation reward: 183.0\n",
      "episode: 1871   score: 180.0  epsilon: 1.0    steps: 473  evaluation reward: 183.75\n",
      "episode: 1872   score: 180.0  epsilon: 1.0    steps: 813  evaluation reward: 184.5\n",
      "episode: 1873   score: 105.0  epsilon: 1.0    steps: 902  evaluation reward: 184.3\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4390: Policy loss: 0.748559. Value loss: 9.709686. Entropy: 0.628785.\n",
      "Iteration 4391: Policy loss: 0.601142. Value loss: 7.591161. Entropy: 0.596217.\n",
      "Iteration 4392: Policy loss: 0.716223. Value loss: 5.826708. Entropy: 0.619657.\n",
      "episode: 1874   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 185.2\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4393: Policy loss: 0.031981. Value loss: 13.123772. Entropy: 0.607918.\n",
      "Iteration 4394: Policy loss: 0.179812. Value loss: 9.262517. Entropy: 0.613802.\n",
      "Iteration 4395: Policy loss: 0.084773. Value loss: 8.255445. Entropy: 0.602107.\n",
      "episode: 1875   score: 180.0  epsilon: 1.0    steps: 308  evaluation reward: 185.2\n",
      "episode: 1876   score: 155.0  epsilon: 1.0    steps: 646  evaluation reward: 184.2\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4396: Policy loss: 0.240191. Value loss: 9.061692. Entropy: 0.738882.\n",
      "Iteration 4397: Policy loss: 0.339460. Value loss: 6.497921. Entropy: 0.748723.\n",
      "Iteration 4398: Policy loss: 0.156917. Value loss: 6.089345. Entropy: 0.746426.\n",
      "episode: 1877   score: 155.0  epsilon: 1.0    steps: 87  evaluation reward: 184.5\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4399: Policy loss: 0.010615. Value loss: 6.504336. Entropy: 0.757037.\n",
      "Iteration 4400: Policy loss: -0.030710. Value loss: 5.035911. Entropy: 0.748816.\n",
      "Iteration 4401: Policy loss: 0.047587. Value loss: 4.294834. Entropy: 0.766247.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4402: Policy loss: -0.410974. Value loss: 5.129576. Entropy: 0.723539.\n",
      "Iteration 4403: Policy loss: -0.479172. Value loss: 2.613474. Entropy: 0.714779.\n",
      "Iteration 4404: Policy loss: -0.393209. Value loss: 3.142079. Entropy: 0.768598.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4405: Policy loss: 0.638965. Value loss: 5.280674. Entropy: 0.597528.\n",
      "Iteration 4406: Policy loss: 0.484420. Value loss: 2.981143. Entropy: 0.636508.\n",
      "Iteration 4407: Policy loss: 0.578481. Value loss: 2.900810. Entropy: 0.607413.\n",
      "episode: 1878   score: 185.0  epsilon: 1.0    steps: 864  evaluation reward: 184.8\n",
      "episode: 1879   score: 155.0  epsilon: 1.0    steps: 945  evaluation reward: 184.6\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4408: Policy loss: 1.527416. Value loss: 8.847467. Entropy: 0.631488.\n",
      "Iteration 4409: Policy loss: 1.514961. Value loss: 5.496538. Entropy: 0.670078.\n",
      "Iteration 4410: Policy loss: 1.528536. Value loss: 4.830125. Entropy: 0.672458.\n",
      "episode: 1880   score: 135.0  epsilon: 1.0    steps: 152  evaluation reward: 184.85\n",
      "episode: 1881   score: 155.0  epsilon: 1.0    steps: 403  evaluation reward: 185.05\n",
      "episode: 1882   score: 135.0  epsilon: 1.0    steps: 553  evaluation reward: 183.5\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4411: Policy loss: 0.686823. Value loss: 11.002933. Entropy: 0.650387.\n",
      "Iteration 4412: Policy loss: 0.576506. Value loss: 8.288662. Entropy: 0.645764.\n",
      "Iteration 4413: Policy loss: 0.580610. Value loss: 7.762343. Entropy: 0.657378.\n",
      "episode: 1883   score: 105.0  epsilon: 1.0    steps: 54  evaluation reward: 182.25\n",
      "episode: 1884   score: 135.0  epsilon: 1.0    steps: 671  evaluation reward: 180.2\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4414: Policy loss: 0.345860. Value loss: 16.794231. Entropy: 0.868943.\n",
      "Iteration 4415: Policy loss: 0.326688. Value loss: 12.104197. Entropy: 0.891772.\n",
      "Iteration 4416: Policy loss: 0.140331. Value loss: 9.619538. Entropy: 0.885456.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4417: Policy loss: -0.354819. Value loss: 10.613240. Entropy: 0.952601.\n",
      "Iteration 4418: Policy loss: -0.557810. Value loss: 5.876768. Entropy: 0.936759.\n",
      "Iteration 4419: Policy loss: -0.502227. Value loss: 6.577779. Entropy: 0.955545.\n",
      "episode: 1885   score: 270.0  epsilon: 1.0    steps: 365  evaluation reward: 181.85\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4420: Policy loss: -0.126048. Value loss: 11.430675. Entropy: 0.830053.\n",
      "Iteration 4421: Policy loss: -0.180443. Value loss: 6.833929. Entropy: 0.826757.\n",
      "Iteration 4422: Policy loss: -0.104396. Value loss: 4.968614. Entropy: 0.839493.\n",
      "episode: 1886   score: 75.0  epsilon: 1.0    steps: 946  evaluation reward: 181.7\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4423: Policy loss: 1.552675. Value loss: 7.096076. Entropy: 0.708161.\n",
      "Iteration 4424: Policy loss: 1.412622. Value loss: 4.756105. Entropy: 0.671040.\n",
      "Iteration 4425: Policy loss: 1.466730. Value loss: 4.017378. Entropy: 0.735075.\n",
      "episode: 1887   score: 80.0  epsilon: 1.0    steps: 131  evaluation reward: 180.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1888   score: 105.0  epsilon: 1.0    steps: 864  evaluation reward: 179.35\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4426: Policy loss: 1.387551. Value loss: 22.443108. Entropy: 0.658085.\n",
      "Iteration 4427: Policy loss: 1.513243. Value loss: 12.234916. Entropy: 0.662600.\n",
      "Iteration 4428: Policy loss: 1.499044. Value loss: 9.934115. Entropy: 0.692081.\n",
      "episode: 1889   score: 155.0  epsilon: 1.0    steps: 441  evaluation reward: 179.35\n",
      "episode: 1890   score: 135.0  epsilon: 1.0    steps: 664  evaluation reward: 178.75\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4429: Policy loss: 0.825264. Value loss: 6.991617. Entropy: 0.769325.\n",
      "Iteration 4430: Policy loss: 0.646334. Value loss: 3.471842. Entropy: 0.781555.\n",
      "Iteration 4431: Policy loss: 0.644597. Value loss: 3.706109. Entropy: 0.800634.\n",
      "episode: 1891   score: 120.0  epsilon: 1.0    steps: 65  evaluation reward: 178.6\n",
      "episode: 1892   score: 260.0  epsilon: 1.0    steps: 622  evaluation reward: 179.65\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4432: Policy loss: -0.091159. Value loss: 10.858375. Entropy: 0.824256.\n",
      "Iteration 4433: Policy loss: -0.045276. Value loss: 5.688000. Entropy: 0.849296.\n",
      "Iteration 4434: Policy loss: 0.052602. Value loss: 5.387084. Entropy: 0.859539.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4435: Policy loss: -0.958337. Value loss: 7.183630. Entropy: 0.833066.\n",
      "Iteration 4436: Policy loss: -0.899688. Value loss: 4.435797. Entropy: 0.828280.\n",
      "Iteration 4437: Policy loss: -1.071877. Value loss: 4.176573. Entropy: 0.805012.\n",
      "episode: 1893   score: 135.0  epsilon: 1.0    steps: 375  evaluation reward: 179.1\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4438: Policy loss: 0.918421. Value loss: 3.990565. Entropy: 0.641375.\n",
      "Iteration 4439: Policy loss: 0.836925. Value loss: 2.851623. Entropy: 0.636112.\n",
      "Iteration 4440: Policy loss: 0.835766. Value loss: 2.208555. Entropy: 0.652270.\n",
      "episode: 1894   score: 135.0  epsilon: 1.0    steps: 997  evaluation reward: 177.1\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4441: Policy loss: 0.397665. Value loss: 8.239605. Entropy: 0.632832.\n",
      "Iteration 4442: Policy loss: 0.419432. Value loss: 4.044149. Entropy: 0.623366.\n",
      "Iteration 4443: Policy loss: 0.329515. Value loss: 3.546102. Entropy: 0.632591.\n",
      "episode: 1895   score: 135.0  epsilon: 1.0    steps: 169  evaluation reward: 176.5\n",
      "episode: 1896   score: 135.0  epsilon: 1.0    steps: 885  evaluation reward: 176.3\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4444: Policy loss: 0.558899. Value loss: 6.023561. Entropy: 0.698156.\n",
      "Iteration 4445: Policy loss: 0.688769. Value loss: 3.455163. Entropy: 0.698731.\n",
      "Iteration 4446: Policy loss: 0.642372. Value loss: 3.130651. Entropy: 0.711846.\n",
      "episode: 1897   score: 105.0  epsilon: 1.0    steps: 694  evaluation reward: 175.35\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4447: Policy loss: 0.412283. Value loss: 10.003485. Entropy: 0.852569.\n",
      "Iteration 4448: Policy loss: 0.664357. Value loss: 5.366563. Entropy: 0.867748.\n",
      "Iteration 4449: Policy loss: 0.455209. Value loss: 4.697304. Entropy: 0.856606.\n",
      "episode: 1898   score: 135.0  epsilon: 1.0    steps: 402  evaluation reward: 174.4\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4450: Policy loss: -0.689393. Value loss: 11.112268. Entropy: 0.872862.\n",
      "Iteration 4451: Policy loss: -0.730645. Value loss: 7.119678. Entropy: 0.882902.\n",
      "Iteration 4452: Policy loss: -0.822615. Value loss: 6.090752. Entropy: 0.867115.\n",
      "episode: 1899   score: 125.0  epsilon: 1.0    steps: 12  evaluation reward: 173.85\n",
      "episode: 1900   score: 155.0  epsilon: 1.0    steps: 523  evaluation reward: 174.15\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4453: Policy loss: -0.282314. Value loss: 5.766957. Entropy: 0.798359.\n",
      "Iteration 4454: Policy loss: -0.334709. Value loss: 2.739965. Entropy: 0.797248.\n",
      "Iteration 4455: Policy loss: -0.317477. Value loss: 3.136205. Entropy: 0.788170.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4456: Policy loss: -0.064318. Value loss: 8.082868. Entropy: 0.754224.\n",
      "Iteration 4457: Policy loss: -0.069219. Value loss: 4.902557. Entropy: 0.772650.\n",
      "Iteration 4458: Policy loss: -0.117853. Value loss: 4.823064. Entropy: 0.779182.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4459: Policy loss: -0.990346. Value loss: 13.717052. Entropy: 0.762517.\n",
      "Iteration 4460: Policy loss: -1.169835. Value loss: 5.119043. Entropy: 0.763292.\n",
      "Iteration 4461: Policy loss: -1.026622. Value loss: 5.533173. Entropy: 0.767982.\n",
      "now time :  2019-02-25 20:03:18.589639\n",
      "episode: 1901   score: 190.0  epsilon: 1.0    steps: 262  evaluation reward: 172.85\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4462: Policy loss: -2.677827. Value loss: 288.839966. Entropy: 0.689558.\n",
      "Iteration 4463: Policy loss: -2.602082. Value loss: 259.117859. Entropy: 0.669067.\n",
      "Iteration 4464: Policy loss: -2.455789. Value loss: 157.585266. Entropy: 0.547399.\n",
      "episode: 1902   score: 105.0  epsilon: 1.0    steps: 85  evaluation reward: 167.85\n",
      "episode: 1903   score: 105.0  epsilon: 1.0    steps: 606  evaluation reward: 167.35\n",
      "episode: 1904   score: 105.0  epsilon: 1.0    steps: 776  evaluation reward: 167.25\n",
      "episode: 1905   score: 240.0  epsilon: 1.0    steps: 908  evaluation reward: 167.6\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4465: Policy loss: 1.280312. Value loss: 24.945396. Entropy: 0.616343.\n",
      "Iteration 4466: Policy loss: 1.085608. Value loss: 11.148396. Entropy: 0.620613.\n",
      "Iteration 4467: Policy loss: 1.074894. Value loss: 9.984277. Entropy: 0.638710.\n",
      "episode: 1906   score: 115.0  epsilon: 1.0    steps: 443  evaluation reward: 167.5\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4468: Policy loss: 0.069627. Value loss: 22.937759. Entropy: 0.701284.\n",
      "Iteration 4469: Policy loss: -0.064774. Value loss: 15.932800. Entropy: 0.707652.\n",
      "Iteration 4470: Policy loss: -0.011429. Value loss: 13.520561. Entropy: 0.692130.\n",
      "episode: 1907   score: 540.0  epsilon: 1.0    steps: 244  evaluation reward: 171.05\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4471: Policy loss: 0.182061. Value loss: 16.625994. Entropy: 0.837781.\n",
      "Iteration 4472: Policy loss: 0.204137. Value loss: 11.288515. Entropy: 0.852851.\n",
      "Iteration 4473: Policy loss: 0.165867. Value loss: 9.772453. Entropy: 0.858514.\n",
      "episode: 1908   score: 225.0  epsilon: 1.0    steps: 694  evaluation reward: 171.65\n",
      "episode: 1909   score: 75.0  epsilon: 1.0    steps: 934  evaluation reward: 171.3\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4474: Policy loss: 1.051186. Value loss: 8.830305. Entropy: 0.618786.\n",
      "Iteration 4475: Policy loss: 0.982393. Value loss: 5.305483. Entropy: 0.647990.\n",
      "Iteration 4476: Policy loss: 1.060515. Value loss: 5.332929. Entropy: 0.651811.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4477: Policy loss: 1.213514. Value loss: 9.249551. Entropy: 0.771451.\n",
      "Iteration 4478: Policy loss: 1.147539. Value loss: 7.580284. Entropy: 0.749676.\n",
      "Iteration 4479: Policy loss: 0.991996. Value loss: 8.244171. Entropy: 0.723997.\n",
      "episode: 1910   score: 155.0  epsilon: 1.0    steps: 265  evaluation reward: 171.8\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4480: Policy loss: 1.778220. Value loss: 9.133316. Entropy: 0.909168.\n",
      "Iteration 4481: Policy loss: 1.339127. Value loss: 8.958375. Entropy: 0.888482.\n",
      "Iteration 4482: Policy loss: 1.421414. Value loss: 6.170331. Entropy: 0.925313.\n",
      "episode: 1911   score: 110.0  epsilon: 1.0    steps: 92  evaluation reward: 169.7\n",
      "episode: 1912   score: 75.0  epsilon: 1.0    steps: 141  evaluation reward: 166.55\n",
      "episode: 1913   score: 75.0  epsilon: 1.0    steps: 391  evaluation reward: 166.55\n",
      "episode: 1914   score: 105.0  epsilon: 1.0    steps: 568  evaluation reward: 164.85\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4483: Policy loss: 0.291700. Value loss: 11.058121. Entropy: 0.713259.\n",
      "Iteration 4484: Policy loss: 0.238373. Value loss: 6.583010. Entropy: 0.713336.\n",
      "Iteration 4485: Policy loss: 0.238331. Value loss: 5.635094. Entropy: 0.697389.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4486: Policy loss: 0.342875. Value loss: 16.342140. Entropy: 0.796647.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4487: Policy loss: 0.385731. Value loss: 12.324015. Entropy: 0.820554.\n",
      "Iteration 4488: Policy loss: 0.310854. Value loss: 10.759770. Entropy: 0.802503.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4489: Policy loss: -2.702842. Value loss: 192.478470. Entropy: 0.888263.\n",
      "Iteration 4490: Policy loss: -3.698209. Value loss: 163.413605. Entropy: 0.823736.\n",
      "Iteration 4491: Policy loss: -2.681087. Value loss: 91.191292. Entropy: 0.830898.\n",
      "episode: 1915   score: 105.0  epsilon: 1.0    steps: 412  evaluation reward: 163.6\n",
      "episode: 1916   score: 80.0  epsilon: 1.0    steps: 576  evaluation reward: 161.95\n",
      "episode: 1917   score: 225.0  epsilon: 1.0    steps: 754  evaluation reward: 163.0\n",
      "episode: 1918   score: 420.0  epsilon: 1.0    steps: 889  evaluation reward: 166.1\n",
      "episode: 1919   score: 120.0  epsilon: 1.0    steps: 934  evaluation reward: 166.25\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4492: Policy loss: -0.315679. Value loss: 14.411426. Entropy: 0.632817.\n",
      "Iteration 4493: Policy loss: -0.102990. Value loss: 7.525080. Entropy: 0.631818.\n",
      "Iteration 4494: Policy loss: -0.196710. Value loss: 7.310558. Entropy: 0.611461.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4495: Policy loss: 0.471186. Value loss: 11.104969. Entropy: 0.700004.\n",
      "Iteration 4496: Policy loss: 0.519951. Value loss: 8.598013. Entropy: 0.690256.\n",
      "Iteration 4497: Policy loss: 0.551608. Value loss: 6.471215. Entropy: 0.722314.\n",
      "episode: 1920   score: 115.0  epsilon: 1.0    steps: 288  evaluation reward: 166.6\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4498: Policy loss: 0.841427. Value loss: 26.907740. Entropy: 0.811949.\n",
      "Iteration 4499: Policy loss: 0.845426. Value loss: 17.617599. Entropy: 0.828719.\n",
      "Iteration 4500: Policy loss: 0.954177. Value loss: 14.738230. Entropy: 0.832250.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4501: Policy loss: -0.212923. Value loss: 11.033607. Entropy: 0.865764.\n",
      "Iteration 4502: Policy loss: -0.194694. Value loss: 6.563570. Entropy: 0.865438.\n",
      "Iteration 4503: Policy loss: -0.200119. Value loss: 5.133552. Entropy: 0.870431.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4504: Policy loss: 0.209048. Value loss: 7.728809. Entropy: 0.791380.\n",
      "Iteration 4505: Policy loss: 0.274401. Value loss: 5.001303. Entropy: 0.774801.\n",
      "Iteration 4506: Policy loss: 0.334886. Value loss: 3.460387. Entropy: 0.791094.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4507: Policy loss: 1.205195. Value loss: 8.278119. Entropy: 0.778280.\n",
      "Iteration 4508: Policy loss: 1.056240. Value loss: 3.903046. Entropy: 0.828681.\n",
      "Iteration 4509: Policy loss: 1.107340. Value loss: 3.688769. Entropy: 0.819094.\n",
      "episode: 1921   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 166.95\n",
      "episode: 1922   score: 105.0  epsilon: 1.0    steps: 551  evaluation reward: 166.8\n",
      "episode: 1923   score: 180.0  epsilon: 1.0    steps: 965  evaluation reward: 167.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4510: Policy loss: -0.429544. Value loss: 17.405893. Entropy: 0.696196.\n",
      "Iteration 4511: Policy loss: -0.479462. Value loss: 9.029899. Entropy: 0.724316.\n",
      "Iteration 4512: Policy loss: -0.626202. Value loss: 6.058775. Entropy: 0.730651.\n",
      "episode: 1924   score: 105.0  epsilon: 1.0    steps: 657  evaluation reward: 167.15\n",
      "episode: 1925   score: 180.0  epsilon: 1.0    steps: 806  evaluation reward: 167.8\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4513: Policy loss: -0.616351. Value loss: 15.309740. Entropy: 0.782343.\n",
      "Iteration 4514: Policy loss: -0.617810. Value loss: 7.019488. Entropy: 0.811311.\n",
      "Iteration 4515: Policy loss: -0.746618. Value loss: 5.690287. Entropy: 0.816105.\n",
      "episode: 1926   score: 290.0  epsilon: 1.0    steps: 27  evaluation reward: 168.7\n",
      "episode: 1927   score: 290.0  epsilon: 1.0    steps: 256  evaluation reward: 169.3\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4516: Policy loss: 1.235114. Value loss: 17.623875. Entropy: 0.743517.\n",
      "Iteration 4517: Policy loss: 1.263458. Value loss: 11.136556. Entropy: 0.743637.\n",
      "Iteration 4518: Policy loss: 1.323614. Value loss: 9.544327. Entropy: 0.754829.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4519: Policy loss: 0.197651. Value loss: 6.848395. Entropy: 0.986002.\n",
      "Iteration 4520: Policy loss: 0.169300. Value loss: 4.412342. Entropy: 0.987048.\n",
      "Iteration 4521: Policy loss: 0.171595. Value loss: 4.113885. Entropy: 0.972152.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4522: Policy loss: -0.131533. Value loss: 8.221900. Entropy: 0.876586.\n",
      "Iteration 4523: Policy loss: -0.291742. Value loss: 6.693815. Entropy: 0.860896.\n",
      "Iteration 4524: Policy loss: -0.195227. Value loss: 3.583909. Entropy: 0.852249.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4525: Policy loss: -1.167891. Value loss: 9.346719. Entropy: 0.796092.\n",
      "Iteration 4526: Policy loss: -1.261558. Value loss: 4.365256. Entropy: 0.783538.\n",
      "Iteration 4527: Policy loss: -1.212303. Value loss: 3.323743. Entropy: 0.768783.\n",
      "episode: 1928   score: 285.0  epsilon: 1.0    steps: 383  evaluation reward: 171.4\n",
      "episode: 1929   score: 155.0  epsilon: 1.0    steps: 1012  evaluation reward: 170.05\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4528: Policy loss: -1.264325. Value loss: 15.379513. Entropy: 0.612848.\n",
      "Iteration 4529: Policy loss: -1.152570. Value loss: 8.378068. Entropy: 0.638376.\n",
      "Iteration 4530: Policy loss: -1.119816. Value loss: 7.520234. Entropy: 0.631355.\n",
      "episode: 1930   score: 180.0  epsilon: 1.0    steps: 418  evaluation reward: 171.05\n",
      "episode: 1931   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 170.35\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4531: Policy loss: -0.422615. Value loss: 14.045560. Entropy: 0.719376.\n",
      "Iteration 4532: Policy loss: -0.395153. Value loss: 8.741496. Entropy: 0.734178.\n",
      "Iteration 4533: Policy loss: -0.390725. Value loss: 7.940612. Entropy: 0.743011.\n",
      "episode: 1932   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 169.4\n",
      "episode: 1933   score: 260.0  epsilon: 1.0    steps: 526  evaluation reward: 168.3\n",
      "episode: 1934   score: 180.0  epsilon: 1.0    steps: 788  evaluation reward: 167.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4534: Policy loss: 1.041192. Value loss: 12.401599. Entropy: 0.699618.\n",
      "Iteration 4535: Policy loss: 0.931603. Value loss: 10.198127. Entropy: 0.721227.\n",
      "Iteration 4536: Policy loss: 1.199767. Value loss: 7.380516. Entropy: 0.682180.\n",
      "episode: 1935   score: 210.0  epsilon: 1.0    steps: 187  evaluation reward: 168.4\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4537: Policy loss: 0.246413. Value loss: 13.281881. Entropy: 0.911873.\n",
      "Iteration 4538: Policy loss: 0.242623. Value loss: 11.329328. Entropy: 0.903773.\n",
      "Iteration 4539: Policy loss: 0.230597. Value loss: 11.265559. Entropy: 0.905369.\n",
      "episode: 1936   score: 50.0  epsilon: 1.0    steps: 713  evaluation reward: 164.9\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4540: Policy loss: 0.670539. Value loss: 7.173004. Entropy: 1.029758.\n",
      "Iteration 4541: Policy loss: 0.983327. Value loss: 5.953924. Entropy: 1.013998.\n",
      "Iteration 4542: Policy loss: 0.815273. Value loss: 4.145673. Entropy: 0.996149.\n",
      "episode: 1937   score: 105.0  epsilon: 1.0    steps: 327  evaluation reward: 164.9\n",
      "episode: 1938   score: 105.0  epsilon: 1.0    steps: 497  evaluation reward: 164.8\n",
      "episode: 1939   score: 105.0  epsilon: 1.0    steps: 965  evaluation reward: 162.7\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4543: Policy loss: 0.380138. Value loss: 7.089535. Entropy: 0.980937.\n",
      "Iteration 4544: Policy loss: 0.359176. Value loss: 4.556682. Entropy: 0.995856.\n",
      "Iteration 4545: Policy loss: 0.388836. Value loss: 3.573320. Entropy: 0.976442.\n",
      "episode: 1940   score: 105.0  epsilon: 1.0    steps: 34  evaluation reward: 162.7\n",
      "episode: 1941   score: 105.0  epsilon: 1.0    steps: 586  evaluation reward: 161.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4546: Policy loss: -0.051332. Value loss: 6.479115. Entropy: 0.917133.\n",
      "Iteration 4547: Policy loss: -0.087957. Value loss: 5.040956. Entropy: 0.908473.\n",
      "Iteration 4548: Policy loss: -0.058809. Value loss: 4.175899. Entropy: 0.918189.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4549: Policy loss: 0.127104. Value loss: 9.213293. Entropy: 0.874261.\n",
      "Iteration 4550: Policy loss: 0.324320. Value loss: 6.210553. Entropy: 0.883306.\n",
      "Iteration 4551: Policy loss: 0.529431. Value loss: 6.372404. Entropy: 0.924317.\n",
      "episode: 1942   score: 105.0  epsilon: 1.0    steps: 130  evaluation reward: 159.2\n",
      "episode: 1943   score: 180.0  epsilon: 1.0    steps: 825  evaluation reward: 159.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4552: Policy loss: 0.630204. Value loss: 12.247446. Entropy: 1.116038.\n",
      "Iteration 4553: Policy loss: 0.341542. Value loss: 9.885705. Entropy: 1.137480.\n",
      "Iteration 4554: Policy loss: 0.509899. Value loss: 6.016371. Entropy: 1.112331.\n",
      "episode: 1944   score: 110.0  epsilon: 1.0    steps: 426  evaluation reward: 159.65\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4555: Policy loss: -0.849389. Value loss: 9.176172. Entropy: 0.972692.\n",
      "Iteration 4556: Policy loss: -0.810290. Value loss: 4.396437. Entropy: 0.979923.\n",
      "Iteration 4557: Policy loss: -0.780653. Value loss: 4.575047. Entropy: 0.977221.\n",
      "episode: 1945   score: 180.0  epsilon: 1.0    steps: 758  evaluation reward: 160.2\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4558: Policy loss: 0.005439. Value loss: 8.111698. Entropy: 0.926842.\n",
      "Iteration 4559: Policy loss: -0.050483. Value loss: 4.891243. Entropy: 0.921934.\n",
      "Iteration 4560: Policy loss: 0.001509. Value loss: 4.081955. Entropy: 0.922598.\n",
      "episode: 1946   score: 110.0  epsilon: 1.0    steps: 47  evaluation reward: 160.15\n",
      "episode: 1947   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 160.9\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4561: Policy loss: -0.036171. Value loss: 11.037095. Entropy: 0.854850.\n",
      "Iteration 4562: Policy loss: -0.118474. Value loss: 5.369667. Entropy: 0.875030.\n",
      "Iteration 4563: Policy loss: -0.118159. Value loss: 4.070015. Entropy: 0.871864.\n",
      "episode: 1948   score: 105.0  epsilon: 1.0    steps: 175  evaluation reward: 160.0\n",
      "episode: 1949   score: 115.0  epsilon: 1.0    steps: 528  evaluation reward: 158.6\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4564: Policy loss: 0.651846. Value loss: 10.967973. Entropy: 0.789949.\n",
      "Iteration 4565: Policy loss: 0.607746. Value loss: 9.157393. Entropy: 0.809730.\n",
      "Iteration 4566: Policy loss: 0.527294. Value loss: 9.537133. Entropy: 0.800830.\n",
      "episode: 1950   score: 105.0  epsilon: 1.0    steps: 451  evaluation reward: 158.25\n",
      "now time :  2019-02-25 20:05:18.350343\n",
      "episode: 1951   score: 105.0  epsilon: 1.0    steps: 859  evaluation reward: 157.2\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4567: Policy loss: 1.128153. Value loss: 9.972013. Entropy: 0.843643.\n",
      "Iteration 4568: Policy loss: 1.229717. Value loss: 6.681881. Entropy: 0.854661.\n",
      "Iteration 4569: Policy loss: 1.171003. Value loss: 6.303933. Entropy: 0.887453.\n",
      "episode: 1952   score: 300.0  epsilon: 1.0    steps: 920  evaluation reward: 158.85\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4570: Policy loss: -0.353930. Value loss: 12.660742. Entropy: 1.056861.\n",
      "Iteration 4571: Policy loss: -0.326245. Value loss: 9.329942. Entropy: 1.054370.\n",
      "Iteration 4572: Policy loss: -0.316530. Value loss: 7.333573. Entropy: 1.035968.\n",
      "episode: 1953   score: 120.0  epsilon: 1.0    steps: 63  evaluation reward: 158.65\n",
      "episode: 1954   score: 105.0  epsilon: 1.0    steps: 549  evaluation reward: 157.5\n",
      "episode: 1955   score: 110.0  epsilon: 1.0    steps: 730  evaluation reward: 156.95\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4573: Policy loss: 0.771278. Value loss: 8.359293. Entropy: 0.872165.\n",
      "Iteration 4574: Policy loss: 0.716798. Value loss: 6.138870. Entropy: 0.863481.\n",
      "Iteration 4575: Policy loss: 0.681294. Value loss: 5.551408. Entropy: 0.863637.\n",
      "episode: 1956   score: 135.0  epsilon: 1.0    steps: 309  evaluation reward: 158.0\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4576: Policy loss: 0.932809. Value loss: 17.938761. Entropy: 1.010287.\n",
      "Iteration 4577: Policy loss: 0.887062. Value loss: 10.775539. Entropy: 0.998661.\n",
      "Iteration 4578: Policy loss: 1.073170. Value loss: 9.060646. Entropy: 0.995494.\n",
      "episode: 1957   score: 105.0  epsilon: 1.0    steps: 799  evaluation reward: 154.2\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4579: Policy loss: -0.557552. Value loss: 18.470663. Entropy: 0.905442.\n",
      "Iteration 4580: Policy loss: -0.812881. Value loss: 12.016337. Entropy: 0.897159.\n",
      "Iteration 4581: Policy loss: -0.548627. Value loss: 10.048962. Entropy: 0.891837.\n",
      "episode: 1958   score: 225.0  epsilon: 1.0    steps: 210  evaluation reward: 155.1\n",
      "episode: 1959   score: 60.0  epsilon: 1.0    steps: 620  evaluation reward: 154.25\n",
      "episode: 1960   score: 100.0  epsilon: 1.0    steps: 916  evaluation reward: 153.7\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4582: Policy loss: 0.717887. Value loss: 18.987183. Entropy: 0.927104.\n",
      "Iteration 4583: Policy loss: 0.634804. Value loss: 11.596221. Entropy: 0.926042.\n",
      "Iteration 4584: Policy loss: 0.958450. Value loss: 7.954741. Entropy: 0.940070.\n",
      "episode: 1961   score: 180.0  epsilon: 1.0    steps: 118  evaluation reward: 154.75\n",
      "episode: 1962   score: 105.0  epsilon: 1.0    steps: 768  evaluation reward: 155.05\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4585: Policy loss: -1.463219. Value loss: 24.135233. Entropy: 0.848029.\n",
      "Iteration 4586: Policy loss: -1.360466. Value loss: 12.365337. Entropy: 0.841325.\n",
      "Iteration 4587: Policy loss: -1.532888. Value loss: 11.639099. Entropy: 0.827222.\n",
      "episode: 1963   score: 135.0  epsilon: 1.0    steps: 344  evaluation reward: 155.2\n",
      "episode: 1964   score: 210.0  epsilon: 1.0    steps: 396  evaluation reward: 156.25\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4588: Policy loss: -0.495201. Value loss: 8.820943. Entropy: 0.878121.\n",
      "Iteration 4589: Policy loss: -0.684933. Value loss: 6.160295. Entropy: 0.884099.\n",
      "Iteration 4590: Policy loss: -0.570458. Value loss: 5.542822. Entropy: 0.879675.\n",
      "episode: 1965   score: 105.0  epsilon: 1.0    steps: 251  evaluation reward: 155.2\n",
      "episode: 1966   score: 110.0  epsilon: 1.0    steps: 940  evaluation reward: 155.55\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4591: Policy loss: 0.747370. Value loss: 13.330463. Entropy: 0.715191.\n",
      "Iteration 4592: Policy loss: 0.695257. Value loss: 9.377140. Entropy: 0.714769.\n",
      "Iteration 4593: Policy loss: 0.675562. Value loss: 9.231387. Entropy: 0.707289.\n",
      "episode: 1967   score: 105.0  epsilon: 1.0    steps: 554  evaluation reward: 154.5\n",
      "episode: 1968   score: 105.0  epsilon: 1.0    steps: 824  evaluation reward: 153.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4594: Policy loss: 1.145535. Value loss: 11.784933. Entropy: 0.906036.\n",
      "Iteration 4595: Policy loss: 1.101417. Value loss: 9.052902. Entropy: 0.923721.\n",
      "Iteration 4596: Policy loss: 0.950305. Value loss: 9.695829. Entropy: 0.899991.\n",
      "episode: 1969   score: 105.0  epsilon: 1.0    steps: 3  evaluation reward: 152.35\n",
      "episode: 1970   score: 75.0  epsilon: 1.0    steps: 429  evaluation reward: 151.3\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4597: Policy loss: 0.946450. Value loss: 10.466352. Entropy: 0.762800.\n",
      "Iteration 4598: Policy loss: 0.812421. Value loss: 6.778131. Entropy: 0.749319.\n",
      "Iteration 4599: Policy loss: 0.875180. Value loss: 6.164658. Entropy: 0.757622.\n",
      "episode: 1971   score: 105.0  epsilon: 1.0    steps: 663  evaluation reward: 150.55\n",
      "episode: 1972   score: 110.0  epsilon: 1.0    steps: 981  evaluation reward: 149.85\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4600: Policy loss: 0.597378. Value loss: 10.645725. Entropy: 0.844588.\n",
      "Iteration 4601: Policy loss: 0.317082. Value loss: 10.057213. Entropy: 0.836416.\n",
      "Iteration 4602: Policy loss: 0.659996. Value loss: 7.466999. Entropy: 0.845246.\n",
      "episode: 1973   score: 75.0  epsilon: 1.0    steps: 147  evaluation reward: 149.55\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4603: Policy loss: 0.249530. Value loss: 11.440588. Entropy: 1.061769.\n",
      "Iteration 4604: Policy loss: 0.129786. Value loss: 7.822587. Entropy: 1.070825.\n",
      "Iteration 4605: Policy loss: 0.193201. Value loss: 6.355398. Entropy: 1.061121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1974   score: 165.0  epsilon: 1.0    steps: 102  evaluation reward: 149.1\n",
      "episode: 1975   score: 185.0  epsilon: 1.0    steps: 335  evaluation reward: 149.15\n",
      "episode: 1976   score: 75.0  epsilon: 1.0    steps: 891  evaluation reward: 148.35\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4606: Policy loss: -1.655109. Value loss: 29.264853. Entropy: 0.947880.\n",
      "Iteration 4607: Policy loss: -1.456448. Value loss: 16.577827. Entropy: 0.923278.\n",
      "Iteration 4608: Policy loss: -1.670082. Value loss: 12.439906. Entropy: 0.926560.\n",
      "episode: 1977   score: 105.0  epsilon: 1.0    steps: 420  evaluation reward: 147.85\n",
      "episode: 1978   score: 135.0  epsilon: 1.0    steps: 561  evaluation reward: 147.35\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4609: Policy loss: 0.713904. Value loss: 9.091210. Entropy: 0.829736.\n",
      "Iteration 4610: Policy loss: 0.635353. Value loss: 6.013665. Entropy: 0.848043.\n",
      "Iteration 4611: Policy loss: 0.525221. Value loss: 4.789754. Entropy: 0.826609.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4612: Policy loss: 1.390189. Value loss: 15.594593. Entropy: 0.818507.\n",
      "Iteration 4613: Policy loss: 1.113105. Value loss: 11.213619. Entropy: 0.862374.\n",
      "Iteration 4614: Policy loss: 1.297723. Value loss: 9.460298. Entropy: 0.831814.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4615: Policy loss: -0.481818. Value loss: 22.264235. Entropy: 1.029974.\n",
      "Iteration 4616: Policy loss: -0.528319. Value loss: 13.327864. Entropy: 1.020567.\n",
      "Iteration 4617: Policy loss: -0.625213. Value loss: 10.849089. Entropy: 0.992589.\n",
      "episode: 1979   score: 195.0  epsilon: 1.0    steps: 226  evaluation reward: 147.75\n",
      "episode: 1980   score: 100.0  epsilon: 1.0    steps: 497  evaluation reward: 147.4\n",
      "episode: 1981   score: 165.0  epsilon: 1.0    steps: 996  evaluation reward: 147.5\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4618: Policy loss: 2.215052. Value loss: 34.582001. Entropy: 0.984928.\n",
      "Iteration 4619: Policy loss: 2.224028. Value loss: 16.445160. Entropy: 0.972496.\n",
      "Iteration 4620: Policy loss: 2.173102. Value loss: 13.654989. Entropy: 0.971562.\n",
      "episode: 1982   score: 85.0  epsilon: 1.0    steps: 294  evaluation reward: 147.0\n",
      "episode: 1983   score: 105.0  epsilon: 1.0    steps: 780  evaluation reward: 147.0\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4621: Policy loss: -0.687542. Value loss: 22.721073. Entropy: 0.971967.\n",
      "Iteration 4622: Policy loss: -0.704109. Value loss: 13.493363. Entropy: 0.960874.\n",
      "Iteration 4623: Policy loss: -0.761420. Value loss: 11.120673. Entropy: 0.955282.\n",
      "episode: 1984   score: 230.0  epsilon: 1.0    steps: 26  evaluation reward: 147.95\n",
      "episode: 1985   score: 330.0  epsilon: 1.0    steps: 672  evaluation reward: 148.55\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4624: Policy loss: 0.703401. Value loss: 21.260656. Entropy: 0.683574.\n",
      "Iteration 4625: Policy loss: 0.677253. Value loss: 14.700680. Entropy: 0.672349.\n",
      "Iteration 4626: Policy loss: 0.581426. Value loss: 13.977427. Entropy: 0.688164.\n",
      "episode: 1986   score: 375.0  epsilon: 1.0    steps: 614  evaluation reward: 151.55\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4627: Policy loss: 0.360624. Value loss: 197.655899. Entropy: 0.886275.\n",
      "Iteration 4628: Policy loss: 0.635951. Value loss: 90.484024. Entropy: 0.804291.\n",
      "Iteration 4629: Policy loss: 0.514925. Value loss: 56.700947. Entropy: 0.845639.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4630: Policy loss: -0.893976. Value loss: 32.759766. Entropy: 0.881779.\n",
      "Iteration 4631: Policy loss: -0.630701. Value loss: 20.917614. Entropy: 0.862521.\n",
      "Iteration 4632: Policy loss: -0.797399. Value loss: 14.924259. Entropy: 0.848444.\n",
      "episode: 1987   score: 180.0  epsilon: 1.0    steps: 421  evaluation reward: 152.55\n",
      "episode: 1988   score: 180.0  epsilon: 1.0    steps: 818  evaluation reward: 153.3\n",
      "episode: 1989   score: 180.0  epsilon: 1.0    steps: 991  evaluation reward: 153.55\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4633: Policy loss: 2.250322. Value loss: 23.494005. Entropy: 0.714162.\n",
      "Iteration 4634: Policy loss: 2.102140. Value loss: 13.053176. Entropy: 0.701057.\n",
      "Iteration 4635: Policy loss: 2.165227. Value loss: 10.965527. Entropy: 0.693896.\n",
      "episode: 1990   score: 150.0  epsilon: 1.0    steps: 76  evaluation reward: 153.7\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4636: Policy loss: 0.799285. Value loss: 43.035992. Entropy: 0.803277.\n",
      "Iteration 4637: Policy loss: 0.814224. Value loss: 27.195936. Entropy: 0.785103.\n",
      "Iteration 4638: Policy loss: 0.588772. Value loss: 22.586473. Entropy: 0.781284.\n",
      "episode: 1991   score: 240.0  epsilon: 1.0    steps: 356  evaluation reward: 154.9\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4639: Policy loss: -0.736450. Value loss: 26.636347. Entropy: 0.740681.\n",
      "Iteration 4640: Policy loss: -0.503877. Value loss: 15.653198. Entropy: 0.741385.\n",
      "Iteration 4641: Policy loss: -0.566928. Value loss: 11.131968. Entropy: 0.733871.\n",
      "episode: 1992   score: 105.0  epsilon: 1.0    steps: 1022  evaluation reward: 153.35\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4642: Policy loss: 0.870290. Value loss: 19.687790. Entropy: 0.803025.\n",
      "Iteration 4643: Policy loss: 0.763611. Value loss: 11.450039. Entropy: 0.802062.\n",
      "Iteration 4644: Policy loss: 0.753765. Value loss: 9.984502. Entropy: 0.839818.\n",
      "episode: 1993   score: 365.0  epsilon: 1.0    steps: 237  evaluation reward: 155.65\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4645: Policy loss: -0.664469. Value loss: 21.451483. Entropy: 0.677155.\n",
      "Iteration 4646: Policy loss: -0.374643. Value loss: 13.586893. Entropy: 0.662023.\n",
      "Iteration 4647: Policy loss: -0.516033. Value loss: 10.211426. Entropy: 0.652133.\n",
      "episode: 1994   score: 210.0  epsilon: 1.0    steps: 553  evaluation reward: 156.4\n",
      "episode: 1995   score: 210.0  epsilon: 1.0    steps: 809  evaluation reward: 157.15\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4648: Policy loss: 1.240531. Value loss: 14.132707. Entropy: 0.818059.\n",
      "Iteration 4649: Policy loss: 1.247195. Value loss: 7.623349. Entropy: 0.827841.\n",
      "Iteration 4650: Policy loss: 1.409719. Value loss: 5.931999. Entropy: 0.824818.\n",
      "episode: 1996   score: 315.0  epsilon: 1.0    steps: 720  evaluation reward: 158.95\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4651: Policy loss: -0.039677. Value loss: 20.103016. Entropy: 0.696247.\n",
      "Iteration 4652: Policy loss: -0.094148. Value loss: 11.309693. Entropy: 0.692019.\n",
      "Iteration 4653: Policy loss: -0.294235. Value loss: 8.971072. Entropy: 0.708742.\n",
      "episode: 1997   score: 240.0  epsilon: 1.0    steps: 501  evaluation reward: 160.3\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4654: Policy loss: 1.438232. Value loss: 15.879607. Entropy: 0.763054.\n",
      "Iteration 4655: Policy loss: 1.426990. Value loss: 9.822987. Entropy: 0.749670.\n",
      "Iteration 4656: Policy loss: 1.119327. Value loss: 8.052561. Entropy: 0.738408.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4657: Policy loss: 1.606624. Value loss: 19.908096. Entropy: 0.877335.\n",
      "Iteration 4658: Policy loss: 2.335967. Value loss: 11.550940. Entropy: 0.913670.\n",
      "Iteration 4659: Policy loss: 1.943200. Value loss: 7.629577. Entropy: 0.888975.\n",
      "episode: 1998   score: 135.0  epsilon: 1.0    steps: 637  evaluation reward: 160.3\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4660: Policy loss: 1.553105. Value loss: 17.492886. Entropy: 0.730690.\n",
      "Iteration 4661: Policy loss: 1.606242. Value loss: 11.556582. Entropy: 0.754576.\n",
      "Iteration 4662: Policy loss: 1.639911. Value loss: 8.477710. Entropy: 0.722986.\n",
      "episode: 1999   score: 80.0  epsilon: 1.0    steps: 651  evaluation reward: 159.85\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4663: Policy loss: 0.399630. Value loss: 18.876942. Entropy: 0.980664.\n",
      "Iteration 4664: Policy loss: 0.333354. Value loss: 11.379194. Entropy: 0.973591.\n",
      "Iteration 4665: Policy loss: 0.238899. Value loss: 7.406997. Entropy: 0.966848.\n",
      "episode: 2000   score: 245.0  epsilon: 1.0    steps: 294  evaluation reward: 160.75\n",
      "now time :  2019-02-25 20:07:09.385841\n",
      "episode: 2001   score: 75.0  epsilon: 1.0    steps: 397  evaluation reward: 159.6\n",
      "episode: 2002   score: 180.0  epsilon: 1.0    steps: 814  evaluation reward: 160.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4666: Policy loss: 0.165287. Value loss: 18.014296. Entropy: 0.812871.\n",
      "Iteration 4667: Policy loss: 0.182680. Value loss: 9.961085. Entropy: 0.824525.\n",
      "Iteration 4668: Policy loss: 0.116493. Value loss: 7.100731. Entropy: 0.795711.\n",
      "episode: 2003   score: 315.0  epsilon: 1.0    steps: 14  evaluation reward: 162.45\n",
      "episode: 2004   score: 260.0  epsilon: 1.0    steps: 189  evaluation reward: 164.0\n",
      "episode: 2005   score: 310.0  epsilon: 1.0    steps: 983  evaluation reward: 164.7\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4669: Policy loss: 1.779582. Value loss: 10.203805. Entropy: 0.754207.\n",
      "Iteration 4670: Policy loss: 1.767440. Value loss: 5.855810. Entropy: 0.767413.\n",
      "Iteration 4671: Policy loss: 1.529659. Value loss: 4.910637. Entropy: 0.787001.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4672: Policy loss: -0.053701. Value loss: 13.978382. Entropy: 0.777168.\n",
      "Iteration 4673: Policy loss: -0.111315. Value loss: 7.738390. Entropy: 0.782044.\n",
      "Iteration 4674: Policy loss: -0.125102. Value loss: 9.477814. Entropy: 0.783629.\n",
      "episode: 2006   score: 105.0  epsilon: 1.0    steps: 752  evaluation reward: 164.6\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4675: Policy loss: 0.339238. Value loss: 14.187262. Entropy: 0.902766.\n",
      "Iteration 4676: Policy loss: 0.634904. Value loss: 8.022473. Entropy: 0.917946.\n",
      "Iteration 4677: Policy loss: 0.430266. Value loss: 8.027285. Entropy: 0.911235.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4678: Policy loss: -0.407537. Value loss: 9.301823. Entropy: 0.682658.\n",
      "Iteration 4679: Policy loss: -0.249677. Value loss: 5.643843. Entropy: 0.679743.\n",
      "Iteration 4680: Policy loss: -0.325941. Value loss: 4.029810. Entropy: 0.696151.\n",
      "episode: 2007   score: 155.0  epsilon: 1.0    steps: 536  evaluation reward: 160.75\n",
      "episode: 2008   score: 135.0  epsilon: 1.0    steps: 852  evaluation reward: 159.85\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4681: Policy loss: 1.682721. Value loss: 12.219979. Entropy: 0.742808.\n",
      "Iteration 4682: Policy loss: 1.807630. Value loss: 7.222397. Entropy: 0.749803.\n",
      "Iteration 4683: Policy loss: 1.632064. Value loss: 4.707785. Entropy: 0.721418.\n",
      "episode: 2009   score: 185.0  epsilon: 1.0    steps: 325  evaluation reward: 160.95\n",
      "episode: 2010   score: 210.0  epsilon: 1.0    steps: 470  evaluation reward: 161.5\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4684: Policy loss: 1.271857. Value loss: 12.981349. Entropy: 0.576614.\n",
      "Iteration 4685: Policy loss: 1.217884. Value loss: 6.724232. Entropy: 0.557573.\n",
      "Iteration 4686: Policy loss: 0.978897. Value loss: 6.761813. Entropy: 0.587076.\n",
      "episode: 2011   score: 155.0  epsilon: 1.0    steps: 53  evaluation reward: 161.95\n",
      "episode: 2012   score: 120.0  epsilon: 1.0    steps: 1000  evaluation reward: 162.4\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4687: Policy loss: 1.347279. Value loss: 15.595336. Entropy: 0.804783.\n",
      "Iteration 4688: Policy loss: 1.629329. Value loss: 9.577789. Entropy: 0.837366.\n",
      "Iteration 4689: Policy loss: 1.395107. Value loss: 7.213718. Entropy: 0.871080.\n",
      "episode: 2013   score: 185.0  epsilon: 1.0    steps: 184  evaluation reward: 163.5\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4690: Policy loss: -0.396556. Value loss: 6.443062. Entropy: 0.891369.\n",
      "Iteration 4691: Policy loss: -0.615250. Value loss: 4.383016. Entropy: 0.876933.\n",
      "Iteration 4692: Policy loss: -0.416866. Value loss: 4.852392. Entropy: 0.858609.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4693: Policy loss: -0.976924. Value loss: 11.032227. Entropy: 0.777506.\n",
      "Iteration 4694: Policy loss: -0.988958. Value loss: 7.257846. Entropy: 0.777471.\n",
      "Iteration 4695: Policy loss: -1.016569. Value loss: 5.567269. Entropy: 0.777349.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4696: Policy loss: 1.442760. Value loss: 7.273084. Entropy: 0.828702.\n",
      "Iteration 4697: Policy loss: 1.305378. Value loss: 4.107479. Entropy: 0.846381.\n",
      "Iteration 4698: Policy loss: 1.401194. Value loss: 3.598498. Entropy: 0.834700.\n",
      "episode: 2014   score: 135.0  epsilon: 1.0    steps: 226  evaluation reward: 163.8\n",
      "episode: 2015   score: 155.0  epsilon: 1.0    steps: 595  evaluation reward: 164.3\n",
      "episode: 2016   score: 240.0  epsilon: 1.0    steps: 725  evaluation reward: 165.9\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4699: Policy loss: -0.536236. Value loss: 12.056977. Entropy: 0.683827.\n",
      "Iteration 4700: Policy loss: -0.628920. Value loss: 7.345304. Entropy: 0.669233.\n",
      "Iteration 4701: Policy loss: -0.511952. Value loss: 5.534774. Entropy: 0.650208.\n",
      "episode: 2017   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 165.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4702: Policy loss: 0.435393. Value loss: 11.182166. Entropy: 0.769741.\n",
      "Iteration 4703: Policy loss: 0.105647. Value loss: 7.818980. Entropy: 0.799371.\n",
      "Iteration 4704: Policy loss: 0.145862. Value loss: 5.861372. Entropy: 0.771772.\n",
      "episode: 2018   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 163.05\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4705: Policy loss: -1.032973. Value loss: 17.744308. Entropy: 0.826810.\n",
      "Iteration 4706: Policy loss: -0.901191. Value loss: 13.023810. Entropy: 0.763187.\n",
      "Iteration 4707: Policy loss: -0.931717. Value loss: 10.885405. Entropy: 0.781219.\n",
      "episode: 2019   score: 105.0  epsilon: 1.0    steps: 242  evaluation reward: 162.9\n",
      "episode: 2020   score: 260.0  epsilon: 1.0    steps: 266  evaluation reward: 164.35\n",
      "episode: 2021   score: 155.0  epsilon: 1.0    steps: 915  evaluation reward: 163.8\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4708: Policy loss: 1.237464. Value loss: 17.037951. Entropy: 0.972818.\n",
      "Iteration 4709: Policy loss: 1.263717. Value loss: 10.686789. Entropy: 0.976246.\n",
      "Iteration 4710: Policy loss: 1.227570. Value loss: 8.559380. Entropy: 0.975080.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4711: Policy loss: 0.153739. Value loss: 11.662518. Entropy: 0.810779.\n",
      "Iteration 4712: Policy loss: 0.189770. Value loss: 8.677741. Entropy: 0.831219.\n",
      "Iteration 4713: Policy loss: 0.197818. Value loss: 7.947951. Entropy: 0.841796.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4714: Policy loss: -0.730707. Value loss: 17.121643. Entropy: 0.799138.\n",
      "Iteration 4715: Policy loss: -0.740098. Value loss: 11.611706. Entropy: 0.794153.\n",
      "Iteration 4716: Policy loss: -0.894225. Value loss: 10.176116. Entropy: 0.790494.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4717: Policy loss: -1.599742. Value loss: 18.699715. Entropy: 0.610283.\n",
      "Iteration 4718: Policy loss: -1.381239. Value loss: 7.784469. Entropy: 0.545423.\n",
      "Iteration 4719: Policy loss: -1.334175. Value loss: 6.788786. Entropy: 0.549598.\n",
      "episode: 2022   score: 180.0  epsilon: 1.0    steps: 839  evaluation reward: 164.55\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4720: Policy loss: -0.065309. Value loss: 8.005713. Entropy: 0.681083.\n",
      "Iteration 4721: Policy loss: -0.057617. Value loss: 4.530354. Entropy: 0.687978.\n",
      "Iteration 4722: Policy loss: -0.051839. Value loss: 3.307162. Entropy: 0.695380.\n",
      "episode: 2023   score: 260.0  epsilon: 1.0    steps: 681  evaluation reward: 165.35\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4723: Policy loss: -1.276367. Value loss: 12.267407. Entropy: 0.734204.\n",
      "Iteration 4724: Policy loss: -1.226315. Value loss: 8.631989. Entropy: 0.740812.\n",
      "Iteration 4725: Policy loss: -1.293964. Value loss: 6.441049. Entropy: 0.727372.\n",
      "episode: 2024   score: 180.0  epsilon: 1.0    steps: 39  evaluation reward: 166.1\n",
      "episode: 2025   score: 210.0  epsilon: 1.0    steps: 346  evaluation reward: 166.4\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4726: Policy loss: 0.112231. Value loss: 19.419764. Entropy: 0.679026.\n",
      "Iteration 4727: Policy loss: 0.259567. Value loss: 11.024214. Entropy: 0.662659.\n",
      "Iteration 4728: Policy loss: -0.060775. Value loss: 8.807287. Entropy: 0.647786.\n",
      "episode: 2026   score: 210.0  epsilon: 1.0    steps: 169  evaluation reward: 165.6\n",
      "episode: 2027   score: 105.0  epsilon: 1.0    steps: 834  evaluation reward: 163.75\n",
      "episode: 2028   score: 260.0  epsilon: 1.0    steps: 1007  evaluation reward: 163.5\n",
      "Training network. lr: 0.000214. clip: 0.085568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4729: Policy loss: 0.204398. Value loss: 12.303585. Entropy: 0.822087.\n",
      "Iteration 4730: Policy loss: 0.196295. Value loss: 8.600326. Entropy: 0.860184.\n",
      "Iteration 4731: Policy loss: 0.059270. Value loss: 6.380945. Entropy: 0.855755.\n",
      "episode: 2029   score: 330.0  epsilon: 1.0    steps: 502  evaluation reward: 165.25\n",
      "episode: 2030   score: 290.0  epsilon: 1.0    steps: 529  evaluation reward: 166.35\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4732: Policy loss: 1.755353. Value loss: 9.251736. Entropy: 0.917924.\n",
      "Iteration 4733: Policy loss: 1.631563. Value loss: 6.927368. Entropy: 0.942637.\n",
      "Iteration 4734: Policy loss: 1.572654. Value loss: 5.476974. Entropy: 0.946900.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4735: Policy loss: 0.526174. Value loss: 18.503078. Entropy: 0.900443.\n",
      "Iteration 4736: Policy loss: 0.716696. Value loss: 12.460983. Entropy: 0.913183.\n",
      "Iteration 4737: Policy loss: 0.829056. Value loss: 11.057864. Entropy: 0.922923.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4738: Policy loss: -0.125414. Value loss: 15.702807. Entropy: 0.885717.\n",
      "Iteration 4739: Policy loss: -0.035444. Value loss: 8.909374. Entropy: 0.880110.\n",
      "Iteration 4740: Policy loss: -0.139133. Value loss: 8.074965. Entropy: 0.859698.\n",
      "episode: 2031   score: 175.0  epsilon: 1.0    steps: 714  evaluation reward: 166.0\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4741: Policy loss: 1.460178. Value loss: 19.434929. Entropy: 0.689753.\n",
      "Iteration 4742: Policy loss: 1.426699. Value loss: 13.319016. Entropy: 0.739326.\n",
      "Iteration 4743: Policy loss: 1.524807. Value loss: 9.995014. Entropy: 0.745774.\n",
      "episode: 2032   score: 215.0  epsilon: 1.0    steps: 851  evaluation reward: 166.35\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4744: Policy loss: 0.700927. Value loss: 17.293011. Entropy: 0.815198.\n",
      "Iteration 4745: Policy loss: 0.722729. Value loss: 7.908981. Entropy: 0.819167.\n",
      "Iteration 4746: Policy loss: 0.786484. Value loss: 7.527089. Entropy: 0.823770.\n",
      "episode: 2033   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 165.85\n",
      "episode: 2034   score: 180.0  epsilon: 1.0    steps: 311  evaluation reward: 165.85\n",
      "episode: 2035   score: 135.0  epsilon: 1.0    steps: 544  evaluation reward: 165.1\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4747: Policy loss: 1.575633. Value loss: 14.580260. Entropy: 0.849748.\n",
      "Iteration 4748: Policy loss: 1.885707. Value loss: 6.219572. Entropy: 0.890592.\n",
      "Iteration 4749: Policy loss: 1.766085. Value loss: 5.019299. Entropy: 0.868591.\n",
      "episode: 2036   score: 155.0  epsilon: 1.0    steps: 905  evaluation reward: 166.15\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4750: Policy loss: -0.468503. Value loss: 30.001314. Entropy: 1.016975.\n",
      "Iteration 4751: Policy loss: -0.576109. Value loss: 18.658190. Entropy: 1.030900.\n",
      "Iteration 4752: Policy loss: -0.404825. Value loss: 14.641887. Entropy: 1.012061.\n",
      "episode: 2037   score: 210.0  epsilon: 1.0    steps: 435  evaluation reward: 167.2\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4753: Policy loss: 0.375414. Value loss: 15.539094. Entropy: 0.934781.\n",
      "Iteration 4754: Policy loss: 0.296753. Value loss: 8.554198. Entropy: 0.939247.\n",
      "Iteration 4755: Policy loss: 0.207742. Value loss: 8.459929. Entropy: 0.946359.\n",
      "episode: 2038   score: 205.0  epsilon: 1.0    steps: 135  evaluation reward: 168.2\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4756: Policy loss: -1.019421. Value loss: 17.360031. Entropy: 0.848995.\n",
      "Iteration 4757: Policy loss: -1.087210. Value loss: 11.952827. Entropy: 0.836143.\n",
      "Iteration 4758: Policy loss: -0.846281. Value loss: 10.507890. Entropy: 0.840065.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4759: Policy loss: 0.590220. Value loss: 12.290565. Entropy: 0.777865.\n",
      "Iteration 4760: Policy loss: 0.724963. Value loss: 6.935495. Entropy: 0.749050.\n",
      "Iteration 4761: Policy loss: 0.635192. Value loss: 5.785243. Entropy: 0.775359.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4762: Policy loss: -0.141194. Value loss: 16.983135. Entropy: 0.840741.\n",
      "Iteration 4763: Policy loss: -0.043523. Value loss: 9.256903. Entropy: 0.829368.\n",
      "Iteration 4764: Policy loss: -0.051925. Value loss: 6.745036. Entropy: 0.841558.\n",
      "episode: 2039   score: 180.0  epsilon: 1.0    steps: 11  evaluation reward: 168.95\n",
      "episode: 2040   score: 110.0  epsilon: 1.0    steps: 172  evaluation reward: 169.0\n",
      "episode: 2041   score: 210.0  epsilon: 1.0    steps: 364  evaluation reward: 170.05\n",
      "episode: 2042   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 170.8\n",
      "episode: 2043   score: 240.0  epsilon: 1.0    steps: 683  evaluation reward: 171.4\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4765: Policy loss: -0.185260. Value loss: 17.444548. Entropy: 0.755302.\n",
      "Iteration 4766: Policy loss: -0.327043. Value loss: 11.306102. Entropy: 0.756410.\n",
      "Iteration 4767: Policy loss: -0.124287. Value loss: 8.020795. Entropy: 0.753696.\n",
      "episode: 2044   score: 440.0  epsilon: 1.0    steps: 830  evaluation reward: 174.7\n",
      "episode: 2045   score: 210.0  epsilon: 1.0    steps: 969  evaluation reward: 175.0\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4768: Policy loss: -2.326010. Value loss: 214.185486. Entropy: 0.848217.\n",
      "Iteration 4769: Policy loss: -2.508065. Value loss: 114.949486. Entropy: 0.804761.\n",
      "Iteration 4770: Policy loss: -2.184966. Value loss: 44.533661. Entropy: 0.787254.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4771: Policy loss: -0.078452. Value loss: 21.467030. Entropy: 0.770314.\n",
      "Iteration 4772: Policy loss: 0.034128. Value loss: 14.508865. Entropy: 0.766070.\n",
      "Iteration 4773: Policy loss: -0.155431. Value loss: 13.449162. Entropy: 0.753488.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4774: Policy loss: -3.178109. Value loss: 37.868458. Entropy: 0.555144.\n",
      "Iteration 4775: Policy loss: -2.873929. Value loss: 24.795383. Entropy: 0.564970.\n",
      "Iteration 4776: Policy loss: -3.261878. Value loss: 20.862465. Entropy: 0.537405.\n",
      "episode: 2046   score: 315.0  epsilon: 1.0    steps: 446  evaluation reward: 177.05\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4777: Policy loss: 1.488210. Value loss: 10.651833. Entropy: 0.485215.\n",
      "Iteration 4778: Policy loss: 1.484213. Value loss: 8.898886. Entropy: 0.469133.\n",
      "Iteration 4779: Policy loss: 1.643920. Value loss: 7.816504. Entropy: 0.485144.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4780: Policy loss: -2.065514. Value loss: 28.842213. Entropy: 0.530703.\n",
      "Iteration 4781: Policy loss: -1.882800. Value loss: 17.088699. Entropy: 0.514086.\n",
      "Iteration 4782: Policy loss: -1.695672. Value loss: 13.960716. Entropy: 0.502125.\n",
      "episode: 2047   score: 255.0  epsilon: 1.0    steps: 46  evaluation reward: 177.5\n",
      "episode: 2048   score: 210.0  epsilon: 1.0    steps: 235  evaluation reward: 178.55\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4783: Policy loss: 0.054891. Value loss: 18.873478. Entropy: 0.598901.\n",
      "Iteration 4784: Policy loss: 0.087775. Value loss: 8.384104. Entropy: 0.604276.\n",
      "Iteration 4785: Policy loss: 0.160622. Value loss: 6.492832. Entropy: 0.597986.\n",
      "episode: 2049   score: 210.0  epsilon: 1.0    steps: 879  evaluation reward: 179.5\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4786: Policy loss: -0.673468. Value loss: 270.056244. Entropy: 0.626095.\n",
      "Iteration 4787: Policy loss: -0.986913. Value loss: 201.054718. Entropy: 0.627169.\n",
      "Iteration 4788: Policy loss: -0.775575. Value loss: 211.338257. Entropy: 0.608080.\n",
      "episode: 2050   score: 345.0  epsilon: 1.0    steps: 627  evaluation reward: 181.9\n",
      "now time :  2019-02-25 20:09:28.538045\n",
      "episode: 2051   score: 260.0  epsilon: 1.0    steps: 684  evaluation reward: 183.45\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4789: Policy loss: 0.708218. Value loss: 15.544798. Entropy: 0.519898.\n",
      "Iteration 4790: Policy loss: 0.536387. Value loss: 8.872721. Entropy: 0.555952.\n",
      "Iteration 4791: Policy loss: 0.732917. Value loss: 6.678185. Entropy: 0.555435.\n",
      "episode: 2052   score: 105.0  epsilon: 1.0    steps: 99  evaluation reward: 181.5\n",
      "episode: 2053   score: 210.0  epsilon: 1.0    steps: 470  evaluation reward: 182.4\n",
      "episode: 2054   score: 485.0  epsilon: 1.0    steps: 933  evaluation reward: 186.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4792: Policy loss: -0.379388. Value loss: 23.531101. Entropy: 0.548043.\n",
      "Iteration 4793: Policy loss: -0.161918. Value loss: 13.985315. Entropy: 0.530107.\n",
      "Iteration 4794: Policy loss: -0.068874. Value loss: 11.600697. Entropy: 0.545696.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4795: Policy loss: -2.267464. Value loss: 152.673096. Entropy: 0.608110.\n",
      "Iteration 4796: Policy loss: -3.455958. Value loss: 170.279495. Entropy: 0.468130.\n",
      "Iteration 4797: Policy loss: -2.991239. Value loss: 61.214588. Entropy: 0.483715.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4798: Policy loss: -3.270819. Value loss: 41.839252. Entropy: 0.273092.\n",
      "Iteration 4799: Policy loss: -3.212539. Value loss: 23.738533. Entropy: 0.256446.\n",
      "Iteration 4800: Policy loss: -3.214823. Value loss: 21.145052. Entropy: 0.267439.\n",
      "episode: 2055   score: 210.0  epsilon: 1.0    steps: 156  evaluation reward: 187.2\n",
      "episode: 2056   score: 565.0  epsilon: 1.0    steps: 317  evaluation reward: 191.5\n",
      "episode: 2057   score: 150.0  epsilon: 1.0    steps: 828  evaluation reward: 191.95\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4801: Policy loss: 2.381366. Value loss: 28.581680. Entropy: 0.367427.\n",
      "Iteration 4802: Policy loss: 2.382988. Value loss: 16.616404. Entropy: 0.416223.\n",
      "Iteration 4803: Policy loss: 2.300712. Value loss: 13.437790. Entropy: 0.419105.\n",
      "episode: 2058   score: 135.0  epsilon: 1.0    steps: 986  evaluation reward: 191.05\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4804: Policy loss: 0.761075. Value loss: 20.367718. Entropy: 0.336145.\n",
      "Iteration 4805: Policy loss: 0.591356. Value loss: 12.343921. Entropy: 0.340367.\n",
      "Iteration 4806: Policy loss: 0.831786. Value loss: 10.953125. Entropy: 0.341448.\n",
      "episode: 2059   score: 210.0  epsilon: 1.0    steps: 73  evaluation reward: 192.55\n",
      "episode: 2060   score: 225.0  epsilon: 1.0    steps: 588  evaluation reward: 193.8\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4807: Policy loss: 0.047536. Value loss: 14.231056. Entropy: 0.560774.\n",
      "Iteration 4808: Policy loss: 0.154788. Value loss: 10.569024. Entropy: 0.524981.\n",
      "Iteration 4809: Policy loss: 0.002244. Value loss: 7.993288. Entropy: 0.539816.\n",
      "episode: 2061   score: 105.0  epsilon: 1.0    steps: 169  evaluation reward: 193.05\n",
      "episode: 2062   score: 105.0  epsilon: 1.0    steps: 859  evaluation reward: 193.05\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4810: Policy loss: 1.209507. Value loss: 12.034022. Entropy: 0.509837.\n",
      "Iteration 4811: Policy loss: 1.047662. Value loss: 6.861928. Entropy: 0.545056.\n",
      "Iteration 4812: Policy loss: 1.127552. Value loss: 5.825979. Entropy: 0.510490.\n",
      "episode: 2063   score: 210.0  epsilon: 1.0    steps: 410  evaluation reward: 193.8\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4813: Policy loss: 0.383144. Value loss: 15.646259. Entropy: 0.596567.\n",
      "Iteration 4814: Policy loss: 0.481909. Value loss: 11.508723. Entropy: 0.591495.\n",
      "Iteration 4815: Policy loss: 0.401726. Value loss: 8.814734. Entropy: 0.602163.\n",
      "episode: 2064   score: 105.0  epsilon: 1.0    steps: 90  evaluation reward: 192.75\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4816: Policy loss: 0.788026. Value loss: 15.466604. Entropy: 0.385157.\n",
      "Iteration 4817: Policy loss: 0.778131. Value loss: 8.238205. Entropy: 0.387828.\n",
      "Iteration 4818: Policy loss: 0.830858. Value loss: 7.553064. Entropy: 0.387288.\n",
      "episode: 2065   score: 75.0  epsilon: 1.0    steps: 184  evaluation reward: 192.45\n",
      "episode: 2066   score: 285.0  epsilon: 1.0    steps: 744  evaluation reward: 194.2\n",
      "episode: 2067   score: 180.0  epsilon: 1.0    steps: 974  evaluation reward: 194.95\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4819: Policy loss: -0.174270. Value loss: 24.149246. Entropy: 0.428988.\n",
      "Iteration 4820: Policy loss: -0.336550. Value loss: 13.453685. Entropy: 0.400697.\n",
      "Iteration 4821: Policy loss: -0.323300. Value loss: 12.622108. Entropy: 0.423557.\n",
      "episode: 2068   score: 210.0  epsilon: 1.0    steps: 625  evaluation reward: 196.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4822: Policy loss: -3.369184. Value loss: 206.737152. Entropy: 0.598603.\n",
      "Iteration 4823: Policy loss: -3.778174. Value loss: 112.719337. Entropy: 0.545953.\n",
      "Iteration 4824: Policy loss: -2.803522. Value loss: 33.954174. Entropy: 0.509756.\n",
      "episode: 2069   score: 505.0  epsilon: 1.0    steps: 377  evaluation reward: 200.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4825: Policy loss: 0.749390. Value loss: 17.539875. Entropy: 0.507568.\n",
      "Iteration 4826: Policy loss: 0.642234. Value loss: 9.880091. Entropy: 0.531639.\n",
      "Iteration 4827: Policy loss: 0.613556. Value loss: 8.447386. Entropy: 0.525823.\n",
      "episode: 2070   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 201.35\n",
      "episode: 2071   score: 160.0  epsilon: 1.0    steps: 987  evaluation reward: 201.9\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4828: Policy loss: 1.852901. Value loss: 16.512808. Entropy: 0.519466.\n",
      "Iteration 4829: Policy loss: 1.670246. Value loss: 11.850633. Entropy: 0.495866.\n",
      "Iteration 4830: Policy loss: 1.701709. Value loss: 9.625760. Entropy: 0.496628.\n",
      "episode: 2072   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 202.9\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4831: Policy loss: 1.510449. Value loss: 34.097599. Entropy: 0.425434.\n",
      "Iteration 4832: Policy loss: 1.426446. Value loss: 20.637064. Entropy: 0.431256.\n",
      "Iteration 4833: Policy loss: 1.721972. Value loss: 16.949314. Entropy: 0.457168.\n",
      "episode: 2073   score: 180.0  epsilon: 1.0    steps: 146  evaluation reward: 203.95\n",
      "episode: 2074   score: 125.0  epsilon: 1.0    steps: 661  evaluation reward: 203.55\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4834: Policy loss: 0.845816. Value loss: 24.731455. Entropy: 0.541982.\n",
      "Iteration 4835: Policy loss: 0.794043. Value loss: 15.544348. Entropy: 0.546182.\n",
      "Iteration 4836: Policy loss: 0.728220. Value loss: 12.223075. Entropy: 0.549630.\n",
      "episode: 2075   score: 210.0  epsilon: 1.0    steps: 11  evaluation reward: 203.8\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4837: Policy loss: -1.918404. Value loss: 45.460789. Entropy: 0.402299.\n",
      "Iteration 4838: Policy loss: -1.969856. Value loss: 24.401381. Entropy: 0.419109.\n",
      "Iteration 4839: Policy loss: -1.677700. Value loss: 21.302664. Entropy: 0.403922.\n",
      "episode: 2076   score: 185.0  epsilon: 1.0    steps: 280  evaluation reward: 204.9\n",
      "episode: 2077   score: 185.0  epsilon: 1.0    steps: 405  evaluation reward: 205.7\n",
      "episode: 2078   score: 115.0  epsilon: 1.0    steps: 809  evaluation reward: 205.5\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4840: Policy loss: -0.553488. Value loss: 16.521618. Entropy: 0.457945.\n",
      "Iteration 4841: Policy loss: -0.741069. Value loss: 9.011504. Entropy: 0.473256.\n",
      "Iteration 4842: Policy loss: -0.729649. Value loss: 9.250260. Entropy: 0.460735.\n",
      "episode: 2079   score: 230.0  epsilon: 1.0    steps: 536  evaluation reward: 205.85\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4843: Policy loss: -5.108641. Value loss: 263.931030. Entropy: 0.331382.\n",
      "Iteration 4844: Policy loss: -5.001669. Value loss: 114.735771. Entropy: 0.282472.\n",
      "Iteration 4845: Policy loss: -5.019297. Value loss: 90.173027. Entropy: 0.236089.\n",
      "episode: 2080   score: 175.0  epsilon: 1.0    steps: 646  evaluation reward: 206.6\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4846: Policy loss: -4.211713. Value loss: 147.580368. Entropy: 0.367565.\n",
      "Iteration 4847: Policy loss: -3.886597. Value loss: 51.806759. Entropy: 0.355426.\n",
      "Iteration 4848: Policy loss: -4.346001. Value loss: 33.354576. Entropy: 0.364517.\n",
      "episode: 2081   score: 105.0  epsilon: 1.0    steps: 337  evaluation reward: 206.0\n",
      "episode: 2082   score: 485.0  epsilon: 1.0    steps: 920  evaluation reward: 210.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4849: Policy loss: 2.348763. Value loss: 42.687283. Entropy: 0.183624.\n",
      "Iteration 4850: Policy loss: 2.315345. Value loss: 30.157177. Entropy: 0.188206.\n",
      "Iteration 4851: Policy loss: 2.294888. Value loss: 18.014254. Entropy: 0.211166.\n",
      "episode: 2083   score: 180.0  epsilon: 1.0    steps: 436  evaluation reward: 210.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2084   score: 210.0  epsilon: 1.0    steps: 826  evaluation reward: 210.55\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4852: Policy loss: 1.130760. Value loss: 21.289869. Entropy: 0.402858.\n",
      "Iteration 4853: Policy loss: 0.940269. Value loss: 14.087255. Entropy: 0.410427.\n",
      "Iteration 4854: Policy loss: 0.961047. Value loss: 9.513206. Entropy: 0.416219.\n",
      "episode: 2085   score: 90.0  epsilon: 1.0    steps: 714  evaluation reward: 208.15\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4855: Policy loss: 0.530545. Value loss: 16.433496. Entropy: 0.426495.\n",
      "Iteration 4856: Policy loss: 0.678122. Value loss: 9.582279. Entropy: 0.453454.\n",
      "Iteration 4857: Policy loss: 0.546086. Value loss: 7.638901. Entropy: 0.476036.\n",
      "episode: 2086   score: 210.0  epsilon: 1.0    steps: 34  evaluation reward: 206.5\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4858: Policy loss: -0.276422. Value loss: 46.584393. Entropy: 0.464549.\n",
      "Iteration 4859: Policy loss: 0.183858. Value loss: 22.644054. Entropy: 0.453660.\n",
      "Iteration 4860: Policy loss: -0.343237. Value loss: 19.471884. Entropy: 0.455179.\n",
      "episode: 2087   score: 485.0  epsilon: 1.0    steps: 162  evaluation reward: 209.55\n",
      "episode: 2088   score: 125.0  epsilon: 1.0    steps: 269  evaluation reward: 209.0\n",
      "episode: 2089   score: 240.0  epsilon: 1.0    steps: 529  evaluation reward: 209.6\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4861: Policy loss: 0.202794. Value loss: 15.262020. Entropy: 0.357838.\n",
      "Iteration 4862: Policy loss: 0.307387. Value loss: 7.544063. Entropy: 0.356732.\n",
      "Iteration 4863: Policy loss: 0.267548. Value loss: 6.187583. Entropy: 0.348843.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4864: Policy loss: -0.365876. Value loss: 26.316921. Entropy: 0.307320.\n",
      "Iteration 4865: Policy loss: -0.297707. Value loss: 16.290865. Entropy: 0.293926.\n",
      "Iteration 4866: Policy loss: -0.195026. Value loss: 13.295083. Entropy: 0.329859.\n",
      "episode: 2090   score: 180.0  epsilon: 1.0    steps: 774  evaluation reward: 209.9\n",
      "episode: 2091   score: 230.0  epsilon: 1.0    steps: 997  evaluation reward: 209.8\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4867: Policy loss: -4.459023. Value loss: 161.638229. Entropy: 0.300067.\n",
      "Iteration 4868: Policy loss: -4.556329. Value loss: 39.068020. Entropy: 0.261059.\n",
      "Iteration 4869: Policy loss: -4.808622. Value loss: 79.643158. Entropy: 0.300022.\n",
      "episode: 2092   score: 210.0  epsilon: 1.0    steps: 121  evaluation reward: 210.85\n",
      "episode: 2093   score: 430.0  epsilon: 1.0    steps: 463  evaluation reward: 211.5\n",
      "episode: 2094   score: 210.0  epsilon: 1.0    steps: 695  evaluation reward: 211.5\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4870: Policy loss: 0.286709. Value loss: 19.762703. Entropy: 0.233028.\n",
      "Iteration 4871: Policy loss: 0.474127. Value loss: 14.705132. Entropy: 0.216353.\n",
      "Iteration 4872: Policy loss: 0.342103. Value loss: 12.919048. Entropy: 0.224090.\n",
      "episode: 2095   score: 180.0  epsilon: 1.0    steps: 347  evaluation reward: 211.2\n",
      "episode: 2096   score: 160.0  epsilon: 1.0    steps: 636  evaluation reward: 209.65\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4873: Policy loss: 0.106241. Value loss: 15.449153. Entropy: 0.339466.\n",
      "Iteration 4874: Policy loss: 0.068804. Value loss: 10.279149. Entropy: 0.321257.\n",
      "Iteration 4875: Policy loss: -0.021835. Value loss: 10.083272. Entropy: 0.327232.\n",
      "episode: 2097   score: 210.0  epsilon: 1.0    steps: 142  evaluation reward: 209.35\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4876: Policy loss: 0.003616. Value loss: 11.301605. Entropy: 0.355971.\n",
      "Iteration 4877: Policy loss: 0.152233. Value loss: 9.304214. Entropy: 0.349613.\n",
      "Iteration 4878: Policy loss: -0.014603. Value loss: 8.998108. Entropy: 0.372501.\n",
      "episode: 2098   score: 125.0  epsilon: 1.0    steps: 1024  evaluation reward: 209.25\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4879: Policy loss: 1.056697. Value loss: 17.627909. Entropy: 0.271943.\n",
      "Iteration 4880: Policy loss: 1.329341. Value loss: 11.574182. Entropy: 0.284164.\n",
      "Iteration 4881: Policy loss: 1.429398. Value loss: 8.748088. Entropy: 0.284480.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4882: Policy loss: 0.197482. Value loss: 19.605658. Entropy: 0.310307.\n",
      "Iteration 4883: Policy loss: 0.256417. Value loss: 14.113175. Entropy: 0.322510.\n",
      "Iteration 4884: Policy loss: 0.268014. Value loss: 10.885283. Entropy: 0.317568.\n",
      "episode: 2099   score: 155.0  epsilon: 1.0    steps: 426  evaluation reward: 210.0\n",
      "episode: 2100   score: 75.0  epsilon: 1.0    steps: 536  evaluation reward: 208.3\n",
      "now time :  2019-02-25 20:11:16.193972\n",
      "episode: 2101   score: 155.0  epsilon: 1.0    steps: 643  evaluation reward: 209.1\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4885: Policy loss: 0.173009. Value loss: 20.907177. Entropy: 0.278026.\n",
      "Iteration 4886: Policy loss: 0.362730. Value loss: 10.335556. Entropy: 0.269431.\n",
      "Iteration 4887: Policy loss: 0.355320. Value loss: 8.258624. Entropy: 0.262781.\n",
      "episode: 2102   score: 155.0  epsilon: 1.0    steps: 308  evaluation reward: 208.85\n",
      "episode: 2103   score: 285.0  epsilon: 1.0    steps: 880  evaluation reward: 208.55\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4888: Policy loss: 1.050864. Value loss: 27.971079. Entropy: 0.286969.\n",
      "Iteration 4889: Policy loss: 0.820582. Value loss: 20.342356. Entropy: 0.236311.\n",
      "Iteration 4890: Policy loss: 1.020591. Value loss: 13.709405. Entropy: 0.262798.\n",
      "episode: 2104   score: 210.0  epsilon: 1.0    steps: 43  evaluation reward: 208.05\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4891: Policy loss: -0.885835. Value loss: 25.697643. Entropy: 0.341266.\n",
      "Iteration 4892: Policy loss: -0.672116. Value loss: 12.950934. Entropy: 0.347398.\n",
      "Iteration 4893: Policy loss: -0.798977. Value loss: 10.399499. Entropy: 0.344957.\n",
      "episode: 2105   score: 265.0  epsilon: 1.0    steps: 146  evaluation reward: 207.6\n",
      "episode: 2106   score: 150.0  epsilon: 1.0    steps: 935  evaluation reward: 208.05\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4894: Policy loss: 0.850440. Value loss: 24.969601. Entropy: 0.245241.\n",
      "Iteration 4895: Policy loss: 0.658342. Value loss: 11.573341. Entropy: 0.228198.\n",
      "Iteration 4896: Policy loss: 0.963603. Value loss: 9.940244. Entropy: 0.252713.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4897: Policy loss: 0.762795. Value loss: 26.338699. Entropy: 0.238352.\n",
      "Iteration 4898: Policy loss: 0.987438. Value loss: 16.948919. Entropy: 0.227022.\n",
      "Iteration 4899: Policy loss: 0.425337. Value loss: 17.630939. Entropy: 0.234970.\n",
      "episode: 2107   score: 180.0  epsilon: 1.0    steps: 339  evaluation reward: 208.3\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4900: Policy loss: 1.647176. Value loss: 20.589066. Entropy: 0.414314.\n",
      "Iteration 4901: Policy loss: 1.393762. Value loss: 13.316736. Entropy: 0.425672.\n",
      "Iteration 4902: Policy loss: 1.501195. Value loss: 10.676060. Entropy: 0.483806.\n",
      "episode: 2108   score: 180.0  epsilon: 1.0    steps: 684  evaluation reward: 208.75\n",
      "episode: 2109   score: 125.0  epsilon: 1.0    steps: 779  evaluation reward: 208.15\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4903: Policy loss: 0.193477. Value loss: 17.737041. Entropy: 0.342333.\n",
      "Iteration 4904: Policy loss: 0.401353. Value loss: 10.043174. Entropy: 0.335424.\n",
      "Iteration 4905: Policy loss: 0.276408. Value loss: 7.832066. Entropy: 0.335834.\n",
      "episode: 2110   score: 180.0  epsilon: 1.0    steps: 1007  evaluation reward: 207.85\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4906: Policy loss: -1.342090. Value loss: 27.008430. Entropy: 0.186420.\n",
      "Iteration 4907: Policy loss: -1.526030. Value loss: 14.498977. Entropy: 0.147879.\n",
      "Iteration 4908: Policy loss: -1.212096. Value loss: 13.719717. Entropy: 0.164838.\n",
      "episode: 2111   score: 285.0  epsilon: 1.0    steps: 430  evaluation reward: 209.15\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4909: Policy loss: 0.213861. Value loss: 23.087135. Entropy: 0.450911.\n",
      "Iteration 4910: Policy loss: 0.137418. Value loss: 11.023472. Entropy: 0.435769.\n",
      "Iteration 4911: Policy loss: -0.030925. Value loss: 8.058761. Entropy: 0.455764.\n",
      "episode: 2112   score: 260.0  epsilon: 1.0    steps: 173  evaluation reward: 210.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2113   score: 300.0  epsilon: 1.0    steps: 555  evaluation reward: 211.7\n",
      "episode: 2114   score: 80.0  epsilon: 1.0    steps: 722  evaluation reward: 211.15\n",
      "episode: 2115   score: 150.0  epsilon: 1.0    steps: 885  evaluation reward: 211.1\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4912: Policy loss: 2.357553. Value loss: 27.007843. Entropy: 0.475875.\n",
      "Iteration 4913: Policy loss: 2.590549. Value loss: 12.292084. Entropy: 0.495084.\n",
      "Iteration 4914: Policy loss: 2.466601. Value loss: 10.702726. Entropy: 0.457344.\n",
      "episode: 2116   score: 240.0  epsilon: 1.0    steps: 7  evaluation reward: 211.1\n",
      "episode: 2117   score: 155.0  epsilon: 1.0    steps: 332  evaluation reward: 210.85\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4915: Policy loss: -1.361840. Value loss: 27.489159. Entropy: 0.428254.\n",
      "Iteration 4916: Policy loss: -1.526537. Value loss: 16.371834. Entropy: 0.405992.\n",
      "Iteration 4917: Policy loss: -1.599912. Value loss: 13.674053. Entropy: 0.422802.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4918: Policy loss: 1.507861. Value loss: 20.618538. Entropy: 0.356800.\n",
      "Iteration 4919: Policy loss: 1.627502. Value loss: 14.644525. Entropy: 0.358585.\n",
      "Iteration 4920: Policy loss: 1.541081. Value loss: 13.031269. Entropy: 0.363801.\n",
      "episode: 2118   score: 125.0  epsilon: 1.0    steps: 903  evaluation reward: 210.3\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4921: Policy loss: -0.160227. Value loss: 18.799635. Entropy: 0.375915.\n",
      "Iteration 4922: Policy loss: -0.053385. Value loss: 9.058381. Entropy: 0.360866.\n",
      "Iteration 4923: Policy loss: -0.283885. Value loss: 7.856205. Entropy: 0.358321.\n",
      "episode: 2119   score: 135.0  epsilon: 1.0    steps: 346  evaluation reward: 210.6\n",
      "episode: 2120   score: 125.0  epsilon: 1.0    steps: 743  evaluation reward: 209.25\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4924: Policy loss: -0.321381. Value loss: 17.480022. Entropy: 0.266993.\n",
      "Iteration 4925: Policy loss: -0.109186. Value loss: 8.594333. Entropy: 0.262719.\n",
      "Iteration 4926: Policy loss: -0.448776. Value loss: 6.209876. Entropy: 0.272457.\n",
      "episode: 2121   score: 105.0  epsilon: 1.0    steps: 217  evaluation reward: 208.75\n",
      "episode: 2122   score: 135.0  epsilon: 1.0    steps: 604  evaluation reward: 208.3\n",
      "episode: 2123   score: 210.0  epsilon: 1.0    steps: 893  evaluation reward: 207.8\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4927: Policy loss: 0.457662. Value loss: 42.430359. Entropy: 0.340053.\n",
      "Iteration 4928: Policy loss: 1.033260. Value loss: 25.240700. Entropy: 0.353337.\n",
      "Iteration 4929: Policy loss: 0.628129. Value loss: 19.920349. Entropy: 0.362023.\n",
      "episode: 2124   score: 180.0  epsilon: 1.0    steps: 18  evaluation reward: 207.8\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4930: Policy loss: 1.453239. Value loss: 19.608030. Entropy: 0.595995.\n",
      "Iteration 4931: Policy loss: 1.512148. Value loss: 12.051702. Entropy: 0.583557.\n",
      "Iteration 4932: Policy loss: 1.347168. Value loss: 9.922117. Entropy: 0.578044.\n",
      "episode: 2125   score: 100.0  epsilon: 1.0    steps: 315  evaluation reward: 206.7\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4933: Policy loss: -1.133493. Value loss: 35.283428. Entropy: 0.475241.\n",
      "Iteration 4934: Policy loss: -1.124475. Value loss: 20.086826. Entropy: 0.453723.\n",
      "Iteration 4935: Policy loss: -1.189572. Value loss: 16.078987. Entropy: 0.486717.\n",
      "episode: 2126   score: 210.0  epsilon: 1.0    steps: 901  evaluation reward: 206.7\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4936: Policy loss: 0.131255. Value loss: 30.457754. Entropy: 0.487224.\n",
      "Iteration 4937: Policy loss: 0.368541. Value loss: 17.609606. Entropy: 0.481948.\n",
      "Iteration 4938: Policy loss: 0.089480. Value loss: 16.585793. Entropy: 0.499858.\n",
      "episode: 2127   score: 120.0  epsilon: 1.0    steps: 35  evaluation reward: 206.85\n",
      "episode: 2128   score: 180.0  epsilon: 1.0    steps: 171  evaluation reward: 206.05\n",
      "episode: 2129   score: 425.0  epsilon: 1.0    steps: 448  evaluation reward: 207.0\n",
      "episode: 2130   score: 170.0  epsilon: 1.0    steps: 590  evaluation reward: 205.8\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4939: Policy loss: -2.354725. Value loss: 38.664406. Entropy: 0.417876.\n",
      "Iteration 4940: Policy loss: -2.195495. Value loss: 19.627733. Entropy: 0.421663.\n",
      "Iteration 4941: Policy loss: -2.059575. Value loss: 13.196512. Entropy: 0.414586.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4942: Policy loss: -2.160007. Value loss: 37.673672. Entropy: 0.494895.\n",
      "Iteration 4943: Policy loss: -2.219198. Value loss: 27.226704. Entropy: 0.495260.\n",
      "Iteration 4944: Policy loss: -2.069816. Value loss: 20.810574. Entropy: 0.493392.\n",
      "episode: 2131   score: 285.0  epsilon: 1.0    steps: 729  evaluation reward: 206.9\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4945: Policy loss: -0.830332. Value loss: 30.602615. Entropy: 0.237473.\n",
      "Iteration 4946: Policy loss: -1.042976. Value loss: 19.183151. Entropy: 0.264717.\n",
      "Iteration 4947: Policy loss: -0.985702. Value loss: 14.518905. Entropy: 0.251698.\n",
      "episode: 2132   score: 110.0  epsilon: 1.0    steps: 423  evaluation reward: 205.85\n",
      "episode: 2133   score: 290.0  epsilon: 1.0    steps: 865  evaluation reward: 206.65\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4948: Policy loss: -3.513153. Value loss: 190.200119. Entropy: 0.394954.\n",
      "Iteration 4949: Policy loss: -2.602075. Value loss: 126.335136. Entropy: 0.397045.\n",
      "Iteration 4950: Policy loss: -3.393892. Value loss: 89.302788. Entropy: 0.326860.\n",
      "episode: 2134   score: 260.0  epsilon: 1.0    steps: 338  evaluation reward: 207.45\n",
      "episode: 2135   score: 210.0  epsilon: 1.0    steps: 907  evaluation reward: 208.2\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4951: Policy loss: 1.192917. Value loss: 64.529694. Entropy: 0.502346.\n",
      "Iteration 4952: Policy loss: 1.073359. Value loss: 35.685184. Entropy: 0.497514.\n",
      "Iteration 4953: Policy loss: 1.029076. Value loss: 31.323521. Entropy: 0.498777.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4954: Policy loss: 3.103552. Value loss: 47.106476. Entropy: 0.423019.\n",
      "Iteration 4955: Policy loss: 3.175619. Value loss: 30.394278. Entropy: 0.401083.\n",
      "Iteration 4956: Policy loss: 3.246604. Value loss: 25.236179. Entropy: 0.408898.\n",
      "episode: 2136   score: 85.0  epsilon: 1.0    steps: 440  evaluation reward: 207.5\n",
      "episode: 2137   score: 65.0  epsilon: 1.0    steps: 856  evaluation reward: 206.05\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4957: Policy loss: 2.419685. Value loss: 46.139736. Entropy: 0.380153.\n",
      "Iteration 4958: Policy loss: 2.087669. Value loss: 30.350754. Entropy: 0.379249.\n",
      "Iteration 4959: Policy loss: 2.772730. Value loss: 22.703445. Entropy: 0.382904.\n",
      "episode: 2138   score: 270.0  epsilon: 1.0    steps: 123  evaluation reward: 206.7\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4960: Policy loss: 2.242294. Value loss: 24.186760. Entropy: 0.366751.\n",
      "Iteration 4961: Policy loss: 2.651165. Value loss: 12.172156. Entropy: 0.367947.\n",
      "Iteration 4962: Policy loss: 2.422534. Value loss: 10.723281. Entropy: 0.377821.\n",
      "episode: 2139   score: 575.0  epsilon: 1.0    steps: 251  evaluation reward: 210.65\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4963: Policy loss: -0.600658. Value loss: 43.080765. Entropy: 0.349655.\n",
      "Iteration 4964: Policy loss: -0.767836. Value loss: 22.902855. Entropy: 0.352131.\n",
      "Iteration 4965: Policy loss: -0.645207. Value loss: 17.759615. Entropy: 0.355085.\n",
      "episode: 2140   score: 155.0  epsilon: 1.0    steps: 277  evaluation reward: 211.1\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4966: Policy loss: 2.564135. Value loss: 22.975201. Entropy: 0.514612.\n",
      "Iteration 4967: Policy loss: 2.641037. Value loss: 10.317841. Entropy: 0.518121.\n",
      "Iteration 4968: Policy loss: 2.466947. Value loss: 8.051456. Entropy: 0.533850.\n",
      "episode: 2141   score: 435.0  epsilon: 1.0    steps: 562  evaluation reward: 213.35\n",
      "episode: 2142   score: 290.0  epsilon: 1.0    steps: 644  evaluation reward: 214.45\n",
      "episode: 2143   score: 95.0  epsilon: 1.0    steps: 857  evaluation reward: 213.0\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4969: Policy loss: 0.254077. Value loss: 32.514725. Entropy: 0.542291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4970: Policy loss: 0.371532. Value loss: 17.897856. Entropy: 0.549663.\n",
      "Iteration 4971: Policy loss: 0.096173. Value loss: 15.612453. Entropy: 0.549198.\n",
      "episode: 2144   score: 105.0  epsilon: 1.0    steps: 57  evaluation reward: 209.65\n",
      "episode: 2145   score: 105.0  epsilon: 1.0    steps: 433  evaluation reward: 208.6\n",
      "episode: 2146   score: 270.0  epsilon: 1.0    steps: 972  evaluation reward: 208.15\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4972: Policy loss: -0.527288. Value loss: 31.924313. Entropy: 0.609651.\n",
      "Iteration 4973: Policy loss: -0.299621. Value loss: 19.484409. Entropy: 0.609545.\n",
      "Iteration 4974: Policy loss: -0.356584. Value loss: 14.803093. Entropy: 0.615183.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4975: Policy loss: 0.283953. Value loss: 29.796848. Entropy: 0.500898.\n",
      "Iteration 4976: Policy loss: 0.212652. Value loss: 20.371794. Entropy: 0.528283.\n",
      "Iteration 4977: Policy loss: 0.325755. Value loss: 15.417240. Entropy: 0.509622.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4978: Policy loss: 0.206546. Value loss: 30.459160. Entropy: 0.301034.\n",
      "Iteration 4979: Policy loss: 0.349242. Value loss: 18.147652. Entropy: 0.235175.\n",
      "Iteration 4980: Policy loss: 0.235477. Value loss: 14.901208. Entropy: 0.261744.\n",
      "episode: 2147   score: 195.0  epsilon: 1.0    steps: 345  evaluation reward: 207.55\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4981: Policy loss: -0.025150. Value loss: 30.655243. Entropy: 0.126214.\n",
      "Iteration 4982: Policy loss: 0.037666. Value loss: 16.412756. Entropy: 0.124531.\n",
      "Iteration 4983: Policy loss: -0.116011. Value loss: 13.403197. Entropy: 0.128001.\n",
      "episode: 2148   score: 185.0  epsilon: 1.0    steps: 884  evaluation reward: 207.3\n",
      "episode: 2149   score: 110.0  epsilon: 1.0    steps: 968  evaluation reward: 206.3\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4984: Policy loss: 0.273630. Value loss: 25.737093. Entropy: 0.236809.\n",
      "Iteration 4985: Policy loss: 0.527488. Value loss: 11.417760. Entropy: 0.254241.\n",
      "Iteration 4986: Policy loss: 0.221606. Value loss: 9.813883. Entropy: 0.271712.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4987: Policy loss: -2.770230. Value loss: 283.232208. Entropy: 0.500232.\n",
      "Iteration 4988: Policy loss: -2.217825. Value loss: 116.708839. Entropy: 0.455040.\n",
      "Iteration 4989: Policy loss: -3.048945. Value loss: 175.915359. Entropy: 0.436572.\n",
      "episode: 2150   score: 180.0  epsilon: 1.0    steps: 398  evaluation reward: 204.65\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4990: Policy loss: -0.742388. Value loss: 209.543045. Entropy: 0.398455.\n",
      "Iteration 4991: Policy loss: -1.913685. Value loss: 143.917847. Entropy: 0.342139.\n",
      "Iteration 4992: Policy loss: -0.892398. Value loss: 112.340355. Entropy: 0.324666.\n",
      "now time :  2019-02-25 20:13:16.215110\n",
      "episode: 2151   score: 270.0  epsilon: 1.0    steps: 120  evaluation reward: 204.75\n",
      "episode: 2152   score: 500.0  epsilon: 1.0    steps: 712  evaluation reward: 208.7\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4993: Policy loss: 1.897249. Value loss: 52.564713. Entropy: 0.228088.\n",
      "Iteration 4994: Policy loss: 2.084973. Value loss: 24.378639. Entropy: 0.274799.\n",
      "Iteration 4995: Policy loss: 1.774558. Value loss: 21.456438. Entropy: 0.267288.\n",
      "episode: 2153   score: 275.0  epsilon: 1.0    steps: 634  evaluation reward: 209.35\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4996: Policy loss: 0.422611. Value loss: 20.275717. Entropy: 0.260580.\n",
      "Iteration 4997: Policy loss: 0.311353. Value loss: 13.845355. Entropy: 0.261005.\n",
      "Iteration 4998: Policy loss: 0.258173. Value loss: 12.198780. Entropy: 0.245886.\n",
      "episode: 2154   score: 465.0  epsilon: 1.0    steps: 136  evaluation reward: 209.15\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4999: Policy loss: -1.020753. Value loss: 20.207796. Entropy: 0.328793.\n",
      "Iteration 5000: Policy loss: -0.889964. Value loss: 13.270538. Entropy: 0.313488.\n",
      "Iteration 5001: Policy loss: -0.949494. Value loss: 9.159850. Entropy: 0.332712.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5002: Policy loss: 1.601851. Value loss: 36.296066. Entropy: 0.123011.\n",
      "Iteration 5003: Policy loss: 1.077150. Value loss: 23.803541. Entropy: 0.097233.\n",
      "Iteration 5004: Policy loss: 0.974868. Value loss: 18.683455. Entropy: 0.094019.\n",
      "episode: 2155   score: 485.0  epsilon: 1.0    steps: 334  evaluation reward: 211.9\n",
      "episode: 2156   score: 180.0  epsilon: 1.0    steps: 419  evaluation reward: 208.05\n",
      "episode: 2157   score: 315.0  epsilon: 1.0    steps: 843  evaluation reward: 209.7\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5005: Policy loss: -1.172576. Value loss: 240.730896. Entropy: 0.414149.\n",
      "Iteration 5006: Policy loss: -0.871843. Value loss: 160.749588. Entropy: 0.415510.\n",
      "Iteration 5007: Policy loss: -1.539297. Value loss: 115.548523. Entropy: 0.309175.\n",
      "episode: 2158   score: 140.0  epsilon: 1.0    steps: 615  evaluation reward: 209.75\n",
      "episode: 2159   score: 285.0  epsilon: 1.0    steps: 951  evaluation reward: 210.5\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5008: Policy loss: 0.889695. Value loss: 42.442368. Entropy: 0.344700.\n",
      "Iteration 5009: Policy loss: 1.194045. Value loss: 26.173040. Entropy: 0.373657.\n",
      "Iteration 5010: Policy loss: 0.954523. Value loss: 20.957880. Entropy: 0.398681.\n",
      "episode: 2160   score: 260.0  epsilon: 1.0    steps: 763  evaluation reward: 210.85\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5011: Policy loss: 0.483885. Value loss: 34.453487. Entropy: 0.289890.\n",
      "Iteration 5012: Policy loss: 0.476515. Value loss: 21.694479. Entropy: 0.278626.\n",
      "Iteration 5013: Policy loss: 0.649930. Value loss: 17.988121. Entropy: 0.286828.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5014: Policy loss: 0.512994. Value loss: 52.432320. Entropy: 0.354091.\n",
      "Iteration 5015: Policy loss: 0.442659. Value loss: 29.270662. Entropy: 0.356297.\n",
      "Iteration 5016: Policy loss: 0.362788. Value loss: 24.438690. Entropy: 0.351366.\n",
      "episode: 2161   score: 180.0  epsilon: 1.0    steps: 359  evaluation reward: 211.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5017: Policy loss: -0.951161. Value loss: 38.190460. Entropy: 0.147371.\n",
      "Iteration 5018: Policy loss: -1.305681. Value loss: 23.979361. Entropy: 0.143259.\n",
      "Iteration 5019: Policy loss: -1.226311. Value loss: 15.208440. Entropy: 0.154478.\n",
      "episode: 2162   score: 290.0  epsilon: 1.0    steps: 241  evaluation reward: 213.45\n",
      "episode: 2163   score: 190.0  epsilon: 1.0    steps: 443  evaluation reward: 213.25\n",
      "episode: 2164   score: 100.0  epsilon: 1.0    steps: 921  evaluation reward: 213.2\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5020: Policy loss: 1.040395. Value loss: 21.665546. Entropy: 0.340189.\n",
      "Iteration 5021: Policy loss: 1.509632. Value loss: 11.556610. Entropy: 0.383985.\n",
      "Iteration 5022: Policy loss: 1.385616. Value loss: 9.370466. Entropy: 0.367509.\n",
      "episode: 2165   score: 195.0  epsilon: 1.0    steps: 630  evaluation reward: 214.4\n",
      "episode: 2166   score: 165.0  epsilon: 1.0    steps: 713  evaluation reward: 213.2\n",
      "episode: 2167   score: 255.0  epsilon: 1.0    steps: 886  evaluation reward: 213.95\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5023: Policy loss: -0.523631. Value loss: 48.918968. Entropy: 0.468661.\n",
      "Iteration 5024: Policy loss: -0.628610. Value loss: 30.677618. Entropy: 0.424962.\n",
      "Iteration 5025: Policy loss: -0.442401. Value loss: 23.272472. Entropy: 0.434284.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5026: Policy loss: 1.538973. Value loss: 24.783278. Entropy: 0.525361.\n",
      "Iteration 5027: Policy loss: 1.513814. Value loss: 15.092309. Entropy: 0.522927.\n",
      "Iteration 5028: Policy loss: 1.285262. Value loss: 12.827986. Entropy: 0.532714.\n",
      "episode: 2168   score: 740.0  epsilon: 1.0    steps: 87  evaluation reward: 219.25\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5029: Policy loss: 0.491722. Value loss: 41.569668. Entropy: 0.185490.\n",
      "Iteration 5030: Policy loss: 0.205158. Value loss: 22.816395. Entropy: 0.174146.\n",
      "Iteration 5031: Policy loss: 0.198502. Value loss: 19.969784. Entropy: 0.159859.\n",
      "episode: 2169   score: 75.0  epsilon: 1.0    steps: 687  evaluation reward: 214.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5032: Policy loss: -1.289111. Value loss: 31.092678. Entropy: 0.356435.\n",
      "Iteration 5033: Policy loss: -1.252738. Value loss: 18.682577. Entropy: 0.380951.\n",
      "Iteration 5034: Policy loss: -1.216848. Value loss: 13.275498. Entropy: 0.387837.\n",
      "episode: 2170   score: 125.0  epsilon: 1.0    steps: 625  evaluation reward: 214.1\n",
      "episode: 2171   score: 210.0  epsilon: 1.0    steps: 934  evaluation reward: 214.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5035: Policy loss: -0.109924. Value loss: 27.278263. Entropy: 0.311300.\n",
      "Iteration 5036: Policy loss: -0.062144. Value loss: 15.176250. Entropy: 0.335692.\n",
      "Iteration 5037: Policy loss: 0.109806. Value loss: 11.267164. Entropy: 0.322316.\n",
      "episode: 2172   score: 215.0  epsilon: 1.0    steps: 130  evaluation reward: 214.65\n",
      "episode: 2173   score: 195.0  epsilon: 1.0    steps: 268  evaluation reward: 214.8\n",
      "episode: 2174   score: 265.0  epsilon: 1.0    steps: 475  evaluation reward: 216.2\n",
      "episode: 2175   score: 185.0  epsilon: 1.0    steps: 895  evaluation reward: 215.95\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5038: Policy loss: -0.500128. Value loss: 25.832373. Entropy: 0.494899.\n",
      "Iteration 5039: Policy loss: -0.363117. Value loss: 15.055000. Entropy: 0.472067.\n",
      "Iteration 5040: Policy loss: -0.277253. Value loss: 10.778382. Entropy: 0.484051.\n",
      "episode: 2176   score: 85.0  epsilon: 1.0    steps: 687  evaluation reward: 214.95\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5041: Policy loss: 0.784597. Value loss: 19.772181. Entropy: 0.474237.\n",
      "Iteration 5042: Policy loss: 0.840534. Value loss: 12.868655. Entropy: 0.454358.\n",
      "Iteration 5043: Policy loss: 0.610600. Value loss: 10.606992. Entropy: 0.455623.\n",
      "episode: 2177   score: 50.0  epsilon: 1.0    steps: 618  evaluation reward: 213.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5044: Policy loss: 1.256281. Value loss: 19.874634. Entropy: 0.353994.\n",
      "Iteration 5045: Policy loss: 1.203175. Value loss: 14.498169. Entropy: 0.355626.\n",
      "Iteration 5046: Policy loss: 1.310260. Value loss: 10.676004. Entropy: 0.354591.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5047: Policy loss: 0.172943. Value loss: 17.526724. Entropy: 0.412914.\n",
      "Iteration 5048: Policy loss: 0.177454. Value loss: 10.037451. Entropy: 0.418170.\n",
      "Iteration 5049: Policy loss: 0.263771. Value loss: 8.370364. Entropy: 0.437027.\n",
      "episode: 2178   score: 70.0  epsilon: 1.0    steps: 976  evaluation reward: 213.15\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5050: Policy loss: -0.279822. Value loss: 30.794710. Entropy: 0.314839.\n",
      "Iteration 5051: Policy loss: -0.017367. Value loss: 19.847086. Entropy: 0.313641.\n",
      "Iteration 5052: Policy loss: 0.005205. Value loss: 15.884630. Entropy: 0.305484.\n",
      "episode: 2179   score: 105.0  epsilon: 1.0    steps: 311  evaluation reward: 211.9\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5053: Policy loss: 0.056595. Value loss: 35.774094. Entropy: 0.285726.\n",
      "Iteration 5054: Policy loss: 0.068609. Value loss: 20.300121. Entropy: 0.299031.\n",
      "Iteration 5055: Policy loss: 0.032837. Value loss: 16.399780. Entropy: 0.286137.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5056: Policy loss: -0.801039. Value loss: 37.112167. Entropy: 0.284090.\n",
      "Iteration 5057: Policy loss: -0.721304. Value loss: 20.210548. Entropy: 0.301043.\n",
      "Iteration 5058: Policy loss: -0.684045. Value loss: 15.493034. Entropy: 0.275292.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5059: Policy loss: 0.835116. Value loss: 32.800526. Entropy: 0.116108.\n",
      "Iteration 5060: Policy loss: 1.202624. Value loss: 16.258142. Entropy: 0.170976.\n",
      "Iteration 5061: Policy loss: 1.200724. Value loss: 14.026664. Entropy: 0.238543.\n",
      "episode: 2180   score: 370.0  epsilon: 1.0    steps: 32  evaluation reward: 213.85\n",
      "episode: 2181   score: 275.0  epsilon: 1.0    steps: 211  evaluation reward: 215.55\n",
      "episode: 2182   score: 345.0  epsilon: 1.0    steps: 882  evaluation reward: 214.15\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5062: Policy loss: -1.139409. Value loss: 31.630297. Entropy: 0.382618.\n",
      "Iteration 5063: Policy loss: -1.305663. Value loss: 20.668598. Entropy: 0.361320.\n",
      "Iteration 5064: Policy loss: -1.044617. Value loss: 15.549911. Entropy: 0.381387.\n",
      "episode: 2183   score: 150.0  epsilon: 1.0    steps: 281  evaluation reward: 213.85\n",
      "episode: 2184   score: 420.0  epsilon: 1.0    steps: 398  evaluation reward: 215.95\n",
      "episode: 2185   score: 260.0  epsilon: 1.0    steps: 557  evaluation reward: 217.65\n",
      "episode: 2186   score: 365.0  epsilon: 1.0    steps: 702  evaluation reward: 219.2\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5065: Policy loss: -0.492893. Value loss: 12.928180. Entropy: 0.585447.\n",
      "Iteration 5066: Policy loss: -0.624803. Value loss: 11.519365. Entropy: 0.576022.\n",
      "Iteration 5067: Policy loss: -0.715098. Value loss: 10.469932. Entropy: 0.570829.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5068: Policy loss: 1.000521. Value loss: 22.135166. Entropy: 0.445418.\n",
      "Iteration 5069: Policy loss: 1.006868. Value loss: 16.118198. Entropy: 0.436426.\n",
      "Iteration 5070: Policy loss: 0.968059. Value loss: 16.521870. Entropy: 0.442251.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5071: Policy loss: 1.523216. Value loss: 18.599770. Entropy: 0.573602.\n",
      "Iteration 5072: Policy loss: 1.521688. Value loss: 12.976631. Entropy: 0.573907.\n",
      "Iteration 5073: Policy loss: 1.440657. Value loss: 11.180398. Entropy: 0.567528.\n",
      "episode: 2187   score: 155.0  epsilon: 1.0    steps: 891  evaluation reward: 215.9\n",
      "episode: 2188   score: 240.0  epsilon: 1.0    steps: 927  evaluation reward: 217.05\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5074: Policy loss: -0.591347. Value loss: 28.997057. Entropy: 0.414634.\n",
      "Iteration 5075: Policy loss: -0.206674. Value loss: 18.139818. Entropy: 0.380705.\n",
      "Iteration 5076: Policy loss: -0.253391. Value loss: 13.503538. Entropy: 0.414998.\n",
      "episode: 2189   score: 180.0  epsilon: 1.0    steps: 155  evaluation reward: 216.45\n",
      "episode: 2190   score: 155.0  epsilon: 1.0    steps: 455  evaluation reward: 216.2\n",
      "episode: 2191   score: 105.0  epsilon: 1.0    steps: 645  evaluation reward: 214.95\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5077: Policy loss: 0.862049. Value loss: 16.958820. Entropy: 0.506991.\n",
      "Iteration 5078: Policy loss: 1.072065. Value loss: 11.264333. Entropy: 0.530768.\n",
      "Iteration 5079: Policy loss: 0.893420. Value loss: 10.162587. Entropy: 0.520431.\n",
      "episode: 2192   score: 110.0  epsilon: 1.0    steps: 71  evaluation reward: 213.95\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5080: Policy loss: 1.189206. Value loss: 21.605673. Entropy: 0.610892.\n",
      "Iteration 5081: Policy loss: 0.975605. Value loss: 14.166890. Entropy: 0.579442.\n",
      "Iteration 5082: Policy loss: 0.721876. Value loss: 11.207074. Entropy: 0.567730.\n",
      "episode: 2193   score: 155.0  epsilon: 1.0    steps: 358  evaluation reward: 211.2\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5083: Policy loss: 0.672981. Value loss: 27.417261. Entropy: 0.492244.\n",
      "Iteration 5084: Policy loss: 0.757349. Value loss: 16.405388. Entropy: 0.462813.\n",
      "Iteration 5085: Policy loss: 0.757309. Value loss: 14.050270. Entropy: 0.457948.\n",
      "episode: 2194   score: 110.0  epsilon: 1.0    steps: 860  evaluation reward: 210.2\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5086: Policy loss: -1.184466. Value loss: 34.670963. Entropy: 0.399625.\n",
      "Iteration 5087: Policy loss: -1.222847. Value loss: 19.707338. Entropy: 0.364062.\n",
      "Iteration 5088: Policy loss: -1.135798. Value loss: 14.653037. Entropy: 0.370344.\n",
      "episode: 2195   score: 155.0  epsilon: 1.0    steps: 399  evaluation reward: 209.95\n",
      "episode: 2196   score: 240.0  epsilon: 1.0    steps: 543  evaluation reward: 210.75\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5089: Policy loss: -1.342094. Value loss: 20.268650. Entropy: 0.422159.\n",
      "Iteration 5090: Policy loss: -1.299649. Value loss: 11.553191. Entropy: 0.439762.\n",
      "Iteration 5091: Policy loss: -1.435791. Value loss: 10.511949. Entropy: 0.429947.\n",
      "episode: 2197   score: 285.0  epsilon: 1.0    steps: 964  evaluation reward: 211.5\n",
      "Training network. lr: 0.000211. clip: 0.084489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5092: Policy loss: 0.421511. Value loss: 22.733677. Entropy: 0.311413.\n",
      "Iteration 5093: Policy loss: 0.260041. Value loss: 14.429324. Entropy: 0.320836.\n",
      "Iteration 5094: Policy loss: 0.142906. Value loss: 11.845476. Entropy: 0.334183.\n",
      "episode: 2198   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 212.35\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5095: Policy loss: 0.418561. Value loss: 15.796069. Entropy: 0.434959.\n",
      "Iteration 5096: Policy loss: 0.359690. Value loss: 10.087809. Entropy: 0.482820.\n",
      "Iteration 5097: Policy loss: 0.541325. Value loss: 7.210877. Entropy: 0.476203.\n",
      "episode: 2199   score: 350.0  epsilon: 1.0    steps: 198  evaluation reward: 214.3\n",
      "episode: 2200   score: 150.0  epsilon: 1.0    steps: 807  evaluation reward: 215.05\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5098: Policy loss: 0.496561. Value loss: 9.375492. Entropy: 0.291731.\n",
      "Iteration 5099: Policy loss: 0.530911. Value loss: 5.794184. Entropy: 0.291353.\n",
      "Iteration 5100: Policy loss: 0.512455. Value loss: 6.963932. Entropy: 0.317161.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5101: Policy loss: 1.847347. Value loss: 14.175125. Entropy: 0.388684.\n",
      "Iteration 5102: Policy loss: 1.990577. Value loss: 7.812299. Entropy: 0.377819.\n",
      "Iteration 5103: Policy loss: 1.939300. Value loss: 6.731364. Entropy: 0.398702.\n",
      "now time :  2019-02-25 20:15:20.102640\n",
      "episode: 2201   score: 240.0  epsilon: 1.0    steps: 36  evaluation reward: 215.9\n",
      "episode: 2202   score: 180.0  epsilon: 1.0    steps: 321  evaluation reward: 216.15\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5104: Policy loss: -0.211143. Value loss: 21.612988. Entropy: 0.536147.\n",
      "Iteration 5105: Policy loss: -0.008353. Value loss: 13.893751. Entropy: 0.553596.\n",
      "Iteration 5106: Policy loss: 0.205133. Value loss: 11.102913. Entropy: 0.541675.\n",
      "episode: 2203   score: 155.0  epsilon: 1.0    steps: 436  evaluation reward: 214.85\n",
      "episode: 2204   score: 145.0  epsilon: 1.0    steps: 599  evaluation reward: 214.2\n",
      "episode: 2205   score: 105.0  epsilon: 1.0    steps: 861  evaluation reward: 212.6\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5107: Policy loss: 3.211245. Value loss: 16.186535. Entropy: 0.479704.\n",
      "Iteration 5108: Policy loss: 3.197901. Value loss: 7.138809. Entropy: 0.476726.\n",
      "Iteration 5109: Policy loss: 2.995400. Value loss: 6.964885. Entropy: 0.502385.\n",
      "episode: 2206   score: 225.0  epsilon: 1.0    steps: 1022  evaluation reward: 213.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5110: Policy loss: 0.825003. Value loss: 17.053652. Entropy: 0.575102.\n",
      "Iteration 5111: Policy loss: 0.975199. Value loss: 9.007938. Entropy: 0.579096.\n",
      "Iteration 5112: Policy loss: 0.782041. Value loss: 9.542389. Entropy: 0.590859.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5113: Policy loss: 0.785394. Value loss: 21.042921. Entropy: 0.508410.\n",
      "Iteration 5114: Policy loss: 0.721208. Value loss: 14.347137. Entropy: 0.498890.\n",
      "Iteration 5115: Policy loss: 0.684662. Value loss: 11.147778. Entropy: 0.505537.\n",
      "episode: 2207   score: 95.0  epsilon: 1.0    steps: 64  evaluation reward: 212.5\n",
      "episode: 2208   score: 150.0  epsilon: 1.0    steps: 265  evaluation reward: 212.2\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5116: Policy loss: 0.763691. Value loss: 16.751364. Entropy: 0.470390.\n",
      "Iteration 5117: Policy loss: 0.598372. Value loss: 10.477199. Entropy: 0.472241.\n",
      "Iteration 5118: Policy loss: 0.732147. Value loss: 8.295151. Entropy: 0.471631.\n",
      "episode: 2209   score: 210.0  epsilon: 1.0    steps: 221  evaluation reward: 213.05\n",
      "episode: 2210   score: 180.0  epsilon: 1.0    steps: 423  evaluation reward: 213.05\n",
      "episode: 2211   score: 515.0  epsilon: 1.0    steps: 755  evaluation reward: 215.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5119: Policy loss: -2.232911. Value loss: 199.225693. Entropy: 0.418848.\n",
      "Iteration 5120: Policy loss: -2.373305. Value loss: 44.743870. Entropy: 0.447568.\n",
      "Iteration 5121: Policy loss: -2.372797. Value loss: 27.838163. Entropy: 0.426346.\n",
      "episode: 2212   score: 105.0  epsilon: 1.0    steps: 805  evaluation reward: 213.8\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5122: Policy loss: 0.537962. Value loss: 21.820992. Entropy: 0.628036.\n",
      "Iteration 5123: Policy loss: 0.056603. Value loss: 10.887305. Entropy: 0.614175.\n",
      "Iteration 5124: Policy loss: 0.272283. Value loss: 8.253807. Entropy: 0.641810.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5125: Policy loss: -0.290747. Value loss: 23.952547. Entropy: 0.373899.\n",
      "Iteration 5126: Policy loss: 0.069901. Value loss: 11.122256. Entropy: 0.379364.\n",
      "Iteration 5127: Policy loss: -0.329450. Value loss: 11.914187. Entropy: 0.359156.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5128: Policy loss: 1.590581. Value loss: 18.316038. Entropy: 0.365094.\n",
      "Iteration 5129: Policy loss: 1.660206. Value loss: 10.311364. Entropy: 0.355366.\n",
      "Iteration 5130: Policy loss: 1.658990. Value loss: 6.932116. Entropy: 0.346914.\n",
      "episode: 2213   score: 75.0  epsilon: 1.0    steps: 843  evaluation reward: 211.55\n",
      "episode: 2214   score: 180.0  epsilon: 1.0    steps: 939  evaluation reward: 212.55\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5131: Policy loss: 0.148650. Value loss: 31.041580. Entropy: 0.376446.\n",
      "Iteration 5132: Policy loss: 0.157587. Value loss: 11.458102. Entropy: 0.369493.\n",
      "Iteration 5133: Policy loss: 0.130361. Value loss: 8.450161. Entropy: 0.380405.\n",
      "episode: 2215   score: 210.0  epsilon: 1.0    steps: 124  evaluation reward: 213.15\n",
      "episode: 2216   score: 155.0  epsilon: 1.0    steps: 251  evaluation reward: 212.3\n",
      "episode: 2217   score: 180.0  epsilon: 1.0    steps: 298  evaluation reward: 212.55\n",
      "episode: 2218   score: 335.0  epsilon: 1.0    steps: 578  evaluation reward: 214.65\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5134: Policy loss: -2.903051. Value loss: 281.823120. Entropy: 0.536351.\n",
      "Iteration 5135: Policy loss: -1.485656. Value loss: 95.528679. Entropy: 0.520870.\n",
      "Iteration 5136: Policy loss: -3.249341. Value loss: 204.838028. Entropy: 0.515465.\n",
      "episode: 2219   score: 210.0  epsilon: 1.0    steps: 492  evaluation reward: 215.4\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5137: Policy loss: -0.531901. Value loss: 21.923573. Entropy: 0.505479.\n",
      "Iteration 5138: Policy loss: -0.710414. Value loss: 14.477587. Entropy: 0.495871.\n",
      "Iteration 5139: Policy loss: -0.692358. Value loss: 10.356993. Entropy: 0.502783.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5140: Policy loss: -1.189140. Value loss: 34.464085. Entropy: 0.305718.\n",
      "Iteration 5141: Policy loss: -1.017082. Value loss: 19.141567. Entropy: 0.292616.\n",
      "Iteration 5142: Policy loss: -1.296423. Value loss: 17.356569. Entropy: 0.283502.\n",
      "episode: 2220   score: 460.0  epsilon: 1.0    steps: 725  evaluation reward: 218.75\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5143: Policy loss: 1.005494. Value loss: 19.286745. Entropy: 0.159224.\n",
      "Iteration 5144: Policy loss: 0.897332. Value loss: 13.081754. Entropy: 0.152265.\n",
      "Iteration 5145: Policy loss: 0.878311. Value loss: 10.108245. Entropy: 0.163699.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5146: Policy loss: 0.463535. Value loss: 11.135730. Entropy: 0.274805.\n",
      "Iteration 5147: Policy loss: 0.642894. Value loss: 6.479252. Entropy: 0.278254.\n",
      "Iteration 5148: Policy loss: 0.461306. Value loss: 5.742980. Entropy: 0.291112.\n",
      "episode: 2221   score: 265.0  epsilon: 1.0    steps: 891  evaluation reward: 220.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5149: Policy loss: -2.558830. Value loss: 198.126465. Entropy: 0.394259.\n",
      "Iteration 5150: Policy loss: -2.231686. Value loss: 108.040886. Entropy: 0.373122.\n",
      "Iteration 5151: Policy loss: -2.501030. Value loss: 44.623550. Entropy: 0.386739.\n",
      "episode: 2222   score: 245.0  epsilon: 1.0    steps: 343  evaluation reward: 221.45\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5152: Policy loss: -0.277290. Value loss: 27.252445. Entropy: 0.244170.\n",
      "Iteration 5153: Policy loss: 0.028766. Value loss: 11.467841. Entropy: 0.228967.\n",
      "Iteration 5154: Policy loss: -0.224992. Value loss: 10.508004. Entropy: 0.239637.\n",
      "episode: 2223   score: 210.0  epsilon: 1.0    steps: 65  evaluation reward: 221.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2224   score: 240.0  epsilon: 1.0    steps: 918  evaluation reward: 222.05\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5155: Policy loss: 1.725693. Value loss: 17.891270. Entropy: 0.309168.\n",
      "Iteration 5156: Policy loss: 1.716899. Value loss: 9.903576. Entropy: 0.348858.\n",
      "Iteration 5157: Policy loss: 1.698728. Value loss: 8.628958. Entropy: 0.378503.\n",
      "episode: 2225   score: 155.0  epsilon: 1.0    steps: 402  evaluation reward: 222.6\n",
      "episode: 2226   score: 260.0  epsilon: 1.0    steps: 533  evaluation reward: 223.1\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5158: Policy loss: 0.497399. Value loss: 18.192749. Entropy: 0.368234.\n",
      "Iteration 5159: Policy loss: 0.333469. Value loss: 12.273008. Entropy: 0.378085.\n",
      "Iteration 5160: Policy loss: 0.292526. Value loss: 10.514172. Entropy: 0.384256.\n",
      "episode: 2227   score: 440.0  epsilon: 1.0    steps: 236  evaluation reward: 226.3\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5161: Policy loss: -1.183523. Value loss: 153.585663. Entropy: 0.405238.\n",
      "Iteration 5162: Policy loss: -1.195177. Value loss: 103.019852. Entropy: 0.438063.\n",
      "Iteration 5163: Policy loss: -1.866693. Value loss: 84.057327. Entropy: 0.436922.\n",
      "episode: 2228   score: 180.0  epsilon: 1.0    steps: 858  evaluation reward: 226.3\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5164: Policy loss: 1.234430. Value loss: 19.909428. Entropy: 0.375433.\n",
      "Iteration 5165: Policy loss: 0.433117. Value loss: 12.729870. Entropy: 0.352370.\n",
      "Iteration 5166: Policy loss: 0.720428. Value loss: 10.484917. Entropy: 0.368624.\n",
      "episode: 2229   score: 75.0  epsilon: 1.0    steps: 626  evaluation reward: 222.8\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5167: Policy loss: -1.146340. Value loss: 31.845320. Entropy: 0.383012.\n",
      "Iteration 5168: Policy loss: -1.328243. Value loss: 16.270790. Entropy: 0.352503.\n",
      "Iteration 5169: Policy loss: -1.777827. Value loss: 12.605401. Entropy: 0.365338.\n",
      "episode: 2230   score: 180.0  epsilon: 1.0    steps: 38  evaluation reward: 222.9\n",
      "episode: 2231   score: 170.0  epsilon: 1.0    steps: 332  evaluation reward: 221.75\n",
      "episode: 2232   score: 515.0  epsilon: 1.0    steps: 659  evaluation reward: 225.8\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5170: Policy loss: 2.489941. Value loss: 53.661369. Entropy: 0.372144.\n",
      "Iteration 5171: Policy loss: 2.823362. Value loss: 29.894976. Entropy: 0.373525.\n",
      "Iteration 5172: Policy loss: 2.197575. Value loss: 19.294609. Entropy: 0.366638.\n",
      "episode: 2233   score: 105.0  epsilon: 1.0    steps: 442  evaluation reward: 223.95\n",
      "episode: 2234   score: 160.0  epsilon: 1.0    steps: 908  evaluation reward: 222.95\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5173: Policy loss: 1.394625. Value loss: 61.605927. Entropy: 0.456692.\n",
      "Iteration 5174: Policy loss: 1.349914. Value loss: 26.451164. Entropy: 0.472171.\n",
      "Iteration 5175: Policy loss: 1.323259. Value loss: 20.420454. Entropy: 0.462649.\n",
      "episode: 2235   score: 210.0  epsilon: 1.0    steps: 220  evaluation reward: 222.95\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5176: Policy loss: -0.807321. Value loss: 25.127583. Entropy: 0.423303.\n",
      "Iteration 5177: Policy loss: -0.744932. Value loss: 16.039339. Entropy: 0.422668.\n",
      "Iteration 5178: Policy loss: -1.033899. Value loss: 16.029903. Entropy: 0.414343.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5179: Policy loss: -2.030633. Value loss: 28.221813. Entropy: 0.403727.\n",
      "Iteration 5180: Policy loss: -1.730099. Value loss: 19.956650. Entropy: 0.409109.\n",
      "Iteration 5181: Policy loss: -2.055951. Value loss: 15.786072. Entropy: 0.411657.\n",
      "episode: 2236   score: 180.0  epsilon: 1.0    steps: 549  evaluation reward: 223.9\n",
      "episode: 2237   score: 210.0  epsilon: 1.0    steps: 749  evaluation reward: 225.35\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5182: Policy loss: 0.597200. Value loss: 24.173851. Entropy: 0.239922.\n",
      "Iteration 5183: Policy loss: 0.411224. Value loss: 13.650240. Entropy: 0.234660.\n",
      "Iteration 5184: Policy loss: 0.471062. Value loss: 12.504616. Entropy: 0.230937.\n",
      "episode: 2238   score: 230.0  epsilon: 1.0    steps: 125  evaluation reward: 224.95\n",
      "episode: 2239   score: 210.0  epsilon: 1.0    steps: 271  evaluation reward: 221.3\n",
      "episode: 2240   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 221.85\n",
      "episode: 2241   score: 210.0  epsilon: 1.0    steps: 974  evaluation reward: 219.6\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5185: Policy loss: -0.735038. Value loss: 24.672293. Entropy: 0.412901.\n",
      "Iteration 5186: Policy loss: -0.548796. Value loss: 15.357324. Entropy: 0.446147.\n",
      "Iteration 5187: Policy loss: -0.612393. Value loss: 11.749633. Entropy: 0.435815.\n",
      "episode: 2242   score: 180.0  epsilon: 1.0    steps: 207  evaluation reward: 218.5\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5188: Policy loss: -1.078129. Value loss: 22.953461. Entropy: 0.509442.\n",
      "Iteration 5189: Policy loss: -1.054424. Value loss: 14.077247. Entropy: 0.498532.\n",
      "Iteration 5190: Policy loss: -1.087217. Value loss: 12.856347. Entropy: 0.485585.\n",
      "episode: 2243   score: 105.0  epsilon: 1.0    steps: 746  evaluation reward: 218.6\n",
      "episode: 2244   score: 380.0  epsilon: 1.0    steps: 836  evaluation reward: 221.35\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5191: Policy loss: 1.796293. Value loss: 30.565842. Entropy: 0.462807.\n",
      "Iteration 5192: Policy loss: 1.954099. Value loss: 18.938629. Entropy: 0.467634.\n",
      "Iteration 5193: Policy loss: 1.816800. Value loss: 14.292806. Entropy: 0.466143.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5194: Policy loss: 1.585066. Value loss: 32.873203. Entropy: 0.394481.\n",
      "Iteration 5195: Policy loss: 1.705592. Value loss: 18.173843. Entropy: 0.409483.\n",
      "Iteration 5196: Policy loss: 1.197650. Value loss: 15.655053. Entropy: 0.407462.\n",
      "episode: 2245   score: 180.0  epsilon: 1.0    steps: 338  evaluation reward: 222.1\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5197: Policy loss: -0.171488. Value loss: 26.564260. Entropy: 0.371348.\n",
      "Iteration 5198: Policy loss: 0.041658. Value loss: 19.533566. Entropy: 0.342583.\n",
      "Iteration 5199: Policy loss: -0.039318. Value loss: 16.433449. Entropy: 0.354160.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5200: Policy loss: -0.031675. Value loss: 29.399313. Entropy: 0.303652.\n",
      "Iteration 5201: Policy loss: -0.046554. Value loss: 17.678143. Entropy: 0.310591.\n",
      "Iteration 5202: Policy loss: -0.168265. Value loss: 16.049480. Entropy: 0.281535.\n",
      "episode: 2246   score: 235.0  epsilon: 1.0    steps: 395  evaluation reward: 221.75\n",
      "episode: 2247   score: 225.0  epsilon: 1.0    steps: 907  evaluation reward: 222.05\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5203: Policy loss: -0.352689. Value loss: 27.136051. Entropy: 0.360063.\n",
      "Iteration 5204: Policy loss: -0.139563. Value loss: 12.739956. Entropy: 0.372016.\n",
      "Iteration 5205: Policy loss: -0.275217. Value loss: 9.979340. Entropy: 0.372292.\n",
      "episode: 2248   score: 295.0  epsilon: 1.0    steps: 221  evaluation reward: 223.15\n",
      "episode: 2249   score: 290.0  epsilon: 1.0    steps: 553  evaluation reward: 224.95\n",
      "episode: 2250   score: 185.0  epsilon: 1.0    steps: 753  evaluation reward: 225.0\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5206: Policy loss: 0.486751. Value loss: 40.154469. Entropy: 0.420064.\n",
      "Iteration 5207: Policy loss: 0.338517. Value loss: 17.702740. Entropy: 0.400381.\n",
      "Iteration 5208: Policy loss: 0.735679. Value loss: 12.300139. Entropy: 0.423867.\n",
      "now time :  2019-02-25 20:17:18.273765\n",
      "episode: 2251   score: 295.0  epsilon: 1.0    steps: 67  evaluation reward: 225.25\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5209: Policy loss: -2.235620. Value loss: 324.885223. Entropy: 0.561864.\n",
      "Iteration 5210: Policy loss: -2.165189. Value loss: 121.340775. Entropy: 0.537526.\n",
      "Iteration 5211: Policy loss: -1.958573. Value loss: 120.172478. Entropy: 0.535512.\n",
      "episode: 2252   score: 125.0  epsilon: 1.0    steps: 467  evaluation reward: 221.5\n",
      "episode: 2253   score: 130.0  epsilon: 1.0    steps: 962  evaluation reward: 220.05\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5212: Policy loss: 1.580456. Value loss: 27.205217. Entropy: 0.407883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5213: Policy loss: 1.780572. Value loss: 18.040421. Entropy: 0.401919.\n",
      "Iteration 5214: Policy loss: 1.525514. Value loss: 15.470521. Entropy: 0.427157.\n",
      "episode: 2254   score: 245.0  epsilon: 1.0    steps: 325  evaluation reward: 217.85\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5215: Policy loss: 0.677269. Value loss: 31.136395. Entropy: 0.469371.\n",
      "Iteration 5216: Policy loss: 0.464937. Value loss: 21.976210. Entropy: 0.457583.\n",
      "Iteration 5217: Policy loss: 0.447656. Value loss: 16.145994. Entropy: 0.449510.\n",
      "episode: 2255   score: 150.0  epsilon: 1.0    steps: 521  evaluation reward: 214.5\n",
      "episode: 2256   score: 125.0  epsilon: 1.0    steps: 698  evaluation reward: 213.95\n",
      "episode: 2257   score: 685.0  epsilon: 1.0    steps: 795  evaluation reward: 217.65\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5218: Policy loss: 0.421110. Value loss: 24.172695. Entropy: 0.482280.\n",
      "Iteration 5219: Policy loss: 0.209611. Value loss: 16.997116. Entropy: 0.521578.\n",
      "Iteration 5220: Policy loss: 0.345288. Value loss: 14.484519. Entropy: 0.504979.\n",
      "episode: 2258   score: 125.0  epsilon: 1.0    steps: 10  evaluation reward: 217.5\n",
      "episode: 2259   score: 110.0  epsilon: 1.0    steps: 969  evaluation reward: 215.75\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5221: Policy loss: 1.535455. Value loss: 26.304642. Entropy: 0.567659.\n",
      "Iteration 5222: Policy loss: 1.753536. Value loss: 17.024506. Entropy: 0.568707.\n",
      "Iteration 5223: Policy loss: 1.332939. Value loss: 15.146197. Entropy: 0.559437.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5224: Policy loss: 0.161687. Value loss: 34.157639. Entropy: 0.449620.\n",
      "Iteration 5225: Policy loss: 0.417010. Value loss: 19.870550. Entropy: 0.433597.\n",
      "Iteration 5226: Policy loss: -0.381382. Value loss: 19.830561. Entropy: 0.449878.\n",
      "episode: 2260   score: 210.0  epsilon: 1.0    steps: 397  evaluation reward: 215.25\n",
      "episode: 2261   score: 135.0  epsilon: 1.0    steps: 827  evaluation reward: 214.8\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5227: Policy loss: 0.416914. Value loss: 31.864794. Entropy: 0.372801.\n",
      "Iteration 5228: Policy loss: 0.207232. Value loss: 22.372993. Entropy: 0.374663.\n",
      "Iteration 5229: Policy loss: 0.513463. Value loss: 17.356804. Entropy: 0.381641.\n",
      "episode: 2262   score: 230.0  epsilon: 1.0    steps: 365  evaluation reward: 214.2\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5230: Policy loss: 0.048277. Value loss: 24.276939. Entropy: 0.396245.\n",
      "Iteration 5231: Policy loss: -0.273976. Value loss: 21.342268. Entropy: 0.394096.\n",
      "Iteration 5232: Policy loss: -0.173675. Value loss: 15.780008. Entropy: 0.410104.\n",
      "episode: 2263   score: 210.0  epsilon: 1.0    steps: 51  evaluation reward: 214.4\n",
      "episode: 2264   score: 395.0  epsilon: 1.0    steps: 230  evaluation reward: 217.35\n",
      "episode: 2265   score: 260.0  epsilon: 1.0    steps: 569  evaluation reward: 218.0\n",
      "episode: 2266   score: 230.0  epsilon: 1.0    steps: 757  evaluation reward: 218.65\n",
      "episode: 2267   score: 150.0  epsilon: 1.0    steps: 937  evaluation reward: 217.6\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5233: Policy loss: 2.065698. Value loss: 19.880196. Entropy: 0.401883.\n",
      "Iteration 5234: Policy loss: 2.099927. Value loss: 12.690445. Entropy: 0.402446.\n",
      "Iteration 5235: Policy loss: 2.195405. Value loss: 11.003257. Entropy: 0.420291.\n",
      "episode: 2268   score: 125.0  epsilon: 1.0    steps: 470  evaluation reward: 211.45\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5236: Policy loss: 1.212691. Value loss: 19.446117. Entropy: 0.629151.\n",
      "Iteration 5237: Policy loss: 1.199142. Value loss: 11.346864. Entropy: 0.601139.\n",
      "Iteration 5238: Policy loss: 1.200413. Value loss: 9.006416. Entropy: 0.638481.\n",
      "episode: 2269   score: 180.0  epsilon: 1.0    steps: 842  evaluation reward: 212.5\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5239: Policy loss: 0.644122. Value loss: 35.766163. Entropy: 0.600417.\n",
      "Iteration 5240: Policy loss: 0.671177. Value loss: 25.747356. Entropy: 0.615253.\n",
      "Iteration 5241: Policy loss: 0.228820. Value loss: 23.091105. Entropy: 0.589293.\n",
      "episode: 2270   score: 105.0  epsilon: 1.0    steps: 1002  evaluation reward: 212.3\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5242: Policy loss: 0.573640. Value loss: 29.551285. Entropy: 0.372613.\n",
      "Iteration 5243: Policy loss: 0.583454. Value loss: 19.732061. Entropy: 0.355533.\n",
      "Iteration 5244: Policy loss: 0.523719. Value loss: 15.875053. Entropy: 0.346413.\n",
      "episode: 2271   score: 105.0  epsilon: 1.0    steps: 515  evaluation reward: 211.25\n",
      "episode: 2272   score: 180.0  epsilon: 1.0    steps: 760  evaluation reward: 210.9\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5245: Policy loss: 2.192004. Value loss: 35.886093. Entropy: 0.348943.\n",
      "Iteration 5246: Policy loss: 2.484552. Value loss: 21.279663. Entropy: 0.424302.\n",
      "Iteration 5247: Policy loss: 1.979244. Value loss: 16.836674. Entropy: 0.431283.\n",
      "episode: 2273   score: 110.0  epsilon: 1.0    steps: 485  evaluation reward: 210.05\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5248: Policy loss: -0.463644. Value loss: 247.396011. Entropy: 0.488231.\n",
      "Iteration 5249: Policy loss: -0.009565. Value loss: 156.234848. Entropy: 0.428772.\n",
      "Iteration 5250: Policy loss: -0.388164. Value loss: 113.272224. Entropy: 0.414978.\n",
      "episode: 2274   score: 290.0  epsilon: 1.0    steps: 338  evaluation reward: 210.3\n",
      "episode: 2275   score: 180.0  epsilon: 1.0    steps: 825  evaluation reward: 210.25\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5251: Policy loss: 0.892574. Value loss: 24.754435. Entropy: 0.387627.\n",
      "Iteration 5252: Policy loss: 0.945270. Value loss: 14.515245. Entropy: 0.388726.\n",
      "Iteration 5253: Policy loss: 0.919937. Value loss: 11.991711. Entropy: 0.400561.\n",
      "episode: 2276   score: 540.0  epsilon: 1.0    steps: 92  evaluation reward: 214.8\n",
      "episode: 2277   score: 230.0  epsilon: 1.0    steps: 178  evaluation reward: 216.6\n",
      "episode: 2278   score: 75.0  epsilon: 1.0    steps: 930  evaluation reward: 216.65\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5254: Policy loss: 1.192013. Value loss: 20.862471. Entropy: 0.463393.\n",
      "Iteration 5255: Policy loss: 1.390456. Value loss: 11.420343. Entropy: 0.447142.\n",
      "Iteration 5256: Policy loss: 1.233971. Value loss: 10.022918. Entropy: 0.455560.\n",
      "episode: 2279   score: 195.0  epsilon: 1.0    steps: 514  evaluation reward: 217.55\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5257: Policy loss: -1.653195. Value loss: 17.752445. Entropy: 0.554519.\n",
      "Iteration 5258: Policy loss: -1.718340. Value loss: 12.243398. Entropy: 0.587022.\n",
      "Iteration 5259: Policy loss: -1.806018. Value loss: 11.801459. Entropy: 0.575384.\n",
      "episode: 2280   score: 210.0  epsilon: 1.0    steps: 455  evaluation reward: 215.95\n",
      "episode: 2281   score: 180.0  epsilon: 1.0    steps: 707  evaluation reward: 215.0\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5260: Policy loss: 0.634048. Value loss: 31.808306. Entropy: 0.465752.\n",
      "Iteration 5261: Policy loss: 0.800266. Value loss: 16.596113. Entropy: 0.449675.\n",
      "Iteration 5262: Policy loss: 0.592637. Value loss: 16.458294. Entropy: 0.458892.\n",
      "episode: 2282   score: 75.0  epsilon: 1.0    steps: 266  evaluation reward: 212.3\n",
      "episode: 2283   score: 155.0  epsilon: 1.0    steps: 841  evaluation reward: 212.35\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5263: Policy loss: 0.441509. Value loss: 24.536451. Entropy: 0.464862.\n",
      "Iteration 5264: Policy loss: 0.383062. Value loss: 14.506023. Entropy: 0.447279.\n",
      "Iteration 5265: Policy loss: 0.472353. Value loss: 12.868157. Entropy: 0.428239.\n",
      "episode: 2284   score: 180.0  epsilon: 1.0    steps: 211  evaluation reward: 209.95\n",
      "episode: 2285   score: 125.0  epsilon: 1.0    steps: 623  evaluation reward: 208.6\n",
      "episode: 2286   score: 180.0  epsilon: 1.0    steps: 936  evaluation reward: 206.75\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5266: Policy loss: 0.474326. Value loss: 28.030952. Entropy: 0.449927.\n",
      "Iteration 5267: Policy loss: 0.513951. Value loss: 16.852505. Entropy: 0.447817.\n",
      "Iteration 5268: Policy loss: 0.234157. Value loss: 15.027144. Entropy: 0.448024.\n",
      "episode: 2287   score: 275.0  epsilon: 1.0    steps: 106  evaluation reward: 207.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5269: Policy loss: -1.370865. Value loss: 21.620834. Entropy: 0.480304.\n",
      "Iteration 5270: Policy loss: -1.342375. Value loss: 14.694747. Entropy: 0.470163.\n",
      "Iteration 5271: Policy loss: -1.165387. Value loss: 12.240931. Entropy: 0.487862.\n",
      "episode: 2288   score: 155.0  epsilon: 1.0    steps: 386  evaluation reward: 207.1\n",
      "episode: 2289   score: 210.0  epsilon: 1.0    steps: 723  evaluation reward: 207.4\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5272: Policy loss: -2.329916. Value loss: 21.264582. Entropy: 0.519257.\n",
      "Iteration 5273: Policy loss: -2.337321. Value loss: 15.294360. Entropy: 0.466812.\n",
      "Iteration 5274: Policy loss: -2.301190. Value loss: 14.341470. Entropy: 0.482541.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5275: Policy loss: -1.728249. Value loss: 37.267693. Entropy: 0.413254.\n",
      "Iteration 5276: Policy loss: -1.229022. Value loss: 21.467798. Entropy: 0.425636.\n",
      "Iteration 5277: Policy loss: -1.706624. Value loss: 20.071085. Entropy: 0.417702.\n",
      "episode: 2290   score: 230.0  epsilon: 1.0    steps: 846  evaluation reward: 208.15\n",
      "episode: 2291   score: 210.0  epsilon: 1.0    steps: 981  evaluation reward: 209.2\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5278: Policy loss: 1.036035. Value loss: 51.374775. Entropy: 0.349454.\n",
      "Iteration 5279: Policy loss: 1.108922. Value loss: 21.478161. Entropy: 0.351528.\n",
      "Iteration 5280: Policy loss: 0.971482. Value loss: 16.525433. Entropy: 0.358337.\n",
      "episode: 2292   score: 255.0  epsilon: 1.0    steps: 194  evaluation reward: 210.65\n",
      "episode: 2293   score: 110.0  epsilon: 1.0    steps: 396  evaluation reward: 210.2\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5281: Policy loss: -0.553928. Value loss: 32.223377. Entropy: 0.435466.\n",
      "Iteration 5282: Policy loss: -0.640876. Value loss: 22.030104. Entropy: 0.447842.\n",
      "Iteration 5283: Policy loss: -0.778079. Value loss: 18.196056. Entropy: 0.444231.\n",
      "episode: 2294   score: 350.0  epsilon: 1.0    steps: 260  evaluation reward: 212.6\n",
      "episode: 2295   score: 105.0  epsilon: 1.0    steps: 687  evaluation reward: 212.1\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5284: Policy loss: 0.437856. Value loss: 42.036907. Entropy: 0.500089.\n",
      "Iteration 5285: Policy loss: 0.318159. Value loss: 20.306089. Entropy: 0.485160.\n",
      "Iteration 5286: Policy loss: 0.373350. Value loss: 14.150048. Entropy: 0.493939.\n",
      "episode: 2296   score: 230.0  epsilon: 1.0    steps: 45  evaluation reward: 212.0\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5287: Policy loss: 1.696216. Value loss: 33.215443. Entropy: 0.359035.\n",
      "Iteration 5288: Policy loss: 1.646564. Value loss: 20.631680. Entropy: 0.353903.\n",
      "Iteration 5289: Policy loss: 1.393695. Value loss: 19.391600. Entropy: 0.371334.\n",
      "episode: 2297   score: 410.0  epsilon: 1.0    steps: 607  evaluation reward: 213.25\n",
      "episode: 2298   score: 210.0  epsilon: 1.0    steps: 1011  evaluation reward: 213.25\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5290: Policy loss: -1.090554. Value loss: 27.135994. Entropy: 0.389448.\n",
      "Iteration 5291: Policy loss: -0.995939. Value loss: 13.647728. Entropy: 0.382892.\n",
      "Iteration 5292: Policy loss: -1.133183. Value loss: 13.827407. Entropy: 0.376872.\n",
      "episode: 2299   score: 170.0  epsilon: 1.0    steps: 508  evaluation reward: 211.45\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5293: Policy loss: 3.077631. Value loss: 32.014107. Entropy: 0.395268.\n",
      "Iteration 5294: Policy loss: 3.383360. Value loss: 21.973808. Entropy: 0.394011.\n",
      "Iteration 5295: Policy loss: 3.240384. Value loss: 18.351101. Entropy: 0.384419.\n",
      "episode: 2300   score: 125.0  epsilon: 1.0    steps: 117  evaluation reward: 211.2\n",
      "now time :  2019-02-25 20:18:56.977614\n",
      "episode: 2301   score: 180.0  epsilon: 1.0    steps: 300  evaluation reward: 210.6\n",
      "episode: 2302   score: 230.0  epsilon: 1.0    steps: 811  evaluation reward: 211.1\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5296: Policy loss: 1.401337. Value loss: 22.070549. Entropy: 0.394505.\n",
      "Iteration 5297: Policy loss: 1.491194. Value loss: 12.445475. Entropy: 0.399206.\n",
      "Iteration 5298: Policy loss: 1.363985. Value loss: 13.108619. Entropy: 0.405142.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5299: Policy loss: -0.540443. Value loss: 32.943993. Entropy: 0.469050.\n",
      "Iteration 5300: Policy loss: -0.587440. Value loss: 20.379416. Entropy: 0.447413.\n",
      "Iteration 5301: Policy loss: -0.396688. Value loss: 14.669146. Entropy: 0.461169.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5302: Policy loss: -6.404737. Value loss: 245.846130. Entropy: 0.332322.\n",
      "Iteration 5303: Policy loss: -6.279105. Value loss: 98.278236. Entropy: 0.276868.\n",
      "Iteration 5304: Policy loss: -5.685551. Value loss: 97.096237. Entropy: 0.296740.\n",
      "episode: 2303   score: 500.0  epsilon: 1.0    steps: 184  evaluation reward: 214.55\n",
      "episode: 2304   score: 230.0  epsilon: 1.0    steps: 630  evaluation reward: 215.4\n",
      "episode: 2305   score: 410.0  epsilon: 1.0    steps: 762  evaluation reward: 218.45\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5305: Policy loss: -2.407919. Value loss: 336.260559. Entropy: 0.109749.\n",
      "Iteration 5306: Policy loss: -1.434871. Value loss: 142.395798. Entropy: 0.135632.\n",
      "Iteration 5307: Policy loss: -1.691344. Value loss: 121.868790. Entropy: 0.139190.\n",
      "episode: 2306   score: 210.0  epsilon: 1.0    steps: 333  evaluation reward: 218.3\n",
      "episode: 2307   score: 265.0  epsilon: 1.0    steps: 924  evaluation reward: 220.0\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5308: Policy loss: -0.728208. Value loss: 24.534725. Entropy: 0.370479.\n",
      "Iteration 5309: Policy loss: -0.576233. Value loss: 17.461300. Entropy: 0.376079.\n",
      "Iteration 5310: Policy loss: -0.617272. Value loss: 14.114338. Entropy: 0.366347.\n",
      "episode: 2308   score: 180.0  epsilon: 1.0    steps: 49  evaluation reward: 220.3\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5311: Policy loss: 0.611294. Value loss: 44.146320. Entropy: 0.436815.\n",
      "Iteration 5312: Policy loss: 0.543255. Value loss: 26.701908. Entropy: 0.420278.\n",
      "Iteration 5313: Policy loss: 0.300901. Value loss: 18.857281. Entropy: 0.446553.\n",
      "episode: 2309   score: 475.0  epsilon: 1.0    steps: 848  evaluation reward: 222.95\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5314: Policy loss: -1.725054. Value loss: 36.823620. Entropy: 0.310325.\n",
      "Iteration 5315: Policy loss: -1.858285. Value loss: 25.037455. Entropy: 0.316065.\n",
      "Iteration 5316: Policy loss: -1.910775. Value loss: 20.926603. Entropy: 0.312943.\n",
      "episode: 2310   score: 155.0  epsilon: 1.0    steps: 207  evaluation reward: 222.7\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5317: Policy loss: 1.592453. Value loss: 24.852726. Entropy: 0.232129.\n",
      "Iteration 5318: Policy loss: 1.894605. Value loss: 13.316356. Entropy: 0.255557.\n",
      "Iteration 5319: Policy loss: 1.625015. Value loss: 10.576221. Entropy: 0.257542.\n",
      "episode: 2311   score: 105.0  epsilon: 1.0    steps: 60  evaluation reward: 218.6\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5320: Policy loss: 1.275406. Value loss: 26.095678. Entropy: 0.284713.\n",
      "Iteration 5321: Policy loss: 1.338658. Value loss: 19.290195. Entropy: 0.285141.\n",
      "Iteration 5322: Policy loss: 1.370735. Value loss: 16.449564. Entropy: 0.280106.\n",
      "episode: 2312   score: 420.0  epsilon: 1.0    steps: 435  evaluation reward: 221.75\n",
      "episode: 2313   score: 215.0  epsilon: 1.0    steps: 549  evaluation reward: 223.15\n",
      "episode: 2314   score: 260.0  epsilon: 1.0    steps: 689  evaluation reward: 223.95\n",
      "episode: 2315   score: 265.0  epsilon: 1.0    steps: 1015  evaluation reward: 224.5\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5323: Policy loss: -0.252809. Value loss: 18.610088. Entropy: 0.401893.\n",
      "Iteration 5324: Policy loss: -0.605498. Value loss: 15.180981. Entropy: 0.377624.\n",
      "Iteration 5325: Policy loss: -0.357673. Value loss: 12.257115. Entropy: 0.396049.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5326: Policy loss: -0.803090. Value loss: 25.749676. Entropy: 0.458276.\n",
      "Iteration 5327: Policy loss: -1.004866. Value loss: 17.028461. Entropy: 0.440531.\n",
      "Iteration 5328: Policy loss: -0.924334. Value loss: 15.687511. Entropy: 0.440199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2316   score: 245.0  epsilon: 1.0    steps: 356  evaluation reward: 225.4\n",
      "episode: 2317   score: 180.0  epsilon: 1.0    steps: 786  evaluation reward: 225.4\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5329: Policy loss: 1.559082. Value loss: 23.992727. Entropy: 0.308920.\n",
      "Iteration 5330: Policy loss: 1.574073. Value loss: 14.563641. Entropy: 0.324385.\n",
      "Iteration 5331: Policy loss: 1.472268. Value loss: 12.883773. Entropy: 0.325522.\n",
      "episode: 2318   score: 210.0  epsilon: 1.0    steps: 107  evaluation reward: 224.15\n",
      "episode: 2319   score: 65.0  epsilon: 1.0    steps: 679  evaluation reward: 222.7\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5332: Policy loss: 1.164787. Value loss: 25.484415. Entropy: 0.296643.\n",
      "Iteration 5333: Policy loss: 1.131589. Value loss: 10.915146. Entropy: 0.286991.\n",
      "Iteration 5334: Policy loss: 1.088923. Value loss: 10.236285. Entropy: 0.285541.\n",
      "episode: 2320   score: 230.0  epsilon: 1.0    steps: 183  evaluation reward: 220.4\n",
      "episode: 2321   score: 180.0  epsilon: 1.0    steps: 594  evaluation reward: 219.55\n",
      "episode: 2322   score: 155.0  epsilon: 1.0    steps: 947  evaluation reward: 218.65\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5335: Policy loss: 2.212911. Value loss: 25.845907. Entropy: 0.433975.\n",
      "Iteration 5336: Policy loss: 2.093946. Value loss: 15.179185. Entropy: 0.444225.\n",
      "Iteration 5337: Policy loss: 2.142036. Value loss: 10.718743. Entropy: 0.444279.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5338: Policy loss: 0.637544. Value loss: 27.323380. Entropy: 0.490855.\n",
      "Iteration 5339: Policy loss: 0.681669. Value loss: 18.681345. Entropy: 0.483211.\n",
      "Iteration 5340: Policy loss: 0.710577. Value loss: 14.502857. Entropy: 0.500972.\n",
      "episode: 2323   score: 235.0  epsilon: 1.0    steps: 422  evaluation reward: 218.9\n",
      "episode: 2324   score: 210.0  epsilon: 1.0    steps: 831  evaluation reward: 218.6\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5341: Policy loss: 3.373070. Value loss: 30.481770. Entropy: 0.315940.\n",
      "Iteration 5342: Policy loss: 3.521639. Value loss: 19.826569. Entropy: 0.323438.\n",
      "Iteration 5343: Policy loss: 3.549999. Value loss: 12.349310. Entropy: 0.333339.\n",
      "episode: 2325   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 218.1\n",
      "episode: 2326   score: 55.0  epsilon: 1.0    steps: 652  evaluation reward: 216.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5344: Policy loss: 2.295510. Value loss: 37.011452. Entropy: 0.507978.\n",
      "Iteration 5345: Policy loss: 2.406103. Value loss: 20.028835. Entropy: 0.483359.\n",
      "Iteration 5346: Policy loss: 2.474091. Value loss: 20.414005. Entropy: 0.510753.\n",
      "episode: 2327   score: 180.0  epsilon: 1.0    steps: 19  evaluation reward: 213.45\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5347: Policy loss: 0.831001. Value loss: 25.548708. Entropy: 0.419775.\n",
      "Iteration 5348: Policy loss: 0.794146. Value loss: 16.570549. Entropy: 0.414730.\n",
      "Iteration 5349: Policy loss: 0.725927. Value loss: 14.010204. Entropy: 0.402652.\n",
      "episode: 2328   score: 180.0  epsilon: 1.0    steps: 547  evaluation reward: 213.45\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5350: Policy loss: 2.725922. Value loss: 25.383120. Entropy: 0.373899.\n",
      "Iteration 5351: Policy loss: 2.883968. Value loss: 10.429091. Entropy: 0.385549.\n",
      "Iteration 5352: Policy loss: 2.496297. Value loss: 15.073341. Entropy: 0.375277.\n",
      "episode: 2329   score: 210.0  epsilon: 1.0    steps: 502  evaluation reward: 214.8\n",
      "episode: 2330   score: 150.0  epsilon: 1.0    steps: 727  evaluation reward: 214.5\n",
      "episode: 2331   score: 215.0  epsilon: 1.0    steps: 904  evaluation reward: 214.95\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5353: Policy loss: -0.753523. Value loss: 25.037231. Entropy: 0.311487.\n",
      "Iteration 5354: Policy loss: -0.671425. Value loss: 16.305565. Entropy: 0.317838.\n",
      "Iteration 5355: Policy loss: -0.786857. Value loss: 15.006465. Entropy: 0.313693.\n",
      "episode: 2332   score: 210.0  epsilon: 1.0    steps: 242  evaluation reward: 211.9\n",
      "episode: 2333   score: 340.0  epsilon: 1.0    steps: 313  evaluation reward: 214.25\n",
      "episode: 2334   score: 210.0  epsilon: 1.0    steps: 795  evaluation reward: 214.75\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5356: Policy loss: 0.868135. Value loss: 15.335691. Entropy: 0.419772.\n",
      "Iteration 5357: Policy loss: 0.868104. Value loss: 10.435966. Entropy: 0.430115.\n",
      "Iteration 5358: Policy loss: 1.018859. Value loss: 8.301575. Entropy: 0.431693.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5359: Policy loss: -0.094227. Value loss: 15.707139. Entropy: 0.478778.\n",
      "Iteration 5360: Policy loss: -0.138552. Value loss: 13.076323. Entropy: 0.467737.\n",
      "Iteration 5361: Policy loss: 0.052103. Value loss: 10.678306. Entropy: 0.463854.\n",
      "episode: 2335   score: 150.0  epsilon: 1.0    steps: 636  evaluation reward: 214.15\n",
      "episode: 2336   score: 80.0  epsilon: 1.0    steps: 975  evaluation reward: 213.15\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5362: Policy loss: 0.819553. Value loss: 30.861073. Entropy: 0.363470.\n",
      "Iteration 5363: Policy loss: 0.583686. Value loss: 18.833523. Entropy: 0.330853.\n",
      "Iteration 5364: Policy loss: 1.213169. Value loss: 13.590259. Entropy: 0.346892.\n",
      "episode: 2337   score: 75.0  epsilon: 1.0    steps: 312  evaluation reward: 211.8\n",
      "episode: 2338   score: 180.0  epsilon: 1.0    steps: 767  evaluation reward: 211.3\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5365: Policy loss: -1.722850. Value loss: 158.359512. Entropy: 0.415926.\n",
      "Iteration 5366: Policy loss: -3.048944. Value loss: 88.441216. Entropy: 0.375985.\n",
      "Iteration 5367: Policy loss: -3.528448. Value loss: 52.303001. Entropy: 0.406910.\n",
      "episode: 2339   score: 485.0  epsilon: 1.0    steps: 46  evaluation reward: 214.05\n",
      "episode: 2340   score: 210.0  epsilon: 1.0    steps: 433  evaluation reward: 214.05\n",
      "episode: 2341   score: 210.0  epsilon: 1.0    steps: 852  evaluation reward: 214.05\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5368: Policy loss: -0.970928. Value loss: 13.666000. Entropy: 0.415477.\n",
      "Iteration 5369: Policy loss: -1.105647. Value loss: 7.571260. Entropy: 0.435199.\n",
      "Iteration 5370: Policy loss: -1.043149. Value loss: 7.352027. Entropy: 0.403877.\n",
      "episode: 2342   score: 210.0  epsilon: 1.0    steps: 156  evaluation reward: 214.35\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5371: Policy loss: -0.075315. Value loss: 17.088406. Entropy: 0.484776.\n",
      "Iteration 5372: Policy loss: -0.116432. Value loss: 12.007756. Entropy: 0.484707.\n",
      "Iteration 5373: Policy loss: -0.048004. Value loss: 10.182921. Entropy: 0.463138.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5374: Policy loss: -2.336418. Value loss: 31.684223. Entropy: 0.418922.\n",
      "Iteration 5375: Policy loss: -2.515024. Value loss: 19.204819. Entropy: 0.421764.\n",
      "Iteration 5376: Policy loss: -2.275655. Value loss: 16.263996. Entropy: 0.409749.\n",
      "episode: 2343   score: 210.0  epsilon: 1.0    steps: 579  evaluation reward: 215.4\n",
      "episode: 2344   score: 180.0  epsilon: 1.0    steps: 902  evaluation reward: 213.4\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5377: Policy loss: -1.485733. Value loss: 20.585655. Entropy: 0.389542.\n",
      "Iteration 5378: Policy loss: -1.274845. Value loss: 12.238043. Entropy: 0.424394.\n",
      "Iteration 5379: Policy loss: -1.506204. Value loss: 10.523546. Entropy: 0.410550.\n",
      "episode: 2345   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 213.7\n",
      "episode: 2346   score: 180.0  epsilon: 1.0    steps: 709  evaluation reward: 213.15\n",
      "episode: 2347   score: 80.0  epsilon: 1.0    steps: 819  evaluation reward: 211.7\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5380: Policy loss: -1.877544. Value loss: 190.652451. Entropy: 0.326260.\n",
      "Iteration 5381: Policy loss: -1.661765. Value loss: 143.837967. Entropy: 0.377514.\n",
      "Iteration 5382: Policy loss: -2.146569. Value loss: 102.162613. Entropy: 0.337677.\n",
      "episode: 2348   score: 255.0  epsilon: 1.0    steps: 61  evaluation reward: 211.3\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5383: Policy loss: -1.032021. Value loss: 19.580793. Entropy: 0.497975.\n",
      "Iteration 5384: Policy loss: -1.145657. Value loss: 12.747370. Entropy: 0.484389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5385: Policy loss: -1.096646. Value loss: 11.835034. Entropy: 0.493282.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5386: Policy loss: 0.954525. Value loss: 28.644260. Entropy: 0.331586.\n",
      "Iteration 5387: Policy loss: 0.869510. Value loss: 16.468729. Entropy: 0.347377.\n",
      "Iteration 5388: Policy loss: 0.738991. Value loss: 14.150226. Entropy: 0.338816.\n",
      "episode: 2349   score: 210.0  epsilon: 1.0    steps: 981  evaluation reward: 210.5\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5389: Policy loss: 3.644655. Value loss: 41.828293. Entropy: 0.175100.\n",
      "Iteration 5390: Policy loss: 3.181190. Value loss: 18.913799. Entropy: 0.167635.\n",
      "Iteration 5391: Policy loss: 3.400955. Value loss: 14.902040. Entropy: 0.182135.\n",
      "episode: 2350   score: 305.0  epsilon: 1.0    steps: 201  evaluation reward: 211.7\n",
      "now time :  2019-02-25 20:20:45.142893\n",
      "episode: 2351   score: 510.0  epsilon: 1.0    steps: 304  evaluation reward: 213.85\n",
      "episode: 2352   score: 55.0  epsilon: 1.0    steps: 833  evaluation reward: 213.15\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5392: Policy loss: 6.559176. Value loss: 47.576412. Entropy: 0.325506.\n",
      "Iteration 5393: Policy loss: 6.225707. Value loss: 22.373848. Entropy: 0.301344.\n",
      "Iteration 5394: Policy loss: 6.761688. Value loss: 18.032763. Entropy: 0.293946.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5395: Policy loss: 0.441514. Value loss: 35.517826. Entropy: 0.428930.\n",
      "Iteration 5396: Policy loss: 0.324200. Value loss: 18.608004. Entropy: 0.441273.\n",
      "Iteration 5397: Policy loss: 0.551586. Value loss: 13.482815. Entropy: 0.446672.\n",
      "episode: 2353   score: 165.0  epsilon: 1.0    steps: 724  evaluation reward: 213.5\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5398: Policy loss: 1.445047. Value loss: 23.392765. Entropy: 0.461126.\n",
      "Iteration 5399: Policy loss: 1.225201. Value loss: 14.173795. Entropy: 0.472051.\n",
      "Iteration 5400: Policy loss: 1.327061. Value loss: 11.682718. Entropy: 0.424830.\n",
      "episode: 2354   score: 275.0  epsilon: 1.0    steps: 542  evaluation reward: 213.8\n",
      "episode: 2355   score: 160.0  epsilon: 1.0    steps: 982  evaluation reward: 213.9\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5401: Policy loss: 1.708550. Value loss: 23.173634. Entropy: 0.485029.\n",
      "Iteration 5402: Policy loss: 1.860474. Value loss: 11.162243. Entropy: 0.488772.\n",
      "Iteration 5403: Policy loss: 1.611564. Value loss: 8.874364. Entropy: 0.476405.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5404: Policy loss: 1.427483. Value loss: 19.448874. Entropy: 0.383525.\n",
      "Iteration 5405: Policy loss: 1.412807. Value loss: 10.194772. Entropy: 0.399615.\n",
      "Iteration 5406: Policy loss: 1.484621. Value loss: 8.204364. Entropy: 0.413338.\n",
      "episode: 2356   score: 140.0  epsilon: 1.0    steps: 132  evaluation reward: 214.05\n",
      "episode: 2357   score: 180.0  epsilon: 1.0    steps: 355  evaluation reward: 209.0\n",
      "episode: 2358   score: 235.0  epsilon: 1.0    steps: 491  evaluation reward: 210.1\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5407: Policy loss: 2.963489. Value loss: 18.031858. Entropy: 0.654834.\n",
      "Iteration 5408: Policy loss: 2.899662. Value loss: 8.942191. Entropy: 0.677821.\n",
      "Iteration 5409: Policy loss: 2.911437. Value loss: 7.337249. Entropy: 0.695461.\n",
      "episode: 2359   score: 210.0  epsilon: 1.0    steps: 124  evaluation reward: 211.1\n",
      "episode: 2360   score: 105.0  epsilon: 1.0    steps: 590  evaluation reward: 210.05\n",
      "episode: 2361   score: 210.0  epsilon: 1.0    steps: 763  evaluation reward: 210.8\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5410: Policy loss: 0.703086. Value loss: 34.666767. Entropy: 0.558454.\n",
      "Iteration 5411: Policy loss: 0.308233. Value loss: 17.490721. Entropy: 0.473228.\n",
      "Iteration 5412: Policy loss: 0.748789. Value loss: 13.004975. Entropy: 0.521265.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5413: Policy loss: 1.180988. Value loss: 21.505638. Entropy: 0.868690.\n",
      "Iteration 5414: Policy loss: 1.167347. Value loss: 10.185910. Entropy: 0.886044.\n",
      "Iteration 5415: Policy loss: 1.184409. Value loss: 7.367905. Entropy: 0.895654.\n",
      "episode: 2362   score: 235.0  epsilon: 1.0    steps: 816  evaluation reward: 210.85\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5416: Policy loss: 3.026043. Value loss: 32.380455. Entropy: 0.634571.\n",
      "Iteration 5417: Policy loss: 3.122165. Value loss: 15.056482. Entropy: 0.648446.\n",
      "Iteration 5418: Policy loss: 2.965883. Value loss: 12.596914. Entropy: 0.664800.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5419: Policy loss: 1.002902. Value loss: 28.456387. Entropy: 0.520477.\n",
      "Iteration 5420: Policy loss: 0.981931. Value loss: 14.093858. Entropy: 0.519306.\n",
      "Iteration 5421: Policy loss: 0.899345. Value loss: 10.138538. Entropy: 0.519531.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5422: Policy loss: 0.229819. Value loss: 40.620216. Entropy: 0.485876.\n",
      "Iteration 5423: Policy loss: -0.129518. Value loss: 21.371609. Entropy: 0.467499.\n",
      "Iteration 5424: Policy loss: 0.255580. Value loss: 16.367765. Entropy: 0.466644.\n",
      "episode: 2363   score: 160.0  epsilon: 1.0    steps: 119  evaluation reward: 210.35\n",
      "episode: 2364   score: 120.0  epsilon: 1.0    steps: 594  evaluation reward: 207.6\n",
      "episode: 2365   score: 75.0  epsilon: 1.0    steps: 686  evaluation reward: 205.75\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5425: Policy loss: 1.070809. Value loss: 34.777058. Entropy: 0.553231.\n",
      "Iteration 5426: Policy loss: 0.869913. Value loss: 18.653238. Entropy: 0.516733.\n",
      "Iteration 5427: Policy loss: 0.864830. Value loss: 14.811070. Entropy: 0.517252.\n",
      "episode: 2366   score: 185.0  epsilon: 1.0    steps: 145  evaluation reward: 205.3\n",
      "episode: 2367   score: 160.0  epsilon: 1.0    steps: 424  evaluation reward: 205.4\n",
      "episode: 2368   score: 375.0  epsilon: 1.0    steps: 934  evaluation reward: 207.9\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5428: Policy loss: 0.644298. Value loss: 12.959665. Entropy: 0.823427.\n",
      "Iteration 5429: Policy loss: 0.660785. Value loss: 7.804621. Entropy: 0.813202.\n",
      "Iteration 5430: Policy loss: 0.560866. Value loss: 6.524595. Entropy: 0.806946.\n",
      "episode: 2369   score: 185.0  epsilon: 1.0    steps: 845  evaluation reward: 207.95\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5431: Policy loss: 0.582593. Value loss: 24.817253. Entropy: 0.789018.\n",
      "Iteration 5432: Policy loss: 0.732548. Value loss: 15.893845. Entropy: 0.793523.\n",
      "Iteration 5433: Policy loss: 0.511040. Value loss: 15.025303. Entropy: 0.803351.\n",
      "episode: 2370   score: 265.0  epsilon: 1.0    steps: 379  evaluation reward: 209.55\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5434: Policy loss: -0.795915. Value loss: 24.261742. Entropy: 0.752258.\n",
      "Iteration 5435: Policy loss: -0.343080. Value loss: 13.311964. Entropy: 0.767004.\n",
      "Iteration 5436: Policy loss: -0.578120. Value loss: 11.080983. Entropy: 0.764720.\n",
      "episode: 2371   score: 105.0  epsilon: 1.0    steps: 622  evaluation reward: 209.55\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5437: Policy loss: 2.230353. Value loss: 28.150684. Entropy: 0.430892.\n",
      "Iteration 5438: Policy loss: 2.272302. Value loss: 14.546882. Entropy: 0.396691.\n",
      "Iteration 5439: Policy loss: 1.978065. Value loss: 12.431268. Entropy: 0.434001.\n",
      "episode: 2372   score: 240.0  epsilon: 1.0    steps: 123  evaluation reward: 210.15\n",
      "episode: 2373   score: 70.0  epsilon: 1.0    steps: 894  evaluation reward: 209.75\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5440: Policy loss: -0.766551. Value loss: 26.624327. Entropy: 0.537149.\n",
      "Iteration 5441: Policy loss: -0.674631. Value loss: 17.534948. Entropy: 0.532705.\n",
      "Iteration 5442: Policy loss: -0.835036. Value loss: 15.190145. Entropy: 0.549989.\n",
      "episode: 2374   score: 155.0  epsilon: 1.0    steps: 214  evaluation reward: 208.4\n",
      "episode: 2375   score: 55.0  epsilon: 1.0    steps: 347  evaluation reward: 207.15\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5443: Policy loss: -1.312890. Value loss: 18.506283. Entropy: 0.726587.\n",
      "Iteration 5444: Policy loss: -1.237643. Value loss: 10.710892. Entropy: 0.738756.\n",
      "Iteration 5445: Policy loss: -1.292554. Value loss: 7.322529. Entropy: 0.714544.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5446: Policy loss: -0.242686. Value loss: 29.921820. Entropy: 0.682448.\n",
      "Iteration 5447: Policy loss: -0.196015. Value loss: 17.393335. Entropy: 0.659675.\n",
      "Iteration 5448: Policy loss: -0.083030. Value loss: 13.175156. Entropy: 0.670458.\n",
      "episode: 2376   score: 240.0  epsilon: 1.0    steps: 491  evaluation reward: 204.15\n",
      "episode: 2377   score: 265.0  epsilon: 1.0    steps: 710  evaluation reward: 204.5\n",
      "episode: 2378   score: 80.0  epsilon: 1.0    steps: 895  evaluation reward: 204.55\n",
      "episode: 2379   score: 225.0  epsilon: 1.0    steps: 899  evaluation reward: 204.85\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5449: Policy loss: 0.175372. Value loss: 16.999170. Entropy: 0.806423.\n",
      "Iteration 5450: Policy loss: 0.032607. Value loss: 9.711875. Entropy: 0.777073.\n",
      "Iteration 5451: Policy loss: 0.106705. Value loss: 8.086752. Entropy: 0.769156.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5452: Policy loss: 0.192491. Value loss: 14.410005. Entropy: 0.691122.\n",
      "Iteration 5453: Policy loss: 0.209184. Value loss: 7.884151. Entropy: 0.667871.\n",
      "Iteration 5454: Policy loss: -0.082299. Value loss: 6.481414. Entropy: 0.662937.\n",
      "episode: 2380   score: 140.0  epsilon: 1.0    steps: 18  evaluation reward: 204.15\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5455: Policy loss: -0.599540. Value loss: 28.371338. Entropy: 0.716974.\n",
      "Iteration 5456: Policy loss: -0.860282. Value loss: 17.988316. Entropy: 0.707903.\n",
      "Iteration 5457: Policy loss: -0.558241. Value loss: 14.405954. Entropy: 0.692000.\n",
      "episode: 2381   score: 135.0  epsilon: 1.0    steps: 129  evaluation reward: 203.7\n",
      "episode: 2382   score: 80.0  epsilon: 1.0    steps: 1019  evaluation reward: 203.75\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5458: Policy loss: 0.507599. Value loss: 23.532782. Entropy: 0.647548.\n",
      "Iteration 5459: Policy loss: 0.590764. Value loss: 15.415874. Entropy: 0.645634.\n",
      "Iteration 5460: Policy loss: 0.535789. Value loss: 11.604578. Entropy: 0.644192.\n",
      "episode: 2383   score: 285.0  epsilon: 1.0    steps: 616  evaluation reward: 205.05\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5461: Policy loss: -0.803484. Value loss: 27.713768. Entropy: 0.431811.\n",
      "Iteration 5462: Policy loss: -0.757076. Value loss: 14.577294. Entropy: 0.391097.\n",
      "Iteration 5463: Policy loss: -0.946765. Value loss: 9.664543. Entropy: 0.361006.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5464: Policy loss: 0.605716. Value loss: 36.048855. Entropy: 0.538321.\n",
      "Iteration 5465: Policy loss: 0.749208. Value loss: 16.973351. Entropy: 0.567251.\n",
      "Iteration 5466: Policy loss: 0.892786. Value loss: 13.944813. Entropy: 0.557726.\n",
      "episode: 2384   score: 215.0  epsilon: 1.0    steps: 655  evaluation reward: 205.4\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5467: Policy loss: 1.056324. Value loss: 38.817162. Entropy: 0.390761.\n",
      "Iteration 5468: Policy loss: 1.795569. Value loss: 21.950350. Entropy: 0.372517.\n",
      "Iteration 5469: Policy loss: 1.237732. Value loss: 15.082289. Entropy: 0.368423.\n",
      "episode: 2385   score: 105.0  epsilon: 1.0    steps: 77  evaluation reward: 205.2\n",
      "episode: 2386   score: 260.0  epsilon: 1.0    steps: 274  evaluation reward: 206.0\n",
      "episode: 2387   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 204.3\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5470: Policy loss: 1.339580. Value loss: 30.452221. Entropy: 0.406997.\n",
      "Iteration 5471: Policy loss: 1.362857. Value loss: 15.222391. Entropy: 0.403547.\n",
      "Iteration 5472: Policy loss: 1.215362. Value loss: 11.490977. Entropy: 0.394176.\n",
      "episode: 2388   score: 110.0  epsilon: 1.0    steps: 152  evaluation reward: 203.85\n",
      "episode: 2389   score: 345.0  epsilon: 1.0    steps: 889  evaluation reward: 205.2\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5473: Policy loss: 0.259360. Value loss: 15.687057. Entropy: 0.553535.\n",
      "Iteration 5474: Policy loss: 0.231723. Value loss: 8.765855. Entropy: 0.534602.\n",
      "Iteration 5475: Policy loss: 0.178336. Value loss: 6.952680. Entropy: 0.557035.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5476: Policy loss: 1.871498. Value loss: 17.600098. Entropy: 0.618973.\n",
      "Iteration 5477: Policy loss: 1.660206. Value loss: 11.537630. Entropy: 0.601848.\n",
      "Iteration 5478: Policy loss: 1.853088. Value loss: 7.928638. Entropy: 0.612501.\n",
      "episode: 2390   score: 440.0  epsilon: 1.0    steps: 388  evaluation reward: 207.3\n",
      "episode: 2391   score: 120.0  epsilon: 1.0    steps: 550  evaluation reward: 206.4\n",
      "episode: 2392   score: 120.0  epsilon: 1.0    steps: 694  evaluation reward: 205.05\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5479: Policy loss: 0.913633. Value loss: 15.451155. Entropy: 0.486598.\n",
      "Iteration 5480: Policy loss: 0.827225. Value loss: 10.869406. Entropy: 0.487177.\n",
      "Iteration 5481: Policy loss: 0.881308. Value loss: 9.596889. Entropy: 0.502629.\n",
      "episode: 2393   score: 155.0  epsilon: 1.0    steps: 341  evaluation reward: 205.5\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5482: Policy loss: -0.951122. Value loss: 21.024389. Entropy: 0.395406.\n",
      "Iteration 5483: Policy loss: -0.864766. Value loss: 13.573565. Entropy: 0.393887.\n",
      "Iteration 5484: Policy loss: -0.990525. Value loss: 13.025823. Entropy: 0.419227.\n",
      "episode: 2394   score: 215.0  epsilon: 1.0    steps: 125  evaluation reward: 204.15\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5485: Policy loss: 0.551041. Value loss: 24.589420. Entropy: 0.601216.\n",
      "Iteration 5486: Policy loss: 0.608581. Value loss: 11.739237. Entropy: 0.576444.\n",
      "Iteration 5487: Policy loss: 0.560777. Value loss: 10.335053. Entropy: 0.596714.\n",
      "episode: 2395   score: 180.0  epsilon: 1.0    steps: 169  evaluation reward: 204.9\n",
      "episode: 2396   score: 180.0  epsilon: 1.0    steps: 814  evaluation reward: 204.4\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5488: Policy loss: -0.431448. Value loss: 9.578845. Entropy: 0.505061.\n",
      "Iteration 5489: Policy loss: -0.503860. Value loss: 7.661478. Entropy: 0.492585.\n",
      "Iteration 5490: Policy loss: -0.390688. Value loss: 5.172995. Entropy: 0.499999.\n",
      "episode: 2397   score: 210.0  epsilon: 1.0    steps: 495  evaluation reward: 202.4\n",
      "episode: 2398   score: 120.0  epsilon: 1.0    steps: 601  evaluation reward: 201.5\n",
      "episode: 2399   score: 185.0  epsilon: 1.0    steps: 764  evaluation reward: 201.65\n",
      "episode: 2400   score: 285.0  epsilon: 1.0    steps: 988  evaluation reward: 203.25\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5491: Policy loss: -0.711237. Value loss: 31.239174. Entropy: 0.289233.\n",
      "Iteration 5492: Policy loss: -0.706965. Value loss: 19.880711. Entropy: 0.294120.\n",
      "Iteration 5493: Policy loss: -0.523202. Value loss: 18.021967. Entropy: 0.291783.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5494: Policy loss: -0.766483. Value loss: 15.432636. Entropy: 0.517170.\n",
      "Iteration 5495: Policy loss: -0.783599. Value loss: 8.436028. Entropy: 0.534484.\n",
      "Iteration 5496: Policy loss: -0.872529. Value loss: 7.374626. Entropy: 0.542089.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5497: Policy loss: -1.158067. Value loss: 24.620932. Entropy: 0.394364.\n",
      "Iteration 5498: Policy loss: -1.309592. Value loss: 20.056162. Entropy: 0.399616.\n",
      "Iteration 5499: Policy loss: -1.159382. Value loss: 15.648303. Entropy: 0.387901.\n",
      "now time :  2019-02-25 20:22:46.710239\n",
      "episode: 2401   score: 255.0  epsilon: 1.0    steps: 310  evaluation reward: 204.0\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5500: Policy loss: -1.846418. Value loss: 24.844830. Entropy: 0.550048.\n",
      "Iteration 5501: Policy loss: -1.795884. Value loss: 19.094162. Entropy: 0.555014.\n",
      "Iteration 5502: Policy loss: -1.790193. Value loss: 13.385940. Entropy: 0.555353.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5503: Policy loss: -2.420532. Value loss: 21.014435. Entropy: 0.368857.\n",
      "Iteration 5504: Policy loss: -2.229310. Value loss: 15.383204. Entropy: 0.363568.\n",
      "Iteration 5505: Policy loss: -2.492147. Value loss: 12.587310. Entropy: 0.375469.\n",
      "episode: 2402   score: 330.0  epsilon: 1.0    steps: 102  evaluation reward: 205.0\n",
      "episode: 2403   score: 255.0  epsilon: 1.0    steps: 138  evaluation reward: 202.55\n",
      "episode: 2404   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 202.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2405   score: 180.0  epsilon: 1.0    steps: 721  evaluation reward: 199.75\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5506: Policy loss: 0.179903. Value loss: 24.028978. Entropy: 0.498994.\n",
      "Iteration 5507: Policy loss: 0.196385. Value loss: 13.168758. Entropy: 0.501572.\n",
      "Iteration 5508: Policy loss: 0.084301. Value loss: 9.911963. Entropy: 0.496589.\n",
      "episode: 2406   score: 215.0  epsilon: 1.0    steps: 436  evaluation reward: 199.8\n",
      "episode: 2407   score: 315.0  epsilon: 1.0    steps: 783  evaluation reward: 200.3\n",
      "episode: 2408   score: 210.0  epsilon: 1.0    steps: 931  evaluation reward: 200.6\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5509: Policy loss: 0.478029. Value loss: 8.067687. Entropy: 0.470395.\n",
      "Iteration 5510: Policy loss: 0.465948. Value loss: 5.662351. Entropy: 0.442598.\n",
      "Iteration 5511: Policy loss: 0.773340. Value loss: 4.618909. Entropy: 0.441243.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5512: Policy loss: -0.877419. Value loss: 26.926205. Entropy: 0.513056.\n",
      "Iteration 5513: Policy loss: -0.530095. Value loss: 17.842319. Entropy: 0.500286.\n",
      "Iteration 5514: Policy loss: -1.018014. Value loss: 16.200748. Entropy: 0.512411.\n",
      "episode: 2409   score: 120.0  epsilon: 1.0    steps: 610  evaluation reward: 197.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5515: Policy loss: -1.394868. Value loss: 21.385305. Entropy: 0.621302.\n",
      "Iteration 5516: Policy loss: -1.507844. Value loss: 14.706183. Entropy: 0.615607.\n",
      "Iteration 5517: Policy loss: -1.707709. Value loss: 11.380131. Entropy: 0.607655.\n",
      "episode: 2410   score: 105.0  epsilon: 1.0    steps: 23  evaluation reward: 196.55\n",
      "episode: 2411   score: 180.0  epsilon: 1.0    steps: 187  evaluation reward: 197.3\n",
      "episode: 2412   score: 285.0  epsilon: 1.0    steps: 330  evaluation reward: 195.95\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5518: Policy loss: -0.510015. Value loss: 14.601406. Entropy: 0.427024.\n",
      "Iteration 5519: Policy loss: -0.541227. Value loss: 8.798780. Entropy: 0.420082.\n",
      "Iteration 5520: Policy loss: -0.615235. Value loss: 7.632303. Entropy: 0.434346.\n",
      "episode: 2413   score: 150.0  epsilon: 1.0    steps: 433  evaluation reward: 195.3\n",
      "episode: 2414   score: 180.0  epsilon: 1.0    steps: 678  evaluation reward: 194.5\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5521: Policy loss: -0.682697. Value loss: 14.541971. Entropy: 0.282475.\n",
      "Iteration 5522: Policy loss: -0.565579. Value loss: 9.807328. Entropy: 0.287187.\n",
      "Iteration 5523: Policy loss: -0.628724. Value loss: 8.097157. Entropy: 0.296041.\n",
      "episode: 2415   score: 260.0  epsilon: 1.0    steps: 860  evaluation reward: 194.45\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5524: Policy loss: 0.237444. Value loss: 21.683372. Entropy: 0.584003.\n",
      "Iteration 5525: Policy loss: 0.320938. Value loss: 14.060950. Entropy: 0.549014.\n",
      "Iteration 5526: Policy loss: 0.295989. Value loss: 15.093592. Entropy: 0.576592.\n",
      "episode: 2416   score: 120.0  epsilon: 1.0    steps: 537  evaluation reward: 193.2\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5527: Policy loss: -0.806902. Value loss: 15.520078. Entropy: 0.481164.\n",
      "Iteration 5528: Policy loss: -0.467374. Value loss: 8.804936. Entropy: 0.500769.\n",
      "Iteration 5529: Policy loss: -0.652344. Value loss: 6.942046. Entropy: 0.508388.\n",
      "episode: 2417   score: 210.0  epsilon: 1.0    steps: 94  evaluation reward: 193.5\n",
      "episode: 2418   score: 365.0  epsilon: 1.0    steps: 1014  evaluation reward: 195.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5530: Policy loss: -0.521630. Value loss: 19.609596. Entropy: 0.328160.\n",
      "Iteration 5531: Policy loss: -0.813654. Value loss: 12.172647. Entropy: 0.319748.\n",
      "Iteration 5532: Policy loss: -0.822062. Value loss: 10.737240. Entropy: 0.309035.\n",
      "episode: 2419   score: 180.0  epsilon: 1.0    steps: 274  evaluation reward: 196.2\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5533: Policy loss: 0.438167. Value loss: 15.775416. Entropy: 0.514804.\n",
      "Iteration 5534: Policy loss: 0.466019. Value loss: 9.545258. Entropy: 0.502963.\n",
      "Iteration 5535: Policy loss: 0.441834. Value loss: 7.591377. Entropy: 0.491316.\n",
      "episode: 2420   score: 210.0  epsilon: 1.0    steps: 435  evaluation reward: 196.0\n",
      "episode: 2421   score: 210.0  epsilon: 1.0    steps: 655  evaluation reward: 196.3\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5536: Policy loss: 1.008816. Value loss: 9.050362. Entropy: 0.287459.\n",
      "Iteration 5537: Policy loss: 0.961782. Value loss: 6.942254. Entropy: 0.298496.\n",
      "Iteration 5538: Policy loss: 0.952050. Value loss: 6.173711. Entropy: 0.285312.\n",
      "episode: 2422   score: 180.0  epsilon: 1.0    steps: 626  evaluation reward: 196.55\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5539: Policy loss: 0.287107. Value loss: 14.544521. Entropy: 0.519068.\n",
      "Iteration 5540: Policy loss: 0.375812. Value loss: 9.560056. Entropy: 0.497192.\n",
      "Iteration 5541: Policy loss: 0.343215. Value loss: 8.422238. Entropy: 0.517705.\n",
      "episode: 2423   score: 225.0  epsilon: 1.0    steps: 817  evaluation reward: 196.45\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5542: Policy loss: 1.033183. Value loss: 17.908953. Entropy: 0.479141.\n",
      "Iteration 5543: Policy loss: 1.008417. Value loss: 9.171394. Entropy: 0.500569.\n",
      "Iteration 5544: Policy loss: 1.125172. Value loss: 7.632664. Entropy: 0.495600.\n",
      "episode: 2424   score: 180.0  epsilon: 1.0    steps: 71  evaluation reward: 196.15\n",
      "episode: 2425   score: 260.0  epsilon: 1.0    steps: 198  evaluation reward: 197.7\n",
      "episode: 2426   score: 210.0  epsilon: 1.0    steps: 373  evaluation reward: 199.25\n",
      "episode: 2427   score: 210.0  epsilon: 1.0    steps: 967  evaluation reward: 199.55\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5545: Policy loss: 0.981933. Value loss: 19.389626. Entropy: 0.209977.\n",
      "Iteration 5546: Policy loss: 0.849944. Value loss: 14.718235. Entropy: 0.180806.\n",
      "Iteration 5547: Policy loss: 0.954588. Value loss: 10.926764. Entropy: 0.186867.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5548: Policy loss: -0.390600. Value loss: 16.420578. Entropy: 0.484186.\n",
      "Iteration 5549: Policy loss: -0.240809. Value loss: 9.858648. Entropy: 0.487341.\n",
      "Iteration 5550: Policy loss: -0.392056. Value loss: 9.699809. Entropy: 0.495401.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5551: Policy loss: -1.425970. Value loss: 23.304737. Entropy: 0.537198.\n",
      "Iteration 5552: Policy loss: -1.486741. Value loss: 14.491172. Entropy: 0.522137.\n",
      "Iteration 5553: Policy loss: -1.452646. Value loss: 11.350142. Entropy: 0.541803.\n",
      "episode: 2428   score: 135.0  epsilon: 1.0    steps: 822  evaluation reward: 199.1\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5554: Policy loss: 0.543963. Value loss: 18.818327. Entropy: 0.318914.\n",
      "Iteration 5555: Policy loss: 0.400794. Value loss: 9.786274. Entropy: 0.327197.\n",
      "Iteration 5556: Policy loss: 0.483668. Value loss: 8.183449. Entropy: 0.316511.\n",
      "episode: 2429   score: 260.0  epsilon: 1.0    steps: 505  evaluation reward: 199.6\n",
      "episode: 2430   score: 215.0  epsilon: 1.0    steps: 589  evaluation reward: 200.25\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5557: Policy loss: 1.024510. Value loss: 23.307062. Entropy: 0.302502.\n",
      "Iteration 5558: Policy loss: 1.018657. Value loss: 15.462421. Entropy: 0.316112.\n",
      "Iteration 5559: Policy loss: 1.172565. Value loss: 9.468962. Entropy: 0.323315.\n",
      "episode: 2431   score: 155.0  epsilon: 1.0    steps: 901  evaluation reward: 199.65\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5560: Policy loss: 0.157086. Value loss: 12.843510. Entropy: 0.480580.\n",
      "Iteration 5561: Policy loss: 0.074366. Value loss: 7.289804. Entropy: 0.490107.\n",
      "Iteration 5562: Policy loss: 0.115332. Value loss: 6.763142. Entropy: 0.473598.\n",
      "episode: 2432   score: 210.0  epsilon: 1.0    steps: 266  evaluation reward: 199.65\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5563: Policy loss: -0.982136. Value loss: 18.654009. Entropy: 0.391230.\n",
      "Iteration 5564: Policy loss: -1.132087. Value loss: 10.880531. Entropy: 0.405558.\n",
      "Iteration 5565: Policy loss: -1.079120. Value loss: 8.984390. Entropy: 0.416160.\n",
      "episode: 2433   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 198.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2434   score: 285.0  epsilon: 1.0    steps: 204  evaluation reward: 199.1\n",
      "episode: 2435   score: 105.0  epsilon: 1.0    steps: 588  evaluation reward: 198.65\n",
      "episode: 2436   score: 395.0  epsilon: 1.0    steps: 751  evaluation reward: 201.8\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5566: Policy loss: 1.215006. Value loss: 17.552172. Entropy: 0.326226.\n",
      "Iteration 5567: Policy loss: 1.207279. Value loss: 11.640192. Entropy: 0.309669.\n",
      "Iteration 5568: Policy loss: 1.298972. Value loss: 10.010635. Entropy: 0.315512.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5569: Policy loss: 0.033236. Value loss: 12.350272. Entropy: 0.550036.\n",
      "Iteration 5570: Policy loss: -0.108268. Value loss: 8.468417. Entropy: 0.579058.\n",
      "Iteration 5571: Policy loss: 0.028675. Value loss: 7.388412. Entropy: 0.556710.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5572: Policy loss: -0.438170. Value loss: 19.143467. Entropy: 0.324644.\n",
      "Iteration 5573: Policy loss: -0.370774. Value loss: 13.933099. Entropy: 0.332693.\n",
      "Iteration 5574: Policy loss: -0.494963. Value loss: 13.411099. Entropy: 0.330710.\n",
      "episode: 2437   score: 105.0  epsilon: 1.0    steps: 267  evaluation reward: 202.1\n",
      "episode: 2438   score: 260.0  epsilon: 1.0    steps: 1002  evaluation reward: 202.9\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5575: Policy loss: 1.253665. Value loss: 24.937979. Entropy: 0.467456.\n",
      "Iteration 5576: Policy loss: 1.186208. Value loss: 13.147004. Entropy: 0.443704.\n",
      "Iteration 5577: Policy loss: 1.227100. Value loss: 10.976858. Entropy: 0.448214.\n",
      "episode: 2439   score: 290.0  epsilon: 1.0    steps: 807  evaluation reward: 200.95\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5578: Policy loss: -0.071443. Value loss: 14.080022. Entropy: 0.297049.\n",
      "Iteration 5579: Policy loss: 0.082296. Value loss: 9.578898. Entropy: 0.324776.\n",
      "Iteration 5580: Policy loss: -0.033153. Value loss: 7.368406. Entropy: 0.320524.\n",
      "episode: 2440   score: 210.0  epsilon: 1.0    steps: 253  evaluation reward: 200.95\n",
      "episode: 2441   score: 180.0  epsilon: 1.0    steps: 737  evaluation reward: 200.65\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5581: Policy loss: -0.075401. Value loss: 17.742893. Entropy: 0.355779.\n",
      "Iteration 5582: Policy loss: -0.053747. Value loss: 11.939539. Entropy: 0.345023.\n",
      "Iteration 5583: Policy loss: -0.235827. Value loss: 10.441051. Entropy: 0.348477.\n",
      "episode: 2442   score: 105.0  epsilon: 1.0    steps: 270  evaluation reward: 199.6\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5584: Policy loss: -0.739913. Value loss: 13.148749. Entropy: 0.355210.\n",
      "Iteration 5585: Policy loss: -0.537524. Value loss: 5.472600. Entropy: 0.366465.\n",
      "Iteration 5586: Policy loss: -0.665345. Value loss: 3.901214. Entropy: 0.357854.\n",
      "episode: 2443   score: 260.0  epsilon: 1.0    steps: 93  evaluation reward: 200.1\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5587: Policy loss: -0.155075. Value loss: 19.113054. Entropy: 0.298751.\n",
      "Iteration 5588: Policy loss: -0.327887. Value loss: 11.660659. Entropy: 0.325258.\n",
      "Iteration 5589: Policy loss: -0.238610. Value loss: 12.268500. Entropy: 0.331397.\n",
      "episode: 2444   score: 390.0  epsilon: 1.0    steps: 427  evaluation reward: 202.2\n",
      "episode: 2445   score: 240.0  epsilon: 1.0    steps: 537  evaluation reward: 202.5\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5590: Policy loss: 0.351509. Value loss: 13.698060. Entropy: 0.441264.\n",
      "Iteration 5591: Policy loss: 0.343361. Value loss: 8.184389. Entropy: 0.497588.\n",
      "Iteration 5592: Policy loss: 0.354092. Value loss: 5.914945. Entropy: 0.493051.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5593: Policy loss: -0.276452. Value loss: 16.450422. Entropy: 0.343604.\n",
      "Iteration 5594: Policy loss: -0.116718. Value loss: 9.301903. Entropy: 0.337911.\n",
      "Iteration 5595: Policy loss: -0.184323. Value loss: 7.959926. Entropy: 0.335730.\n",
      "episode: 2446   score: 155.0  epsilon: 1.0    steps: 220  evaluation reward: 202.25\n",
      "episode: 2447   score: 345.0  epsilon: 1.0    steps: 888  evaluation reward: 204.9\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5596: Policy loss: 0.136930. Value loss: 31.972778. Entropy: 0.399158.\n",
      "Iteration 5597: Policy loss: 0.234733. Value loss: 11.449980. Entropy: 0.421398.\n",
      "Iteration 5598: Policy loss: 0.032852. Value loss: 9.678732. Entropy: 0.419673.\n",
      "episode: 2448   score: 255.0  epsilon: 1.0    steps: 683  evaluation reward: 204.9\n",
      "episode: 2449   score: 260.0  epsilon: 1.0    steps: 973  evaluation reward: 205.4\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5599: Policy loss: -0.151077. Value loss: 11.926812. Entropy: 0.373038.\n",
      "Iteration 5600: Policy loss: -0.259551. Value loss: 5.981857. Entropy: 0.374755.\n",
      "Iteration 5601: Policy loss: -0.109248. Value loss: 3.918312. Entropy: 0.392538.\n",
      "episode: 2450   score: 155.0  epsilon: 1.0    steps: 59  evaluation reward: 203.9\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5602: Policy loss: 1.557594. Value loss: 23.431505. Entropy: 0.266037.\n",
      "Iteration 5603: Policy loss: 1.602990. Value loss: 16.739183. Entropy: 0.295597.\n",
      "Iteration 5604: Policy loss: 1.279842. Value loss: 16.110216. Entropy: 0.288926.\n",
      "now time :  2019-02-25 20:24:44.636606\n",
      "episode: 2451   score: 285.0  epsilon: 1.0    steps: 257  evaluation reward: 201.65\n",
      "episode: 2452   score: 180.0  epsilon: 1.0    steps: 402  evaluation reward: 202.9\n",
      "episode: 2453   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 203.05\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5605: Policy loss: 1.466304. Value loss: 10.359382. Entropy: 0.506301.\n",
      "Iteration 5606: Policy loss: 1.507657. Value loss: 6.187443. Entropy: 0.530604.\n",
      "Iteration 5607: Policy loss: 1.345709. Value loss: 6.403211. Entropy: 0.538070.\n",
      "episode: 2454   score: 110.0  epsilon: 1.0    steps: 231  evaluation reward: 201.4\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5608: Policy loss: 2.618703. Value loss: 19.387394. Entropy: 0.447134.\n",
      "Iteration 5609: Policy loss: 2.681612. Value loss: 10.815610. Entropy: 0.447614.\n",
      "Iteration 5610: Policy loss: 2.515006. Value loss: 8.917822. Entropy: 0.464282.\n",
      "episode: 2455   score: 105.0  epsilon: 1.0    steps: 708  evaluation reward: 200.85\n",
      "episode: 2456   score: 210.0  epsilon: 1.0    steps: 893  evaluation reward: 201.55\n",
      "episode: 2457   score: 105.0  epsilon: 1.0    steps: 998  evaluation reward: 200.8\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5611: Policy loss: 1.904944. Value loss: 17.234097. Entropy: 0.483053.\n",
      "Iteration 5612: Policy loss: 1.865071. Value loss: 8.782769. Entropy: 0.551090.\n",
      "Iteration 5613: Policy loss: 1.763839. Value loss: 8.248683. Entropy: 0.586292.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5614: Policy loss: 1.665410. Value loss: 13.265748. Entropy: 0.468663.\n",
      "Iteration 5615: Policy loss: 2.122800. Value loss: 7.180222. Entropy: 0.494528.\n",
      "Iteration 5616: Policy loss: 1.782200. Value loss: 5.214721. Entropy: 0.466810.\n",
      "episode: 2458   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 200.55\n",
      "episode: 2459   score: 180.0  epsilon: 1.0    steps: 507  evaluation reward: 200.25\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5617: Policy loss: 2.034039. Value loss: 25.687466. Entropy: 0.369043.\n",
      "Iteration 5618: Policy loss: 1.877564. Value loss: 15.505350. Entropy: 0.349643.\n",
      "Iteration 5619: Policy loss: 1.980380. Value loss: 12.656144. Entropy: 0.378718.\n",
      "episode: 2460   score: 180.0  epsilon: 1.0    steps: 573  evaluation reward: 201.0\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5620: Policy loss: 1.128183. Value loss: 9.465937. Entropy: 0.548221.\n",
      "Iteration 5621: Policy loss: 0.911474. Value loss: 5.039584. Entropy: 0.579321.\n",
      "Iteration 5622: Policy loss: 0.963396. Value loss: 3.325394. Entropy: 0.581690.\n",
      "episode: 2461   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 201.0\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5623: Policy loss: 1.871784. Value loss: 11.242078. Entropy: 0.615214.\n",
      "Iteration 5624: Policy loss: 1.869894. Value loss: 6.620200. Entropy: 0.624001.\n",
      "Iteration 5625: Policy loss: 1.879455. Value loss: 5.389533. Entropy: 0.620560.\n",
      "episode: 2462   score: 105.0  epsilon: 1.0    steps: 732  evaluation reward: 199.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5626: Policy loss: 1.354437. Value loss: 19.591219. Entropy: 0.473332.\n",
      "Iteration 5627: Policy loss: 1.325829. Value loss: 9.316572. Entropy: 0.493891.\n",
      "Iteration 5628: Policy loss: 1.441732. Value loss: 6.952620. Entropy: 0.491906.\n",
      "episode: 2463   score: 180.0  epsilon: 1.0    steps: 157  evaluation reward: 199.9\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5629: Policy loss: 0.929460. Value loss: 11.777409. Entropy: 0.469686.\n",
      "Iteration 5630: Policy loss: 0.875056. Value loss: 6.036533. Entropy: 0.482646.\n",
      "Iteration 5631: Policy loss: 0.929494. Value loss: 4.131516. Entropy: 0.487149.\n",
      "episode: 2464   score: 115.0  epsilon: 1.0    steps: 829  evaluation reward: 199.85\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5632: Policy loss: 0.578521. Value loss: 13.689707. Entropy: 0.470765.\n",
      "Iteration 5633: Policy loss: 0.429023. Value loss: 7.797975. Entropy: 0.489150.\n",
      "Iteration 5634: Policy loss: 0.301854. Value loss: 5.888644. Entropy: 0.477765.\n",
      "episode: 2465   score: 185.0  epsilon: 1.0    steps: 950  evaluation reward: 200.95\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5635: Policy loss: -0.085892. Value loss: 12.288304. Entropy: 0.459299.\n",
      "Iteration 5636: Policy loss: 0.037971. Value loss: 7.163601. Entropy: 0.475302.\n",
      "Iteration 5637: Policy loss: -0.228233. Value loss: 5.001069. Entropy: 0.468773.\n",
      "episode: 2466   score: 210.0  epsilon: 1.0    steps: 434  evaluation reward: 201.2\n",
      "episode: 2467   score: 240.0  epsilon: 1.0    steps: 616  evaluation reward: 202.0\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5638: Policy loss: -1.110325. Value loss: 14.262815. Entropy: 0.547197.\n",
      "Iteration 5639: Policy loss: -0.835111. Value loss: 8.743999. Entropy: 0.541859.\n",
      "Iteration 5640: Policy loss: -0.937404. Value loss: 7.058392. Entropy: 0.538072.\n",
      "episode: 2468   score: 180.0  epsilon: 1.0    steps: 378  evaluation reward: 200.05\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5641: Policy loss: -0.383646. Value loss: 15.855946. Entropy: 0.559424.\n",
      "Iteration 5642: Policy loss: -0.440904. Value loss: 7.992476. Entropy: 0.559448.\n",
      "Iteration 5643: Policy loss: -0.432417. Value loss: 5.574956. Entropy: 0.552162.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5644: Policy loss: 0.685827. Value loss: 17.580393. Entropy: 0.441400.\n",
      "Iteration 5645: Policy loss: 0.575865. Value loss: 8.580832. Entropy: 0.427604.\n",
      "Iteration 5646: Policy loss: 0.713079. Value loss: 7.838225. Entropy: 0.434148.\n",
      "episode: 2469   score: 365.0  epsilon: 1.0    steps: 21  evaluation reward: 201.85\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5647: Policy loss: 1.213578. Value loss: 9.526701. Entropy: 0.606225.\n",
      "Iteration 5648: Policy loss: 1.394632. Value loss: 4.661850. Entropy: 0.633169.\n",
      "Iteration 5649: Policy loss: 1.167123. Value loss: 3.975999. Entropy: 0.619078.\n",
      "episode: 2470   score: 135.0  epsilon: 1.0    steps: 131  evaluation reward: 200.55\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5650: Policy loss: -0.071316. Value loss: 13.206182. Entropy: 0.546294.\n",
      "Iteration 5651: Policy loss: -0.342844. Value loss: 9.342525. Entropy: 0.538176.\n",
      "Iteration 5652: Policy loss: -0.245505. Value loss: 6.974669. Entropy: 0.530613.\n",
      "episode: 2471   score: 260.0  epsilon: 1.0    steps: 716  evaluation reward: 202.1\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5653: Policy loss: 0.295440. Value loss: 14.953098. Entropy: 0.580609.\n",
      "Iteration 5654: Policy loss: 0.021934. Value loss: 9.750059. Entropy: 0.606322.\n",
      "Iteration 5655: Policy loss: 0.243675. Value loss: 7.093303. Entropy: 0.606986.\n",
      "episode: 2472   score: 225.0  epsilon: 1.0    steps: 918  evaluation reward: 201.95\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5656: Policy loss: 0.032188. Value loss: 9.752198. Entropy: 0.626083.\n",
      "Iteration 5657: Policy loss: 0.041449. Value loss: 4.430290. Entropy: 0.619260.\n",
      "Iteration 5658: Policy loss: -0.006873. Value loss: 3.290980. Entropy: 0.620915.\n",
      "episode: 2473   score: 180.0  epsilon: 1.0    steps: 419  evaluation reward: 203.05\n",
      "episode: 2474   score: 125.0  epsilon: 1.0    steps: 517  evaluation reward: 202.75\n",
      "episode: 2475   score: 310.0  epsilon: 1.0    steps: 781  evaluation reward: 205.3\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5659: Policy loss: 1.029496. Value loss: 4.911716. Entropy: 0.519616.\n",
      "Iteration 5660: Policy loss: 1.016029. Value loss: 4.010760. Entropy: 0.525874.\n",
      "Iteration 5661: Policy loss: 0.988304. Value loss: 3.097456. Entropy: 0.534593.\n",
      "episode: 2476   score: 110.0  epsilon: 1.0    steps: 110  evaluation reward: 204.0\n",
      "episode: 2477   score: 110.0  epsilon: 1.0    steps: 299  evaluation reward: 202.45\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5662: Policy loss: 1.100026. Value loss: 8.321539. Entropy: 0.525910.\n",
      "Iteration 5663: Policy loss: 1.006371. Value loss: 5.311464. Entropy: 0.535433.\n",
      "Iteration 5664: Policy loss: 1.049101. Value loss: 5.532746. Entropy: 0.536424.\n",
      "episode: 2478   score: 105.0  epsilon: 1.0    steps: 176  evaluation reward: 202.7\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5665: Policy loss: 1.463399. Value loss: 7.539308. Entropy: 0.771734.\n",
      "Iteration 5666: Policy loss: 1.470237. Value loss: 3.397184. Entropy: 0.786090.\n",
      "Iteration 5667: Policy loss: 1.461105. Value loss: 3.590842. Entropy: 0.808387.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5668: Policy loss: 1.213488. Value loss: 2.971082. Entropy: 0.787176.\n",
      "Iteration 5669: Policy loss: 1.210377. Value loss: 1.417225. Entropy: 0.799348.\n",
      "Iteration 5670: Policy loss: 1.192881. Value loss: 2.500903. Entropy: 0.808452.\n",
      "episode: 2479   score: 105.0  epsilon: 1.0    steps: 689  evaluation reward: 201.5\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5671: Policy loss: 0.988823. Value loss: 7.040772. Entropy: 0.666748.\n",
      "Iteration 5672: Policy loss: 1.073324. Value loss: 4.269224. Entropy: 0.698506.\n",
      "Iteration 5673: Policy loss: 0.983859. Value loss: 3.651056. Entropy: 0.690375.\n",
      "episode: 2480   score: 140.0  epsilon: 1.0    steps: 977  evaluation reward: 201.5\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5674: Policy loss: -2.390253. Value loss: 259.461456. Entropy: 0.641378.\n",
      "Iteration 5675: Policy loss: -2.802520. Value loss: 210.997986. Entropy: 0.578858.\n",
      "Iteration 5676: Policy loss: -3.009196. Value loss: 182.279236. Entropy: 0.544329.\n",
      "episode: 2481   score: 210.0  epsilon: 1.0    steps: 492  evaluation reward: 202.25\n",
      "episode: 2482   score: 130.0  epsilon: 1.0    steps: 795  evaluation reward: 202.75\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5677: Policy loss: -0.036840. Value loss: 7.783954. Entropy: 0.442890.\n",
      "Iteration 5678: Policy loss: 0.101894. Value loss: 5.032525. Entropy: 0.451705.\n",
      "Iteration 5679: Policy loss: 0.132815. Value loss: 3.701310. Entropy: 0.446159.\n",
      "episode: 2483   score: 110.0  epsilon: 1.0    steps: 245  evaluation reward: 201.0\n",
      "episode: 2484   score: 180.0  epsilon: 1.0    steps: 362  evaluation reward: 200.65\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5680: Policy loss: 1.047458. Value loss: 11.565788. Entropy: 0.425096.\n",
      "Iteration 5681: Policy loss: 1.044580. Value loss: 7.588661. Entropy: 0.431208.\n",
      "Iteration 5682: Policy loss: 1.076775. Value loss: 6.779064. Entropy: 0.441429.\n",
      "episode: 2485   score: 155.0  epsilon: 1.0    steps: 25  evaluation reward: 201.15\n",
      "episode: 2486   score: 385.0  epsilon: 1.0    steps: 568  evaluation reward: 202.4\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5683: Policy loss: 0.265490. Value loss: 5.490757. Entropy: 0.497171.\n",
      "Iteration 5684: Policy loss: 0.239241. Value loss: 3.829900. Entropy: 0.485483.\n",
      "Iteration 5685: Policy loss: 0.247594. Value loss: 2.857383. Entropy: 0.462659.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5686: Policy loss: -0.708531. Value loss: 9.829626. Entropy: 0.505342.\n",
      "Iteration 5687: Policy loss: -0.643259. Value loss: 5.983346. Entropy: 0.526390.\n",
      "Iteration 5688: Policy loss: -0.691172. Value loss: 4.015211. Entropy: 0.497705.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5689: Policy loss: -1.526290. Value loss: 14.006255. Entropy: 0.590633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5690: Policy loss: -1.640856. Value loss: 8.338089. Entropy: 0.588648.\n",
      "Iteration 5691: Policy loss: -1.295399. Value loss: 9.022756. Entropy: 0.602259.\n",
      "episode: 2487   score: 180.0  epsilon: 1.0    steps: 674  evaluation reward: 203.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5692: Policy loss: -0.345383. Value loss: 14.203615. Entropy: 0.603910.\n",
      "Iteration 5693: Policy loss: -0.069751. Value loss: 6.912871. Entropy: 0.591386.\n",
      "Iteration 5694: Policy loss: -0.521674. Value loss: 4.714059. Entropy: 0.598091.\n",
      "episode: 2488   score: 210.0  epsilon: 1.0    steps: 918  evaluation reward: 204.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5695: Policy loss: -0.844088. Value loss: 9.836941. Entropy: 0.400666.\n",
      "Iteration 5696: Policy loss: -0.722080. Value loss: 5.878652. Entropy: 0.404271.\n",
      "Iteration 5697: Policy loss: -0.800555. Value loss: 5.759713. Entropy: 0.391063.\n",
      "episode: 2489   score: 155.0  epsilon: 1.0    steps: 412  evaluation reward: 202.25\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5698: Policy loss: -0.211494. Value loss: 15.293641. Entropy: 0.505048.\n",
      "Iteration 5699: Policy loss: -0.013987. Value loss: 8.277538. Entropy: 0.520653.\n",
      "Iteration 5700: Policy loss: -0.150024. Value loss: 6.202806. Entropy: 0.519096.\n",
      "episode: 2490   score: 210.0  epsilon: 1.0    steps: 203  evaluation reward: 199.95\n",
      "episode: 2491   score: 155.0  epsilon: 1.0    steps: 277  evaluation reward: 200.3\n",
      "episode: 2492   score: 270.0  epsilon: 1.0    steps: 856  evaluation reward: 201.8\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5701: Policy loss: 0.508529. Value loss: 9.058394. Entropy: 0.526059.\n",
      "Iteration 5702: Policy loss: 0.482982. Value loss: 6.530937. Entropy: 0.543761.\n",
      "Iteration 5703: Policy loss: 0.410223. Value loss: 4.743001. Entropy: 0.549738.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5704: Policy loss: -3.202685. Value loss: 331.270996. Entropy: 0.585419.\n",
      "Iteration 5705: Policy loss: -2.756414. Value loss: 92.843384. Entropy: 0.487720.\n",
      "Iteration 5706: Policy loss: -2.095769. Value loss: 89.564011. Entropy: 0.477329.\n",
      "episode: 2493   score: 110.0  epsilon: 1.0    steps: 922  evaluation reward: 201.35\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5707: Policy loss: -0.140498. Value loss: 9.488384. Entropy: 0.466648.\n",
      "Iteration 5708: Policy loss: -0.206815. Value loss: 7.250008. Entropy: 0.477663.\n",
      "Iteration 5709: Policy loss: -0.212895. Value loss: 7.049021. Entropy: 0.487446.\n",
      "episode: 2494   score: 210.0  epsilon: 1.0    steps: 747  evaluation reward: 201.3\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5710: Policy loss: -3.451306. Value loss: 23.959143. Entropy: 0.296911.\n",
      "Iteration 5711: Policy loss: -3.436857. Value loss: 13.499624. Entropy: 0.285826.\n",
      "Iteration 5712: Policy loss: -3.437110. Value loss: 11.277690. Entropy: 0.283979.\n",
      "episode: 2495   score: 350.0  epsilon: 1.0    steps: 114  evaluation reward: 203.0\n",
      "episode: 2496   score: 530.0  epsilon: 1.0    steps: 583  evaluation reward: 206.5\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5713: Policy loss: -1.418814. Value loss: 23.883852. Entropy: 0.374239.\n",
      "Iteration 5714: Policy loss: -1.349410. Value loss: 14.083854. Entropy: 0.380321.\n",
      "Iteration 5715: Policy loss: -1.343200. Value loss: 11.197371. Entropy: 0.381807.\n",
      "episode: 2497   score: 180.0  epsilon: 1.0    steps: 818  evaluation reward: 206.2\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5716: Policy loss: 0.503540. Value loss: 22.250805. Entropy: 0.371932.\n",
      "Iteration 5717: Policy loss: 0.272827. Value loss: 10.301832. Entropy: 0.378268.\n",
      "Iteration 5718: Policy loss: 0.117210. Value loss: 7.497254. Entropy: 0.378076.\n",
      "episode: 2498   score: 320.0  epsilon: 1.0    steps: 403  evaluation reward: 208.2\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5719: Policy loss: -0.433298. Value loss: 11.427711. Entropy: 0.309211.\n",
      "Iteration 5720: Policy loss: -0.248452. Value loss: 7.317181. Entropy: 0.299110.\n",
      "Iteration 5721: Policy loss: -0.451900. Value loss: 5.186109. Entropy: 0.306988.\n",
      "episode: 2499   score: 210.0  epsilon: 1.0    steps: 909  evaluation reward: 208.45\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5722: Policy loss: 0.660918. Value loss: 21.975023. Entropy: 0.489109.\n",
      "Iteration 5723: Policy loss: 0.784362. Value loss: 10.842190. Entropy: 0.462805.\n",
      "Iteration 5724: Policy loss: 0.723371. Value loss: 6.358340. Entropy: 0.469667.\n",
      "episode: 2500   score: 105.0  epsilon: 1.0    steps: 121  evaluation reward: 206.65\n",
      "now time :  2019-02-25 20:26:59.543155\n",
      "episode: 2501   score: 280.0  epsilon: 1.0    steps: 248  evaluation reward: 206.9\n",
      "episode: 2502   score: 210.0  epsilon: 1.0    steps: 766  evaluation reward: 205.7\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5725: Policy loss: 1.091875. Value loss: 31.334852. Entropy: 0.498822.\n",
      "Iteration 5726: Policy loss: 1.083632. Value loss: 14.482280. Entropy: 0.487767.\n",
      "Iteration 5727: Policy loss: 1.036395. Value loss: 10.112433. Entropy: 0.455669.\n",
      "episode: 2503   score: 210.0  epsilon: 1.0    steps: 586  evaluation reward: 205.25\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5728: Policy loss: -0.323192. Value loss: 20.204895. Entropy: 0.473523.\n",
      "Iteration 5729: Policy loss: -0.308044. Value loss: 9.109302. Entropy: 0.443911.\n",
      "Iteration 5730: Policy loss: -0.365398. Value loss: 7.709511. Entropy: 0.455870.\n",
      "episode: 2504   score: 180.0  epsilon: 1.0    steps: 815  evaluation reward: 205.25\n",
      "episode: 2505   score: 75.0  epsilon: 1.0    steps: 1008  evaluation reward: 204.2\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5731: Policy loss: 0.591376. Value loss: 32.502190. Entropy: 0.342374.\n",
      "Iteration 5732: Policy loss: 0.695393. Value loss: 19.577732. Entropy: 0.333119.\n",
      "Iteration 5733: Policy loss: 0.749375. Value loss: 13.706436. Entropy: 0.339038.\n",
      "episode: 2506   score: 515.0  epsilon: 1.0    steps: 369  evaluation reward: 207.2\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5734: Policy loss: 0.931863. Value loss: 11.843036. Entropy: 0.616518.\n",
      "Iteration 5735: Policy loss: 0.895733. Value loss: 6.661611. Entropy: 0.608649.\n",
      "Iteration 5736: Policy loss: 0.920958. Value loss: 5.344755. Entropy: 0.628113.\n",
      "episode: 2507   score: 110.0  epsilon: 1.0    steps: 128  evaluation reward: 205.15\n",
      "episode: 2508   score: 75.0  epsilon: 1.0    steps: 251  evaluation reward: 203.8\n",
      "episode: 2509   score: 185.0  epsilon: 1.0    steps: 438  evaluation reward: 204.45\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5737: Policy loss: 2.152774. Value loss: 22.850016. Entropy: 0.589189.\n",
      "Iteration 5738: Policy loss: 2.688100. Value loss: 13.858477. Entropy: 0.552403.\n",
      "Iteration 5739: Policy loss: 2.481242. Value loss: 14.990095. Entropy: 0.578274.\n",
      "episode: 2510   score: 110.0  epsilon: 1.0    steps: 646  evaluation reward: 204.5\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5740: Policy loss: -0.613812. Value loss: 12.701774. Entropy: 0.489937.\n",
      "Iteration 5741: Policy loss: -0.615770. Value loss: 5.655053. Entropy: 0.492150.\n",
      "Iteration 5742: Policy loss: -0.441594. Value loss: 7.013949. Entropy: 0.499284.\n",
      "episode: 2511   score: 110.0  epsilon: 1.0    steps: 823  evaluation reward: 203.8\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5743: Policy loss: 0.626153. Value loss: 21.865377. Entropy: 0.428173.\n",
      "Iteration 5744: Policy loss: 0.362732. Value loss: 12.994299. Entropy: 0.419180.\n",
      "Iteration 5745: Policy loss: 0.364328. Value loss: 11.185348. Entropy: 0.430578.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5746: Policy loss: 0.936015. Value loss: 17.240419. Entropy: 0.502698.\n",
      "Iteration 5747: Policy loss: 0.816737. Value loss: 7.134477. Entropy: 0.514256.\n",
      "Iteration 5748: Policy loss: 0.795328. Value loss: 6.081764. Entropy: 0.527410.\n",
      "episode: 2512   score: 180.0  epsilon: 1.0    steps: 349  evaluation reward: 202.75\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5749: Policy loss: 0.765171. Value loss: 18.265009. Entropy: 0.328685.\n",
      "Iteration 5750: Policy loss: 0.684747. Value loss: 11.656822. Entropy: 0.341770.\n",
      "Iteration 5751: Policy loss: 1.043748. Value loss: 9.119329. Entropy: 0.331456.\n",
      "episode: 2513   score: 155.0  epsilon: 1.0    steps: 241  evaluation reward: 202.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2514   score: 355.0  epsilon: 1.0    steps: 631  evaluation reward: 204.55\n",
      "episode: 2515   score: 260.0  epsilon: 1.0    steps: 901  evaluation reward: 204.55\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5752: Policy loss: 0.834293. Value loss: 25.171749. Entropy: 0.458976.\n",
      "Iteration 5753: Policy loss: 0.667077. Value loss: 13.674945. Entropy: 0.465010.\n",
      "Iteration 5754: Policy loss: 0.848762. Value loss: 9.184001. Entropy: 0.472444.\n",
      "episode: 2516   score: 50.0  epsilon: 1.0    steps: 790  evaluation reward: 203.85\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5755: Policy loss: 0.668602. Value loss: 10.850893. Entropy: 0.415151.\n",
      "Iteration 5756: Policy loss: 0.597607. Value loss: 7.377614. Entropy: 0.410081.\n",
      "Iteration 5757: Policy loss: 0.573162. Value loss: 5.237402. Entropy: 0.412264.\n",
      "episode: 2517   score: 180.0  epsilon: 1.0    steps: 51  evaluation reward: 203.55\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5758: Policy loss: 0.338089. Value loss: 12.047591. Entropy: 0.525351.\n",
      "Iteration 5759: Policy loss: 0.373727. Value loss: 9.780892. Entropy: 0.553455.\n",
      "Iteration 5760: Policy loss: 0.372194. Value loss: 7.544057. Entropy: 0.564702.\n",
      "episode: 2518   score: 290.0  epsilon: 1.0    steps: 725  evaluation reward: 202.8\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5761: Policy loss: -0.700156. Value loss: 13.127381. Entropy: 0.509013.\n",
      "Iteration 5762: Policy loss: -0.651933. Value loss: 7.511421. Entropy: 0.483845.\n",
      "Iteration 5763: Policy loss: -0.659472. Value loss: 7.156768. Entropy: 0.467677.\n",
      "episode: 2519   score: 300.0  epsilon: 1.0    steps: 423  evaluation reward: 204.0\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5764: Policy loss: -0.437124. Value loss: 4.885110. Entropy: 0.396307.\n",
      "Iteration 5765: Policy loss: -0.420236. Value loss: 3.990510. Entropy: 0.389433.\n",
      "Iteration 5766: Policy loss: -0.339657. Value loss: 4.073198. Entropy: 0.400998.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5767: Policy loss: -2.458055. Value loss: 194.082291. Entropy: 0.561878.\n",
      "Iteration 5768: Policy loss: -2.592769. Value loss: 155.493439. Entropy: 0.445483.\n",
      "Iteration 5769: Policy loss: -2.688433. Value loss: 83.674217. Entropy: 0.451447.\n",
      "episode: 2520   score: 210.0  epsilon: 1.0    steps: 940  evaluation reward: 204.0\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5770: Policy loss: 2.102389. Value loss: 17.706388. Entropy: 0.447539.\n",
      "Iteration 5771: Policy loss: 2.020475. Value loss: 9.059989. Entropy: 0.459437.\n",
      "Iteration 5772: Policy loss: 1.862391. Value loss: 7.869840. Entropy: 0.491388.\n",
      "episode: 2521   score: 240.0  epsilon: 1.0    steps: 302  evaluation reward: 204.3\n",
      "episode: 2522   score: 410.0  epsilon: 1.0    steps: 610  evaluation reward: 206.6\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5773: Policy loss: 0.709352. Value loss: 14.383559. Entropy: 0.449944.\n",
      "Iteration 5774: Policy loss: 0.730078. Value loss: 9.842091. Entropy: 0.466938.\n",
      "Iteration 5775: Policy loss: 0.595503. Value loss: 7.506112. Entropy: 0.464455.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5776: Policy loss: 0.413864. Value loss: 26.973703. Entropy: 0.470206.\n",
      "Iteration 5777: Policy loss: 0.573917. Value loss: 11.873489. Entropy: 0.467500.\n",
      "Iteration 5778: Policy loss: 0.511208. Value loss: 9.291558. Entropy: 0.486110.\n",
      "episode: 2523   score: 205.0  epsilon: 1.0    steps: 14  evaluation reward: 206.4\n",
      "episode: 2524   score: 300.0  epsilon: 1.0    steps: 188  evaluation reward: 207.6\n",
      "episode: 2525   score: 280.0  epsilon: 1.0    steps: 836  evaluation reward: 207.8\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5779: Policy loss: 0.486719. Value loss: 13.407024. Entropy: 0.551877.\n",
      "Iteration 5780: Policy loss: 0.531691. Value loss: 7.537851. Entropy: 0.535628.\n",
      "Iteration 5781: Policy loss: 0.343374. Value loss: 5.291458. Entropy: 0.559746.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5782: Policy loss: 0.377679. Value loss: 13.657044. Entropy: 0.436025.\n",
      "Iteration 5783: Policy loss: 0.398068. Value loss: 7.962125. Entropy: 0.435812.\n",
      "Iteration 5784: Policy loss: 0.580070. Value loss: 5.709041. Entropy: 0.440144.\n",
      "episode: 2526   score: 210.0  epsilon: 1.0    steps: 386  evaluation reward: 207.8\n",
      "episode: 2527   score: 180.0  epsilon: 1.0    steps: 937  evaluation reward: 207.5\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5785: Policy loss: -0.919592. Value loss: 17.820080. Entropy: 0.449714.\n",
      "Iteration 5786: Policy loss: -0.852473. Value loss: 11.294899. Entropy: 0.437974.\n",
      "Iteration 5787: Policy loss: -0.819495. Value loss: 10.679288. Entropy: 0.442586.\n",
      "episode: 2528   score: 155.0  epsilon: 1.0    steps: 280  evaluation reward: 207.7\n",
      "episode: 2529   score: 210.0  epsilon: 1.0    steps: 611  evaluation reward: 207.2\n",
      "episode: 2530   score: 290.0  epsilon: 1.0    steps: 683  evaluation reward: 207.95\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5788: Policy loss: -1.999217. Value loss: 26.125109. Entropy: 0.420140.\n",
      "Iteration 5789: Policy loss: -1.786442. Value loss: 14.221093. Entropy: 0.435948.\n",
      "Iteration 5790: Policy loss: -1.905993. Value loss: 12.037199. Entropy: 0.418963.\n",
      "episode: 2531   score: 105.0  epsilon: 1.0    steps: 817  evaluation reward: 207.45\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5791: Policy loss: -0.671135. Value loss: 14.158472. Entropy: 0.486390.\n",
      "Iteration 5792: Policy loss: -0.854754. Value loss: 8.523714. Entropy: 0.465571.\n",
      "Iteration 5793: Policy loss: -0.661177. Value loss: 6.915964. Entropy: 0.479780.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5794: Policy loss: -1.258650. Value loss: 14.156159. Entropy: 0.490645.\n",
      "Iteration 5795: Policy loss: -1.270788. Value loss: 10.527190. Entropy: 0.526146.\n",
      "Iteration 5796: Policy loss: -1.276070. Value loss: 9.196063. Entropy: 0.511147.\n",
      "episode: 2532   score: 290.0  epsilon: 1.0    steps: 107  evaluation reward: 208.25\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5797: Policy loss: -1.228138. Value loss: 17.852491. Entropy: 0.353906.\n",
      "Iteration 5798: Policy loss: -1.369938. Value loss: 10.724287. Entropy: 0.359971.\n",
      "Iteration 5799: Policy loss: -1.275655. Value loss: 8.236642. Entropy: 0.364498.\n",
      "episode: 2533   score: 330.0  epsilon: 1.0    steps: 174  evaluation reward: 209.45\n",
      "episode: 2534   score: 180.0  epsilon: 1.0    steps: 937  evaluation reward: 208.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5800: Policy loss: 2.481045. Value loss: 13.110081. Entropy: 0.391957.\n",
      "Iteration 5801: Policy loss: 2.448777. Value loss: 9.141050. Entropy: 0.409020.\n",
      "Iteration 5802: Policy loss: 2.223074. Value loss: 7.867054. Entropy: 0.408043.\n",
      "episode: 2535   score: 210.0  epsilon: 1.0    steps: 289  evaluation reward: 209.45\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5803: Policy loss: -0.044491. Value loss: 15.610711. Entropy: 0.434366.\n",
      "Iteration 5804: Policy loss: -0.171936. Value loss: 8.583410. Entropy: 0.442781.\n",
      "Iteration 5805: Policy loss: -0.029151. Value loss: 7.015572. Entropy: 0.452466.\n",
      "episode: 2536   score: 240.0  epsilon: 1.0    steps: 469  evaluation reward: 207.9\n",
      "episode: 2537   score: 210.0  epsilon: 1.0    steps: 737  evaluation reward: 208.95\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5806: Policy loss: 1.247948. Value loss: 17.537544. Entropy: 0.571985.\n",
      "Iteration 5807: Policy loss: 1.233898. Value loss: 8.876576. Entropy: 0.560476.\n",
      "Iteration 5808: Policy loss: 1.441480. Value loss: 7.029218. Entropy: 0.556646.\n",
      "episode: 2538   score: 210.0  epsilon: 1.0    steps: 543  evaluation reward: 208.45\n",
      "episode: 2539   score: 260.0  epsilon: 1.0    steps: 819  evaluation reward: 208.15\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5809: Policy loss: 0.503288. Value loss: 9.893853. Entropy: 0.431197.\n",
      "Iteration 5810: Policy loss: 0.602296. Value loss: 4.721069. Entropy: 0.457519.\n",
      "Iteration 5811: Policy loss: 0.428625. Value loss: 4.219240. Entropy: 0.454227.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5812: Policy loss: 0.265014. Value loss: 27.423887. Entropy: 0.349945.\n",
      "Iteration 5813: Policy loss: 0.227244. Value loss: 18.794947. Entropy: 0.332019.\n",
      "Iteration 5814: Policy loss: -0.150458. Value loss: 12.212746. Entropy: 0.340459.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5815: Policy loss: -0.679827. Value loss: 22.918814. Entropy: 0.588192.\n",
      "Iteration 5816: Policy loss: -0.928987. Value loss: 12.723268. Entropy: 0.569316.\n",
      "Iteration 5817: Policy loss: -0.487754. Value loss: 8.740479. Entropy: 0.588134.\n",
      "episode: 2540   score: 275.0  epsilon: 1.0    steps: 94  evaluation reward: 208.8\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5818: Policy loss: 0.331510. Value loss: 9.265335. Entropy: 0.475188.\n",
      "Iteration 5819: Policy loss: 0.364489. Value loss: 5.332398. Entropy: 0.484081.\n",
      "Iteration 5820: Policy loss: 0.315529. Value loss: 4.312104. Entropy: 0.484249.\n",
      "episode: 2541   score: 210.0  epsilon: 1.0    steps: 227  evaluation reward: 209.1\n",
      "episode: 2542   score: 240.0  epsilon: 1.0    steps: 274  evaluation reward: 210.45\n",
      "episode: 2543   score: 180.0  epsilon: 1.0    steps: 455  evaluation reward: 209.65\n",
      "episode: 2544   score: 185.0  epsilon: 1.0    steps: 629  evaluation reward: 207.6\n",
      "episode: 2545   score: 265.0  epsilon: 1.0    steps: 959  evaluation reward: 207.85\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5821: Policy loss: 0.752839. Value loss: 18.057621. Entropy: 0.412795.\n",
      "Iteration 5822: Policy loss: 0.874558. Value loss: 13.323500. Entropy: 0.382008.\n",
      "Iteration 5823: Policy loss: 0.775714. Value loss: 11.857805. Entropy: 0.425269.\n",
      "episode: 2546   score: 240.0  epsilon: 1.0    steps: 742  evaluation reward: 208.7\n",
      "episode: 2547   score: 210.0  epsilon: 1.0    steps: 836  evaluation reward: 207.35\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5824: Policy loss: 0.837796. Value loss: 12.065951. Entropy: 0.421713.\n",
      "Iteration 5825: Policy loss: 0.947271. Value loss: 7.459478. Entropy: 0.448708.\n",
      "Iteration 5826: Policy loss: 1.008654. Value loss: 6.141797. Entropy: 0.419030.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5827: Policy loss: -0.290387. Value loss: 12.601334. Entropy: 0.591166.\n",
      "Iteration 5828: Policy loss: -0.363473. Value loss: 8.979568. Entropy: 0.569257.\n",
      "Iteration 5829: Policy loss: -0.374206. Value loss: 8.555392. Entropy: 0.571513.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5830: Policy loss: -0.164840. Value loss: 14.029782. Entropy: 0.423490.\n",
      "Iteration 5831: Policy loss: -0.320746. Value loss: 11.541820. Entropy: 0.413144.\n",
      "Iteration 5832: Policy loss: -0.197528. Value loss: 7.895497. Entropy: 0.415418.\n",
      "episode: 2548   score: 135.0  epsilon: 1.0    steps: 15  evaluation reward: 206.15\n",
      "episode: 2549   score: 80.0  epsilon: 1.0    steps: 536  evaluation reward: 204.35\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5833: Policy loss: -0.496144. Value loss: 16.545725. Entropy: 0.345417.\n",
      "Iteration 5834: Policy loss: -0.656660. Value loss: 10.561804. Entropy: 0.333423.\n",
      "Iteration 5835: Policy loss: -0.543745. Value loss: 9.351960. Entropy: 0.327419.\n",
      "episode: 2550   score: 210.0  epsilon: 1.0    steps: 285  evaluation reward: 204.9\n",
      "now time :  2019-02-25 20:29:04.781517\n",
      "episode: 2551   score: 210.0  epsilon: 1.0    steps: 415  evaluation reward: 204.15\n",
      "episode: 2552   score: 210.0  epsilon: 1.0    steps: 932  evaluation reward: 204.45\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5836: Policy loss: -0.487636. Value loss: 17.016312. Entropy: 0.335822.\n",
      "Iteration 5837: Policy loss: -0.424169. Value loss: 10.760831. Entropy: 0.336862.\n",
      "Iteration 5838: Policy loss: -0.664275. Value loss: 10.783764. Entropy: 0.324452.\n",
      "episode: 2553   score: 215.0  epsilon: 1.0    steps: 196  evaluation reward: 204.8\n",
      "episode: 2554   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 205.8\n",
      "episode: 2555   score: 180.0  epsilon: 1.0    steps: 785  evaluation reward: 206.55\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5839: Policy loss: -0.004968. Value loss: 14.954299. Entropy: 0.324328.\n",
      "Iteration 5840: Policy loss: -0.070616. Value loss: 10.579570. Entropy: 0.312120.\n",
      "Iteration 5841: Policy loss: 0.134792. Value loss: 8.605266. Entropy: 0.327980.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5842: Policy loss: -0.458791. Value loss: 11.297251. Entropy: 0.403493.\n",
      "Iteration 5843: Policy loss: -0.398217. Value loss: 7.712491. Entropy: 0.420906.\n",
      "Iteration 5844: Policy loss: -0.638789. Value loss: 6.821701. Entropy: 0.388749.\n",
      "episode: 2556   score: 180.0  epsilon: 1.0    steps: 117  evaluation reward: 206.25\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5845: Policy loss: -0.113902. Value loss: 13.655610. Entropy: 0.417423.\n",
      "Iteration 5846: Policy loss: 0.002161. Value loss: 9.158244. Entropy: 0.440229.\n",
      "Iteration 5847: Policy loss: 0.053392. Value loss: 8.593351. Entropy: 0.421973.\n",
      "episode: 2557   score: 155.0  epsilon: 1.0    steps: 290  evaluation reward: 206.75\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5848: Policy loss: 0.097471. Value loss: 11.064637. Entropy: 0.488052.\n",
      "Iteration 5849: Policy loss: -0.123335. Value loss: 6.634231. Entropy: 0.473037.\n",
      "Iteration 5850: Policy loss: -0.079418. Value loss: 5.844353. Entropy: 0.493935.\n",
      "episode: 2558   score: 180.0  epsilon: 1.0    steps: 434  evaluation reward: 206.45\n",
      "episode: 2559   score: 210.0  epsilon: 1.0    steps: 534  evaluation reward: 206.75\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5851: Policy loss: -3.456328. Value loss: 291.818573. Entropy: 0.262737.\n",
      "Iteration 5852: Policy loss: -3.944503. Value loss: 236.631195. Entropy: 0.345440.\n",
      "Iteration 5853: Policy loss: -3.212810. Value loss: 132.178650. Entropy: 0.350936.\n",
      "episode: 2560   score: 210.0  epsilon: 1.0    steps: 826  evaluation reward: 207.05\n",
      "episode: 2561   score: 425.0  epsilon: 1.0    steps: 913  evaluation reward: 209.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5854: Policy loss: 0.509564. Value loss: 16.901203. Entropy: 0.373531.\n",
      "Iteration 5855: Policy loss: 0.668225. Value loss: 9.986954. Entropy: 0.371048.\n",
      "Iteration 5856: Policy loss: 0.530080. Value loss: 7.276402. Entropy: 0.352608.\n",
      "episode: 2562   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 210.25\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5857: Policy loss: 0.541518. Value loss: 11.747676. Entropy: 0.460922.\n",
      "Iteration 5858: Policy loss: 0.643864. Value loss: 8.541778. Entropy: 0.462370.\n",
      "Iteration 5859: Policy loss: 0.578835. Value loss: 5.749256. Entropy: 0.468040.\n",
      "episode: 2563   score: 135.0  epsilon: 1.0    steps: 30  evaluation reward: 209.8\n",
      "episode: 2564   score: 210.0  epsilon: 1.0    steps: 147  evaluation reward: 210.75\n",
      "episode: 2565   score: 120.0  epsilon: 1.0    steps: 339  evaluation reward: 210.1\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5860: Policy loss: 3.329675. Value loss: 35.633427. Entropy: 0.413078.\n",
      "Iteration 5861: Policy loss: 2.902091. Value loss: 17.531567. Entropy: 0.431688.\n",
      "Iteration 5862: Policy loss: 2.979706. Value loss: 15.151846. Entropy: 0.437970.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5863: Policy loss: 0.826103. Value loss: 19.503330. Entropy: 0.384101.\n",
      "Iteration 5864: Policy loss: 0.982008. Value loss: 13.633299. Entropy: 0.390199.\n",
      "Iteration 5865: Policy loss: 0.859780. Value loss: 11.274317. Entropy: 0.404592.\n",
      "episode: 2566   score: 125.0  epsilon: 1.0    steps: 847  evaluation reward: 209.25\n",
      "episode: 2567   score: 165.0  epsilon: 1.0    steps: 934  evaluation reward: 208.5\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5866: Policy loss: 0.989449. Value loss: 12.756707. Entropy: 0.442513.\n",
      "Iteration 5867: Policy loss: 1.035763. Value loss: 8.752254. Entropy: 0.429562.\n",
      "Iteration 5868: Policy loss: 0.869046. Value loss: 6.650286. Entropy: 0.425556.\n",
      "episode: 2568   score: 210.0  epsilon: 1.0    steps: 535  evaluation reward: 208.8\n",
      "episode: 2569   score: 180.0  epsilon: 1.0    steps: 714  evaluation reward: 206.95\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5869: Policy loss: -1.219768. Value loss: 18.609985. Entropy: 0.451947.\n",
      "Iteration 5870: Policy loss: -1.012412. Value loss: 14.494363. Entropy: 0.450514.\n",
      "Iteration 5871: Policy loss: -1.482946. Value loss: 12.284285. Entropy: 0.440444.\n",
      "episode: 2570   score: 210.0  epsilon: 1.0    steps: 121  evaluation reward: 207.7\n",
      "episode: 2571   score: 165.0  epsilon: 1.0    steps: 202  evaluation reward: 206.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2572   score: 390.0  epsilon: 1.0    steps: 503  evaluation reward: 208.4\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5872: Policy loss: -2.800666. Value loss: 219.970169. Entropy: 0.364620.\n",
      "Iteration 5873: Policy loss: -2.442656. Value loss: 132.258057. Entropy: 0.337674.\n",
      "Iteration 5874: Policy loss: -2.873480. Value loss: 92.010544. Entropy: 0.326027.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5875: Policy loss: 0.546630. Value loss: 25.243422. Entropy: 0.478294.\n",
      "Iteration 5876: Policy loss: 0.486519. Value loss: 12.740969. Entropy: 0.455781.\n",
      "Iteration 5877: Policy loss: 0.592621. Value loss: 9.643446. Entropy: 0.458677.\n",
      "episode: 2573   score: 230.0  epsilon: 1.0    steps: 328  evaluation reward: 208.9\n",
      "episode: 2574   score: 80.0  epsilon: 1.0    steps: 577  evaluation reward: 208.45\n",
      "episode: 2575   score: 135.0  epsilon: 1.0    steps: 874  evaluation reward: 206.7\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5878: Policy loss: 2.664894. Value loss: 22.499798. Entropy: 0.354579.\n",
      "Iteration 5879: Policy loss: 2.694932. Value loss: 16.434223. Entropy: 0.384499.\n",
      "Iteration 5880: Policy loss: 2.587428. Value loss: 13.636916. Entropy: 0.358786.\n",
      "episode: 2576   score: 240.0  epsilon: 1.0    steps: 1019  evaluation reward: 208.0\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5881: Policy loss: 0.183166. Value loss: 20.552273. Entropy: 0.556197.\n",
      "Iteration 5882: Policy loss: 0.122660. Value loss: 12.618365. Entropy: 0.563761.\n",
      "Iteration 5883: Policy loss: 0.222990. Value loss: 8.522969. Entropy: 0.553670.\n",
      "episode: 2577   score: 45.0  epsilon: 1.0    steps: 27  evaluation reward: 207.35\n",
      "episode: 2578   score: 105.0  epsilon: 1.0    steps: 144  evaluation reward: 207.35\n",
      "episode: 2579   score: 105.0  epsilon: 1.0    steps: 418  evaluation reward: 207.35\n",
      "episode: 2580   score: 155.0  epsilon: 1.0    steps: 661  evaluation reward: 207.5\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5884: Policy loss: 0.967494. Value loss: 7.196489. Entropy: 0.502990.\n",
      "Iteration 5885: Policy loss: 1.028116. Value loss: 4.412655. Entropy: 0.521981.\n",
      "Iteration 5886: Policy loss: 0.900563. Value loss: 5.373040. Entropy: 0.504533.\n",
      "episode: 2581   score: 105.0  epsilon: 1.0    steps: 380  evaluation reward: 206.45\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5887: Policy loss: -0.456206. Value loss: 17.323780. Entropy: 0.352463.\n",
      "Iteration 5888: Policy loss: -0.417611. Value loss: 11.941450. Entropy: 0.355215.\n",
      "Iteration 5889: Policy loss: -0.400762. Value loss: 10.589183. Entropy: 0.359235.\n",
      "episode: 2582   score: 110.0  epsilon: 1.0    steps: 518  evaluation reward: 206.25\n",
      "episode: 2583   score: 105.0  epsilon: 1.0    steps: 792  evaluation reward: 206.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5890: Policy loss: -0.477130. Value loss: 17.051601. Entropy: 0.623996.\n",
      "Iteration 5891: Policy loss: -0.461530. Value loss: 13.918883. Entropy: 0.619883.\n",
      "Iteration 5892: Policy loss: -0.499410. Value loss: 10.002455. Entropy: 0.614724.\n",
      "episode: 2584   score: 105.0  epsilon: 1.0    steps: 65  evaluation reward: 205.45\n",
      "episode: 2585   score: 110.0  epsilon: 1.0    steps: 488  evaluation reward: 205.0\n",
      "episode: 2586   score: 105.0  epsilon: 1.0    steps: 700  evaluation reward: 202.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5893: Policy loss: -0.033227. Value loss: 15.546066. Entropy: 0.297405.\n",
      "Iteration 5894: Policy loss: 0.062463. Value loss: 10.778423. Entropy: 0.302715.\n",
      "Iteration 5895: Policy loss: 0.051120. Value loss: 8.666368. Entropy: 0.312249.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5896: Policy loss: -2.520687. Value loss: 27.016291. Entropy: 0.578799.\n",
      "Iteration 5897: Policy loss: -2.460377. Value loss: 16.031898. Entropy: 0.557252.\n",
      "Iteration 5898: Policy loss: -2.454684. Value loss: 14.886670. Entropy: 0.578615.\n",
      "episode: 2587   score: 180.0  epsilon: 1.0    steps: 637  evaluation reward: 202.2\n",
      "episode: 2588   score: 90.0  epsilon: 1.0    steps: 865  evaluation reward: 201.0\n",
      "episode: 2589   score: 230.0  epsilon: 1.0    steps: 1011  evaluation reward: 201.75\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5899: Policy loss: 0.187925. Value loss: 23.531235. Entropy: 0.371914.\n",
      "Iteration 5900: Policy loss: 0.010117. Value loss: 14.655290. Entropy: 0.401186.\n",
      "Iteration 5901: Policy loss: 0.166625. Value loss: 13.858416. Entropy: 0.395676.\n",
      "episode: 2590   score: 105.0  epsilon: 1.0    steps: 728  evaluation reward: 200.7\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5902: Policy loss: -0.456810. Value loss: 13.834798. Entropy: 0.544626.\n",
      "Iteration 5903: Policy loss: -0.459178. Value loss: 8.391853. Entropy: 0.535100.\n",
      "Iteration 5904: Policy loss: -0.615288. Value loss: 6.568260. Entropy: 0.519690.\n",
      "episode: 2591   score: 125.0  epsilon: 1.0    steps: 54  evaluation reward: 200.4\n",
      "episode: 2592   score: 355.0  epsilon: 1.0    steps: 231  evaluation reward: 201.25\n",
      "episode: 2593   score: 285.0  epsilon: 1.0    steps: 363  evaluation reward: 203.0\n",
      "episode: 2594   score: 110.0  epsilon: 1.0    steps: 393  evaluation reward: 202.0\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5905: Policy loss: 0.117315. Value loss: 17.979988. Entropy: 0.389137.\n",
      "Iteration 5906: Policy loss: -0.074742. Value loss: 11.105330. Entropy: 0.419361.\n",
      "Iteration 5907: Policy loss: -0.182092. Value loss: 10.234160. Entropy: 0.399660.\n",
      "episode: 2595   score: 45.0  epsilon: 1.0    steps: 894  evaluation reward: 198.95\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5908: Policy loss: -0.034958. Value loss: 19.737150. Entropy: 0.707460.\n",
      "Iteration 5909: Policy loss: 0.065972. Value loss: 13.839865. Entropy: 0.706123.\n",
      "Iteration 5910: Policy loss: 0.027596. Value loss: 12.129411. Entropy: 0.672782.\n",
      "episode: 2596   score: 110.0  epsilon: 1.0    steps: 765  evaluation reward: 194.75\n",
      "episode: 2597   score: 125.0  epsilon: 1.0    steps: 1010  evaluation reward: 194.2\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5911: Policy loss: -0.949152. Value loss: 14.937440. Entropy: 0.595185.\n",
      "Iteration 5912: Policy loss: -0.961411. Value loss: 8.706440. Entropy: 0.588993.\n",
      "Iteration 5913: Policy loss: -0.851557. Value loss: 7.291720. Entropy: 0.586594.\n",
      "episode: 2598   score: 205.0  epsilon: 1.0    steps: 634  evaluation reward: 193.05\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5914: Policy loss: -1.056838. Value loss: 13.001805. Entropy: 0.570878.\n",
      "Iteration 5915: Policy loss: -1.007474. Value loss: 8.300481. Entropy: 0.570785.\n",
      "Iteration 5916: Policy loss: -1.042048. Value loss: 7.502873. Entropy: 0.570695.\n",
      "episode: 2599   score: 110.0  epsilon: 1.0    steps: 132  evaluation reward: 192.05\n",
      "episode: 2600   score: 120.0  epsilon: 1.0    steps: 275  evaluation reward: 192.2\n",
      "now time :  2019-02-25 20:30:35.821379\n",
      "episode: 2601   score: 180.0  epsilon: 1.0    steps: 456  evaluation reward: 191.2\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5917: Policy loss: 0.244925. Value loss: 14.679596. Entropy: 0.373843.\n",
      "Iteration 5918: Policy loss: 0.113907. Value loss: 8.098070. Entropy: 0.393904.\n",
      "Iteration 5919: Policy loss: 0.164312. Value loss: 8.382166. Entropy: 0.387468.\n",
      "episode: 2602   score: 210.0  epsilon: 1.0    steps: 107  evaluation reward: 191.2\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5920: Policy loss: 0.849047. Value loss: 12.145137. Entropy: 0.391968.\n",
      "Iteration 5921: Policy loss: 0.978886. Value loss: 9.200804. Entropy: 0.388500.\n",
      "Iteration 5922: Policy loss: 0.949681. Value loss: 7.110672. Entropy: 0.405835.\n",
      "episode: 2603   score: 120.0  epsilon: 1.0    steps: 938  evaluation reward: 190.3\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5923: Policy loss: -0.669895. Value loss: 25.675261. Entropy: 0.624466.\n",
      "Iteration 5924: Policy loss: -0.555298. Value loss: 15.244167. Entropy: 0.608243.\n",
      "Iteration 5925: Policy loss: -0.556301. Value loss: 13.358776. Entropy: 0.609489.\n",
      "episode: 2604   score: 240.0  epsilon: 1.0    steps: 857  evaluation reward: 190.9\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5926: Policy loss: -2.440919. Value loss: 157.703094. Entropy: 0.349091.\n",
      "Iteration 5927: Policy loss: -2.262871. Value loss: 51.136246. Entropy: 0.282804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5928: Policy loss: -2.426757. Value loss: 37.401104. Entropy: 0.293263.\n",
      "episode: 2605   score: 185.0  epsilon: 1.0    steps: 418  evaluation reward: 192.0\n",
      "episode: 2606   score: 210.0  epsilon: 1.0    steps: 545  evaluation reward: 188.95\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5929: Policy loss: 0.773762. Value loss: 53.324047. Entropy: 0.540682.\n",
      "Iteration 5930: Policy loss: 0.807047. Value loss: 18.695154. Entropy: 0.487290.\n",
      "Iteration 5931: Policy loss: 1.033228. Value loss: 14.472447. Entropy: 0.470039.\n",
      "episode: 2607   score: 165.0  epsilon: 1.0    steps: 372  evaluation reward: 189.5\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5932: Policy loss: 4.085889. Value loss: 37.483574. Entropy: 0.311820.\n",
      "Iteration 5933: Policy loss: 3.501954. Value loss: 20.700846. Entropy: 0.331255.\n",
      "Iteration 5934: Policy loss: 3.735038. Value loss: 18.137947. Entropy: 0.334454.\n",
      "episode: 2608   score: 220.0  epsilon: 1.0    steps: 199  evaluation reward: 190.95\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5935: Policy loss: 2.452543. Value loss: 15.514275. Entropy: 0.521929.\n",
      "Iteration 5936: Policy loss: 2.793586. Value loss: 7.798671. Entropy: 0.539628.\n",
      "Iteration 5937: Policy loss: 2.777606. Value loss: 6.572305. Entropy: 0.540898.\n",
      "episode: 2609   score: 595.0  epsilon: 1.0    steps: 666  evaluation reward: 195.05\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5938: Policy loss: 1.276148. Value loss: 22.700388. Entropy: 0.351849.\n",
      "Iteration 5939: Policy loss: 1.387518. Value loss: 10.694674. Entropy: 0.349934.\n",
      "Iteration 5940: Policy loss: 1.153283. Value loss: 10.489176. Entropy: 0.344300.\n",
      "episode: 2610   score: 240.0  epsilon: 1.0    steps: 30  evaluation reward: 196.35\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5941: Policy loss: 2.514941. Value loss: 24.368805. Entropy: 0.500383.\n",
      "Iteration 5942: Policy loss: 2.375252. Value loss: 12.191847. Entropy: 0.514005.\n",
      "Iteration 5943: Policy loss: 2.467208. Value loss: 10.427396. Entropy: 0.515507.\n",
      "episode: 2611   score: 105.0  epsilon: 1.0    steps: 234  evaluation reward: 196.3\n",
      "episode: 2612   score: 180.0  epsilon: 1.0    steps: 895  evaluation reward: 196.3\n",
      "episode: 2613   score: 230.0  epsilon: 1.0    steps: 908  evaluation reward: 197.05\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5944: Policy loss: 0.620106. Value loss: 227.891754. Entropy: 0.494951.\n",
      "Iteration 5945: Policy loss: -0.599014. Value loss: 193.707565. Entropy: 0.453547.\n",
      "Iteration 5946: Policy loss: 0.751688. Value loss: 143.576401. Entropy: 0.422624.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5947: Policy loss: 0.724948. Value loss: 17.114819. Entropy: 0.472488.\n",
      "Iteration 5948: Policy loss: 1.145192. Value loss: 9.091356. Entropy: 0.481123.\n",
      "Iteration 5949: Policy loss: 0.975766. Value loss: 6.417624. Entropy: 0.493829.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5950: Policy loss: 3.119701. Value loss: 30.181683. Entropy: 0.401954.\n",
      "Iteration 5951: Policy loss: 2.807443. Value loss: 17.557278. Entropy: 0.451405.\n",
      "Iteration 5952: Policy loss: 2.740899. Value loss: 13.868474. Entropy: 0.438477.\n",
      "episode: 2614   score: 515.0  epsilon: 1.0    steps: 428  evaluation reward: 198.65\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5953: Policy loss: 0.130140. Value loss: 18.681259. Entropy: 0.548585.\n",
      "Iteration 5954: Policy loss: -0.032163. Value loss: 11.613297. Entropy: 0.567635.\n",
      "Iteration 5955: Policy loss: 0.099843. Value loss: 9.696571. Entropy: 0.538509.\n",
      "episode: 2615   score: 165.0  epsilon: 1.0    steps: 307  evaluation reward: 197.7\n",
      "episode: 2616   score: 215.0  epsilon: 1.0    steps: 566  evaluation reward: 199.35\n",
      "episode: 2617   score: 180.0  epsilon: 1.0    steps: 713  evaluation reward: 199.35\n",
      "episode: 2618   score: 110.0  epsilon: 1.0    steps: 801  evaluation reward: 197.55\n",
      "episode: 2619   score: 210.0  epsilon: 1.0    steps: 946  evaluation reward: 196.65\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5956: Policy loss: 2.335308. Value loss: 11.361820. Entropy: 0.480450.\n",
      "Iteration 5957: Policy loss: 2.415325. Value loss: 7.423350. Entropy: 0.488546.\n",
      "Iteration 5958: Policy loss: 2.153248. Value loss: 7.125583. Entropy: 0.535011.\n",
      "episode: 2620   score: 180.0  epsilon: 1.0    steps: 143  evaluation reward: 196.35\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5959: Policy loss: 0.572101. Value loss: 9.971469. Entropy: 0.511852.\n",
      "Iteration 5960: Policy loss: 0.426420. Value loss: 7.024201. Entropy: 0.517809.\n",
      "Iteration 5961: Policy loss: 0.344298. Value loss: 6.849345. Entropy: 0.490400.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5962: Policy loss: 0.637247. Value loss: 15.619632. Entropy: 0.673681.\n",
      "Iteration 5963: Policy loss: 0.850414. Value loss: 12.961045. Entropy: 0.689937.\n",
      "Iteration 5964: Policy loss: 0.813058. Value loss: 10.009199. Entropy: 0.686806.\n",
      "episode: 2621   score: 230.0  epsilon: 1.0    steps: 18  evaluation reward: 196.25\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5965: Policy loss: 0.135701. Value loss: 12.757229. Entropy: 0.708449.\n",
      "Iteration 5966: Policy loss: 0.087631. Value loss: 8.241654. Entropy: 0.679764.\n",
      "Iteration 5967: Policy loss: 0.043926. Value loss: 6.908836. Entropy: 0.710836.\n",
      "episode: 2622   score: 155.0  epsilon: 1.0    steps: 877  evaluation reward: 193.7\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5968: Policy loss: -1.459657. Value loss: 24.463371. Entropy: 0.253386.\n",
      "Iteration 5969: Policy loss: -1.503017. Value loss: 13.411011. Entropy: 0.285919.\n",
      "Iteration 5970: Policy loss: -1.576422. Value loss: 11.510319. Entropy: 0.294372.\n",
      "episode: 2623   score: 210.0  epsilon: 1.0    steps: 237  evaluation reward: 193.75\n",
      "episode: 2624   score: 210.0  epsilon: 1.0    steps: 545  evaluation reward: 192.85\n",
      "episode: 2625   score: 210.0  epsilon: 1.0    steps: 676  evaluation reward: 192.15\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5971: Policy loss: 0.602075. Value loss: 14.140597. Entropy: 0.532568.\n",
      "Iteration 5972: Policy loss: 0.532164. Value loss: 7.416215. Entropy: 0.549647.\n",
      "Iteration 5973: Policy loss: 0.472289. Value loss: 5.459297. Entropy: 0.562482.\n",
      "episode: 2626   score: 195.0  epsilon: 1.0    steps: 324  evaluation reward: 192.0\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5974: Policy loss: 0.408826. Value loss: 14.481642. Entropy: 0.480211.\n",
      "Iteration 5975: Policy loss: 0.340322. Value loss: 7.419232. Entropy: 0.499660.\n",
      "Iteration 5976: Policy loss: 0.252901. Value loss: 6.878854. Entropy: 0.499318.\n",
      "episode: 2627   score: 180.0  epsilon: 1.0    steps: 93  evaluation reward: 192.0\n",
      "episode: 2628   score: 210.0  epsilon: 1.0    steps: 1001  evaluation reward: 192.55\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5977: Policy loss: -2.068604. Value loss: 30.142637. Entropy: 0.359170.\n",
      "Iteration 5978: Policy loss: -2.081079. Value loss: 16.229975. Entropy: 0.373171.\n",
      "Iteration 5979: Policy loss: -2.119129. Value loss: 13.109446. Entropy: 0.357399.\n",
      "episode: 2629   score: 360.0  epsilon: 1.0    steps: 443  evaluation reward: 194.05\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5980: Policy loss: 0.188710. Value loss: 16.243647. Entropy: 0.599066.\n",
      "Iteration 5981: Policy loss: 0.285127. Value loss: 7.866739. Entropy: 0.559612.\n",
      "Iteration 5982: Policy loss: 0.190153. Value loss: 7.312804. Entropy: 0.596896.\n",
      "episode: 2630   score: 210.0  epsilon: 1.0    steps: 637  evaluation reward: 193.25\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5983: Policy loss: -0.420734. Value loss: 22.081499. Entropy: 0.377505.\n",
      "Iteration 5984: Policy loss: -0.471031. Value loss: 14.268577. Entropy: 0.381396.\n",
      "Iteration 5985: Policy loss: -0.433442. Value loss: 11.355407. Entropy: 0.381327.\n",
      "episode: 2631   score: 270.0  epsilon: 1.0    steps: 852  evaluation reward: 194.9\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5986: Policy loss: -0.205705. Value loss: 19.340925. Entropy: 0.321570.\n",
      "Iteration 5987: Policy loss: -0.302980. Value loss: 11.246850. Entropy: 0.330060.\n",
      "Iteration 5988: Policy loss: -0.176276. Value loss: 9.027515. Entropy: 0.338785.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5989: Policy loss: -0.449442. Value loss: 26.347429. Entropy: 0.298537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5990: Policy loss: -0.606037. Value loss: 14.583551. Entropy: 0.263223.\n",
      "Iteration 5991: Policy loss: -0.769790. Value loss: 11.842278. Entropy: 0.268112.\n",
      "episode: 2632   score: 255.0  epsilon: 1.0    steps: 183  evaluation reward: 194.55\n",
      "episode: 2633   score: 240.0  epsilon: 1.0    steps: 285  evaluation reward: 193.65\n",
      "episode: 2634   score: 290.0  epsilon: 1.0    steps: 641  evaluation reward: 194.75\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5992: Policy loss: 1.220258. Value loss: 9.095880. Entropy: 0.364113.\n",
      "Iteration 5993: Policy loss: 0.973296. Value loss: 6.653890. Entropy: 0.371579.\n",
      "Iteration 5994: Policy loss: 0.899800. Value loss: 4.168577. Entropy: 0.377039.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5995: Policy loss: -0.431065. Value loss: 17.091276. Entropy: 0.417467.\n",
      "Iteration 5996: Policy loss: -0.446587. Value loss: 12.430593. Entropy: 0.412826.\n",
      "Iteration 5997: Policy loss: -0.391711. Value loss: 8.654578. Entropy: 0.421656.\n",
      "episode: 2635   score: 260.0  epsilon: 1.0    steps: 93  evaluation reward: 195.25\n",
      "episode: 2636   score: 255.0  epsilon: 1.0    steps: 908  evaluation reward: 195.4\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5998: Policy loss: 1.604101. Value loss: 16.440840. Entropy: 0.458831.\n",
      "Iteration 5999: Policy loss: 1.684939. Value loss: 8.368988. Entropy: 0.467680.\n",
      "Iteration 6000: Policy loss: 1.617342. Value loss: 7.768973. Entropy: 0.448901.\n",
      "episode: 2637   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 194.35\n",
      "episode: 2638   score: 110.0  epsilon: 1.0    steps: 296  evaluation reward: 193.35\n",
      "episode: 2639   score: 225.0  epsilon: 1.0    steps: 508  evaluation reward: 193.0\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6001: Policy loss: 1.050822. Value loss: 19.540415. Entropy: 0.735719.\n",
      "Iteration 6002: Policy loss: 0.946238. Value loss: 11.341971. Entropy: 0.740775.\n",
      "Iteration 6003: Policy loss: 1.009001. Value loss: 9.354752. Entropy: 0.709906.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6004: Policy loss: -0.502772. Value loss: 16.074762. Entropy: 0.573883.\n",
      "Iteration 6005: Policy loss: -0.583204. Value loss: 9.881553. Entropy: 0.578631.\n",
      "Iteration 6006: Policy loss: -0.410207. Value loss: 7.823799. Entropy: 0.596212.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6007: Policy loss: 1.229674. Value loss: 27.225536. Entropy: 0.336306.\n",
      "Iteration 6008: Policy loss: 0.829164. Value loss: 17.009481. Entropy: 0.319028.\n",
      "Iteration 6009: Policy loss: 0.961250. Value loss: 13.018798. Entropy: 0.351901.\n",
      "episode: 2640   score: 105.0  epsilon: 1.0    steps: 187  evaluation reward: 191.3\n",
      "episode: 2641   score: 260.0  epsilon: 1.0    steps: 730  evaluation reward: 191.8\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6010: Policy loss: -1.896118. Value loss: 180.761520. Entropy: 0.736838.\n",
      "Iteration 6011: Policy loss: -2.838211. Value loss: 146.479187. Entropy: 0.618821.\n",
      "Iteration 6012: Policy loss: -3.363939. Value loss: 121.591362. Entropy: 0.624257.\n",
      "episode: 2642   score: 135.0  epsilon: 1.0    steps: 11  evaluation reward: 190.75\n",
      "episode: 2643   score: 180.0  epsilon: 1.0    steps: 477  evaluation reward: 190.75\n",
      "episode: 2644   score: 285.0  epsilon: 1.0    steps: 819  evaluation reward: 191.75\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6013: Policy loss: 1.889697. Value loss: 28.548243. Entropy: 0.464896.\n",
      "Iteration 6014: Policy loss: 1.820229. Value loss: 15.354454. Entropy: 0.473506.\n",
      "Iteration 6015: Policy loss: 1.836732. Value loss: 13.055655. Entropy: 0.501923.\n",
      "episode: 2645   score: 295.0  epsilon: 1.0    steps: 594  evaluation reward: 192.05\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6016: Policy loss: -0.271630. Value loss: 30.762629. Entropy: 0.391528.\n",
      "Iteration 6017: Policy loss: -0.120320. Value loss: 18.649279. Entropy: 0.426869.\n",
      "Iteration 6018: Policy loss: -0.114898. Value loss: 14.809114. Entropy: 0.409885.\n",
      "episode: 2646   score: 515.0  epsilon: 1.0    steps: 361  evaluation reward: 194.8\n",
      "episode: 2647   score: 215.0  epsilon: 1.0    steps: 971  evaluation reward: 194.85\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6019: Policy loss: 1.876552. Value loss: 25.811321. Entropy: 0.606577.\n",
      "Iteration 6020: Policy loss: 1.842715. Value loss: 13.870257. Entropy: 0.597436.\n",
      "Iteration 6021: Policy loss: 1.952555. Value loss: 11.832180. Entropy: 0.619587.\n",
      "episode: 2648   score: 105.0  epsilon: 1.0    steps: 480  evaluation reward: 194.55\n",
      "episode: 2649   score: 180.0  epsilon: 1.0    steps: 729  evaluation reward: 195.55\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6022: Policy loss: 0.216828. Value loss: 26.309753. Entropy: 0.549553.\n",
      "Iteration 6023: Policy loss: 0.527128. Value loss: 11.845147. Entropy: 0.495916.\n",
      "Iteration 6024: Policy loss: 0.579738. Value loss: 8.015287. Entropy: 0.494876.\n",
      "episode: 2650   score: 180.0  epsilon: 1.0    steps: 144  evaluation reward: 195.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6025: Policy loss: -0.300433. Value loss: 20.612204. Entropy: 0.492156.\n",
      "Iteration 6026: Policy loss: -0.283893. Value loss: 11.588125. Entropy: 0.484349.\n",
      "Iteration 6027: Policy loss: -0.398330. Value loss: 11.408230. Entropy: 0.482722.\n",
      "now time :  2019-02-25 20:32:39.622993\n",
      "episode: 2651   score: 155.0  epsilon: 1.0    steps: 40  evaluation reward: 194.7\n",
      "episode: 2652   score: 155.0  epsilon: 1.0    steps: 523  evaluation reward: 194.15\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6028: Policy loss: 0.941688. Value loss: 14.319733. Entropy: 0.542500.\n",
      "Iteration 6029: Policy loss: 0.996134. Value loss: 9.416961. Entropy: 0.548176.\n",
      "Iteration 6030: Policy loss: 0.978615. Value loss: 8.144440. Entropy: 0.553433.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6031: Policy loss: 0.692479. Value loss: 21.887163. Entropy: 0.634943.\n",
      "Iteration 6032: Policy loss: 0.475409. Value loss: 15.292008. Entropy: 0.646886.\n",
      "Iteration 6033: Policy loss: 0.667968. Value loss: 12.882953. Entropy: 0.619393.\n",
      "episode: 2653   score: 110.0  epsilon: 1.0    steps: 218  evaluation reward: 193.1\n",
      "episode: 2654   score: 155.0  epsilon: 1.0    steps: 403  evaluation reward: 192.55\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6034: Policy loss: -1.114545. Value loss: 24.429022. Entropy: 0.590752.\n",
      "Iteration 6035: Policy loss: -1.034286. Value loss: 14.133556. Entropy: 0.595386.\n",
      "Iteration 6036: Policy loss: -1.288784. Value loss: 12.909825. Entropy: 0.567152.\n",
      "episode: 2655   score: 270.0  epsilon: 1.0    steps: 778  evaluation reward: 193.45\n",
      "episode: 2656   score: 185.0  epsilon: 1.0    steps: 959  evaluation reward: 193.5\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6037: Policy loss: -1.307796. Value loss: 33.922554. Entropy: 0.428380.\n",
      "Iteration 6038: Policy loss: -0.866168. Value loss: 19.220053. Entropy: 0.410193.\n",
      "Iteration 6039: Policy loss: -1.039388. Value loss: 14.811149. Entropy: 0.391172.\n",
      "episode: 2657   score: 180.0  epsilon: 1.0    steps: 50  evaluation reward: 193.75\n",
      "episode: 2658   score: 230.0  epsilon: 1.0    steps: 700  evaluation reward: 194.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6040: Policy loss: -0.233173. Value loss: 17.402338. Entropy: 0.551339.\n",
      "Iteration 6041: Policy loss: -0.202248. Value loss: 11.911505. Entropy: 0.503892.\n",
      "Iteration 6042: Policy loss: 0.039972. Value loss: 9.527497. Entropy: 0.507307.\n",
      "episode: 2659   score: 110.0  epsilon: 1.0    steps: 230  evaluation reward: 193.25\n",
      "episode: 2660   score: 295.0  epsilon: 1.0    steps: 341  evaluation reward: 194.1\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6043: Policy loss: 2.306257. Value loss: 18.424021. Entropy: 0.648940.\n",
      "Iteration 6044: Policy loss: 2.306609. Value loss: 9.974157. Entropy: 0.637706.\n",
      "Iteration 6045: Policy loss: 2.326070. Value loss: 8.283822. Entropy: 0.632069.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6046: Policy loss: -0.079493. Value loss: 16.898075. Entropy: 0.545357.\n",
      "Iteration 6047: Policy loss: -0.258825. Value loss: 10.555540. Entropy: 0.553741.\n",
      "Iteration 6048: Policy loss: -0.141545. Value loss: 8.776974. Entropy: 0.545004.\n",
      "episode: 2661   score: 220.0  epsilon: 1.0    steps: 387  evaluation reward: 192.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2662   score: 320.0  epsilon: 1.0    steps: 631  evaluation reward: 193.15\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6049: Policy loss: -2.304451. Value loss: 33.349949. Entropy: 0.511124.\n",
      "Iteration 6050: Policy loss: -2.195524. Value loss: 17.911903. Entropy: 0.518488.\n",
      "Iteration 6051: Policy loss: -2.718853. Value loss: 18.799520. Entropy: 0.503822.\n",
      "episode: 2663   score: 285.0  epsilon: 1.0    steps: 859  evaluation reward: 194.65\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6052: Policy loss: 0.429782. Value loss: 30.653851. Entropy: 0.563625.\n",
      "Iteration 6053: Policy loss: 0.517095. Value loss: 17.513180. Entropy: 0.587450.\n",
      "Iteration 6054: Policy loss: 0.839190. Value loss: 11.033463. Entropy: 0.586105.\n",
      "episode: 2664   score: 180.0  epsilon: 1.0    steps: 349  evaluation reward: 194.35\n",
      "episode: 2665   score: 240.0  epsilon: 1.0    steps: 745  evaluation reward: 195.55\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6055: Policy loss: -0.113222. Value loss: 15.343755. Entropy: 0.526982.\n",
      "Iteration 6056: Policy loss: 0.032159. Value loss: 10.980509. Entropy: 0.532206.\n",
      "Iteration 6057: Policy loss: -0.094711. Value loss: 8.381567. Entropy: 0.527780.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6058: Policy loss: -1.019476. Value loss: 20.979927. Entropy: 0.541402.\n",
      "Iteration 6059: Policy loss: -0.856042. Value loss: 9.969744. Entropy: 0.515202.\n",
      "Iteration 6060: Policy loss: -0.999194. Value loss: 8.948539. Entropy: 0.518622.\n",
      "episode: 2666   score: 210.0  epsilon: 1.0    steps: 167  evaluation reward: 196.4\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6061: Policy loss: 1.275507. Value loss: 22.157526. Entropy: 0.546454.\n",
      "Iteration 6062: Policy loss: 1.291638. Value loss: 10.457886. Entropy: 0.563751.\n",
      "Iteration 6063: Policy loss: 1.263855. Value loss: 10.840107. Entropy: 0.554670.\n",
      "episode: 2667   score: 225.0  epsilon: 1.0    steps: 89  evaluation reward: 197.0\n",
      "episode: 2668   score: 230.0  epsilon: 1.0    steps: 415  evaluation reward: 197.2\n",
      "episode: 2669   score: 180.0  epsilon: 1.0    steps: 577  evaluation reward: 197.2\n",
      "episode: 2670   score: 90.0  epsilon: 1.0    steps: 838  evaluation reward: 196.0\n",
      "episode: 2671   score: 315.0  epsilon: 1.0    steps: 948  evaluation reward: 197.5\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6064: Policy loss: 4.043123. Value loss: 18.305809. Entropy: 0.539556.\n",
      "Iteration 6065: Policy loss: 3.900049. Value loss: 7.737563. Entropy: 0.560267.\n",
      "Iteration 6066: Policy loss: 3.957145. Value loss: 7.155835. Entropy: 0.586044.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6067: Policy loss: -0.318841. Value loss: 20.042343. Entropy: 0.387396.\n",
      "Iteration 6068: Policy loss: -0.179153. Value loss: 11.821974. Entropy: 0.382575.\n",
      "Iteration 6069: Policy loss: -0.434742. Value loss: 11.705324. Entropy: 0.427578.\n",
      "episode: 2672   score: 150.0  epsilon: 1.0    steps: 271  evaluation reward: 195.1\n",
      "episode: 2673   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 194.6\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6070: Policy loss: 2.227704. Value loss: 17.387749. Entropy: 0.541407.\n",
      "Iteration 6071: Policy loss: 2.215232. Value loss: 10.481884. Entropy: 0.526340.\n",
      "Iteration 6072: Policy loss: 2.376009. Value loss: 9.553210. Entropy: 0.543538.\n",
      "episode: 2674   score: 105.0  epsilon: 1.0    steps: 161  evaluation reward: 194.85\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6073: Policy loss: -0.111506. Value loss: 19.812664. Entropy: 0.561283.\n",
      "Iteration 6074: Policy loss: -0.130091. Value loss: 12.901900. Entropy: 0.575659.\n",
      "Iteration 6075: Policy loss: -0.102573. Value loss: 10.732201. Entropy: 0.573930.\n",
      "episode: 2675   score: 135.0  epsilon: 1.0    steps: 851  evaluation reward: 194.85\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6076: Policy loss: -0.729285. Value loss: 27.984482. Entropy: 0.494964.\n",
      "Iteration 6077: Policy loss: -0.579487. Value loss: 17.753716. Entropy: 0.499989.\n",
      "Iteration 6078: Policy loss: -0.912527. Value loss: 14.617858. Entropy: 0.483919.\n",
      "episode: 2676   score: 210.0  epsilon: 1.0    steps: 1  evaluation reward: 194.55\n",
      "episode: 2677   score: 110.0  epsilon: 1.0    steps: 713  evaluation reward: 195.2\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6079: Policy loss: 1.420122. Value loss: 11.472574. Entropy: 0.733071.\n",
      "Iteration 6080: Policy loss: 1.308937. Value loss: 5.345633. Entropy: 0.718098.\n",
      "Iteration 6081: Policy loss: 1.289173. Value loss: 4.504779. Entropy: 0.728218.\n",
      "episode: 2678   score: 210.0  epsilon: 1.0    steps: 385  evaluation reward: 196.25\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6082: Policy loss: 0.249494. Value loss: 9.750199. Entropy: 0.485548.\n",
      "Iteration 6083: Policy loss: 0.102818. Value loss: 6.947136. Entropy: 0.505655.\n",
      "Iteration 6084: Policy loss: 0.212009. Value loss: 5.134916. Entropy: 0.512471.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6085: Policy loss: -0.031887. Value loss: 22.276382. Entropy: 0.526538.\n",
      "Iteration 6086: Policy loss: -0.025395. Value loss: 14.225993. Entropy: 0.547255.\n",
      "Iteration 6087: Policy loss: -0.110318. Value loss: 12.968532. Entropy: 0.543473.\n",
      "episode: 2679   score: 180.0  epsilon: 1.0    steps: 117  evaluation reward: 197.0\n",
      "episode: 2680   score: 215.0  epsilon: 1.0    steps: 246  evaluation reward: 197.6\n",
      "episode: 2681   score: 215.0  epsilon: 1.0    steps: 535  evaluation reward: 198.7\n",
      "episode: 2682   score: 155.0  epsilon: 1.0    steps: 810  evaluation reward: 199.15\n",
      "episode: 2683   score: 225.0  epsilon: 1.0    steps: 937  evaluation reward: 200.35\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6088: Policy loss: 1.275943. Value loss: 10.151005. Entropy: 0.791303.\n",
      "Iteration 6089: Policy loss: 1.063745. Value loss: 6.483815. Entropy: 0.823787.\n",
      "Iteration 6090: Policy loss: 1.160802. Value loss: 5.035543. Entropy: 0.830627.\n",
      "episode: 2684   score: 215.0  epsilon: 1.0    steps: 340  evaluation reward: 201.45\n",
      "episode: 2685   score: 135.0  epsilon: 1.0    steps: 736  evaluation reward: 201.7\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6091: Policy loss: 2.104944. Value loss: 15.873959. Entropy: 0.481108.\n",
      "Iteration 6092: Policy loss: 1.990258. Value loss: 11.718829. Entropy: 0.495927.\n",
      "Iteration 6093: Policy loss: 2.093499. Value loss: 11.424556. Entropy: 0.508558.\n",
      "episode: 2686   score: 155.0  epsilon: 1.0    steps: 406  evaluation reward: 202.2\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6094: Policy loss: -0.576092. Value loss: 15.220109. Entropy: 0.609763.\n",
      "Iteration 6095: Policy loss: -0.481238. Value loss: 9.521713. Entropy: 0.616352.\n",
      "Iteration 6096: Policy loss: -0.458052. Value loss: 7.992120. Entropy: 0.631372.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6097: Policy loss: -0.185962. Value loss: 25.254494. Entropy: 0.620344.\n",
      "Iteration 6098: Policy loss: -0.375841. Value loss: 17.825750. Entropy: 0.595819.\n",
      "Iteration 6099: Policy loss: 0.054393. Value loss: 14.749324. Entropy: 0.630465.\n",
      "episode: 2687   score: 105.0  epsilon: 1.0    steps: 343  evaluation reward: 201.45\n",
      "episode: 2688   score: 150.0  epsilon: 1.0    steps: 569  evaluation reward: 202.05\n",
      "episode: 2689   score: 180.0  epsilon: 1.0    steps: 943  evaluation reward: 201.55\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6100: Policy loss: -0.288651. Value loss: 17.649206. Entropy: 0.498242.\n",
      "Iteration 6101: Policy loss: -0.067576. Value loss: 8.850307. Entropy: 0.548368.\n",
      "Iteration 6102: Policy loss: -0.062796. Value loss: 8.537031. Entropy: 0.574228.\n",
      "episode: 2690   score: 135.0  epsilon: 1.0    steps: 159  evaluation reward: 201.85\n",
      "episode: 2691   score: 155.0  epsilon: 1.0    steps: 770  evaluation reward: 202.15\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6103: Policy loss: 0.655936. Value loss: 9.593639. Entropy: 0.610908.\n",
      "Iteration 6104: Policy loss: 0.593940. Value loss: 6.204087. Entropy: 0.641055.\n",
      "Iteration 6105: Policy loss: 0.585141. Value loss: 5.052524. Entropy: 0.645015.\n",
      "episode: 2692   score: 210.0  epsilon: 1.0    steps: 23  evaluation reward: 200.7\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6106: Policy loss: -0.210146. Value loss: 13.889439. Entropy: 0.373895.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6107: Policy loss: -0.230442. Value loss: 10.890643. Entropy: 0.385091.\n",
      "Iteration 6108: Policy loss: -0.192661. Value loss: 9.470179. Entropy: 0.396792.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6109: Policy loss: 3.264207. Value loss: 19.181974. Entropy: 0.586547.\n",
      "Iteration 6110: Policy loss: 2.914472. Value loss: 10.384481. Entropy: 0.581742.\n",
      "Iteration 6111: Policy loss: 3.085068. Value loss: 8.945456. Entropy: 0.578465.\n",
      "episode: 2693   score: 125.0  epsilon: 1.0    steps: 204  evaluation reward: 199.1\n",
      "episode: 2694   score: 75.0  epsilon: 1.0    steps: 274  evaluation reward: 198.75\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6112: Policy loss: 0.376848. Value loss: 29.930138. Entropy: 0.558422.\n",
      "Iteration 6113: Policy loss: 0.533303. Value loss: 14.247811. Entropy: 0.510568.\n",
      "Iteration 6114: Policy loss: 0.355699. Value loss: 10.310543. Entropy: 0.487603.\n",
      "episode: 2695   score: 260.0  epsilon: 1.0    steps: 509  evaluation reward: 200.9\n",
      "episode: 2696   score: 180.0  epsilon: 1.0    steps: 842  evaluation reward: 201.6\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6115: Policy loss: -0.520106. Value loss: 23.230278. Entropy: 0.563736.\n",
      "Iteration 6116: Policy loss: -0.531132. Value loss: 14.621874. Entropy: 0.537939.\n",
      "Iteration 6117: Policy loss: -0.797911. Value loss: 13.997368. Entropy: 0.555285.\n",
      "episode: 2697   score: 210.0  epsilon: 1.0    steps: 598  evaluation reward: 202.45\n",
      "episode: 2698   score: 330.0  epsilon: 1.0    steps: 657  evaluation reward: 203.7\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6118: Policy loss: -0.311055. Value loss: 15.073910. Entropy: 0.647183.\n",
      "Iteration 6119: Policy loss: -0.437288. Value loss: 9.531198. Entropy: 0.632323.\n",
      "Iteration 6120: Policy loss: -0.424018. Value loss: 7.886854. Entropy: 0.634399.\n",
      "episode: 2699   score: 80.0  epsilon: 1.0    steps: 225  evaluation reward: 203.4\n",
      "episode: 2700   score: 255.0  epsilon: 1.0    steps: 940  evaluation reward: 204.75\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6121: Policy loss: 0.685428. Value loss: 16.528860. Entropy: 0.568668.\n",
      "Iteration 6122: Policy loss: 0.559026. Value loss: 11.055885. Entropy: 0.576791.\n",
      "Iteration 6123: Policy loss: 0.776097. Value loss: 11.678439. Entropy: 0.552209.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6124: Policy loss: 0.012694. Value loss: 22.587353. Entropy: 0.556289.\n",
      "Iteration 6125: Policy loss: 0.164457. Value loss: 14.573550. Entropy: 0.586369.\n",
      "Iteration 6126: Policy loss: 0.128520. Value loss: 10.686487. Entropy: 0.568623.\n",
      "now time :  2019-02-25 20:34:31.404820\n",
      "episode: 2701   score: 230.0  epsilon: 1.0    steps: 104  evaluation reward: 205.25\n",
      "episode: 2702   score: 210.0  epsilon: 1.0    steps: 310  evaluation reward: 205.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6127: Policy loss: -0.430596. Value loss: 19.375198. Entropy: 0.563877.\n",
      "Iteration 6128: Policy loss: -0.551671. Value loss: 9.066156. Entropy: 0.556970.\n",
      "Iteration 6129: Policy loss: -0.701885. Value loss: 8.862671. Entropy: 0.561801.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6130: Policy loss: 0.317137. Value loss: 8.547125. Entropy: 0.523033.\n",
      "Iteration 6131: Policy loss: 0.320795. Value loss: 5.916555. Entropy: 0.470369.\n",
      "Iteration 6132: Policy loss: 0.362772. Value loss: 4.059043. Entropy: 0.489008.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6133: Policy loss: -0.207278. Value loss: 11.221082. Entropy: 0.519302.\n",
      "Iteration 6134: Policy loss: -0.187225. Value loss: 5.483317. Entropy: 0.537156.\n",
      "Iteration 6135: Policy loss: -0.287506. Value loss: 5.900683. Entropy: 0.532843.\n",
      "episode: 2703   score: 180.0  epsilon: 1.0    steps: 202  evaluation reward: 205.85\n",
      "episode: 2704   score: 225.0  epsilon: 1.0    steps: 440  evaluation reward: 205.7\n",
      "episode: 2705   score: 180.0  epsilon: 1.0    steps: 638  evaluation reward: 205.65\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6136: Policy loss: 0.597437. Value loss: 15.135323. Entropy: 0.632619.\n",
      "Iteration 6137: Policy loss: 0.627881. Value loss: 8.070449. Entropy: 0.632716.\n",
      "Iteration 6138: Policy loss: 0.591006. Value loss: 6.537755. Entropy: 0.641433.\n",
      "episode: 2706   score: 260.0  epsilon: 1.0    steps: 740  evaluation reward: 206.15\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6139: Policy loss: -2.273892. Value loss: 15.094036. Entropy: 0.521320.\n",
      "Iteration 6140: Policy loss: -2.288437. Value loss: 10.580995. Entropy: 0.503151.\n",
      "Iteration 6141: Policy loss: -2.214951. Value loss: 9.346909. Entropy: 0.515972.\n",
      "episode: 2707   score: 180.0  epsilon: 1.0    steps: 23  evaluation reward: 206.3\n",
      "episode: 2708   score: 180.0  epsilon: 1.0    steps: 283  evaluation reward: 205.9\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6142: Policy loss: -0.749048. Value loss: 13.642438. Entropy: 0.593171.\n",
      "Iteration 6143: Policy loss: -0.803259. Value loss: 8.647600. Entropy: 0.564201.\n",
      "Iteration 6144: Policy loss: -0.503206. Value loss: 5.201641. Entropy: 0.572177.\n",
      "episode: 2709   score: 300.0  epsilon: 1.0    steps: 855  evaluation reward: 202.95\n",
      "episode: 2710   score: 345.0  epsilon: 1.0    steps: 1011  evaluation reward: 204.0\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6145: Policy loss: -0.849856. Value loss: 18.598217. Entropy: 0.481857.\n",
      "Iteration 6146: Policy loss: -0.645918. Value loss: 10.388958. Entropy: 0.461260.\n",
      "Iteration 6147: Policy loss: -0.861085. Value loss: 9.210762. Entropy: 0.470382.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6148: Policy loss: -1.238008. Value loss: 20.144312. Entropy: 0.590557.\n",
      "Iteration 6149: Policy loss: -1.232562. Value loss: 12.944678. Entropy: 0.578939.\n",
      "Iteration 6150: Policy loss: -1.245189. Value loss: 11.237123. Entropy: 0.570505.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6151: Policy loss: 0.469021. Value loss: 7.083107. Entropy: 0.525778.\n",
      "Iteration 6152: Policy loss: 0.484368. Value loss: 4.361990. Entropy: 0.533896.\n",
      "Iteration 6153: Policy loss: 0.396688. Value loss: 5.568472. Entropy: 0.512541.\n",
      "episode: 2711   score: 180.0  epsilon: 1.0    steps: 83  evaluation reward: 204.75\n",
      "episode: 2712   score: 210.0  epsilon: 1.0    steps: 180  evaluation reward: 205.05\n",
      "episode: 2713   score: 180.0  epsilon: 1.0    steps: 573  evaluation reward: 204.55\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6154: Policy loss: 0.717446. Value loss: 13.089911. Entropy: 0.471333.\n",
      "Iteration 6155: Policy loss: 0.572021. Value loss: 8.772882. Entropy: 0.487275.\n",
      "Iteration 6156: Policy loss: 0.545341. Value loss: 6.617737. Entropy: 0.494043.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6157: Policy loss: -0.490382. Value loss: 15.699213. Entropy: 0.653680.\n",
      "Iteration 6158: Policy loss: -0.491908. Value loss: 8.600585. Entropy: 0.622271.\n",
      "Iteration 6159: Policy loss: -0.718418. Value loss: 6.458332. Entropy: 0.653101.\n",
      "episode: 2714   score: 260.0  epsilon: 1.0    steps: 404  evaluation reward: 202.0\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6160: Policy loss: -0.637672. Value loss: 9.774823. Entropy: 0.531863.\n",
      "Iteration 6161: Policy loss: -0.581352. Value loss: 5.503625. Entropy: 0.514085.\n",
      "Iteration 6162: Policy loss: -0.539912. Value loss: 6.192216. Entropy: 0.531010.\n",
      "episode: 2715   score: 260.0  epsilon: 1.0    steps: 366  evaluation reward: 202.95\n",
      "episode: 2716   score: 260.0  epsilon: 1.0    steps: 705  evaluation reward: 203.4\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6163: Policy loss: 1.232100. Value loss: 13.373264. Entropy: 0.461388.\n",
      "Iteration 6164: Policy loss: 1.106373. Value loss: 10.988552. Entropy: 0.466790.\n",
      "Iteration 6165: Policy loss: 1.124025. Value loss: 8.768652. Entropy: 0.461266.\n",
      "episode: 2717   score: 155.0  epsilon: 1.0    steps: 81  evaluation reward: 203.15\n",
      "episode: 2718   score: 125.0  epsilon: 1.0    steps: 600  evaluation reward: 203.3\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6166: Policy loss: 1.770676. Value loss: 20.661718. Entropy: 0.620341.\n",
      "Iteration 6167: Policy loss: 1.455656. Value loss: 9.862726. Entropy: 0.602132.\n",
      "Iteration 6168: Policy loss: 1.706538. Value loss: 8.011683. Entropy: 0.591692.\n",
      "episode: 2719   score: 105.0  epsilon: 1.0    steps: 457  evaluation reward: 202.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2720   score: 260.0  epsilon: 1.0    steps: 807  evaluation reward: 203.05\n",
      "episode: 2721   score: 285.0  epsilon: 1.0    steps: 958  evaluation reward: 203.6\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6169: Policy loss: -0.322445. Value loss: 15.786114. Entropy: 0.629484.\n",
      "Iteration 6170: Policy loss: -0.485089. Value loss: 9.063816. Entropy: 0.617546.\n",
      "Iteration 6171: Policy loss: -0.506371. Value loss: 8.829552. Entropy: 0.625314.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6172: Policy loss: -0.904330. Value loss: 24.971134. Entropy: 0.541578.\n",
      "Iteration 6173: Policy loss: -0.669767. Value loss: 13.809347. Entropy: 0.513287.\n",
      "Iteration 6174: Policy loss: -0.883942. Value loss: 10.102118. Entropy: 0.494322.\n",
      "episode: 2722   score: 265.0  epsilon: 1.0    steps: 134  evaluation reward: 204.7\n",
      "episode: 2723   score: 135.0  epsilon: 1.0    steps: 722  evaluation reward: 203.95\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6175: Policy loss: -0.193748. Value loss: 25.828869. Entropy: 0.563596.\n",
      "Iteration 6176: Policy loss: -0.331514. Value loss: 13.996407. Entropy: 0.567849.\n",
      "Iteration 6177: Policy loss: -0.110429. Value loss: 12.233892. Entropy: 0.556636.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6178: Policy loss: 0.822891. Value loss: 10.372147. Entropy: 0.571204.\n",
      "Iteration 6179: Policy loss: 0.742787. Value loss: 7.295257. Entropy: 0.553187.\n",
      "Iteration 6180: Policy loss: 0.610546. Value loss: 6.811535. Entropy: 0.564224.\n",
      "episode: 2724   score: 105.0  epsilon: 1.0    steps: 462  evaluation reward: 202.9\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6181: Policy loss: 0.252827. Value loss: 23.607416. Entropy: 0.618828.\n",
      "Iteration 6182: Policy loss: 0.215595. Value loss: 15.405436. Entropy: 0.604550.\n",
      "Iteration 6183: Policy loss: 0.385501. Value loss: 11.530699. Entropy: 0.613809.\n",
      "episode: 2725   score: 125.0  epsilon: 1.0    steps: 907  evaluation reward: 202.05\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6184: Policy loss: -0.278742. Value loss: 15.886048. Entropy: 0.637758.\n",
      "Iteration 6185: Policy loss: 0.186648. Value loss: 8.226166. Entropy: 0.588104.\n",
      "Iteration 6186: Policy loss: -0.130310. Value loss: 6.560696. Entropy: 0.613062.\n",
      "episode: 2726   score: 155.0  epsilon: 1.0    steps: 675  evaluation reward: 201.65\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6187: Policy loss: -1.801094. Value loss: 18.326429. Entropy: 0.495901.\n",
      "Iteration 6188: Policy loss: -1.914226. Value loss: 12.230834. Entropy: 0.484379.\n",
      "Iteration 6189: Policy loss: -1.825375. Value loss: 6.771861. Entropy: 0.488293.\n",
      "episode: 2727   score: 285.0  epsilon: 1.0    steps: 32  evaluation reward: 202.7\n",
      "episode: 2728   score: 240.0  epsilon: 1.0    steps: 239  evaluation reward: 203.0\n",
      "episode: 2729   score: 275.0  epsilon: 1.0    steps: 603  evaluation reward: 202.15\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6190: Policy loss: -0.916831. Value loss: 22.509834. Entropy: 0.545277.\n",
      "Iteration 6191: Policy loss: -1.073926. Value loss: 11.666092. Entropy: 0.512712.\n",
      "Iteration 6192: Policy loss: -0.984928. Value loss: 6.939800. Entropy: 0.515901.\n",
      "episode: 2730   score: 180.0  epsilon: 1.0    steps: 461  evaluation reward: 201.85\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6193: Policy loss: -0.827794. Value loss: 21.630552. Entropy: 0.658951.\n",
      "Iteration 6194: Policy loss: -0.710306. Value loss: 11.542815. Entropy: 0.663416.\n",
      "Iteration 6195: Policy loss: -0.668392. Value loss: 8.485325. Entropy: 0.636857.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6196: Policy loss: -2.869030. Value loss: 186.825684. Entropy: 0.508542.\n",
      "Iteration 6197: Policy loss: -1.451923. Value loss: 32.125961. Entropy: 0.463513.\n",
      "Iteration 6198: Policy loss: -1.844237. Value loss: 24.550694. Entropy: 0.482268.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6199: Policy loss: 0.834377. Value loss: 27.499346. Entropy: 0.510772.\n",
      "Iteration 6200: Policy loss: 0.798789. Value loss: 13.882559. Entropy: 0.520142.\n",
      "Iteration 6201: Policy loss: 0.866952. Value loss: 8.372970. Entropy: 0.511451.\n",
      "episode: 2731   score: 595.0  epsilon: 1.0    steps: 852  evaluation reward: 205.1\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6202: Policy loss: 0.385199. Value loss: 25.104902. Entropy: 0.424216.\n",
      "Iteration 6203: Policy loss: 0.083563. Value loss: 14.060814. Entropy: 0.384178.\n",
      "Iteration 6204: Policy loss: 0.265533. Value loss: 10.960144. Entropy: 0.402686.\n",
      "episode: 2732   score: 440.0  epsilon: 1.0    steps: 292  evaluation reward: 206.95\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6205: Policy loss: 1.602897. Value loss: 15.458366. Entropy: 0.636139.\n",
      "Iteration 6206: Policy loss: 1.720729. Value loss: 7.473301. Entropy: 0.684089.\n",
      "Iteration 6207: Policy loss: 1.426559. Value loss: 4.784507. Entropy: 0.666882.\n",
      "episode: 2733   score: 260.0  epsilon: 1.0    steps: 674  evaluation reward: 207.15\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6208: Policy loss: 0.640021. Value loss: 19.306425. Entropy: 0.483846.\n",
      "Iteration 6209: Policy loss: 0.371457. Value loss: 8.521328. Entropy: 0.481554.\n",
      "Iteration 6210: Policy loss: 0.540210. Value loss: 6.375288. Entropy: 0.502895.\n",
      "episode: 2734   score: 260.0  epsilon: 1.0    steps: 102  evaluation reward: 206.85\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6211: Policy loss: 1.205794. Value loss: 17.211926. Entropy: 0.459486.\n",
      "Iteration 6212: Policy loss: 1.200114. Value loss: 8.722775. Entropy: 0.443765.\n",
      "Iteration 6213: Policy loss: 1.376709. Value loss: 7.626610. Entropy: 0.454697.\n",
      "episode: 2735   score: 180.0  epsilon: 1.0    steps: 212  evaluation reward: 206.05\n",
      "episode: 2736   score: 260.0  epsilon: 1.0    steps: 551  evaluation reward: 206.1\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6214: Policy loss: -0.512577. Value loss: 17.187368. Entropy: 0.698083.\n",
      "Iteration 6215: Policy loss: -0.645895. Value loss: 9.125328. Entropy: 0.726510.\n",
      "Iteration 6216: Policy loss: -0.559871. Value loss: 5.857847. Entropy: 0.704224.\n",
      "episode: 2737   score: 180.0  epsilon: 1.0    steps: 349  evaluation reward: 206.85\n",
      "episode: 2738   score: 320.0  epsilon: 1.0    steps: 906  evaluation reward: 208.95\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6217: Policy loss: 0.884359. Value loss: 11.958715. Entropy: 0.507303.\n",
      "Iteration 6218: Policy loss: 0.684640. Value loss: 8.697530. Entropy: 0.532237.\n",
      "Iteration 6219: Policy loss: 0.864114. Value loss: 8.141672. Entropy: 0.551255.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6220: Policy loss: 0.404388. Value loss: 14.611872. Entropy: 0.553281.\n",
      "Iteration 6221: Policy loss: 0.450207. Value loss: 10.483685. Entropy: 0.572095.\n",
      "Iteration 6222: Policy loss: 0.545055. Value loss: 7.129550. Entropy: 0.581389.\n",
      "episode: 2739   score: 290.0  epsilon: 1.0    steps: 415  evaluation reward: 209.6\n",
      "episode: 2740   score: 230.0  epsilon: 1.0    steps: 884  evaluation reward: 210.85\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6223: Policy loss: -0.256421. Value loss: 23.911310. Entropy: 0.645564.\n",
      "Iteration 6224: Policy loss: -0.312959. Value loss: 13.706730. Entropy: 0.658307.\n",
      "Iteration 6225: Policy loss: -0.358913. Value loss: 9.961338. Entropy: 0.617510.\n",
      "episode: 2741   score: 90.0  epsilon: 1.0    steps: 146  evaluation reward: 209.15\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6226: Policy loss: -0.470977. Value loss: 23.047131. Entropy: 0.548178.\n",
      "Iteration 6227: Policy loss: -0.351321. Value loss: 11.093725. Entropy: 0.531646.\n",
      "Iteration 6228: Policy loss: -0.732276. Value loss: 10.185678. Entropy: 0.523305.\n",
      "episode: 2742   score: 170.0  epsilon: 1.0    steps: 616  evaluation reward: 209.5\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6229: Policy loss: 0.158863. Value loss: 23.976185. Entropy: 0.657623.\n",
      "Iteration 6230: Policy loss: -0.018356. Value loss: 10.935727. Entropy: 0.652533.\n",
      "Iteration 6231: Policy loss: 0.045109. Value loss: 8.184238. Entropy: 0.638968.\n",
      "episode: 2743   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 209.5\n",
      "episode: 2744   score: 210.0  epsilon: 1.0    steps: 364  evaluation reward: 208.75\n",
      "episode: 2745   score: 105.0  epsilon: 1.0    steps: 443  evaluation reward: 206.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2746   score: 195.0  epsilon: 1.0    steps: 647  evaluation reward: 203.65\n",
      "episode: 2747   score: 210.0  epsilon: 1.0    steps: 929  evaluation reward: 203.6\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6232: Policy loss: -1.076785. Value loss: 10.264053. Entropy: 0.692033.\n",
      "Iteration 6233: Policy loss: -0.931731. Value loss: 8.026813. Entropy: 0.680092.\n",
      "Iteration 6234: Policy loss: -0.853935. Value loss: 4.969696. Entropy: 0.709407.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6235: Policy loss: -0.132101. Value loss: 27.164579. Entropy: 0.428559.\n",
      "Iteration 6236: Policy loss: -0.327942. Value loss: 16.879675. Entropy: 0.475739.\n",
      "Iteration 6237: Policy loss: -0.064920. Value loss: 14.874904. Entropy: 0.439136.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6238: Policy loss: 0.091908. Value loss: 27.845161. Entropy: 0.588247.\n",
      "Iteration 6239: Policy loss: 0.162840. Value loss: 15.620988. Entropy: 0.599234.\n",
      "Iteration 6240: Policy loss: 0.195421. Value loss: 12.714391. Entropy: 0.577168.\n",
      "episode: 2748   score: 105.0  epsilon: 1.0    steps: 549  evaluation reward: 203.6\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6241: Policy loss: 1.422591. Value loss: 20.556292. Entropy: 0.500004.\n",
      "Iteration 6242: Policy loss: 1.276088. Value loss: 9.996685. Entropy: 0.510114.\n",
      "Iteration 6243: Policy loss: 1.366467. Value loss: 7.624808. Entropy: 0.529593.\n",
      "episode: 2749   score: 105.0  epsilon: 1.0    steps: 263  evaluation reward: 202.85\n",
      "episode: 2750   score: 150.0  epsilon: 1.0    steps: 693  evaluation reward: 202.55\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6244: Policy loss: 0.700499. Value loss: 22.077621. Entropy: 0.613960.\n",
      "Iteration 6245: Policy loss: 0.672173. Value loss: 11.208242. Entropy: 0.658610.\n",
      "Iteration 6246: Policy loss: 0.549879. Value loss: 11.646822. Entropy: 0.624016.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6247: Policy loss: 0.526842. Value loss: 14.487644. Entropy: 0.653833.\n",
      "Iteration 6248: Policy loss: 0.511378. Value loss: 6.321907. Entropy: 0.636666.\n",
      "Iteration 6249: Policy loss: 0.536766. Value loss: 5.802545. Entropy: 0.626167.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6250: Policy loss: 0.703533. Value loss: 10.463536. Entropy: 0.611959.\n",
      "Iteration 6251: Policy loss: 0.745466. Value loss: 5.094276. Entropy: 0.625319.\n",
      "Iteration 6252: Policy loss: 0.677294. Value loss: 4.084629. Entropy: 0.619866.\n",
      "now time :  2019-02-25 20:36:52.863707\n",
      "episode: 2751   score: 225.0  epsilon: 1.0    steps: 68  evaluation reward: 203.25\n",
      "episode: 2752   score: 315.0  epsilon: 1.0    steps: 438  evaluation reward: 204.85\n",
      "episode: 2753   score: 155.0  epsilon: 1.0    steps: 608  evaluation reward: 205.3\n",
      "episode: 2754   score: 270.0  epsilon: 1.0    steps: 998  evaluation reward: 206.45\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6253: Policy loss: 1.109100. Value loss: 24.294546. Entropy: 0.482291.\n",
      "Iteration 6254: Policy loss: 1.140462. Value loss: 10.549373. Entropy: 0.476666.\n",
      "Iteration 6255: Policy loss: 1.148536. Value loss: 9.250554. Entropy: 0.470358.\n",
      "episode: 2755   score: 210.0  epsilon: 1.0    steps: 747  evaluation reward: 205.85\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6256: Policy loss: -1.181384. Value loss: 15.150983. Entropy: 0.595791.\n",
      "Iteration 6257: Policy loss: -1.071108. Value loss: 9.398421. Entropy: 0.569701.\n",
      "Iteration 6258: Policy loss: -1.254832. Value loss: 8.490749. Entropy: 0.550543.\n",
      "episode: 2756   score: 435.0  epsilon: 1.0    steps: 231  evaluation reward: 208.35\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6259: Policy loss: -0.740056. Value loss: 12.126002. Entropy: 0.430837.\n",
      "Iteration 6260: Policy loss: -0.807048. Value loss: 7.526555. Entropy: 0.463703.\n",
      "Iteration 6261: Policy loss: -0.859470. Value loss: 8.048632. Entropy: 0.459318.\n",
      "episode: 2757   score: 210.0  epsilon: 1.0    steps: 341  evaluation reward: 208.65\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6262: Policy loss: -0.531752. Value loss: 16.174894. Entropy: 0.375442.\n",
      "Iteration 6263: Policy loss: -0.639523. Value loss: 11.277678. Entropy: 0.375575.\n",
      "Iteration 6264: Policy loss: -0.435067. Value loss: 8.080521. Entropy: 0.383023.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6265: Policy loss: -0.040397. Value loss: 10.664635. Entropy: 0.538805.\n",
      "Iteration 6266: Policy loss: -0.047937. Value loss: 5.903940. Entropy: 0.540686.\n",
      "Iteration 6267: Policy loss: -0.065584. Value loss: 3.710119. Entropy: 0.548924.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6268: Policy loss: -1.717871. Value loss: 14.984976. Entropy: 0.564536.\n",
      "Iteration 6269: Policy loss: -1.708056. Value loss: 8.100490. Entropy: 0.556290.\n",
      "Iteration 6270: Policy loss: -1.780244. Value loss: 6.360506. Entropy: 0.566063.\n",
      "episode: 2758   score: 265.0  epsilon: 1.0    steps: 62  evaluation reward: 209.0\n",
      "episode: 2759   score: 210.0  epsilon: 1.0    steps: 677  evaluation reward: 210.0\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6271: Policy loss: 0.413398. Value loss: 15.216168. Entropy: 0.485455.\n",
      "Iteration 6272: Policy loss: 0.508855. Value loss: 7.898801. Entropy: 0.517473.\n",
      "Iteration 6273: Policy loss: 0.540733. Value loss: 6.837112. Entropy: 0.523787.\n",
      "episode: 2760   score: 410.0  epsilon: 1.0    steps: 828  evaluation reward: 211.15\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6274: Policy loss: -0.224607. Value loss: 10.464566. Entropy: 0.511245.\n",
      "Iteration 6275: Policy loss: -0.291803. Value loss: 8.115193. Entropy: 0.536873.\n",
      "Iteration 6276: Policy loss: -0.318264. Value loss: 7.680307. Entropy: 0.540534.\n",
      "episode: 2761   score: 240.0  epsilon: 1.0    steps: 396  evaluation reward: 211.35\n",
      "episode: 2762   score: 260.0  epsilon: 1.0    steps: 561  evaluation reward: 210.75\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6277: Policy loss: -1.167667. Value loss: 10.147532. Entropy: 0.395058.\n",
      "Iteration 6278: Policy loss: -1.080512. Value loss: 7.018925. Entropy: 0.427296.\n",
      "Iteration 6279: Policy loss: -1.126656. Value loss: 6.087391. Entropy: 0.390843.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6280: Policy loss: -0.036399. Value loss: 14.144688. Entropy: 0.314809.\n",
      "Iteration 6281: Policy loss: 0.050261. Value loss: 10.331729. Entropy: 0.311554.\n",
      "Iteration 6282: Policy loss: 0.036498. Value loss: 9.337134. Entropy: 0.315274.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6283: Policy loss: 0.457823. Value loss: 19.616531. Entropy: 0.478680.\n",
      "Iteration 6284: Policy loss: 0.505136. Value loss: 8.871803. Entropy: 0.474054.\n",
      "Iteration 6285: Policy loss: 0.381458. Value loss: 7.024915. Entropy: 0.480936.\n",
      "episode: 2763   score: 325.0  epsilon: 1.0    steps: 193  evaluation reward: 211.15\n",
      "episode: 2764   score: 275.0  epsilon: 1.0    steps: 337  evaluation reward: 212.1\n",
      "episode: 2765   score: 75.0  epsilon: 1.0    steps: 626  evaluation reward: 210.45\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6286: Policy loss: -1.593247. Value loss: 276.330719. Entropy: 0.572250.\n",
      "Iteration 6287: Policy loss: -1.350970. Value loss: 75.760841. Entropy: 0.593639.\n",
      "Iteration 6288: Policy loss: -1.848024. Value loss: 75.617554. Entropy: 0.584961.\n",
      "episode: 2766   score: 590.0  epsilon: 1.0    steps: 959  evaluation reward: 214.25\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6289: Policy loss: -1.472979. Value loss: 139.382278. Entropy: 0.382327.\n",
      "Iteration 6290: Policy loss: -1.472305. Value loss: 114.886467. Entropy: 0.376164.\n",
      "Iteration 6291: Policy loss: -1.708028. Value loss: 107.733383. Entropy: 0.404474.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6292: Policy loss: 2.488248. Value loss: 50.292645. Entropy: 0.403649.\n",
      "Iteration 6293: Policy loss: 2.786650. Value loss: 24.473057. Entropy: 0.371668.\n",
      "Iteration 6294: Policy loss: 2.372575. Value loss: 16.918818. Entropy: 0.393649.\n",
      "episode: 2767   score: 260.0  epsilon: 1.0    steps: 15  evaluation reward: 214.6\n",
      "episode: 2768   score: 260.0  epsilon: 1.0    steps: 414  evaluation reward: 214.9\n",
      "episode: 2769   score: 460.0  epsilon: 1.0    steps: 642  evaluation reward: 217.7\n",
      "episode: 2770   score: 260.0  epsilon: 1.0    steps: 772  evaluation reward: 219.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6295: Policy loss: 2.144563. Value loss: 18.404728. Entropy: 0.660342.\n",
      "Iteration 6296: Policy loss: 2.273962. Value loss: 10.577043. Entropy: 0.634072.\n",
      "Iteration 6297: Policy loss: 2.194955. Value loss: 11.572173. Entropy: 0.650888.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6298: Policy loss: 0.462230. Value loss: 14.276697. Entropy: 0.336565.\n",
      "Iteration 6299: Policy loss: 0.268602. Value loss: 13.685552. Entropy: 0.327097.\n",
      "Iteration 6300: Policy loss: 0.316896. Value loss: 12.165421. Entropy: 0.353349.\n",
      "episode: 2771   score: 180.0  epsilon: 1.0    steps: 356  evaluation reward: 218.05\n",
      "episode: 2772   score: 120.0  epsilon: 1.0    steps: 984  evaluation reward: 217.75\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6301: Policy loss: 2.302907. Value loss: 39.566227. Entropy: 0.589135.\n",
      "Iteration 6302: Policy loss: 2.157692. Value loss: 21.342579. Entropy: 0.593882.\n",
      "Iteration 6303: Policy loss: 2.173975. Value loss: 17.118761. Entropy: 0.572306.\n",
      "episode: 2773   score: 215.0  epsilon: 1.0    steps: 617  evaluation reward: 218.1\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6304: Policy loss: 1.505738. Value loss: 13.478526. Entropy: 0.670480.\n",
      "Iteration 6305: Policy loss: 1.395760. Value loss: 7.177929. Entropy: 0.655118.\n",
      "Iteration 6306: Policy loss: 1.717220. Value loss: 4.860089. Entropy: 0.691863.\n",
      "episode: 2774   score: 150.0  epsilon: 1.0    steps: 82  evaluation reward: 218.55\n",
      "episode: 2775   score: 180.0  epsilon: 1.0    steps: 687  evaluation reward: 219.0\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6307: Policy loss: -0.603023. Value loss: 19.441652. Entropy: 0.367673.\n",
      "Iteration 6308: Policy loss: -0.294180. Value loss: 12.564157. Entropy: 0.365077.\n",
      "Iteration 6309: Policy loss: -0.352330. Value loss: 11.257233. Entropy: 0.347870.\n",
      "episode: 2776   score: 285.0  epsilon: 1.0    steps: 147  evaluation reward: 219.75\n",
      "episode: 2777   score: 255.0  epsilon: 1.0    steps: 510  evaluation reward: 221.2\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6310: Policy loss: -0.079920. Value loss: 20.446552. Entropy: 0.476691.\n",
      "Iteration 6311: Policy loss: 0.037000. Value loss: 10.394917. Entropy: 0.452858.\n",
      "Iteration 6312: Policy loss: -0.071547. Value loss: 10.763573. Entropy: 0.464028.\n",
      "episode: 2778   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 221.2\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6313: Policy loss: 2.082081. Value loss: 20.591696. Entropy: 0.493663.\n",
      "Iteration 6314: Policy loss: 1.868243. Value loss: 11.936905. Entropy: 0.495188.\n",
      "Iteration 6315: Policy loss: 2.071488. Value loss: 11.630399. Entropy: 0.512627.\n",
      "episode: 2779   score: 105.0  epsilon: 1.0    steps: 624  evaluation reward: 220.45\n",
      "episode: 2780   score: 110.0  epsilon: 1.0    steps: 734  evaluation reward: 219.4\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6316: Policy loss: 2.833616. Value loss: 17.833088. Entropy: 0.517500.\n",
      "Iteration 6317: Policy loss: 2.766953. Value loss: 8.603065. Entropy: 0.516390.\n",
      "Iteration 6318: Policy loss: 2.562024. Value loss: 7.169734. Entropy: 0.546597.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6319: Policy loss: 0.153187. Value loss: 13.763657. Entropy: 0.727788.\n",
      "Iteration 6320: Policy loss: 0.472728. Value loss: 8.400081. Entropy: 0.743309.\n",
      "Iteration 6321: Policy loss: 0.380623. Value loss: 6.935505. Entropy: 0.728410.\n",
      "episode: 2781   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 219.35\n",
      "episode: 2782   score: 260.0  epsilon: 1.0    steps: 312  evaluation reward: 220.4\n",
      "episode: 2783   score: 75.0  epsilon: 1.0    steps: 504  evaluation reward: 218.9\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6322: Policy loss: 2.272286. Value loss: 21.322645. Entropy: 0.475716.\n",
      "Iteration 6323: Policy loss: 2.401937. Value loss: 11.324592. Entropy: 0.484197.\n",
      "Iteration 6324: Policy loss: 2.236861. Value loss: 9.327446. Entropy: 0.490911.\n",
      "episode: 2784   score: 265.0  epsilon: 1.0    steps: 978  evaluation reward: 219.4\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6325: Policy loss: 0.566330. Value loss: 16.313763. Entropy: 0.460108.\n",
      "Iteration 6326: Policy loss: 0.855587. Value loss: 12.735092. Entropy: 0.475410.\n",
      "Iteration 6327: Policy loss: 0.664824. Value loss: 10.556171. Entropy: 0.465974.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6328: Policy loss: -0.839637. Value loss: 14.929778. Entropy: 0.337912.\n",
      "Iteration 6329: Policy loss: -0.811092. Value loss: 8.641848. Entropy: 0.337516.\n",
      "Iteration 6330: Policy loss: -0.854484. Value loss: 8.872726. Entropy: 0.347602.\n",
      "episode: 2785   score: 260.0  epsilon: 1.0    steps: 55  evaluation reward: 220.65\n",
      "episode: 2786   score: 215.0  epsilon: 1.0    steps: 826  evaluation reward: 221.25\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6331: Policy loss: 1.702986. Value loss: 14.208127. Entropy: 0.464339.\n",
      "Iteration 6332: Policy loss: 1.595204. Value loss: 8.326952. Entropy: 0.474873.\n",
      "Iteration 6333: Policy loss: 1.563526. Value loss: 6.881047. Entropy: 0.480944.\n",
      "episode: 2787   score: 180.0  epsilon: 1.0    steps: 554  evaluation reward: 222.0\n",
      "episode: 2788   score: 260.0  epsilon: 1.0    steps: 767  evaluation reward: 223.1\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6334: Policy loss: -1.701927. Value loss: 15.479845. Entropy: 0.608595.\n",
      "Iteration 6335: Policy loss: -1.942152. Value loss: 9.363408. Entropy: 0.611619.\n",
      "Iteration 6336: Policy loss: -1.912418. Value loss: 9.038160. Entropy: 0.632382.\n",
      "episode: 2789   score: 210.0  epsilon: 1.0    steps: 321  evaluation reward: 223.4\n",
      "episode: 2790   score: 140.0  epsilon: 1.0    steps: 992  evaluation reward: 223.45\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6337: Policy loss: 1.255623. Value loss: 22.305014. Entropy: 0.539683.\n",
      "Iteration 6338: Policy loss: 1.520942. Value loss: 11.594499. Entropy: 0.562382.\n",
      "Iteration 6339: Policy loss: 1.251080. Value loss: 11.084535. Entropy: 0.557842.\n",
      "episode: 2791   score: 215.0  epsilon: 1.0    steps: 150  evaluation reward: 224.05\n",
      "episode: 2792   score: 210.0  epsilon: 1.0    steps: 445  evaluation reward: 224.05\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6340: Policy loss: -0.470803. Value loss: 11.104973. Entropy: 0.561576.\n",
      "Iteration 6341: Policy loss: -0.274339. Value loss: 8.768407. Entropy: 0.535543.\n",
      "Iteration 6342: Policy loss: -0.568879. Value loss: 6.303964. Entropy: 0.574586.\n",
      "episode: 2793   score: 155.0  epsilon: 1.0    steps: 878  evaluation reward: 224.35\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6343: Policy loss: 0.802694. Value loss: 20.843431. Entropy: 0.464867.\n",
      "Iteration 6344: Policy loss: 0.803437. Value loss: 13.762076. Entropy: 0.464075.\n",
      "Iteration 6345: Policy loss: 0.742880. Value loss: 11.501017. Entropy: 0.478349.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6346: Policy loss: 0.207507. Value loss: 21.452736. Entropy: 0.693374.\n",
      "Iteration 6347: Policy loss: 0.312450. Value loss: 11.886757. Entropy: 0.673579.\n",
      "Iteration 6348: Policy loss: 0.259438. Value loss: 10.145105. Entropy: 0.673912.\n",
      "episode: 2794   score: 255.0  epsilon: 1.0    steps: 70  evaluation reward: 226.15\n",
      "episode: 2795   score: 105.0  epsilon: 1.0    steps: 161  evaluation reward: 224.6\n",
      "episode: 2796   score: 210.0  epsilon: 1.0    steps: 689  evaluation reward: 224.9\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6349: Policy loss: 0.165576. Value loss: 18.751011. Entropy: 0.500988.\n",
      "Iteration 6350: Policy loss: 0.226150. Value loss: 9.993909. Entropy: 0.516454.\n",
      "Iteration 6351: Policy loss: 0.139218. Value loss: 9.471893. Entropy: 0.520918.\n",
      "episode: 2797   score: 245.0  epsilon: 1.0    steps: 384  evaluation reward: 225.25\n",
      "episode: 2798   score: 125.0  epsilon: 1.0    steps: 901  evaluation reward: 223.2\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6352: Policy loss: -1.471569. Value loss: 11.456647. Entropy: 0.465361.\n",
      "Iteration 6353: Policy loss: -1.380866. Value loss: 7.156668. Entropy: 0.479733.\n",
      "Iteration 6354: Policy loss: -1.413139. Value loss: 8.065569. Entropy: 0.487784.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6355: Policy loss: -0.508264. Value loss: 17.081482. Entropy: 0.421658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6356: Policy loss: -0.579541. Value loss: 11.688658. Entropy: 0.402339.\n",
      "Iteration 6357: Policy loss: -0.476607. Value loss: 11.763608. Entropy: 0.387084.\n",
      "episode: 2799   score: 240.0  epsilon: 1.0    steps: 523  evaluation reward: 224.8\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6358: Policy loss: 0.871492. Value loss: 15.079495. Entropy: 0.483442.\n",
      "Iteration 6359: Policy loss: 0.894980. Value loss: 8.890687. Entropy: 0.476924.\n",
      "Iteration 6360: Policy loss: 0.921607. Value loss: 7.421604. Entropy: 0.474628.\n",
      "episode: 2800   score: 155.0  epsilon: 1.0    steps: 60  evaluation reward: 223.8\n",
      "now time :  2019-02-25 20:38:54.606854\n",
      "episode: 2801   score: 285.0  epsilon: 1.0    steps: 408  evaluation reward: 224.35\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6361: Policy loss: 1.021776. Value loss: 16.105803. Entropy: 0.543774.\n",
      "Iteration 6362: Policy loss: 0.866455. Value loss: 10.374643. Entropy: 0.542875.\n",
      "Iteration 6363: Policy loss: 0.984181. Value loss: 8.319901. Entropy: 0.556634.\n",
      "episode: 2802   score: 210.0  epsilon: 1.0    steps: 198  evaluation reward: 224.35\n",
      "episode: 2803   score: 215.0  epsilon: 1.0    steps: 736  evaluation reward: 224.7\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6364: Policy loss: -0.576630. Value loss: 202.303253. Entropy: 0.565180.\n",
      "Iteration 6365: Policy loss: -1.674077. Value loss: 85.601341. Entropy: 0.505483.\n",
      "Iteration 6366: Policy loss: -0.573030. Value loss: 59.485203. Entropy: 0.547625.\n",
      "episode: 2804   score: 105.0  epsilon: 1.0    steps: 534  evaluation reward: 223.5\n",
      "episode: 2805   score: 460.0  epsilon: 1.0    steps: 837  evaluation reward: 226.3\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6367: Policy loss: 0.456854. Value loss: 14.073398. Entropy: 0.535018.\n",
      "Iteration 6368: Policy loss: 0.310216. Value loss: 12.000978. Entropy: 0.540352.\n",
      "Iteration 6369: Policy loss: 0.431705. Value loss: 8.777774. Entropy: 0.554228.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6370: Policy loss: 0.278818. Value loss: 25.573591. Entropy: 0.412936.\n",
      "Iteration 6371: Policy loss: 0.212865. Value loss: 18.796368. Entropy: 0.394947.\n",
      "Iteration 6372: Policy loss: 0.192140. Value loss: 12.694118. Entropy: 0.404561.\n",
      "episode: 2806   score: 225.0  epsilon: 1.0    steps: 977  evaluation reward: 225.95\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6373: Policy loss: 1.159901. Value loss: 18.695763. Entropy: 0.426666.\n",
      "Iteration 6374: Policy loss: 1.243244. Value loss: 11.109121. Entropy: 0.405877.\n",
      "Iteration 6375: Policy loss: 1.039635. Value loss: 9.495124. Entropy: 0.425715.\n",
      "episode: 2807   score: 180.0  epsilon: 1.0    steps: 181  evaluation reward: 225.95\n",
      "episode: 2808   score: 285.0  epsilon: 1.0    steps: 339  evaluation reward: 227.0\n",
      "episode: 2809   score: 210.0  epsilon: 1.0    steps: 449  evaluation reward: 226.1\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6376: Policy loss: -1.040360. Value loss: 18.925167. Entropy: 0.619081.\n",
      "Iteration 6377: Policy loss: -0.984094. Value loss: 13.764462. Entropy: 0.607037.\n",
      "Iteration 6378: Policy loss: -0.984933. Value loss: 11.838251. Entropy: 0.618986.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6379: Policy loss: -0.648847. Value loss: 18.277159. Entropy: 0.606021.\n",
      "Iteration 6380: Policy loss: -0.522490. Value loss: 10.323577. Entropy: 0.608549.\n",
      "Iteration 6381: Policy loss: -0.622839. Value loss: 8.872284. Entropy: 0.599885.\n",
      "episode: 2810   score: 290.0  epsilon: 1.0    steps: 40  evaluation reward: 225.55\n",
      "episode: 2811   score: 210.0  epsilon: 1.0    steps: 646  evaluation reward: 225.85\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6382: Policy loss: 0.022631. Value loss: 15.023824. Entropy: 0.436498.\n",
      "Iteration 6383: Policy loss: 0.061688. Value loss: 10.785048. Entropy: 0.473044.\n",
      "Iteration 6384: Policy loss: 0.037092. Value loss: 9.684673. Entropy: 0.461427.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6385: Policy loss: 0.192161. Value loss: 14.096433. Entropy: 0.484041.\n",
      "Iteration 6386: Policy loss: 0.193762. Value loss: 12.846434. Entropy: 0.450326.\n",
      "Iteration 6387: Policy loss: 0.237946. Value loss: 9.992014. Entropy: 0.462166.\n",
      "episode: 2812   score: 150.0  epsilon: 1.0    steps: 352  evaluation reward: 225.25\n",
      "episode: 2813   score: 215.0  epsilon: 1.0    steps: 1024  evaluation reward: 225.6\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6388: Policy loss: 1.778929. Value loss: 25.570578. Entropy: 0.617797.\n",
      "Iteration 6389: Policy loss: 1.926982. Value loss: 12.969998. Entropy: 0.616149.\n",
      "Iteration 6390: Policy loss: 1.745633. Value loss: 10.322798. Entropy: 0.601957.\n",
      "episode: 2814   score: 325.0  epsilon: 1.0    steps: 605  evaluation reward: 226.25\n",
      "episode: 2815   score: 120.0  epsilon: 1.0    steps: 714  evaluation reward: 224.85\n",
      "episode: 2816   score: 260.0  epsilon: 1.0    steps: 787  evaluation reward: 224.85\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6391: Policy loss: -3.491337. Value loss: 261.360718. Entropy: 0.629107.\n",
      "Iteration 6392: Policy loss: -3.406671. Value loss: 149.410110. Entropy: 0.592792.\n",
      "Iteration 6393: Policy loss: -3.068607. Value loss: 105.407722. Entropy: 0.591060.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6394: Policy loss: 2.222302. Value loss: 31.649326. Entropy: 0.436755.\n",
      "Iteration 6395: Policy loss: 2.219600. Value loss: 16.313330. Entropy: 0.421304.\n",
      "Iteration 6396: Policy loss: 2.202117. Value loss: 10.284983. Entropy: 0.410315.\n",
      "episode: 2817   score: 260.0  epsilon: 1.0    steps: 134  evaluation reward: 225.9\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6397: Policy loss: 4.750563. Value loss: 38.600140. Entropy: 0.404166.\n",
      "Iteration 6398: Policy loss: 4.736615. Value loss: 22.297476. Entropy: 0.424162.\n",
      "Iteration 6399: Policy loss: 4.714341. Value loss: 17.680481. Entropy: 0.412050.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6400: Policy loss: 0.212625. Value loss: 24.077169. Entropy: 0.441707.\n",
      "Iteration 6401: Policy loss: 0.259219. Value loss: 10.701178. Entropy: 0.467688.\n",
      "Iteration 6402: Policy loss: 0.426827. Value loss: 9.015387. Entropy: 0.446675.\n",
      "episode: 2818   score: 155.0  epsilon: 1.0    steps: 683  evaluation reward: 226.2\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6403: Policy loss: 1.407393. Value loss: 18.075073. Entropy: 0.334491.\n",
      "Iteration 6404: Policy loss: 1.466599. Value loss: 10.642361. Entropy: 0.319500.\n",
      "Iteration 6405: Policy loss: 1.326098. Value loss: 7.722892. Entropy: 0.344472.\n",
      "episode: 2819   score: 275.0  epsilon: 1.0    steps: 37  evaluation reward: 227.9\n",
      "episode: 2820   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 227.1\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6406: Policy loss: 0.629369. Value loss: 16.977791. Entropy: 0.409585.\n",
      "Iteration 6407: Policy loss: 0.377547. Value loss: 8.001246. Entropy: 0.407388.\n",
      "Iteration 6408: Policy loss: 0.730605. Value loss: 5.362190. Entropy: 0.434639.\n",
      "episode: 2821   score: 310.0  epsilon: 1.0    steps: 322  evaluation reward: 227.35\n",
      "episode: 2822   score: 530.0  epsilon: 1.0    steps: 481  evaluation reward: 230.0\n",
      "episode: 2823   score: 240.0  epsilon: 1.0    steps: 984  evaluation reward: 231.05\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6409: Policy loss: 3.319365. Value loss: 48.626930. Entropy: 0.245428.\n",
      "Iteration 6410: Policy loss: 3.184917. Value loss: 18.289686. Entropy: 0.222991.\n",
      "Iteration 6411: Policy loss: 3.439806. Value loss: 13.057972. Entropy: 0.225038.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6412: Policy loss: 2.049722. Value loss: 16.527193. Entropy: 0.497114.\n",
      "Iteration 6413: Policy loss: 2.034668. Value loss: 10.199373. Entropy: 0.502275.\n",
      "Iteration 6414: Policy loss: 2.031178. Value loss: 7.525153. Entropy: 0.479147.\n",
      "episode: 2824   score: 260.0  epsilon: 1.0    steps: 774  evaluation reward: 232.6\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6415: Policy loss: 0.561063. Value loss: 21.122944. Entropy: 0.473562.\n",
      "Iteration 6416: Policy loss: 0.558006. Value loss: 15.395186. Entropy: 0.485601.\n",
      "Iteration 6417: Policy loss: 0.622112. Value loss: 14.596385. Entropy: 0.479989.\n",
      "episode: 2825   score: 180.0  epsilon: 1.0    steps: 576  evaluation reward: 233.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2826   score: 225.0  epsilon: 1.0    steps: 752  evaluation reward: 233.85\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6418: Policy loss: 1.250571. Value loss: 16.723721. Entropy: 0.645951.\n",
      "Iteration 6419: Policy loss: 1.144503. Value loss: 11.683695. Entropy: 0.664365.\n",
      "Iteration 6420: Policy loss: 1.089853. Value loss: 9.092857. Entropy: 0.642370.\n",
      "episode: 2827   score: 180.0  epsilon: 1.0    steps: 365  evaluation reward: 232.8\n",
      "episode: 2828   score: 135.0  epsilon: 1.0    steps: 1009  evaluation reward: 231.75\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6421: Policy loss: -5.193030. Value loss: 445.643250. Entropy: 0.464994.\n",
      "Iteration 6422: Policy loss: -4.606608. Value loss: 207.935242. Entropy: 0.415375.\n",
      "Iteration 6423: Policy loss: -4.013222. Value loss: 126.743484. Entropy: 0.412936.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6424: Policy loss: -1.145135. Value loss: 253.171082. Entropy: 0.309670.\n",
      "Iteration 6425: Policy loss: -1.281608. Value loss: 86.804146. Entropy: 0.317991.\n",
      "Iteration 6426: Policy loss: -0.966696. Value loss: 49.164951. Entropy: 0.352301.\n",
      "episode: 2829   score: 590.0  epsilon: 1.0    steps: 173  evaluation reward: 234.9\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6427: Policy loss: -0.736111. Value loss: 43.690746. Entropy: 0.378283.\n",
      "Iteration 6428: Policy loss: -1.251846. Value loss: 26.561356. Entropy: 0.360262.\n",
      "Iteration 6429: Policy loss: -0.918816. Value loss: 20.062271. Entropy: 0.356644.\n",
      "episode: 2830   score: 525.0  epsilon: 1.0    steps: 123  evaluation reward: 238.35\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6430: Policy loss: 3.560959. Value loss: 64.874741. Entropy: 0.459792.\n",
      "Iteration 6431: Policy loss: 3.991459. Value loss: 26.057171. Entropy: 0.419061.\n",
      "Iteration 6432: Policy loss: 3.632648. Value loss: 15.596437. Entropy: 0.451225.\n",
      "episode: 2831   score: 110.0  epsilon: 1.0    steps: 277  evaluation reward: 233.5\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6433: Policy loss: 2.852157. Value loss: 29.225216. Entropy: 0.549295.\n",
      "Iteration 6434: Policy loss: 2.707880. Value loss: 13.271445. Entropy: 0.577185.\n",
      "Iteration 6435: Policy loss: 2.582213. Value loss: 9.904283. Entropy: 0.600372.\n",
      "episode: 2832   score: 120.0  epsilon: 1.0    steps: 253  evaluation reward: 230.3\n",
      "episode: 2833   score: 180.0  epsilon: 1.0    steps: 693  evaluation reward: 229.5\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6436: Policy loss: 1.134806. Value loss: 45.079350. Entropy: 0.374508.\n",
      "Iteration 6437: Policy loss: 1.492843. Value loss: 18.315714. Entropy: 0.402858.\n",
      "Iteration 6438: Policy loss: 1.180653. Value loss: 13.938575. Entropy: 0.392355.\n",
      "episode: 2834   score: 215.0  epsilon: 1.0    steps: 947  evaluation reward: 229.05\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6439: Policy loss: 1.505973. Value loss: 21.550682. Entropy: 0.650065.\n",
      "Iteration 6440: Policy loss: 1.400052. Value loss: 12.949121. Entropy: 0.674123.\n",
      "Iteration 6441: Policy loss: 1.492482. Value loss: 10.068799. Entropy: 0.664235.\n",
      "episode: 2835   score: 290.0  epsilon: 1.0    steps: 772  evaluation reward: 230.15\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6442: Policy loss: 1.656433. Value loss: 31.136463. Entropy: 0.659885.\n",
      "Iteration 6443: Policy loss: 1.800626. Value loss: 17.438406. Entropy: 0.672787.\n",
      "Iteration 6444: Policy loss: 1.477456. Value loss: 13.618908. Entropy: 0.669328.\n",
      "episode: 2836   score: 180.0  epsilon: 1.0    steps: 15  evaluation reward: 229.35\n",
      "episode: 2837   score: 640.0  epsilon: 1.0    steps: 397  evaluation reward: 233.95\n",
      "episode: 2838   score: 300.0  epsilon: 1.0    steps: 517  evaluation reward: 233.75\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6445: Policy loss: 0.848036. Value loss: 29.283630. Entropy: 0.871844.\n",
      "Iteration 6446: Policy loss: 0.691682. Value loss: 17.002136. Entropy: 0.864132.\n",
      "Iteration 6447: Policy loss: 0.464622. Value loss: 13.437314. Entropy: 0.857555.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6448: Policy loss: 1.169340. Value loss: 30.996599. Entropy: 0.527533.\n",
      "Iteration 6449: Policy loss: 1.078215. Value loss: 19.917570. Entropy: 0.500958.\n",
      "Iteration 6450: Policy loss: 1.111236. Value loss: 17.375862. Entropy: 0.504458.\n",
      "episode: 2839   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 232.65\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6451: Policy loss: 2.991081. Value loss: 19.025408. Entropy: 0.660037.\n",
      "Iteration 6452: Policy loss: 2.855829. Value loss: 10.764222. Entropy: 0.642019.\n",
      "Iteration 6453: Policy loss: 2.807218. Value loss: 8.304480. Entropy: 0.648031.\n",
      "episode: 2840   score: 110.0  epsilon: 1.0    steps: 48  evaluation reward: 231.45\n",
      "episode: 2841   score: 300.0  epsilon: 1.0    steps: 325  evaluation reward: 233.55\n",
      "episode: 2842   score: 75.0  epsilon: 1.0    steps: 465  evaluation reward: 232.6\n",
      "episode: 2843   score: 110.0  epsilon: 1.0    steps: 539  evaluation reward: 231.9\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6454: Policy loss: 1.933553. Value loss: 19.337582. Entropy: 0.660503.\n",
      "Iteration 6455: Policy loss: 2.001163. Value loss: 10.032202. Entropy: 0.673366.\n",
      "Iteration 6456: Policy loss: 2.145930. Value loss: 8.041127. Entropy: 0.669083.\n",
      "episode: 2844   score: 250.0  epsilon: 1.0    steps: 184  evaluation reward: 232.3\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6457: Policy loss: 3.337524. Value loss: 26.751368. Entropy: 0.543460.\n",
      "Iteration 6458: Policy loss: 3.157985. Value loss: 14.644533. Entropy: 0.576043.\n",
      "Iteration 6459: Policy loss: 3.338410. Value loss: 10.359225. Entropy: 0.572153.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6460: Policy loss: 0.776635. Value loss: 17.281076. Entropy: 0.809716.\n",
      "Iteration 6461: Policy loss: 0.906941. Value loss: 13.039807. Entropy: 0.806522.\n",
      "Iteration 6462: Policy loss: 0.721106. Value loss: 9.399805. Entropy: 0.842699.\n",
      "episode: 2845   score: 290.0  epsilon: 1.0    steps: 641  evaluation reward: 234.15\n",
      "episode: 2846   score: 240.0  epsilon: 1.0    steps: 865  evaluation reward: 234.6\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6463: Policy loss: 1.630151. Value loss: 21.096367. Entropy: 0.835542.\n",
      "Iteration 6464: Policy loss: 1.609935. Value loss: 13.485989. Entropy: 0.850661.\n",
      "Iteration 6465: Policy loss: 1.651018. Value loss: 10.854176. Entropy: 0.852284.\n",
      "episode: 2847   score: 110.0  epsilon: 1.0    steps: 49  evaluation reward: 233.6\n",
      "episode: 2848   score: 30.0  epsilon: 1.0    steps: 195  evaluation reward: 232.85\n",
      "episode: 2849   score: 105.0  epsilon: 1.0    steps: 402  evaluation reward: 232.85\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6466: Policy loss: 2.484332. Value loss: 21.347284. Entropy: 0.801046.\n",
      "Iteration 6467: Policy loss: 2.390737. Value loss: 9.056377. Entropy: 0.789006.\n",
      "Iteration 6468: Policy loss: 2.275477. Value loss: 9.090965. Entropy: 0.798066.\n",
      "episode: 2850   score: 180.0  epsilon: 1.0    steps: 899  evaluation reward: 233.15\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6469: Policy loss: -1.372831. Value loss: 19.464777. Entropy: 0.733891.\n",
      "Iteration 6470: Policy loss: -1.260335. Value loss: 11.064921. Entropy: 0.730089.\n",
      "Iteration 6471: Policy loss: -1.149193. Value loss: 10.126388. Entropy: 0.726484.\n",
      "now time :  2019-02-25 20:40:59.612033\n",
      "episode: 2851   score: 135.0  epsilon: 1.0    steps: 258  evaluation reward: 232.25\n",
      "episode: 2852   score: 105.0  epsilon: 1.0    steps: 878  evaluation reward: 230.15\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6472: Policy loss: -1.217278. Value loss: 14.801580. Entropy: 0.830205.\n",
      "Iteration 6473: Policy loss: -0.762227. Value loss: 7.436944. Entropy: 0.817922.\n",
      "Iteration 6474: Policy loss: -1.128820. Value loss: 7.860586. Entropy: 0.818128.\n",
      "episode: 2853   score: 75.0  epsilon: 1.0    steps: 223  evaluation reward: 229.35\n",
      "episode: 2854   score: 235.0  epsilon: 1.0    steps: 604  evaluation reward: 229.0\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6475: Policy loss: 1.401818. Value loss: 17.387413. Entropy: 0.731283.\n",
      "Iteration 6476: Policy loss: 1.531334. Value loss: 8.734048. Entropy: 0.724677.\n",
      "Iteration 6477: Policy loss: 1.566749. Value loss: 7.181897. Entropy: 0.730205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6478: Policy loss: 0.328652. Value loss: 11.321465. Entropy: 0.728018.\n",
      "Iteration 6479: Policy loss: 0.237843. Value loss: 7.062061. Entropy: 0.757642.\n",
      "Iteration 6480: Policy loss: 0.225195. Value loss: 6.002151. Entropy: 0.765338.\n",
      "episode: 2855   score: 185.0  epsilon: 1.0    steps: 500  evaluation reward: 228.75\n",
      "episode: 2856   score: 135.0  epsilon: 1.0    steps: 914  evaluation reward: 225.75\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6481: Policy loss: 1.888680. Value loss: 14.057236. Entropy: 0.841385.\n",
      "Iteration 6482: Policy loss: 1.939135. Value loss: 7.159658. Entropy: 0.815507.\n",
      "Iteration 6483: Policy loss: 1.838467. Value loss: 5.863451. Entropy: 0.811205.\n",
      "episode: 2857   score: 135.0  epsilon: 1.0    steps: 314  evaluation reward: 225.0\n",
      "episode: 2858   score: 215.0  epsilon: 1.0    steps: 722  evaluation reward: 224.5\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6484: Policy loss: 1.669205. Value loss: 16.333660. Entropy: 0.699548.\n",
      "Iteration 6485: Policy loss: 1.733047. Value loss: 8.862506. Entropy: 0.688056.\n",
      "Iteration 6486: Policy loss: 1.391932. Value loss: 6.796424. Entropy: 0.675656.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6487: Policy loss: 1.221968. Value loss: 19.230644. Entropy: 0.724229.\n",
      "Iteration 6488: Policy loss: 1.338289. Value loss: 13.031318. Entropy: 0.780722.\n",
      "Iteration 6489: Policy loss: 1.255584. Value loss: 9.952057. Entropy: 0.742968.\n",
      "episode: 2859   score: 240.0  epsilon: 1.0    steps: 7  evaluation reward: 224.8\n",
      "episode: 2860   score: 80.0  epsilon: 1.0    steps: 146  evaluation reward: 221.5\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6490: Policy loss: -0.094163. Value loss: 26.066406. Entropy: 0.808930.\n",
      "Iteration 6491: Policy loss: 0.203705. Value loss: 12.794973. Entropy: 0.806115.\n",
      "Iteration 6492: Policy loss: 0.204464. Value loss: 9.924883. Entropy: 0.811773.\n",
      "episode: 2861   score: 105.0  epsilon: 1.0    steps: 337  evaluation reward: 220.15\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6493: Policy loss: 0.480961. Value loss: 10.685983. Entropy: 0.666575.\n",
      "Iteration 6494: Policy loss: 0.566816. Value loss: 6.630174. Entropy: 0.652989.\n",
      "Iteration 6495: Policy loss: 0.500684. Value loss: 6.484241. Entropy: 0.661305.\n",
      "episode: 2862   score: 150.0  epsilon: 1.0    steps: 767  evaluation reward: 219.05\n",
      "episode: 2863   score: 240.0  epsilon: 1.0    steps: 832  evaluation reward: 218.2\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6496: Policy loss: -0.863232. Value loss: 15.249679. Entropy: 0.753613.\n",
      "Iteration 6497: Policy loss: -0.763873. Value loss: 6.981137. Entropy: 0.735492.\n",
      "Iteration 6498: Policy loss: -1.033241. Value loss: 5.663129. Entropy: 0.738512.\n",
      "episode: 2864   score: 240.0  epsilon: 1.0    steps: 577  evaluation reward: 217.85\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6499: Policy loss: -1.778739. Value loss: 16.055742. Entropy: 0.656917.\n",
      "Iteration 6500: Policy loss: -1.623294. Value loss: 9.633629. Entropy: 0.661092.\n",
      "Iteration 6501: Policy loss: -1.826640. Value loss: 6.426727. Entropy: 0.674790.\n",
      "episode: 2865   score: 180.0  epsilon: 1.0    steps: 191  evaluation reward: 218.9\n",
      "episode: 2866   score: 80.0  epsilon: 1.0    steps: 360  evaluation reward: 213.8\n",
      "episode: 2867   score: 210.0  epsilon: 1.0    steps: 436  evaluation reward: 213.3\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6502: Policy loss: 0.649077. Value loss: 11.438912. Entropy: 0.794858.\n",
      "Iteration 6503: Policy loss: 0.825361. Value loss: 5.197919. Entropy: 0.841862.\n",
      "Iteration 6504: Policy loss: 0.762974. Value loss: 5.682709. Entropy: 0.847144.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6505: Policy loss: -0.587910. Value loss: 18.417282. Entropy: 0.806573.\n",
      "Iteration 6506: Policy loss: -0.597486. Value loss: 8.323373. Entropy: 0.834489.\n",
      "Iteration 6507: Policy loss: -0.640583. Value loss: 9.032660. Entropy: 0.807306.\n",
      "episode: 2868   score: 135.0  epsilon: 1.0    steps: 758  evaluation reward: 212.05\n",
      "episode: 2869   score: 120.0  epsilon: 1.0    steps: 839  evaluation reward: 208.65\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6508: Policy loss: 0.917290. Value loss: 14.334833. Entropy: 0.820648.\n",
      "Iteration 6509: Policy loss: 0.759723. Value loss: 10.388124. Entropy: 0.823273.\n",
      "Iteration 6510: Policy loss: 0.832670. Value loss: 8.488515. Entropy: 0.818561.\n",
      "episode: 2870   score: 105.0  epsilon: 1.0    steps: 249  evaluation reward: 207.1\n",
      "episode: 2871   score: 110.0  epsilon: 1.0    steps: 489  evaluation reward: 206.4\n",
      "episode: 2872   score: 150.0  epsilon: 1.0    steps: 600  evaluation reward: 206.7\n",
      "episode: 2873   score: 390.0  epsilon: 1.0    steps: 987  evaluation reward: 208.45\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6511: Policy loss: 0.288227. Value loss: 14.389113. Entropy: 0.798832.\n",
      "Iteration 6512: Policy loss: 0.361821. Value loss: 8.820751. Entropy: 0.827313.\n",
      "Iteration 6513: Policy loss: 0.291472. Value loss: 7.582158. Entropy: 0.815938.\n",
      "episode: 2874   score: 125.0  epsilon: 1.0    steps: 368  evaluation reward: 208.2\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6514: Policy loss: -0.956279. Value loss: 25.537491. Entropy: 0.975607.\n",
      "Iteration 6515: Policy loss: -0.874527. Value loss: 13.278775. Entropy: 0.992790.\n",
      "Iteration 6516: Policy loss: -0.945113. Value loss: 12.015089. Entropy: 0.984361.\n",
      "episode: 2875   score: 315.0  epsilon: 1.0    steps: 70  evaluation reward: 209.55\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6517: Policy loss: -0.654217. Value loss: 36.070492. Entropy: 0.807490.\n",
      "Iteration 6518: Policy loss: -0.155451. Value loss: 18.100471. Entropy: 0.802387.\n",
      "Iteration 6519: Policy loss: -0.242319. Value loss: 13.390032. Entropy: 0.777242.\n",
      "episode: 2876   score: 60.0  epsilon: 1.0    steps: 589  evaluation reward: 207.3\n",
      "episode: 2877   score: 155.0  epsilon: 1.0    steps: 756  evaluation reward: 206.3\n",
      "episode: 2878   score: 155.0  epsilon: 1.0    steps: 826  evaluation reward: 205.75\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6520: Policy loss: 0.582682. Value loss: 21.638878. Entropy: 0.947556.\n",
      "Iteration 6521: Policy loss: 0.890896. Value loss: 13.392630. Entropy: 0.956085.\n",
      "Iteration 6522: Policy loss: 0.390441. Value loss: 12.157969. Entropy: 0.941032.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6523: Policy loss: -1.704518. Value loss: 23.715258. Entropy: 0.924227.\n",
      "Iteration 6524: Policy loss: -1.662780. Value loss: 13.883224. Entropy: 0.914009.\n",
      "Iteration 6525: Policy loss: -1.696482. Value loss: 10.865669. Entropy: 0.910342.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6526: Policy loss: -0.121790. Value loss: 32.782608. Entropy: 0.781497.\n",
      "Iteration 6527: Policy loss: -0.076609. Value loss: 17.991451. Entropy: 0.755668.\n",
      "Iteration 6528: Policy loss: -0.222606. Value loss: 12.800705. Entropy: 0.767069.\n",
      "episode: 2879   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 206.8\n",
      "episode: 2880   score: 130.0  epsilon: 1.0    steps: 401  evaluation reward: 207.0\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6529: Policy loss: -1.180513. Value loss: 22.144276. Entropy: 0.879517.\n",
      "Iteration 6530: Policy loss: -1.005490. Value loss: 10.909594. Entropy: 0.877991.\n",
      "Iteration 6531: Policy loss: -1.049475. Value loss: 7.918295. Entropy: 0.869370.\n",
      "episode: 2881   score: 210.0  epsilon: 1.0    steps: 115  evaluation reward: 207.0\n",
      "episode: 2882   score: 230.0  epsilon: 1.0    steps: 350  evaluation reward: 206.7\n",
      "episode: 2883   score: 310.0  epsilon: 1.0    steps: 985  evaluation reward: 209.05\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6532: Policy loss: -1.055268. Value loss: 31.898043. Entropy: 0.689562.\n",
      "Iteration 6533: Policy loss: -0.694766. Value loss: 16.694710. Entropy: 0.662727.\n",
      "Iteration 6534: Policy loss: -0.978555. Value loss: 10.783203. Entropy: 0.678359.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6535: Policy loss: 0.747480. Value loss: 24.131214. Entropy: 0.913999.\n",
      "Iteration 6536: Policy loss: 0.934007. Value loss: 11.127904. Entropy: 0.888106.\n",
      "Iteration 6537: Policy loss: 0.765752. Value loss: 7.816715. Entropy: 0.912656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2884   score: 235.0  epsilon: 1.0    steps: 514  evaluation reward: 208.75\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6538: Policy loss: -5.343370. Value loss: 315.784973. Entropy: 0.679002.\n",
      "Iteration 6539: Policy loss: -5.396975. Value loss: 129.080002. Entropy: 0.590292.\n",
      "Iteration 6540: Policy loss: -5.512131. Value loss: 145.082779. Entropy: 0.583428.\n",
      "episode: 2885   score: 225.0  epsilon: 1.0    steps: 769  evaluation reward: 208.4\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6541: Policy loss: -1.746175. Value loss: 39.554108. Entropy: 0.524962.\n",
      "Iteration 6542: Policy loss: -1.671353. Value loss: 20.888775. Entropy: 0.519625.\n",
      "Iteration 6543: Policy loss: -1.976876. Value loss: 15.369419. Entropy: 0.522449.\n",
      "episode: 2886   score: 440.0  epsilon: 1.0    steps: 745  evaluation reward: 210.65\n",
      "episode: 2887   score: 130.0  epsilon: 1.0    steps: 1004  evaluation reward: 210.15\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6544: Policy loss: 2.250071. Value loss: 43.479019. Entropy: 0.646741.\n",
      "Iteration 6545: Policy loss: 2.496707. Value loss: 20.462299. Entropy: 0.667167.\n",
      "Iteration 6546: Policy loss: 2.881287. Value loss: 19.326437. Entropy: 0.644470.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6547: Policy loss: -1.307349. Value loss: 26.980906. Entropy: 0.643257.\n",
      "Iteration 6548: Policy loss: -0.944301. Value loss: 13.148827. Entropy: 0.640052.\n",
      "Iteration 6549: Policy loss: -0.853761. Value loss: 8.607368. Entropy: 0.659218.\n",
      "episode: 2888   score: 275.0  epsilon: 1.0    steps: 209  evaluation reward: 210.3\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6550: Policy loss: -4.108080. Value loss: 265.268951. Entropy: 0.605000.\n",
      "Iteration 6551: Policy loss: -4.475850. Value loss: 197.518341. Entropy: 0.598724.\n",
      "Iteration 6552: Policy loss: -3.755347. Value loss: 112.371132. Entropy: 0.610313.\n",
      "episode: 2889   score: 155.0  epsilon: 1.0    steps: 748  evaluation reward: 209.75\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6553: Policy loss: 2.005387. Value loss: 41.759708. Entropy: 0.692790.\n",
      "Iteration 6554: Policy loss: 1.928441. Value loss: 19.617439. Entropy: 0.686532.\n",
      "Iteration 6555: Policy loss: 2.054623. Value loss: 15.842260. Entropy: 0.716493.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6556: Policy loss: -3.472382. Value loss: 302.603027. Entropy: 0.492954.\n",
      "Iteration 6557: Policy loss: -2.880396. Value loss: 173.553192. Entropy: 0.455334.\n",
      "Iteration 6558: Policy loss: -3.102127. Value loss: 140.716736. Entropy: 0.438758.\n",
      "episode: 2890   score: 515.0  epsilon: 1.0    steps: 126  evaluation reward: 213.5\n",
      "episode: 2891   score: 345.0  epsilon: 1.0    steps: 343  evaluation reward: 214.8\n",
      "episode: 2892   score: 315.0  epsilon: 1.0    steps: 620  evaluation reward: 215.85\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6559: Policy loss: 0.829246. Value loss: 34.161579. Entropy: 0.280261.\n",
      "Iteration 6560: Policy loss: 0.642694. Value loss: 16.173454. Entropy: 0.307027.\n",
      "Iteration 6561: Policy loss: 0.639581. Value loss: 13.898828. Entropy: 0.294025.\n",
      "episode: 2893   score: 620.0  epsilon: 1.0    steps: 403  evaluation reward: 220.5\n",
      "episode: 2894   score: 515.0  epsilon: 1.0    steps: 893  evaluation reward: 223.1\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6562: Policy loss: 0.472737. Value loss: 228.733337. Entropy: 0.594549.\n",
      "Iteration 6563: Policy loss: 0.555742. Value loss: 137.809052. Entropy: 0.558887.\n",
      "Iteration 6564: Policy loss: 0.939144. Value loss: 108.108902. Entropy: 0.601040.\n",
      "episode: 2895   score: 330.0  epsilon: 1.0    steps: 937  evaluation reward: 225.35\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6565: Policy loss: 2.056388. Value loss: 38.681355. Entropy: 0.396176.\n",
      "Iteration 6566: Policy loss: 2.274120. Value loss: 23.943726. Entropy: 0.395614.\n",
      "Iteration 6567: Policy loss: 2.420776. Value loss: 19.488302. Entropy: 0.400326.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6568: Policy loss: 2.033127. Value loss: 19.964766. Entropy: 0.722826.\n",
      "Iteration 6569: Policy loss: 1.831999. Value loss: 12.528769. Entropy: 0.732538.\n",
      "Iteration 6570: Policy loss: 1.971404. Value loss: 11.499125. Entropy: 0.725831.\n",
      "episode: 2896   score: 105.0  epsilon: 1.0    steps: 67  evaluation reward: 224.3\n",
      "episode: 2897   score: 135.0  epsilon: 1.0    steps: 308  evaluation reward: 223.2\n",
      "episode: 2898   score: 75.0  epsilon: 1.0    steps: 549  evaluation reward: 222.7\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6571: Policy loss: 2.842526. Value loss: 41.415428. Entropy: 0.734855.\n",
      "Iteration 6572: Policy loss: 2.589710. Value loss: 23.902626. Entropy: 0.713111.\n",
      "Iteration 6573: Policy loss: 2.528081. Value loss: 18.483988. Entropy: 0.749438.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6574: Policy loss: 1.778715. Value loss: 31.180143. Entropy: 0.472737.\n",
      "Iteration 6575: Policy loss: 1.367753. Value loss: 18.720394. Entropy: 0.456430.\n",
      "Iteration 6576: Policy loss: 1.113897. Value loss: 16.376089. Entropy: 0.455972.\n",
      "episode: 2899   score: 210.0  epsilon: 1.0    steps: 462  evaluation reward: 222.4\n",
      "episode: 2900   score: 530.0  epsilon: 1.0    steps: 689  evaluation reward: 226.15\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6577: Policy loss: 3.595600. Value loss: 36.855133. Entropy: 0.514904.\n",
      "Iteration 6578: Policy loss: 4.033959. Value loss: 24.552748. Entropy: 0.562182.\n",
      "Iteration 6579: Policy loss: 3.473870. Value loss: 18.527252. Entropy: 0.488164.\n",
      "now time :  2019-02-25 20:43:00.793623\n",
      "episode: 2901   score: 315.0  epsilon: 1.0    steps: 150  evaluation reward: 226.45\n",
      "episode: 2902   score: 210.0  epsilon: 1.0    steps: 811  evaluation reward: 226.45\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6580: Policy loss: 0.762246. Value loss: 24.975046. Entropy: 0.665099.\n",
      "Iteration 6581: Policy loss: 0.700302. Value loss: 13.046693. Entropy: 0.679627.\n",
      "Iteration 6582: Policy loss: 0.491338. Value loss: 10.134023. Entropy: 0.668147.\n",
      "episode: 2903   score: 135.0  epsilon: 1.0    steps: 75  evaluation reward: 225.65\n",
      "episode: 2904   score: 135.0  epsilon: 1.0    steps: 323  evaluation reward: 225.95\n",
      "episode: 2905   score: 155.0  epsilon: 1.0    steps: 600  evaluation reward: 222.9\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6583: Policy loss: 3.918005. Value loss: 34.941978. Entropy: 0.503304.\n",
      "Iteration 6584: Policy loss: 3.747027. Value loss: 17.208733. Entropy: 0.558648.\n",
      "Iteration 6585: Policy loss: 3.663514. Value loss: 15.140047. Entropy: 0.561832.\n",
      "episode: 2906   score: 285.0  epsilon: 1.0    steps: 980  evaluation reward: 223.5\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6586: Policy loss: 2.264521. Value loss: 11.035642. Entropy: 0.804554.\n",
      "Iteration 6587: Policy loss: 2.157957. Value loss: 5.844598. Entropy: 0.794534.\n",
      "Iteration 6588: Policy loss: 2.316083. Value loss: 4.598083. Entropy: 0.797681.\n",
      "episode: 2907   score: 105.0  epsilon: 1.0    steps: 401  evaluation reward: 222.75\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6589: Policy loss: 1.072168. Value loss: 16.217829. Entropy: 0.676409.\n",
      "Iteration 6590: Policy loss: 1.187580. Value loss: 11.047904. Entropy: 0.667421.\n",
      "Iteration 6591: Policy loss: 1.048237. Value loss: 11.896870. Entropy: 0.684589.\n",
      "episode: 2908   score: 80.0  epsilon: 1.0    steps: 380  evaluation reward: 220.7\n",
      "episode: 2909   score: 120.0  epsilon: 1.0    steps: 706  evaluation reward: 219.8\n",
      "episode: 2910   score: 125.0  epsilon: 1.0    steps: 869  evaluation reward: 218.15\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6592: Policy loss: 3.108707. Value loss: 25.631794. Entropy: 0.788716.\n",
      "Iteration 6593: Policy loss: 2.749874. Value loss: 13.614238. Entropy: 0.804592.\n",
      "Iteration 6594: Policy loss: 3.329859. Value loss: 11.743904. Entropy: 0.790765.\n",
      "episode: 2911   score: 105.0  epsilon: 1.0    steps: 6  evaluation reward: 217.1\n",
      "episode: 2912   score: 120.0  epsilon: 1.0    steps: 540  evaluation reward: 216.8\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6595: Policy loss: 1.477245. Value loss: 9.514070. Entropy: 0.694815.\n",
      "Iteration 6596: Policy loss: 1.663349. Value loss: 4.018619. Entropy: 0.692514.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6597: Policy loss: 1.358149. Value loss: 3.941272. Entropy: 0.703459.\n",
      "episode: 2913   score: 105.0  epsilon: 1.0    steps: 463  evaluation reward: 215.7\n",
      "episode: 2914   score: 75.0  epsilon: 1.0    steps: 919  evaluation reward: 213.2\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6598: Policy loss: 1.159881. Value loss: 9.145762. Entropy: 0.385774.\n",
      "Iteration 6599: Policy loss: 1.156647. Value loss: 6.822482. Entropy: 0.414379.\n",
      "Iteration 6600: Policy loss: 1.173955. Value loss: 6.367426. Entropy: 0.439182.\n",
      "episode: 2915   score: 265.0  epsilon: 1.0    steps: 242  evaluation reward: 214.65\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6601: Policy loss: 1.086706. Value loss: 10.715366. Entropy: 0.912331.\n",
      "Iteration 6602: Policy loss: 1.163842. Value loss: 5.722205. Entropy: 0.904486.\n",
      "Iteration 6603: Policy loss: 1.048041. Value loss: 4.279373. Entropy: 0.927205.\n",
      "episode: 2916   score: 75.0  epsilon: 1.0    steps: 313  evaluation reward: 212.8\n",
      "episode: 2917   score: 105.0  epsilon: 1.0    steps: 599  evaluation reward: 211.25\n",
      "episode: 2918   score: 75.0  epsilon: 1.0    steps: 645  evaluation reward: 210.45\n",
      "episode: 2919   score: 75.0  epsilon: 1.0    steps: 800  evaluation reward: 208.45\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6604: Policy loss: -0.057904. Value loss: 17.181971. Entropy: 0.822290.\n",
      "Iteration 6605: Policy loss: 0.079124. Value loss: 9.072806. Entropy: 0.823794.\n",
      "Iteration 6606: Policy loss: -0.095834. Value loss: 7.791219. Entropy: 0.822500.\n",
      "episode: 2920   score: 195.0  epsilon: 1.0    steps: 119  evaluation reward: 208.6\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6607: Policy loss: 0.237256. Value loss: 14.085970. Entropy: 0.549565.\n",
      "Iteration 6608: Policy loss: 0.514473. Value loss: 10.002866. Entropy: 0.530477.\n",
      "Iteration 6609: Policy loss: 0.208792. Value loss: 8.603233. Entropy: 0.544300.\n",
      "episode: 2921   score: 75.0  epsilon: 1.0    steps: 403  evaluation reward: 206.25\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6610: Policy loss: -0.736537. Value loss: 7.670973. Entropy: 0.914944.\n",
      "Iteration 6611: Policy loss: -0.806483. Value loss: 7.230447. Entropy: 0.907482.\n",
      "Iteration 6612: Policy loss: -0.814901. Value loss: 5.420953. Entropy: 0.931518.\n",
      "episode: 2922   score: 105.0  epsilon: 1.0    steps: 205  evaluation reward: 202.0\n",
      "episode: 2923   score: 105.0  epsilon: 1.0    steps: 736  evaluation reward: 200.65\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6613: Policy loss: 0.504222. Value loss: 27.049805. Entropy: 0.724088.\n",
      "Iteration 6614: Policy loss: 0.029770. Value loss: 17.032059. Entropy: 0.726877.\n",
      "Iteration 6615: Policy loss: 0.109092. Value loss: 14.373244. Entropy: 0.731702.\n",
      "episode: 2924   score: 120.0  epsilon: 1.0    steps: 276  evaluation reward: 199.25\n",
      "episode: 2925   score: 105.0  epsilon: 1.0    steps: 540  evaluation reward: 198.5\n",
      "episode: 2926   score: 150.0  epsilon: 1.0    steps: 875  evaluation reward: 197.75\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6616: Policy loss: 0.659732. Value loss: 11.848197. Entropy: 0.897884.\n",
      "Iteration 6617: Policy loss: 0.649352. Value loss: 4.679750. Entropy: 0.889652.\n",
      "Iteration 6618: Policy loss: 0.726900. Value loss: 3.384565. Entropy: 0.891849.\n",
      "episode: 2927   score: 75.0  epsilon: 1.0    steps: 468  evaluation reward: 196.7\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6619: Policy loss: 0.501122. Value loss: 14.316459. Entropy: 0.578046.\n",
      "Iteration 6620: Policy loss: 0.244360. Value loss: 9.196784. Entropy: 0.560965.\n",
      "Iteration 6621: Policy loss: 0.478502. Value loss: 7.687218. Entropy: 0.593298.\n",
      "episode: 2928   score: 330.0  epsilon: 1.0    steps: 940  evaluation reward: 198.65\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6622: Policy loss: -1.143193. Value loss: 15.932628. Entropy: 0.920102.\n",
      "Iteration 6623: Policy loss: -0.986065. Value loss: 8.907579. Entropy: 0.913504.\n",
      "Iteration 6624: Policy loss: -1.162215. Value loss: 9.671670. Entropy: 0.908394.\n",
      "episode: 2929   score: 110.0  epsilon: 1.0    steps: 599  evaluation reward: 193.85\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6625: Policy loss: -0.994971. Value loss: 20.756531. Entropy: 0.652409.\n",
      "Iteration 6626: Policy loss: -0.793282. Value loss: 12.855718. Entropy: 0.707440.\n",
      "Iteration 6627: Policy loss: -1.045813. Value loss: 8.815392. Entropy: 0.668141.\n",
      "episode: 2930   score: 255.0  epsilon: 1.0    steps: 112  evaluation reward: 191.15\n",
      "episode: 2931   score: 150.0  epsilon: 1.0    steps: 684  evaluation reward: 191.55\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6628: Policy loss: 1.118100. Value loss: 31.626347. Entropy: 0.750344.\n",
      "Iteration 6629: Policy loss: 1.212354. Value loss: 16.274389. Entropy: 0.736372.\n",
      "Iteration 6630: Policy loss: 1.027455. Value loss: 12.646935. Entropy: 0.742715.\n",
      "episode: 2932   score: 155.0  epsilon: 1.0    steps: 307  evaluation reward: 191.9\n",
      "episode: 2933   score: 80.0  epsilon: 1.0    steps: 397  evaluation reward: 190.9\n",
      "episode: 2934   score: 110.0  epsilon: 1.0    steps: 998  evaluation reward: 189.85\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6631: Policy loss: 0.055060. Value loss: 12.795253. Entropy: 0.632505.\n",
      "Iteration 6632: Policy loss: 0.168044. Value loss: 6.287032. Entropy: 0.627860.\n",
      "Iteration 6633: Policy loss: 0.123404. Value loss: 5.583253. Entropy: 0.643743.\n",
      "episode: 2935   score: 315.0  epsilon: 1.0    steps: 212  evaluation reward: 190.1\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6634: Policy loss: 1.727400. Value loss: 9.113465. Entropy: 0.594392.\n",
      "Iteration 6635: Policy loss: 1.620353. Value loss: 5.702512. Entropy: 0.601211.\n",
      "Iteration 6636: Policy loss: 1.511918. Value loss: 6.366734. Entropy: 0.595859.\n",
      "episode: 2936   score: 80.0  epsilon: 1.0    steps: 528  evaluation reward: 189.1\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6637: Policy loss: -0.201602. Value loss: 15.391486. Entropy: 0.843250.\n",
      "Iteration 6638: Policy loss: -0.217900. Value loss: 10.128652. Entropy: 0.873698.\n",
      "Iteration 6639: Policy loss: -0.238819. Value loss: 7.378162. Entropy: 0.865950.\n",
      "episode: 2937   score: 105.0  epsilon: 1.0    steps: 454  evaluation reward: 183.75\n",
      "episode: 2938   score: 130.0  epsilon: 1.0    steps: 648  evaluation reward: 182.05\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6640: Policy loss: -1.104872. Value loss: 14.107799. Entropy: 0.733612.\n",
      "Iteration 6641: Policy loss: -1.068877. Value loss: 10.749724. Entropy: 0.752623.\n",
      "Iteration 6642: Policy loss: -1.221058. Value loss: 8.435252. Entropy: 0.719488.\n",
      "episode: 2939   score: 130.0  epsilon: 1.0    steps: 101  evaluation reward: 181.55\n",
      "episode: 2940   score: 355.0  epsilon: 1.0    steps: 822  evaluation reward: 184.0\n",
      "episode: 2941   score: 155.0  epsilon: 1.0    steps: 967  evaluation reward: 182.55\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6643: Policy loss: -0.174320. Value loss: 13.002744. Entropy: 0.581122.\n",
      "Iteration 6644: Policy loss: -0.315011. Value loss: 7.666932. Entropy: 0.574600.\n",
      "Iteration 6645: Policy loss: -0.233615. Value loss: 6.037731. Entropy: 0.590133.\n",
      "episode: 2942   score: 75.0  epsilon: 1.0    steps: 592  evaluation reward: 182.55\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6646: Policy loss: 0.101518. Value loss: 18.290743. Entropy: 0.662356.\n",
      "Iteration 6647: Policy loss: -0.102018. Value loss: 12.287297. Entropy: 0.661480.\n",
      "Iteration 6648: Policy loss: -0.111258. Value loss: 10.637288. Entropy: 0.678788.\n",
      "episode: 2943   score: 75.0  epsilon: 1.0    steps: 716  evaluation reward: 182.2\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6649: Policy loss: 0.690675. Value loss: 16.716137. Entropy: 0.663730.\n",
      "Iteration 6650: Policy loss: 0.737773. Value loss: 9.678475. Entropy: 0.699044.\n",
      "Iteration 6651: Policy loss: 0.699532. Value loss: 7.862526. Entropy: 0.714286.\n",
      "episode: 2944   score: 310.0  epsilon: 1.0    steps: 235  evaluation reward: 182.8\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6652: Policy loss: -0.718615. Value loss: 15.411266. Entropy: 0.777407.\n",
      "Iteration 6653: Policy loss: -0.689980. Value loss: 9.033309. Entropy: 0.817718.\n",
      "Iteration 6654: Policy loss: -0.808332. Value loss: 5.913266. Entropy: 0.766269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2945   score: 300.0  epsilon: 1.0    steps: 349  evaluation reward: 182.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6655: Policy loss: -0.265650. Value loss: 18.497187. Entropy: 0.663269.\n",
      "Iteration 6656: Policy loss: -0.211773. Value loss: 9.511113. Entropy: 0.670053.\n",
      "Iteration 6657: Policy loss: -0.341519. Value loss: 8.560772. Entropy: 0.632695.\n",
      "episode: 2946   score: 105.0  epsilon: 1.0    steps: 521  evaluation reward: 181.55\n",
      "episode: 2947   score: 195.0  epsilon: 1.0    steps: 872  evaluation reward: 182.4\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6658: Policy loss: -0.057782. Value loss: 21.695015. Entropy: 0.608800.\n",
      "Iteration 6659: Policy loss: 0.031747. Value loss: 10.630980. Entropy: 0.610400.\n",
      "Iteration 6660: Policy loss: -0.119199. Value loss: 8.356744. Entropy: 0.619884.\n",
      "episode: 2948   score: 270.0  epsilon: 1.0    steps: 507  evaluation reward: 184.8\n",
      "episode: 2949   score: 135.0  epsilon: 1.0    steps: 741  evaluation reward: 185.1\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6661: Policy loss: 2.065963. Value loss: 21.171862. Entropy: 0.595708.\n",
      "Iteration 6662: Policy loss: 2.103859. Value loss: 8.914177. Entropy: 0.580159.\n",
      "Iteration 6663: Policy loss: 1.963907. Value loss: 7.493310. Entropy: 0.602596.\n",
      "episode: 2950   score: 255.0  epsilon: 1.0    steps: 10  evaluation reward: 185.85\n",
      "now time :  2019-02-25 20:44:35.369652\n",
      "episode: 2951   score: 80.0  epsilon: 1.0    steps: 164  evaluation reward: 185.3\n",
      "episode: 2952   score: 75.0  epsilon: 1.0    steps: 369  evaluation reward: 185.0\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6664: Policy loss: 0.387534. Value loss: 9.118813. Entropy: 0.840873.\n",
      "Iteration 6665: Policy loss: 0.475431. Value loss: 4.749743. Entropy: 0.835434.\n",
      "Iteration 6666: Policy loss: 0.305483. Value loss: 3.561009. Entropy: 0.851409.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6667: Policy loss: -0.285911. Value loss: 9.274961. Entropy: 0.575075.\n",
      "Iteration 6668: Policy loss: -0.314784. Value loss: 6.195820. Entropy: 0.585453.\n",
      "Iteration 6669: Policy loss: -0.153176. Value loss: 5.373406. Entropy: 0.588692.\n",
      "episode: 2953   score: 105.0  epsilon: 1.0    steps: 803  evaluation reward: 185.3\n",
      "episode: 2954   score: 420.0  epsilon: 1.0    steps: 984  evaluation reward: 187.15\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6670: Policy loss: -0.201811. Value loss: 9.900056. Entropy: 0.838906.\n",
      "Iteration 6671: Policy loss: -0.279133. Value loss: 6.593271. Entropy: 0.842820.\n",
      "Iteration 6672: Policy loss: -0.183695. Value loss: 5.123716. Entropy: 0.849152.\n",
      "episode: 2955   score: 75.0  epsilon: 1.0    steps: 79  evaluation reward: 186.05\n",
      "episode: 2956   score: 105.0  epsilon: 1.0    steps: 223  evaluation reward: 185.75\n",
      "episode: 2957   score: 75.0  epsilon: 1.0    steps: 439  evaluation reward: 185.15\n",
      "episode: 2958   score: 105.0  epsilon: 1.0    steps: 672  evaluation reward: 184.05\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6673: Policy loss: 1.512198. Value loss: 15.493941. Entropy: 0.769595.\n",
      "Iteration 6674: Policy loss: 1.677194. Value loss: 9.238214. Entropy: 0.776351.\n",
      "Iteration 6675: Policy loss: 1.558657. Value loss: 7.095156. Entropy: 0.792513.\n",
      "episode: 2959   score: 80.0  epsilon: 1.0    steps: 313  evaluation reward: 182.45\n",
      "episode: 2960   score: 195.0  epsilon: 1.0    steps: 569  evaluation reward: 183.6\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6676: Policy loss: 0.492635. Value loss: 10.705899. Entropy: 0.619496.\n",
      "Iteration 6677: Policy loss: 0.470022. Value loss: 7.125866. Entropy: 0.645680.\n",
      "Iteration 6678: Policy loss: 0.590848. Value loss: 6.439992. Entropy: 0.646081.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6679: Policy loss: -1.406084. Value loss: 9.567361. Entropy: 0.795577.\n",
      "Iteration 6680: Policy loss: -1.524675. Value loss: 6.495742. Entropy: 0.787668.\n",
      "Iteration 6681: Policy loss: -1.468909. Value loss: 5.853137. Entropy: 0.766393.\n",
      "episode: 2961   score: 75.0  epsilon: 1.0    steps: 500  evaluation reward: 183.3\n",
      "episode: 2962   score: 105.0  epsilon: 1.0    steps: 949  evaluation reward: 182.85\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6682: Policy loss: 1.354061. Value loss: 22.416660. Entropy: 1.003848.\n",
      "Iteration 6683: Policy loss: 1.363883. Value loss: 16.066095. Entropy: 1.024060.\n",
      "Iteration 6684: Policy loss: 1.146819. Value loss: 12.163301. Entropy: 1.027261.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6685: Policy loss: -0.356536. Value loss: 11.700008. Entropy: 0.848953.\n",
      "Iteration 6686: Policy loss: -0.311869. Value loss: 6.592462. Entropy: 0.873940.\n",
      "Iteration 6687: Policy loss: -0.201592. Value loss: 5.515475. Entropy: 0.874087.\n",
      "episode: 2963   score: 115.0  epsilon: 1.0    steps: 310  evaluation reward: 181.6\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6688: Policy loss: 0.606970. Value loss: 20.731298. Entropy: 0.672696.\n",
      "Iteration 6689: Policy loss: 0.627465. Value loss: 13.130577. Entropy: 0.662066.\n",
      "Iteration 6690: Policy loss: 0.737731. Value loss: 10.273041. Entropy: 0.653715.\n",
      "episode: 2964   score: 380.0  epsilon: 1.0    steps: 201  evaluation reward: 183.0\n",
      "episode: 2965   score: 125.0  epsilon: 1.0    steps: 585  evaluation reward: 182.45\n",
      "episode: 2966   score: 105.0  epsilon: 1.0    steps: 1009  evaluation reward: 182.7\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6691: Policy loss: -2.675897. Value loss: 247.064606. Entropy: 0.775721.\n",
      "Iteration 6692: Policy loss: -3.093738. Value loss: 169.474167. Entropy: 0.721554.\n",
      "Iteration 6693: Policy loss: -1.929441. Value loss: 96.417900. Entropy: 0.688615.\n",
      "episode: 2967   score: 240.0  epsilon: 1.0    steps: 80  evaluation reward: 183.0\n",
      "episode: 2968   score: 105.0  epsilon: 1.0    steps: 398  evaluation reward: 182.7\n",
      "episode: 2969   score: 265.0  epsilon: 1.0    steps: 660  evaluation reward: 184.15\n",
      "episode: 2970   score: 385.0  epsilon: 1.0    steps: 824  evaluation reward: 186.95\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6694: Policy loss: 0.082077. Value loss: 16.418652. Entropy: 0.715090.\n",
      "Iteration 6695: Policy loss: 0.376425. Value loss: 12.227243. Entropy: 0.727404.\n",
      "Iteration 6696: Policy loss: -0.077763. Value loss: 11.046666. Entropy: 0.731045.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6697: Policy loss: 0.533515. Value loss: 22.253513. Entropy: 0.543111.\n",
      "Iteration 6698: Policy loss: 0.587491. Value loss: 13.375238. Entropy: 0.585208.\n",
      "Iteration 6699: Policy loss: 0.643260. Value loss: 10.555286. Entropy: 0.592758.\n",
      "episode: 2971   score: 130.0  epsilon: 1.0    steps: 331  evaluation reward: 187.15\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6700: Policy loss: 0.594694. Value loss: 14.124692. Entropy: 0.827429.\n",
      "Iteration 6701: Policy loss: 0.836460. Value loss: 9.362935. Entropy: 0.827350.\n",
      "Iteration 6702: Policy loss: 0.577742. Value loss: 10.338082. Entropy: 0.829687.\n",
      "episode: 2972   score: 105.0  epsilon: 1.0    steps: 889  evaluation reward: 186.7\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6703: Policy loss: -0.479910. Value loss: 18.036802. Entropy: 0.785097.\n",
      "Iteration 6704: Policy loss: -0.484032. Value loss: 9.875843. Entropy: 0.805086.\n",
      "Iteration 6705: Policy loss: -0.494610. Value loss: 6.873387. Entropy: 0.800572.\n",
      "episode: 2973   score: 135.0  epsilon: 1.0    steps: 228  evaluation reward: 184.15\n",
      "episode: 2974   score: 180.0  epsilon: 1.0    steps: 582  evaluation reward: 184.7\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6706: Policy loss: 2.141582. Value loss: 22.427656. Entropy: 0.717405.\n",
      "Iteration 6707: Policy loss: 2.204259. Value loss: 13.277578. Entropy: 0.729158.\n",
      "Iteration 6708: Policy loss: 2.317951. Value loss: 11.250278. Entropy: 0.706398.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6709: Policy loss: -0.795937. Value loss: 36.403122. Entropy: 0.723350.\n",
      "Iteration 6710: Policy loss: -0.670785. Value loss: 19.015789. Entropy: 0.691630.\n",
      "Iteration 6711: Policy loss: -0.855006. Value loss: 14.893230. Entropy: 0.703272.\n",
      "episode: 2975   score: 295.0  epsilon: 1.0    steps: 467  evaluation reward: 184.5\n",
      "episode: 2976   score: 305.0  epsilon: 1.0    steps: 1013  evaluation reward: 186.95\n",
      "Training network. lr: 0.000199. clip: 0.079421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6712: Policy loss: -1.619627. Value loss: 33.682491. Entropy: 0.785547.\n",
      "Iteration 6713: Policy loss: -1.708935. Value loss: 17.940725. Entropy: 0.770255.\n",
      "Iteration 6714: Policy loss: -1.705692. Value loss: 13.752329. Entropy: 0.772272.\n",
      "episode: 2977   score: 220.0  epsilon: 1.0    steps: 335  evaluation reward: 187.6\n",
      "episode: 2978   score: 345.0  epsilon: 1.0    steps: 677  evaluation reward: 189.5\n",
      "episode: 2979   score: 105.0  epsilon: 1.0    steps: 828  evaluation reward: 188.45\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6715: Policy loss: 0.319424. Value loss: 18.312464. Entropy: 0.906632.\n",
      "Iteration 6716: Policy loss: 0.193300. Value loss: 8.750050. Entropy: 0.884528.\n",
      "Iteration 6717: Policy loss: 0.076127. Value loss: 7.023168. Entropy: 0.918474.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6718: Policy loss: -0.999981. Value loss: 19.348555. Entropy: 0.648786.\n",
      "Iteration 6719: Policy loss: -0.864450. Value loss: 11.183462. Entropy: 0.660541.\n",
      "Iteration 6720: Policy loss: -0.987762. Value loss: 8.992579. Entropy: 0.652927.\n",
      "episode: 2980   score: 210.0  epsilon: 1.0    steps: 593  evaluation reward: 189.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6721: Policy loss: 1.261383. Value loss: 36.559414. Entropy: 0.718978.\n",
      "Iteration 6722: Policy loss: 1.358748. Value loss: 17.101606. Entropy: 0.690834.\n",
      "Iteration 6723: Policy loss: 1.321978. Value loss: 13.020170. Entropy: 0.699048.\n",
      "episode: 2981   score: 430.0  epsilon: 1.0    steps: 27  evaluation reward: 191.45\n",
      "episode: 2982   score: 190.0  epsilon: 1.0    steps: 239  evaluation reward: 191.05\n",
      "episode: 2983   score: 80.0  epsilon: 1.0    steps: 711  evaluation reward: 188.75\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6724: Policy loss: 0.804834. Value loss: 22.868568. Entropy: 0.769252.\n",
      "Iteration 6725: Policy loss: 1.329482. Value loss: 14.084663. Entropy: 0.761831.\n",
      "Iteration 6726: Policy loss: 1.019382. Value loss: 9.580918. Entropy: 0.724638.\n",
      "episode: 2984   score: 185.0  epsilon: 1.0    steps: 424  evaluation reward: 188.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6727: Policy loss: 0.301000. Value loss: 26.023069. Entropy: 0.644463.\n",
      "Iteration 6728: Policy loss: 0.506427. Value loss: 13.612699. Entropy: 0.645432.\n",
      "Iteration 6729: Policy loss: 0.362787. Value loss: 10.550898. Entropy: 0.631055.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6730: Policy loss: -0.931439. Value loss: 30.528818. Entropy: 0.656183.\n",
      "Iteration 6731: Policy loss: -0.913105. Value loss: 16.075468. Entropy: 0.678071.\n",
      "Iteration 6732: Policy loss: -1.186048. Value loss: 10.974441. Entropy: 0.654340.\n",
      "episode: 2985   score: 235.0  epsilon: 1.0    steps: 844  evaluation reward: 188.35\n",
      "episode: 2986   score: 180.0  epsilon: 1.0    steps: 966  evaluation reward: 185.75\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6733: Policy loss: -0.926155. Value loss: 22.901625. Entropy: 0.664072.\n",
      "Iteration 6734: Policy loss: -0.915603. Value loss: 12.933369. Entropy: 0.693836.\n",
      "Iteration 6735: Policy loss: -1.070371. Value loss: 10.723910. Entropy: 0.682704.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6736: Policy loss: 2.165510. Value loss: 19.925776. Entropy: 0.758440.\n",
      "Iteration 6737: Policy loss: 2.156288. Value loss: 11.624377. Entropy: 0.787184.\n",
      "Iteration 6738: Policy loss: 2.078528. Value loss: 8.776000. Entropy: 0.763076.\n",
      "episode: 2987   score: 245.0  epsilon: 1.0    steps: 62  evaluation reward: 186.9\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6739: Policy loss: -4.531189. Value loss: 263.854523. Entropy: 0.651324.\n",
      "Iteration 6740: Policy loss: -4.339442. Value loss: 123.167427. Entropy: 0.625214.\n",
      "Iteration 6741: Policy loss: -4.229002. Value loss: 55.754333. Entropy: 0.623570.\n",
      "episode: 2988   score: 200.0  epsilon: 1.0    steps: 199  evaluation reward: 186.15\n",
      "episode: 2989   score: 430.0  epsilon: 1.0    steps: 277  evaluation reward: 188.9\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6742: Policy loss: 0.962751. Value loss: 48.281975. Entropy: 0.741818.\n",
      "Iteration 6743: Policy loss: 0.927774. Value loss: 23.407343. Entropy: 0.766308.\n",
      "Iteration 6744: Policy loss: 0.931874. Value loss: 16.586929. Entropy: 0.741030.\n",
      "episode: 2990   score: 445.0  epsilon: 1.0    steps: 475  evaluation reward: 188.2\n",
      "episode: 2991   score: 285.0  epsilon: 1.0    steps: 694  evaluation reward: 187.6\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6745: Policy loss: -4.029719. Value loss: 270.710114. Entropy: 0.641440.\n",
      "Iteration 6746: Policy loss: -3.734869. Value loss: 104.839233. Entropy: 0.597468.\n",
      "Iteration 6747: Policy loss: -3.497440. Value loss: 48.294693. Entropy: 0.604697.\n",
      "episode: 2992   score: 395.0  epsilon: 1.0    steps: 544  evaluation reward: 188.4\n",
      "episode: 2993   score: 260.0  epsilon: 1.0    steps: 855  evaluation reward: 184.8\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6748: Policy loss: 1.955075. Value loss: 43.444782. Entropy: 0.610076.\n",
      "Iteration 6749: Policy loss: 2.319417. Value loss: 28.442631. Entropy: 0.625569.\n",
      "Iteration 6750: Policy loss: 1.973517. Value loss: 23.907084. Entropy: 0.659381.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6751: Policy loss: 3.486275. Value loss: 46.987617. Entropy: 0.648938.\n",
      "Iteration 6752: Policy loss: 3.678195. Value loss: 25.500631. Entropy: 0.663243.\n",
      "Iteration 6753: Policy loss: 3.541517. Value loss: 19.566868. Entropy: 0.656672.\n",
      "episode: 2994   score: 180.0  epsilon: 1.0    steps: 36  evaluation reward: 181.45\n",
      "episode: 2995   score: 495.0  epsilon: 1.0    steps: 978  evaluation reward: 183.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6754: Policy loss: 3.581412. Value loss: 83.254494. Entropy: 0.553592.\n",
      "Iteration 6755: Policy loss: 3.629098. Value loss: 36.063480. Entropy: 0.568108.\n",
      "Iteration 6756: Policy loss: 2.895716. Value loss: 20.084496. Entropy: 0.562909.\n",
      "episode: 2996   score: 200.0  epsilon: 1.0    steps: 266  evaluation reward: 184.05\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6757: Policy loss: -0.833386. Value loss: 39.186459. Entropy: 0.692755.\n",
      "Iteration 6758: Policy loss: -0.604038. Value loss: 22.382277. Entropy: 0.674196.\n",
      "Iteration 6759: Policy loss: -0.452455. Value loss: 15.498215. Entropy: 0.689899.\n",
      "episode: 2997   score: 240.0  epsilon: 1.0    steps: 707  evaluation reward: 185.1\n",
      "episode: 2998   score: 65.0  epsilon: 1.0    steps: 806  evaluation reward: 185.0\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6760: Policy loss: -2.172263. Value loss: 52.247726. Entropy: 0.604017.\n",
      "Iteration 6761: Policy loss: -2.056500. Value loss: 26.003801. Entropy: 0.621705.\n",
      "Iteration 6762: Policy loss: -1.784456. Value loss: 19.516830. Entropy: 0.584338.\n",
      "episode: 2999   score: 285.0  epsilon: 1.0    steps: 166  evaluation reward: 185.75\n",
      "episode: 3000   score: 195.0  epsilon: 1.0    steps: 522  evaluation reward: 182.4\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6763: Policy loss: 2.796143. Value loss: 62.914288. Entropy: 0.651868.\n",
      "Iteration 6764: Policy loss: 3.136364. Value loss: 35.748013. Entropy: 0.722707.\n",
      "Iteration 6765: Policy loss: 2.849759. Value loss: 25.020422. Entropy: 0.700513.\n",
      "now time :  2019-02-25 20:46:29.271776\n",
      "episode: 3001   score: 80.0  epsilon: 1.0    steps: 25  evaluation reward: 180.05\n",
      "episode: 3002   score: 370.0  epsilon: 1.0    steps: 444  evaluation reward: 181.65\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6766: Policy loss: -0.402839. Value loss: 33.426060. Entropy: 0.662034.\n",
      "Iteration 6767: Policy loss: -0.274476. Value loss: 20.477814. Entropy: 0.676369.\n",
      "Iteration 6768: Policy loss: -0.519862. Value loss: 17.670696. Entropy: 0.656341.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6769: Policy loss: 1.579607. Value loss: 50.764111. Entropy: 0.662787.\n",
      "Iteration 6770: Policy loss: 0.956779. Value loss: 23.364679. Entropy: 0.656422.\n",
      "Iteration 6771: Policy loss: 1.359515. Value loss: 18.075695. Entropy: 0.664989.\n",
      "episode: 3003   score: 210.0  epsilon: 1.0    steps: 290  evaluation reward: 182.4\n",
      "episode: 3004   score: 225.0  epsilon: 1.0    steps: 924  evaluation reward: 183.3\n",
      "Training network. lr: 0.000198. clip: 0.079273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6772: Policy loss: 0.102297. Value loss: 19.080988. Entropy: 0.618566.\n",
      "Iteration 6773: Policy loss: 0.023363. Value loss: 12.078233. Entropy: 0.637619.\n",
      "Iteration 6774: Policy loss: -0.037581. Value loss: 8.775872. Entropy: 0.624885.\n",
      "episode: 3005   score: 225.0  epsilon: 1.0    steps: 839  evaluation reward: 184.0\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6775: Policy loss: 1.081723. Value loss: 32.675533. Entropy: 0.572535.\n",
      "Iteration 6776: Policy loss: 0.660296. Value loss: 19.124628. Entropy: 0.575306.\n",
      "Iteration 6777: Policy loss: 0.799957. Value loss: 14.340330. Entropy: 0.569248.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6778: Policy loss: -1.253003. Value loss: 32.973351. Entropy: 0.507162.\n",
      "Iteration 6779: Policy loss: -1.168996. Value loss: 15.910953. Entropy: 0.511597.\n",
      "Iteration 6780: Policy loss: -1.064971. Value loss: 12.388615. Entropy: 0.499340.\n",
      "episode: 3006   score: 125.0  epsilon: 1.0    steps: 355  evaluation reward: 182.4\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6781: Policy loss: -0.295610. Value loss: 39.808514. Entropy: 0.642140.\n",
      "Iteration 6782: Policy loss: -0.282316. Value loss: 22.126966. Entropy: 0.629344.\n",
      "Iteration 6783: Policy loss: -0.220159. Value loss: 16.127998. Entropy: 0.647707.\n",
      "episode: 3007   score: 310.0  epsilon: 1.0    steps: 563  evaluation reward: 184.45\n",
      "episode: 3008   score: 365.0  epsilon: 1.0    steps: 708  evaluation reward: 187.3\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6784: Policy loss: -2.207474. Value loss: 47.987335. Entropy: 0.704784.\n",
      "Iteration 6785: Policy loss: -2.524168. Value loss: 22.497927. Entropy: 0.699666.\n",
      "Iteration 6786: Policy loss: -2.602249. Value loss: 15.644845. Entropy: 0.688505.\n",
      "episode: 3009   score: 440.0  epsilon: 1.0    steps: 194  evaluation reward: 190.5\n",
      "episode: 3010   score: 245.0  epsilon: 1.0    steps: 989  evaluation reward: 191.7\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6787: Policy loss: 1.001471. Value loss: 27.641727. Entropy: 0.659556.\n",
      "Iteration 6788: Policy loss: 1.280168. Value loss: 15.045596. Entropy: 0.683563.\n",
      "Iteration 6789: Policy loss: 1.099221. Value loss: 12.562266. Entropy: 0.681348.\n",
      "episode: 3011   score: 395.0  epsilon: 1.0    steps: 37  evaluation reward: 194.6\n",
      "episode: 3012   score: 280.0  epsilon: 1.0    steps: 419  evaluation reward: 196.2\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6790: Policy loss: 0.100972. Value loss: 24.531307. Entropy: 0.620160.\n",
      "Iteration 6791: Policy loss: 0.241187. Value loss: 14.817258. Entropy: 0.624837.\n",
      "Iteration 6792: Policy loss: 0.359939. Value loss: 9.976668. Entropy: 0.619204.\n",
      "episode: 3013   score: 245.0  epsilon: 1.0    steps: 811  evaluation reward: 197.6\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6793: Policy loss: -0.009832. Value loss: 26.584986. Entropy: 0.623914.\n",
      "Iteration 6794: Policy loss: -0.106807. Value loss: 18.548388. Entropy: 0.603094.\n",
      "Iteration 6795: Policy loss: 0.115987. Value loss: 14.795140. Entropy: 0.609846.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6796: Policy loss: 1.267093. Value loss: 34.367641. Entropy: 0.425865.\n",
      "Iteration 6797: Policy loss: 1.494778. Value loss: 20.837328. Entropy: 0.440865.\n",
      "Iteration 6798: Policy loss: 1.286789. Value loss: 16.662304. Entropy: 0.462682.\n",
      "episode: 3014   score: 260.0  epsilon: 1.0    steps: 290  evaluation reward: 199.45\n",
      "episode: 3015   score: 210.0  epsilon: 1.0    steps: 606  evaluation reward: 198.9\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6799: Policy loss: 1.034351. Value loss: 20.719458. Entropy: 0.644120.\n",
      "Iteration 6800: Policy loss: 1.134225. Value loss: 15.472550. Entropy: 0.626387.\n",
      "Iteration 6801: Policy loss: 1.148734. Value loss: 11.240154. Entropy: 0.638554.\n",
      "episode: 3016   score: 135.0  epsilon: 1.0    steps: 418  evaluation reward: 199.5\n",
      "episode: 3017   score: 260.0  epsilon: 1.0    steps: 688  evaluation reward: 201.05\n",
      "episode: 3018   score: 50.0  epsilon: 1.0    steps: 800  evaluation reward: 200.8\n",
      "episode: 3019   score: 215.0  epsilon: 1.0    steps: 1024  evaluation reward: 202.2\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6802: Policy loss: 0.362083. Value loss: 11.949570. Entropy: 0.715996.\n",
      "Iteration 6803: Policy loss: 0.493673. Value loss: 6.781619. Entropy: 0.730275.\n",
      "Iteration 6804: Policy loss: 0.466290. Value loss: 6.463255. Entropy: 0.729921.\n",
      "episode: 3020   score: 240.0  epsilon: 1.0    steps: 127  evaluation reward: 202.65\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6805: Policy loss: -2.925654. Value loss: 240.466721. Entropy: 0.659163.\n",
      "Iteration 6806: Policy loss: -3.608695. Value loss: 211.072708. Entropy: 0.629416.\n",
      "Iteration 6807: Policy loss: -4.364306. Value loss: 218.739822. Entropy: 0.607629.\n",
      "episode: 3021   score: 125.0  epsilon: 1.0    steps: 338  evaluation reward: 203.15\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6808: Policy loss: -2.244439. Value loss: 35.882511. Entropy: 0.593078.\n",
      "Iteration 6809: Policy loss: -1.981684. Value loss: 20.401031. Entropy: 0.589010.\n",
      "Iteration 6810: Policy loss: -2.117882. Value loss: 16.149385. Entropy: 0.591287.\n",
      "episode: 3022   score: 545.0  epsilon: 1.0    steps: 169  evaluation reward: 207.55\n",
      "episode: 3023   score: 125.0  epsilon: 1.0    steps: 737  evaluation reward: 207.75\n",
      "episode: 3024   score: 55.0  epsilon: 1.0    steps: 848  evaluation reward: 207.1\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6811: Policy loss: 1.103465. Value loss: 32.638889. Entropy: 0.495294.\n",
      "Iteration 6812: Policy loss: 0.790154. Value loss: 17.359646. Entropy: 0.498789.\n",
      "Iteration 6813: Policy loss: 1.107309. Value loss: 15.782517. Entropy: 0.530413.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6814: Policy loss: -0.580140. Value loss: 34.907654. Entropy: 0.645056.\n",
      "Iteration 6815: Policy loss: -0.571801. Value loss: 20.761539. Entropy: 0.624411.\n",
      "Iteration 6816: Policy loss: -0.896303. Value loss: 14.953070. Entropy: 0.641857.\n",
      "episode: 3025   score: 210.0  epsilon: 1.0    steps: 426  evaluation reward: 208.15\n",
      "episode: 3026   score: 225.0  epsilon: 1.0    steps: 517  evaluation reward: 208.9\n",
      "episode: 3027   score: 155.0  epsilon: 1.0    steps: 929  evaluation reward: 209.7\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6817: Policy loss: 0.001159. Value loss: 20.946650. Entropy: 0.545053.\n",
      "Iteration 6818: Policy loss: -0.006398. Value loss: 12.127476. Entropy: 0.557042.\n",
      "Iteration 6819: Policy loss: -0.028959. Value loss: 11.074662. Entropy: 0.560233.\n",
      "episode: 3028   score: 135.0  epsilon: 1.0    steps: 268  evaluation reward: 207.75\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6820: Policy loss: 0.062123. Value loss: 30.461039. Entropy: 0.641807.\n",
      "Iteration 6821: Policy loss: -0.331257. Value loss: 19.364836. Entropy: 0.626061.\n",
      "Iteration 6822: Policy loss: 0.157296. Value loss: 13.807681. Entropy: 0.625141.\n",
      "episode: 3029   score: 215.0  epsilon: 1.0    steps: 21  evaluation reward: 208.8\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6823: Policy loss: -3.379381. Value loss: 34.912018. Entropy: 0.465754.\n",
      "Iteration 6824: Policy loss: -3.221407. Value loss: 20.659380. Entropy: 0.449232.\n",
      "Iteration 6825: Policy loss: -3.320711. Value loss: 15.681005. Entropy: 0.452891.\n",
      "episode: 3030   score: 215.0  epsilon: 1.0    steps: 163  evaluation reward: 208.4\n",
      "episode: 3031   score: 210.0  epsilon: 1.0    steps: 758  evaluation reward: 209.0\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6826: Policy loss: 1.042020. Value loss: 30.582272. Entropy: 0.583156.\n",
      "Iteration 6827: Policy loss: 0.913131. Value loss: 16.326361. Entropy: 0.578270.\n",
      "Iteration 6828: Policy loss: 1.135022. Value loss: 13.442662. Entropy: 0.591396.\n",
      "episode: 3032   score: 215.0  epsilon: 1.0    steps: 769  evaluation reward: 209.6\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6829: Policy loss: 0.324982. Value loss: 29.457855. Entropy: 0.569119.\n",
      "Iteration 6830: Policy loss: 0.504674. Value loss: 15.223193. Entropy: 0.571412.\n",
      "Iteration 6831: Policy loss: 0.528239. Value loss: 12.197673. Entropy: 0.573202.\n",
      "episode: 3033   score: 140.0  epsilon: 1.0    steps: 122  evaluation reward: 210.2\n",
      "episode: 3034   score: 265.0  epsilon: 1.0    steps: 481  evaluation reward: 211.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3035   score: 240.0  epsilon: 1.0    steps: 586  evaluation reward: 211.0\n",
      "episode: 3036   score: 260.0  epsilon: 1.0    steps: 991  evaluation reward: 212.8\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6832: Policy loss: -1.712093. Value loss: 28.053623. Entropy: 0.444875.\n",
      "Iteration 6833: Policy loss: -1.518338. Value loss: 17.743361. Entropy: 0.467636.\n",
      "Iteration 6834: Policy loss: -1.709108. Value loss: 14.728077. Entropy: 0.455742.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6835: Policy loss: 0.010831. Value loss: 26.711437. Entropy: 0.530274.\n",
      "Iteration 6836: Policy loss: 0.014025. Value loss: 15.248648. Entropy: 0.542772.\n",
      "Iteration 6837: Policy loss: 0.015266. Value loss: 11.272017. Entropy: 0.565387.\n",
      "episode: 3037   score: 275.0  epsilon: 1.0    steps: 262  evaluation reward: 214.5\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6838: Policy loss: 0.484220. Value loss: 31.281155. Entropy: 0.616234.\n",
      "Iteration 6839: Policy loss: 0.528777. Value loss: 19.871861. Entropy: 0.612316.\n",
      "Iteration 6840: Policy loss: 0.672417. Value loss: 15.576859. Entropy: 0.640325.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6841: Policy loss: 2.544414. Value loss: 27.698412. Entropy: 0.503861.\n",
      "Iteration 6842: Policy loss: 2.455526. Value loss: 15.522062. Entropy: 0.482449.\n",
      "Iteration 6843: Policy loss: 2.487332. Value loss: 10.929087. Entropy: 0.523742.\n",
      "episode: 3038   score: 255.0  epsilon: 1.0    steps: 710  evaluation reward: 215.75\n",
      "episode: 3039   score: 135.0  epsilon: 1.0    steps: 976  evaluation reward: 215.8\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6844: Policy loss: 0.025824. Value loss: 29.047333. Entropy: 0.503827.\n",
      "Iteration 6845: Policy loss: -0.466108. Value loss: 18.629215. Entropy: 0.453389.\n",
      "Iteration 6846: Policy loss: 0.151254. Value loss: 14.359588. Entropy: 0.481003.\n",
      "episode: 3040   score: 180.0  epsilon: 1.0    steps: 408  evaluation reward: 214.05\n",
      "episode: 3041   score: 275.0  epsilon: 1.0    steps: 834  evaluation reward: 215.25\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6847: Policy loss: 1.477165. Value loss: 22.426918. Entropy: 0.561312.\n",
      "Iteration 6848: Policy loss: 1.120981. Value loss: 9.580231. Entropy: 0.578696.\n",
      "Iteration 6849: Policy loss: 1.431805. Value loss: 8.197011. Entropy: 0.584231.\n",
      "episode: 3042   score: 185.0  epsilon: 1.0    steps: 280  evaluation reward: 216.35\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6850: Policy loss: 1.339854. Value loss: 16.280481. Entropy: 0.648384.\n",
      "Iteration 6851: Policy loss: 1.598466. Value loss: 9.643079. Entropy: 0.677174.\n",
      "Iteration 6852: Policy loss: 1.539443. Value loss: 9.486212. Entropy: 0.682797.\n",
      "episode: 3043   score: 255.0  epsilon: 1.0    steps: 45  evaluation reward: 218.15\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6853: Policy loss: 0.301389. Value loss: 21.920250. Entropy: 0.789047.\n",
      "Iteration 6854: Policy loss: 0.288951. Value loss: 13.044295. Entropy: 0.807240.\n",
      "Iteration 6855: Policy loss: 0.336131. Value loss: 10.463733. Entropy: 0.803425.\n",
      "episode: 3044   score: 105.0  epsilon: 1.0    steps: 440  evaluation reward: 216.1\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6856: Policy loss: -2.656603. Value loss: 34.871372. Entropy: 0.640852.\n",
      "Iteration 6857: Policy loss: -2.743971. Value loss: 16.891798. Entropy: 0.627565.\n",
      "Iteration 6858: Policy loss: -2.808287. Value loss: 14.003021. Entropy: 0.616250.\n",
      "episode: 3045   score: 330.0  epsilon: 1.0    steps: 136  evaluation reward: 216.4\n",
      "episode: 3046   score: 275.0  epsilon: 1.0    steps: 615  evaluation reward: 218.1\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6859: Policy loss: -0.583990. Value loss: 17.497250. Entropy: 0.700898.\n",
      "Iteration 6860: Policy loss: -0.725245. Value loss: 11.267824. Entropy: 0.689942.\n",
      "Iteration 6861: Policy loss: -0.825496. Value loss: 9.689734. Entropy: 0.679802.\n",
      "episode: 3047   score: 260.0  epsilon: 1.0    steps: 649  evaluation reward: 218.75\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6862: Policy loss: 0.066859. Value loss: 18.663389. Entropy: 0.769183.\n",
      "Iteration 6863: Policy loss: 0.140246. Value loss: 12.047024. Entropy: 0.759322.\n",
      "Iteration 6864: Policy loss: -0.179156. Value loss: 10.368055. Entropy: 0.763044.\n",
      "episode: 3048   score: 245.0  epsilon: 1.0    steps: 271  evaluation reward: 218.5\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6865: Policy loss: -0.723890. Value loss: 30.191666. Entropy: 0.629265.\n",
      "Iteration 6866: Policy loss: -0.640540. Value loss: 17.101206. Entropy: 0.625743.\n",
      "Iteration 6867: Policy loss: -0.839388. Value loss: 12.672495. Entropy: 0.628719.\n",
      "episode: 3049   score: 320.0  epsilon: 1.0    steps: 929  evaluation reward: 220.35\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6868: Policy loss: 1.607987. Value loss: 23.273397. Entropy: 0.610471.\n",
      "Iteration 6869: Policy loss: 1.745402. Value loss: 12.136156. Entropy: 0.595971.\n",
      "Iteration 6870: Policy loss: 1.705298. Value loss: 9.643666. Entropy: 0.602190.\n",
      "episode: 3050   score: 285.0  epsilon: 1.0    steps: 49  evaluation reward: 220.65\n",
      "now time :  2019-02-25 20:48:27.302631\n",
      "episode: 3051   score: 180.0  epsilon: 1.0    steps: 188  evaluation reward: 221.65\n",
      "episode: 3052   score: 225.0  epsilon: 1.0    steps: 496  evaluation reward: 223.15\n",
      "episode: 3053   score: 75.0  epsilon: 1.0    steps: 562  evaluation reward: 222.85\n",
      "episode: 3054   score: 135.0  epsilon: 1.0    steps: 765  evaluation reward: 220.0\n",
      "episode: 3055   score: 340.0  epsilon: 1.0    steps: 813  evaluation reward: 222.65\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6871: Policy loss: 2.315828. Value loss: 22.996086. Entropy: 0.466899.\n",
      "Iteration 6872: Policy loss: 2.540726. Value loss: 12.501977. Entropy: 0.458421.\n",
      "Iteration 6873: Policy loss: 2.262877. Value loss: 10.683876. Entropy: 0.461713.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6874: Policy loss: -1.441978. Value loss: 24.654579. Entropy: 0.752514.\n",
      "Iteration 6875: Policy loss: -1.569484. Value loss: 15.779448. Entropy: 0.753132.\n",
      "Iteration 6876: Policy loss: -1.651130. Value loss: 14.872987. Entropy: 0.741881.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6877: Policy loss: -2.585346. Value loss: 33.823200. Entropy: 0.546129.\n",
      "Iteration 6878: Policy loss: -2.389007. Value loss: 17.677130. Entropy: 0.560968.\n",
      "Iteration 6879: Policy loss: -2.603629. Value loss: 15.721848. Entropy: 0.561154.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6880: Policy loss: -1.106558. Value loss: 32.723167. Entropy: 0.589968.\n",
      "Iteration 6881: Policy loss: -1.229152. Value loss: 20.410128. Entropy: 0.569494.\n",
      "Iteration 6882: Policy loss: -0.909153. Value loss: 16.024351. Entropy: 0.552023.\n",
      "episode: 3056   score: 290.0  epsilon: 1.0    steps: 260  evaluation reward: 224.5\n",
      "episode: 3057   score: 270.0  epsilon: 1.0    steps: 1024  evaluation reward: 226.45\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6883: Policy loss: 2.043543. Value loss: 30.071688. Entropy: 0.607056.\n",
      "Iteration 6884: Policy loss: 1.963999. Value loss: 15.191462. Entropy: 0.608211.\n",
      "Iteration 6885: Policy loss: 2.019554. Value loss: 11.078337. Entropy: 0.602165.\n",
      "episode: 3058   score: 240.0  epsilon: 1.0    steps: 102  evaluation reward: 227.8\n",
      "episode: 3059   score: 155.0  epsilon: 1.0    steps: 508  evaluation reward: 228.55\n",
      "episode: 3060   score: 180.0  epsilon: 1.0    steps: 667  evaluation reward: 228.4\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6886: Policy loss: 1.074465. Value loss: 25.328650. Entropy: 0.617119.\n",
      "Iteration 6887: Policy loss: 1.083655. Value loss: 11.986985. Entropy: 0.633825.\n",
      "Iteration 6888: Policy loss: 1.175288. Value loss: 9.767703. Entropy: 0.609435.\n",
      "episode: 3061   score: 260.0  epsilon: 1.0    steps: 605  evaluation reward: 230.25\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6889: Policy loss: 0.896433. Value loss: 29.098515. Entropy: 0.678971.\n",
      "Iteration 6890: Policy loss: 1.222728. Value loss: 14.208375. Entropy: 0.662648.\n",
      "Iteration 6891: Policy loss: 1.056393. Value loss: 11.936532. Entropy: 0.698554.\n",
      "episode: 3062   score: 360.0  epsilon: 1.0    steps: 242  evaluation reward: 232.8\n",
      "Training network. lr: 0.000197. clip: 0.078960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6892: Policy loss: -2.425145. Value loss: 28.995779. Entropy: 0.622169.\n",
      "Iteration 6893: Policy loss: -2.231010. Value loss: 16.142897. Entropy: 0.637219.\n",
      "Iteration 6894: Policy loss: -2.222157. Value loss: 12.265715. Entropy: 0.625284.\n",
      "episode: 3063   score: 345.0  epsilon: 1.0    steps: 769  evaluation reward: 235.1\n",
      "episode: 3064   score: 150.0  epsilon: 1.0    steps: 990  evaluation reward: 232.8\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6895: Policy loss: -0.935072. Value loss: 27.089642. Entropy: 0.518793.\n",
      "Iteration 6896: Policy loss: -0.930653. Value loss: 15.873392. Entropy: 0.483056.\n",
      "Iteration 6897: Policy loss: -0.827516. Value loss: 14.229106. Entropy: 0.502205.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6898: Policy loss: 1.362264. Value loss: 20.216970. Entropy: 0.720522.\n",
      "Iteration 6899: Policy loss: 1.344995. Value loss: 11.612790. Entropy: 0.706785.\n",
      "Iteration 6900: Policy loss: 1.411379. Value loss: 9.955474. Entropy: 0.721975.\n",
      "episode: 3065   score: 315.0  epsilon: 1.0    steps: 263  evaluation reward: 234.7\n",
      "episode: 3066   score: 260.0  epsilon: 1.0    steps: 751  evaluation reward: 236.25\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6901: Policy loss: -0.290108. Value loss: 32.682789. Entropy: 0.369106.\n",
      "Iteration 6902: Policy loss: -0.476667. Value loss: 24.020849. Entropy: 0.367537.\n",
      "Iteration 6903: Policy loss: -0.478230. Value loss: 19.395264. Entropy: 0.347059.\n",
      "episode: 3067   score: 260.0  epsilon: 1.0    steps: 77  evaluation reward: 236.45\n",
      "episode: 3068   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 238.0\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6904: Policy loss: 0.149482. Value loss: 24.547220. Entropy: 0.642792.\n",
      "Iteration 6905: Policy loss: 0.420134. Value loss: 15.245855. Entropy: 0.638493.\n",
      "Iteration 6906: Policy loss: 0.197166. Value loss: 12.012667. Entropy: 0.638793.\n",
      "episode: 3069   score: 155.0  epsilon: 1.0    steps: 782  evaluation reward: 236.9\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6907: Policy loss: 1.287341. Value loss: 26.451025. Entropy: 0.612328.\n",
      "Iteration 6908: Policy loss: 1.423169. Value loss: 17.379881. Entropy: 0.625106.\n",
      "Iteration 6909: Policy loss: 1.122274. Value loss: 13.144523. Entropy: 0.594965.\n",
      "episode: 3070   score: 290.0  epsilon: 1.0    steps: 182  evaluation reward: 235.95\n",
      "episode: 3071   score: 75.0  epsilon: 1.0    steps: 756  evaluation reward: 235.4\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6910: Policy loss: 0.368357. Value loss: 251.048782. Entropy: 0.557711.\n",
      "Iteration 6911: Policy loss: 1.960553. Value loss: 53.171902. Entropy: 0.564706.\n",
      "Iteration 6912: Policy loss: 0.975296. Value loss: 30.328531. Entropy: 0.544832.\n",
      "episode: 3072   score: 210.0  epsilon: 1.0    steps: 276  evaluation reward: 236.45\n",
      "episode: 3073   score: 105.0  epsilon: 1.0    steps: 436  evaluation reward: 236.15\n",
      "episode: 3074   score: 240.0  epsilon: 1.0    steps: 919  evaluation reward: 236.75\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6913: Policy loss: 3.472308. Value loss: 21.522346. Entropy: 0.656355.\n",
      "Iteration 6914: Policy loss: 3.524296. Value loss: 12.106863. Entropy: 0.683876.\n",
      "Iteration 6915: Policy loss: 3.644193. Value loss: 9.691485. Entropy: 0.681823.\n",
      "episode: 3075   score: 95.0  epsilon: 1.0    steps: 22  evaluation reward: 234.75\n",
      "episode: 3076   score: 530.0  epsilon: 1.0    steps: 592  evaluation reward: 237.0\n",
      "episode: 3077   score: 125.0  epsilon: 1.0    steps: 854  evaluation reward: 236.05\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6916: Policy loss: 0.208484. Value loss: 36.939213. Entropy: 0.637483.\n",
      "Iteration 6917: Policy loss: 0.162016. Value loss: 19.240549. Entropy: 0.616650.\n",
      "Iteration 6918: Policy loss: 0.072407. Value loss: 16.262453. Entropy: 0.611142.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6919: Policy loss: 0.022850. Value loss: 34.844681. Entropy: 0.509119.\n",
      "Iteration 6920: Policy loss: -0.410050. Value loss: 25.260136. Entropy: 0.485448.\n",
      "Iteration 6921: Policy loss: -0.035332. Value loss: 20.014874. Entropy: 0.470617.\n",
      "episode: 3078   score: 120.0  epsilon: 1.0    steps: 237  evaluation reward: 233.8\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6922: Policy loss: -0.359090. Value loss: 28.883291. Entropy: 0.346343.\n",
      "Iteration 6923: Policy loss: -0.432744. Value loss: 18.354591. Entropy: 0.354920.\n",
      "Iteration 6924: Policy loss: -0.264107. Value loss: 14.149268. Entropy: 0.369289.\n",
      "episode: 3079   score: 180.0  epsilon: 1.0    steps: 758  evaluation reward: 234.55\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6925: Policy loss: 2.767581. Value loss: 25.201075. Entropy: 0.507007.\n",
      "Iteration 6926: Policy loss: 3.008366. Value loss: 15.955400. Entropy: 0.506774.\n",
      "Iteration 6927: Policy loss: 2.772557. Value loss: 12.224777. Entropy: 0.508968.\n",
      "episode: 3080   score: 235.0  epsilon: 1.0    steps: 344  evaluation reward: 234.8\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6928: Policy loss: 1.801460. Value loss: 31.179178. Entropy: 0.445063.\n",
      "Iteration 6929: Policy loss: 1.848431. Value loss: 16.158569. Entropy: 0.468333.\n",
      "Iteration 6930: Policy loss: 1.916154. Value loss: 12.649257. Entropy: 0.471125.\n",
      "episode: 3081   score: 225.0  epsilon: 1.0    steps: 79  evaluation reward: 232.75\n",
      "episode: 3082   score: 305.0  epsilon: 1.0    steps: 499  evaluation reward: 233.9\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6931: Policy loss: 1.851044. Value loss: 33.445473. Entropy: 0.572071.\n",
      "Iteration 6932: Policy loss: 1.875816. Value loss: 17.225016. Entropy: 0.566615.\n",
      "Iteration 6933: Policy loss: 1.861878. Value loss: 14.334571. Entropy: 0.568402.\n",
      "episode: 3083   score: 60.0  epsilon: 1.0    steps: 142  evaluation reward: 233.7\n",
      "episode: 3084   score: 260.0  epsilon: 1.0    steps: 535  evaluation reward: 234.45\n",
      "episode: 3085   score: 240.0  epsilon: 1.0    steps: 800  evaluation reward: 234.5\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6934: Policy loss: 0.991622. Value loss: 34.543167. Entropy: 0.639657.\n",
      "Iteration 6935: Policy loss: 1.166645. Value loss: 19.703403. Entropy: 0.649347.\n",
      "Iteration 6936: Policy loss: 1.271253. Value loss: 15.065385. Entropy: 0.660354.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6937: Policy loss: 0.929689. Value loss: 36.669380. Entropy: 0.550026.\n",
      "Iteration 6938: Policy loss: 0.781751. Value loss: 19.393089. Entropy: 0.544354.\n",
      "Iteration 6939: Policy loss: 1.384621. Value loss: 16.149748. Entropy: 0.573022.\n",
      "episode: 3086   score: 125.0  epsilon: 1.0    steps: 291  evaluation reward: 233.95\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6940: Policy loss: -1.523895. Value loss: 279.901428. Entropy: 0.506148.\n",
      "Iteration 6941: Policy loss: -0.991258. Value loss: 96.627983. Entropy: 0.624866.\n",
      "Iteration 6942: Policy loss: -1.177352. Value loss: 76.433571. Entropy: 0.557620.\n",
      "episode: 3087   score: 125.0  epsilon: 1.0    steps: 434  evaluation reward: 232.75\n",
      "episode: 3088   score: 285.0  epsilon: 1.0    steps: 720  evaluation reward: 233.6\n",
      "episode: 3089   score: 105.0  epsilon: 1.0    steps: 881  evaluation reward: 230.35\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6943: Policy loss: 0.594424. Value loss: 37.684868. Entropy: 0.553270.\n",
      "Iteration 6944: Policy loss: 0.270427. Value loss: 19.143229. Entropy: 0.560595.\n",
      "Iteration 6945: Policy loss: 0.244453. Value loss: 16.044092. Entropy: 0.577662.\n",
      "episode: 3090   score: 455.0  epsilon: 1.0    steps: 1004  evaluation reward: 230.45\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6946: Policy loss: -0.526203. Value loss: 26.711981. Entropy: 0.609051.\n",
      "Iteration 6947: Policy loss: -0.609807. Value loss: 14.921617. Entropy: 0.594383.\n",
      "Iteration 6948: Policy loss: -0.551188. Value loss: 12.219275. Entropy: 0.593939.\n",
      "episode: 3091   score: 260.0  epsilon: 1.0    steps: 78  evaluation reward: 230.2\n",
      "episode: 3092   score: 290.0  epsilon: 1.0    steps: 243  evaluation reward: 229.15\n",
      "episode: 3093   score: 135.0  epsilon: 1.0    steps: 317  evaluation reward: 227.9\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6949: Policy loss: 0.249199. Value loss: 37.288757. Entropy: 0.518352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6950: Policy loss: 0.447834. Value loss: 23.251740. Entropy: 0.547277.\n",
      "Iteration 6951: Policy loss: 0.360525. Value loss: 16.231928. Entropy: 0.524687.\n",
      "episode: 3094   score: 65.0  epsilon: 1.0    steps: 453  evaluation reward: 226.75\n",
      "episode: 3095   score: 260.0  epsilon: 1.0    steps: 513  evaluation reward: 224.4\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6952: Policy loss: -0.563435. Value loss: 29.358601. Entropy: 0.633174.\n",
      "Iteration 6953: Policy loss: -0.938138. Value loss: 17.671539. Entropy: 0.614109.\n",
      "Iteration 6954: Policy loss: -1.006961. Value loss: 19.928648. Entropy: 0.647847.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6955: Policy loss: -1.367084. Value loss: 24.032316. Entropy: 0.425646.\n",
      "Iteration 6956: Policy loss: -1.383660. Value loss: 15.664126. Entropy: 0.417901.\n",
      "Iteration 6957: Policy loss: -1.417666. Value loss: 12.959441. Entropy: 0.430791.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6958: Policy loss: 0.409715. Value loss: 31.046377. Entropy: 0.418184.\n",
      "Iteration 6959: Policy loss: 0.509900. Value loss: 17.840141. Entropy: 0.432932.\n",
      "Iteration 6960: Policy loss: 0.484965. Value loss: 14.554202. Entropy: 0.433384.\n",
      "episode: 3096   score: 210.0  epsilon: 1.0    steps: 278  evaluation reward: 224.5\n",
      "episode: 3097   score: 225.0  epsilon: 1.0    steps: 690  evaluation reward: 224.35\n",
      "episode: 3098   score: 235.0  epsilon: 1.0    steps: 804  evaluation reward: 226.05\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6961: Policy loss: 0.638214. Value loss: 18.490419. Entropy: 0.471335.\n",
      "Iteration 6962: Policy loss: 0.683939. Value loss: 12.764095. Entropy: 0.469977.\n",
      "Iteration 6963: Policy loss: 0.565846. Value loss: 9.300401. Entropy: 0.466017.\n",
      "episode: 3099   score: 210.0  epsilon: 1.0    steps: 587  evaluation reward: 225.3\n",
      "episode: 3100   score: 240.0  epsilon: 1.0    steps: 926  evaluation reward: 225.75\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6964: Policy loss: 1.301463. Value loss: 32.654427. Entropy: 0.517885.\n",
      "Iteration 6965: Policy loss: 1.308991. Value loss: 19.727276. Entropy: 0.525854.\n",
      "Iteration 6966: Policy loss: 1.158790. Value loss: 14.015650. Entropy: 0.536815.\n",
      "now time :  2019-02-25 20:50:15.118596\n",
      "episode: 3101   score: 375.0  epsilon: 1.0    steps: 69  evaluation reward: 228.7\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6967: Policy loss: 2.155281. Value loss: 36.174133. Entropy: 0.551487.\n",
      "Iteration 6968: Policy loss: 2.250752. Value loss: 19.907331. Entropy: 0.577769.\n",
      "Iteration 6969: Policy loss: 1.966794. Value loss: 16.630396. Entropy: 0.598034.\n",
      "episode: 3102   score: 260.0  epsilon: 1.0    steps: 424  evaluation reward: 227.6\n",
      "episode: 3103   score: 50.0  epsilon: 1.0    steps: 861  evaluation reward: 226.0\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6970: Policy loss: 1.229543. Value loss: 25.221107. Entropy: 0.641795.\n",
      "Iteration 6971: Policy loss: 1.363940. Value loss: 13.025003. Entropy: 0.643002.\n",
      "Iteration 6972: Policy loss: 1.448997. Value loss: 9.817105. Entropy: 0.670730.\n",
      "episode: 3104   score: 365.0  epsilon: 1.0    steps: 166  evaluation reward: 227.4\n",
      "episode: 3105   score: 210.0  epsilon: 1.0    steps: 325  evaluation reward: 227.25\n",
      "episode: 3106   score: 105.0  epsilon: 1.0    steps: 583  evaluation reward: 227.05\n",
      "episode: 3107   score: 210.0  epsilon: 1.0    steps: 691  evaluation reward: 226.05\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6973: Policy loss: -0.887921. Value loss: 31.747219. Entropy: 0.643925.\n",
      "Iteration 6974: Policy loss: -0.718626. Value loss: 20.686562. Entropy: 0.625925.\n",
      "Iteration 6975: Policy loss: -0.839094. Value loss: 16.777843. Entropy: 0.581688.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6976: Policy loss: 0.061441. Value loss: 25.625179. Entropy: 0.590757.\n",
      "Iteration 6977: Policy loss: -0.011470. Value loss: 17.616348. Entropy: 0.576513.\n",
      "Iteration 6978: Policy loss: 0.076764. Value loss: 14.072708. Entropy: 0.568726.\n",
      "episode: 3108   score: 110.0  epsilon: 1.0    steps: 888  evaluation reward: 223.5\n",
      "episode: 3109   score: 240.0  epsilon: 1.0    steps: 963  evaluation reward: 221.5\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6979: Policy loss: -0.656763. Value loss: 23.473442. Entropy: 0.491179.\n",
      "Iteration 6980: Policy loss: -0.817426. Value loss: 16.415808. Entropy: 0.472931.\n",
      "Iteration 6981: Policy loss: -0.744495. Value loss: 12.919962. Entropy: 0.466029.\n",
      "episode: 3110   score: 155.0  epsilon: 1.0    steps: 397  evaluation reward: 220.6\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6982: Policy loss: -1.818766. Value loss: 23.922571. Entropy: 0.549689.\n",
      "Iteration 6983: Policy loss: -1.924637. Value loss: 13.781664. Entropy: 0.533674.\n",
      "Iteration 6984: Policy loss: -1.941093. Value loss: 11.535971. Entropy: 0.557487.\n",
      "episode: 3111   score: 180.0  epsilon: 1.0    steps: 150  evaluation reward: 218.45\n",
      "episode: 3112   score: 180.0  epsilon: 1.0    steps: 337  evaluation reward: 217.45\n",
      "episode: 3113   score: 135.0  epsilon: 1.0    steps: 559  evaluation reward: 216.35\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6985: Policy loss: -0.648854. Value loss: 22.613586. Entropy: 0.533847.\n",
      "Iteration 6986: Policy loss: -0.545756. Value loss: 13.474083. Entropy: 0.559533.\n",
      "Iteration 6987: Policy loss: -0.826163. Value loss: 10.882172. Entropy: 0.547187.\n",
      "episode: 3114   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 216.35\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6988: Policy loss: -0.546474. Value loss: 30.168955. Entropy: 0.524415.\n",
      "Iteration 6989: Policy loss: -0.525294. Value loss: 19.209623. Entropy: 0.520537.\n",
      "Iteration 6990: Policy loss: -0.495120. Value loss: 14.742452. Entropy: 0.513326.\n",
      "episode: 3115   score: 360.0  epsilon: 1.0    steps: 60  evaluation reward: 217.85\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6991: Policy loss: 1.010811. Value loss: 17.791994. Entropy: 0.525442.\n",
      "Iteration 6992: Policy loss: 0.960492. Value loss: 10.835935. Entropy: 0.519745.\n",
      "Iteration 6993: Policy loss: 0.991106. Value loss: 8.011643. Entropy: 0.515268.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6994: Policy loss: 1.749186. Value loss: 27.108330. Entropy: 0.563653.\n",
      "Iteration 6995: Policy loss: 1.882013. Value loss: 14.449092. Entropy: 0.582501.\n",
      "Iteration 6996: Policy loss: 1.908743. Value loss: 12.358704. Entropy: 0.566178.\n",
      "episode: 3116   score: 75.0  epsilon: 1.0    steps: 270  evaluation reward: 217.25\n",
      "episode: 3117   score: 260.0  epsilon: 1.0    steps: 496  evaluation reward: 217.25\n",
      "episode: 3118   score: 230.0  epsilon: 1.0    steps: 880  evaluation reward: 219.05\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6997: Policy loss: 0.957339. Value loss: 33.864971. Entropy: 0.431429.\n",
      "Iteration 6998: Policy loss: 0.882990. Value loss: 15.476130. Entropy: 0.449113.\n",
      "Iteration 6999: Policy loss: 0.784533. Value loss: 14.681686. Entropy: 0.431958.\n",
      "episode: 3119   score: 210.0  epsilon: 1.0    steps: 556  evaluation reward: 219.0\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 7000: Policy loss: -0.716849. Value loss: 27.308168. Entropy: 0.587362.\n",
      "Iteration 7001: Policy loss: -0.563259. Value loss: 18.208485. Entropy: 0.576148.\n",
      "Iteration 7002: Policy loss: -0.912594. Value loss: 15.694768. Entropy: 0.570736.\n",
      "episode: 3120   score: 285.0  epsilon: 1.0    steps: 133  evaluation reward: 219.45\n",
      "episode: 3121   score: 365.0  epsilon: 1.0    steps: 953  evaluation reward: 221.85\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7003: Policy loss: -0.539680. Value loss: 24.721493. Entropy: 0.531776.\n",
      "Iteration 7004: Policy loss: -0.485555. Value loss: 12.688179. Entropy: 0.537672.\n",
      "Iteration 7005: Policy loss: -0.602804. Value loss: 9.944741. Entropy: 0.566352.\n",
      "episode: 3122   score: 210.0  epsilon: 1.0    steps: 63  evaluation reward: 218.5\n",
      "episode: 3123   score: 270.0  epsilon: 1.0    steps: 733  evaluation reward: 219.95\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7006: Policy loss: 0.855161. Value loss: 26.732470. Entropy: 0.458304.\n",
      "Iteration 7007: Policy loss: 0.827504. Value loss: 15.464183. Entropy: 0.442773.\n",
      "Iteration 7008: Policy loss: 0.926331. Value loss: 14.129349. Entropy: 0.459336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7009: Policy loss: 1.053966. Value loss: 28.339737. Entropy: 0.505438.\n",
      "Iteration 7010: Policy loss: 1.255273. Value loss: 19.736921. Entropy: 0.485190.\n",
      "Iteration 7011: Policy loss: 1.075701. Value loss: 14.306350. Entropy: 0.501321.\n",
      "episode: 3124   score: 80.0  epsilon: 1.0    steps: 192  evaluation reward: 220.2\n",
      "episode: 3125   score: 215.0  epsilon: 1.0    steps: 319  evaluation reward: 220.25\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7012: Policy loss: -0.120111. Value loss: 25.602386. Entropy: 0.385186.\n",
      "Iteration 7013: Policy loss: -0.246532. Value loss: 12.783136. Entropy: 0.389006.\n",
      "Iteration 7014: Policy loss: -0.453015. Value loss: 10.086871. Entropy: 0.393587.\n",
      "episode: 3126   score: 260.0  epsilon: 1.0    steps: 460  evaluation reward: 220.6\n",
      "episode: 3127   score: 275.0  epsilon: 1.0    steps: 885  evaluation reward: 221.8\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7015: Policy loss: 1.448109. Value loss: 29.586535. Entropy: 0.480285.\n",
      "Iteration 7016: Policy loss: 1.602355. Value loss: 16.442692. Entropy: 0.519219.\n",
      "Iteration 7017: Policy loss: 1.424416. Value loss: 14.110924. Entropy: 0.505304.\n",
      "episode: 3128   score: 260.0  epsilon: 1.0    steps: 549  evaluation reward: 223.05\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7018: Policy loss: -0.267004. Value loss: 20.368202. Entropy: 0.524698.\n",
      "Iteration 7019: Policy loss: -0.181008. Value loss: 11.980187. Entropy: 0.539440.\n",
      "Iteration 7020: Policy loss: -0.313477. Value loss: 9.376728. Entropy: 0.525960.\n",
      "episode: 3129   score: 80.0  epsilon: 1.0    steps: 370  evaluation reward: 221.7\n",
      "episode: 3130   score: 275.0  epsilon: 1.0    steps: 962  evaluation reward: 222.3\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7021: Policy loss: 0.834527. Value loss: 32.358406. Entropy: 0.452286.\n",
      "Iteration 7022: Policy loss: 0.778133. Value loss: 18.693031. Entropy: 0.459307.\n",
      "Iteration 7023: Policy loss: 0.811072. Value loss: 14.528662. Entropy: 0.460373.\n",
      "episode: 3131   score: 225.0  epsilon: 1.0    steps: 6  evaluation reward: 222.45\n",
      "episode: 3132   score: 270.0  epsilon: 1.0    steps: 702  evaluation reward: 223.0\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7024: Policy loss: 0.024907. Value loss: 21.008055. Entropy: 0.568302.\n",
      "Iteration 7025: Policy loss: -0.214904. Value loss: 11.196264. Entropy: 0.548267.\n",
      "Iteration 7026: Policy loss: 0.172270. Value loss: 9.435872. Entropy: 0.554289.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7027: Policy loss: -0.799363. Value loss: 29.190748. Entropy: 0.461557.\n",
      "Iteration 7028: Policy loss: -0.656047. Value loss: 17.652369. Entropy: 0.453882.\n",
      "Iteration 7029: Policy loss: -0.745113. Value loss: 12.621857. Entropy: 0.449894.\n",
      "episode: 3133   score: 230.0  epsilon: 1.0    steps: 174  evaluation reward: 223.9\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7030: Policy loss: 0.158518. Value loss: 36.772320. Entropy: 0.385946.\n",
      "Iteration 7031: Policy loss: 0.302772. Value loss: 17.672865. Entropy: 0.401519.\n",
      "Iteration 7032: Policy loss: 0.119019. Value loss: 12.816222. Entropy: 0.384525.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7033: Policy loss: -3.886977. Value loss: 392.201660. Entropy: 0.355443.\n",
      "Iteration 7034: Policy loss: -4.836308. Value loss: 241.537888. Entropy: 0.411530.\n",
      "Iteration 7035: Policy loss: -4.822672. Value loss: 184.736771. Entropy: 0.373965.\n",
      "episode: 3134   score: 370.0  epsilon: 1.0    steps: 550  evaluation reward: 224.95\n",
      "episode: 3135   score: 425.0  epsilon: 1.0    steps: 846  evaluation reward: 226.8\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7036: Policy loss: 0.828573. Value loss: 37.491982. Entropy: 0.401581.\n",
      "Iteration 7037: Policy loss: 1.077545. Value loss: 17.774162. Entropy: 0.403044.\n",
      "Iteration 7038: Policy loss: 0.551245. Value loss: 14.658902. Entropy: 0.388753.\n",
      "episode: 3136   score: 315.0  epsilon: 1.0    steps: 413  evaluation reward: 227.35\n",
      "episode: 3137   score: 230.0  epsilon: 1.0    steps: 977  evaluation reward: 226.9\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7039: Policy loss: -0.900106. Value loss: 68.830009. Entropy: 0.411303.\n",
      "Iteration 7040: Policy loss: -0.660357. Value loss: 24.887911. Entropy: 0.378383.\n",
      "Iteration 7041: Policy loss: -0.195690. Value loss: 15.824187. Entropy: 0.380436.\n",
      "episode: 3138   score: 245.0  epsilon: 1.0    steps: 44  evaluation reward: 226.8\n",
      "episode: 3139   score: 360.0  epsilon: 1.0    steps: 296  evaluation reward: 229.05\n",
      "episode: 3140   score: 255.0  epsilon: 1.0    steps: 707  evaluation reward: 229.8\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7042: Policy loss: 1.307091. Value loss: 39.068008. Entropy: 0.360623.\n",
      "Iteration 7043: Policy loss: 1.806153. Value loss: 24.453945. Entropy: 0.394097.\n",
      "Iteration 7044: Policy loss: 1.649006. Value loss: 20.192976. Entropy: 0.416152.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7045: Policy loss: -0.116530. Value loss: 166.556778. Entropy: 0.515387.\n",
      "Iteration 7046: Policy loss: -1.333774. Value loss: 187.715134. Entropy: 0.527784.\n",
      "Iteration 7047: Policy loss: 0.327814. Value loss: 88.694031. Entropy: 0.570846.\n",
      "episode: 3141   score: 100.0  epsilon: 1.0    steps: 772  evaluation reward: 228.05\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7048: Policy loss: -3.293238. Value loss: 34.025822. Entropy: 0.400113.\n",
      "Iteration 7049: Policy loss: -3.478328. Value loss: 18.027615. Entropy: 0.360544.\n",
      "Iteration 7050: Policy loss: -3.457456. Value loss: 15.757822. Entropy: 0.375895.\n",
      "episode: 3142   score: 165.0  epsilon: 1.0    steps: 992  evaluation reward: 227.85\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7051: Policy loss: -2.008785. Value loss: 194.486115. Entropy: 0.319092.\n",
      "Iteration 7052: Policy loss: -1.972207. Value loss: 111.856781. Entropy: 0.309658.\n",
      "Iteration 7053: Policy loss: -2.121001. Value loss: 87.356255. Entropy: 0.307126.\n",
      "episode: 3143   score: 180.0  epsilon: 1.0    steps: 410  evaluation reward: 227.1\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7054: Policy loss: 3.104711. Value loss: 75.104652. Entropy: 0.398000.\n",
      "Iteration 7055: Policy loss: 3.301247. Value loss: 39.106548. Entropy: 0.399012.\n",
      "Iteration 7056: Policy loss: 3.548099. Value loss: 31.821524. Entropy: 0.427238.\n",
      "episode: 3144   score: 505.0  epsilon: 1.0    steps: 576  evaluation reward: 231.1\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7057: Policy loss: 2.373180. Value loss: 41.129101. Entropy: 0.378764.\n",
      "Iteration 7058: Policy loss: 2.592213. Value loss: 27.312002. Entropy: 0.392402.\n",
      "Iteration 7059: Policy loss: 2.566395. Value loss: 19.778631. Entropy: 0.394391.\n",
      "episode: 3145   score: 260.0  epsilon: 1.0    steps: 73  evaluation reward: 230.4\n",
      "episode: 3146   score: 240.0  epsilon: 1.0    steps: 296  evaluation reward: 230.05\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7060: Policy loss: -0.091755. Value loss: 39.708035. Entropy: 0.391325.\n",
      "Iteration 7061: Policy loss: -0.243617. Value loss: 22.702143. Entropy: 0.412355.\n",
      "Iteration 7062: Policy loss: 0.058530. Value loss: 17.763384. Entropy: 0.406776.\n",
      "episode: 3147   score: 695.0  epsilon: 1.0    steps: 178  evaluation reward: 234.4\n",
      "episode: 3148   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 234.55\n",
      "episode: 3149   score: 135.0  epsilon: 1.0    steps: 913  evaluation reward: 232.7\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7063: Policy loss: 0.422004. Value loss: 32.755226. Entropy: 0.354107.\n",
      "Iteration 7064: Policy loss: 0.711953. Value loss: 16.840649. Entropy: 0.398825.\n",
      "Iteration 7065: Policy loss: 0.551139. Value loss: 15.470746. Entropy: 0.413034.\n",
      "episode: 3150   score: 315.0  epsilon: 1.0    steps: 656  evaluation reward: 233.0\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7066: Policy loss: -1.788512. Value loss: 23.728603. Entropy: 0.428929.\n",
      "Iteration 7067: Policy loss: -1.863800. Value loss: 13.189658. Entropy: 0.438136.\n",
      "Iteration 7068: Policy loss: -1.915523. Value loss: 11.260312. Entropy: 0.459321.\n",
      "now time :  2019-02-25 20:52:10.029406\n",
      "episode: 3151   score: 120.0  epsilon: 1.0    steps: 58  evaluation reward: 232.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7069: Policy loss: 2.062272. Value loss: 35.264587. Entropy: 0.324681.\n",
      "Iteration 7070: Policy loss: 1.838333. Value loss: 16.845415. Entropy: 0.374040.\n",
      "Iteration 7071: Policy loss: 1.792724. Value loss: 15.647941. Entropy: 0.378385.\n",
      "episode: 3152   score: 215.0  epsilon: 1.0    steps: 582  evaluation reward: 232.3\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7072: Policy loss: 0.085856. Value loss: 33.580696. Entropy: 0.336491.\n",
      "Iteration 7073: Policy loss: 0.226360. Value loss: 14.731666. Entropy: 0.339967.\n",
      "Iteration 7074: Policy loss: 0.129568. Value loss: 12.368567. Entropy: 0.338155.\n",
      "episode: 3153   score: 240.0  epsilon: 1.0    steps: 365  evaluation reward: 233.95\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7075: Policy loss: -1.925846. Value loss: 37.213593. Entropy: 0.337988.\n",
      "Iteration 7076: Policy loss: -1.625578. Value loss: 23.570738. Entropy: 0.315134.\n",
      "Iteration 7077: Policy loss: -1.977047. Value loss: 20.741640. Entropy: 0.339344.\n",
      "episode: 3154   score: 315.0  epsilon: 1.0    steps: 415  evaluation reward: 235.75\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7078: Policy loss: 0.861321. Value loss: 34.962952. Entropy: 0.294538.\n",
      "Iteration 7079: Policy loss: 1.079319. Value loss: 18.985531. Entropy: 0.294173.\n",
      "Iteration 7080: Policy loss: 0.975904. Value loss: 15.550169. Entropy: 0.300616.\n",
      "episode: 3155   score: 35.0  epsilon: 1.0    steps: 30  evaluation reward: 232.7\n",
      "episode: 3156   score: 260.0  epsilon: 1.0    steps: 764  evaluation reward: 232.4\n",
      "episode: 3157   score: 260.0  epsilon: 1.0    steps: 848  evaluation reward: 232.3\n",
      "episode: 3158   score: 385.0  epsilon: 1.0    steps: 1004  evaluation reward: 233.75\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7081: Policy loss: -0.381849. Value loss: 36.620338. Entropy: 0.428821.\n",
      "Iteration 7082: Policy loss: -0.082579. Value loss: 23.067335. Entropy: 0.430206.\n",
      "Iteration 7083: Policy loss: -0.552069. Value loss: 17.459450. Entropy: 0.400405.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7084: Policy loss: 0.080632. Value loss: 29.699690. Entropy: 0.325780.\n",
      "Iteration 7085: Policy loss: 0.250590. Value loss: 15.874944. Entropy: 0.356163.\n",
      "Iteration 7086: Policy loss: 0.383376. Value loss: 12.969803. Entropy: 0.378766.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7087: Policy loss: -0.662881. Value loss: 245.437576. Entropy: 0.456179.\n",
      "Iteration 7088: Policy loss: -1.381075. Value loss: 115.815788. Entropy: 0.379160.\n",
      "Iteration 7089: Policy loss: -0.736181. Value loss: 67.174911. Entropy: 0.428102.\n",
      "episode: 3159   score: 240.0  epsilon: 1.0    steps: 527  evaluation reward: 234.6\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7090: Policy loss: -0.979423. Value loss: 38.594124. Entropy: 0.321443.\n",
      "Iteration 7091: Policy loss: -1.113610. Value loss: 20.629005. Entropy: 0.336079.\n",
      "Iteration 7092: Policy loss: -0.973751. Value loss: 15.730364. Entropy: 0.347908.\n",
      "episode: 3160   score: 710.0  epsilon: 1.0    steps: 207  evaluation reward: 239.9\n",
      "episode: 3161   score: 285.0  epsilon: 1.0    steps: 276  evaluation reward: 240.15\n",
      "episode: 3162   score: 215.0  epsilon: 1.0    steps: 422  evaluation reward: 238.7\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7093: Policy loss: -0.479655. Value loss: 26.942511. Entropy: 0.246176.\n",
      "Iteration 7094: Policy loss: -0.671121. Value loss: 17.330185. Entropy: 0.230045.\n",
      "Iteration 7095: Policy loss: -0.722035. Value loss: 14.711517. Entropy: 0.227545.\n",
      "episode: 3163   score: 195.0  epsilon: 1.0    steps: 763  evaluation reward: 237.2\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7096: Policy loss: -0.034467. Value loss: 24.905087. Entropy: 0.294442.\n",
      "Iteration 7097: Policy loss: 0.119517. Value loss: 15.383357. Entropy: 0.303844.\n",
      "Iteration 7098: Policy loss: 0.076990. Value loss: 12.819369. Entropy: 0.320297.\n",
      "episode: 3164   score: 490.0  epsilon: 1.0    steps: 81  evaluation reward: 240.6\n",
      "episode: 3165   score: 260.0  epsilon: 1.0    steps: 817  evaluation reward: 240.05\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7099: Policy loss: -1.110684. Value loss: 219.138489. Entropy: 0.283221.\n",
      "Iteration 7100: Policy loss: -1.118791. Value loss: 166.104813. Entropy: 0.307744.\n",
      "Iteration 7101: Policy loss: -0.747802. Value loss: 90.771439. Entropy: 0.335897.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7102: Policy loss: 1.701175. Value loss: 35.695942. Entropy: 0.238929.\n",
      "Iteration 7103: Policy loss: 1.531668. Value loss: 19.891722. Entropy: 0.232068.\n",
      "Iteration 7104: Policy loss: 1.531399. Value loss: 15.052116. Entropy: 0.230910.\n",
      "episode: 3166   score: 210.0  epsilon: 1.0    steps: 639  evaluation reward: 239.55\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7105: Policy loss: 0.177836. Value loss: 42.062050. Entropy: 0.221161.\n",
      "Iteration 7106: Policy loss: -0.009630. Value loss: 23.808247. Entropy: 0.225098.\n",
      "Iteration 7107: Policy loss: 0.064008. Value loss: 21.426168. Entropy: 0.228760.\n",
      "episode: 3167   score: 405.0  epsilon: 1.0    steps: 967  evaluation reward: 241.0\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7108: Policy loss: 0.023968. Value loss: 57.588833. Entropy: 0.329574.\n",
      "Iteration 7109: Policy loss: -0.683358. Value loss: 28.105988. Entropy: 0.312338.\n",
      "Iteration 7110: Policy loss: -0.054711. Value loss: 22.480328. Entropy: 0.317265.\n",
      "episode: 3168   score: 215.0  epsilon: 1.0    steps: 176  evaluation reward: 240.55\n",
      "episode: 3169   score: 285.0  epsilon: 1.0    steps: 284  evaluation reward: 241.85\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7111: Policy loss: -1.470863. Value loss: 44.943672. Entropy: 0.219726.\n",
      "Iteration 7112: Policy loss: -1.355672. Value loss: 22.597597. Entropy: 0.201592.\n",
      "Iteration 7113: Policy loss: -1.391585. Value loss: 18.972979. Entropy: 0.200678.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7114: Policy loss: -0.732778. Value loss: 51.770924. Entropy: 0.094907.\n",
      "Iteration 7115: Policy loss: -0.268170. Value loss: 28.889761. Entropy: 0.095744.\n",
      "Iteration 7116: Policy loss: -1.059457. Value loss: 18.945538. Entropy: 0.095140.\n",
      "episode: 3170   score: 240.0  epsilon: 1.0    steps: 648  evaluation reward: 241.35\n",
      "episode: 3171   score: 275.0  epsilon: 1.0    steps: 829  evaluation reward: 243.35\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7117: Policy loss: 2.234640. Value loss: 30.875267. Entropy: 0.158364.\n",
      "Iteration 7118: Policy loss: 2.069155. Value loss: 15.557741. Entropy: 0.178011.\n",
      "Iteration 7119: Policy loss: 2.390294. Value loss: 14.256762. Entropy: 0.189082.\n",
      "episode: 3172   score: 365.0  epsilon: 1.0    steps: 428  evaluation reward: 244.9\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7120: Policy loss: 2.275007. Value loss: 44.835922. Entropy: 0.239285.\n",
      "Iteration 7121: Policy loss: 2.245841. Value loss: 23.530386. Entropy: 0.256447.\n",
      "Iteration 7122: Policy loss: 2.632516. Value loss: 17.023508. Entropy: 0.261313.\n",
      "episode: 3173   score: 400.0  epsilon: 1.0    steps: 52  evaluation reward: 247.85\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7123: Policy loss: 1.310465. Value loss: 30.160389. Entropy: 0.391423.\n",
      "Iteration 7124: Policy loss: 1.508166. Value loss: 19.949060. Entropy: 0.420775.\n",
      "Iteration 7125: Policy loss: 1.388156. Value loss: 15.607845. Entropy: 0.454266.\n",
      "episode: 3174   score: 225.0  epsilon: 1.0    steps: 286  evaluation reward: 247.7\n",
      "episode: 3175   score: 320.0  epsilon: 1.0    steps: 571  evaluation reward: 249.95\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7126: Policy loss: 2.616109. Value loss: 21.775150. Entropy: 0.287027.\n",
      "Iteration 7127: Policy loss: 2.451141. Value loss: 13.085940. Entropy: 0.300862.\n",
      "Iteration 7128: Policy loss: 2.539917. Value loss: 11.530126. Entropy: 0.296741.\n",
      "episode: 3176   score: 285.0  epsilon: 1.0    steps: 163  evaluation reward: 247.5\n",
      "episode: 3177   score: 105.0  epsilon: 1.0    steps: 480  evaluation reward: 247.3\n",
      "episode: 3178   score: 210.0  epsilon: 1.0    steps: 753  evaluation reward: 248.2\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7129: Policy loss: 0.476448. Value loss: 36.650230. Entropy: 0.258054.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7130: Policy loss: 0.758296. Value loss: 16.579914. Entropy: 0.274508.\n",
      "Iteration 7131: Policy loss: 0.386564. Value loss: 13.758310. Entropy: 0.246743.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7132: Policy loss: 0.819066. Value loss: 26.848883. Entropy: 0.309898.\n",
      "Iteration 7133: Policy loss: 0.854260. Value loss: 18.268108. Entropy: 0.330561.\n",
      "Iteration 7134: Policy loss: 0.681075. Value loss: 12.618000. Entropy: 0.302800.\n",
      "episode: 3179   score: 260.0  epsilon: 1.0    steps: 799  evaluation reward: 249.0\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7135: Policy loss: 1.459106. Value loss: 32.530529. Entropy: 0.328775.\n",
      "Iteration 7136: Policy loss: 1.663697. Value loss: 22.821297. Entropy: 0.340160.\n",
      "Iteration 7137: Policy loss: 1.414118. Value loss: 17.350342. Entropy: 0.337689.\n",
      "episode: 3180   score: 225.0  epsilon: 1.0    steps: 98  evaluation reward: 248.9\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7138: Policy loss: 0.040178. Value loss: 35.371147. Entropy: 0.340285.\n",
      "Iteration 7139: Policy loss: -0.054424. Value loss: 19.620043. Entropy: 0.311893.\n",
      "Iteration 7140: Policy loss: -0.222708. Value loss: 17.202555. Entropy: 0.332363.\n",
      "episode: 3181   score: 210.0  epsilon: 1.0    steps: 250  evaluation reward: 248.75\n",
      "episode: 3182   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 247.8\n",
      "episode: 3183   score: 135.0  epsilon: 1.0    steps: 646  evaluation reward: 248.55\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7141: Policy loss: 0.626382. Value loss: 44.409100. Entropy: 0.297513.\n",
      "Iteration 7142: Policy loss: 0.665195. Value loss: 20.877897. Entropy: 0.291977.\n",
      "Iteration 7143: Policy loss: 0.512120. Value loss: 16.747459. Entropy: 0.307132.\n",
      "episode: 3184   score: 210.0  epsilon: 1.0    steps: 447  evaluation reward: 248.05\n",
      "episode: 3185   score: 495.0  epsilon: 1.0    steps: 934  evaluation reward: 250.6\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7144: Policy loss: -0.232123. Value loss: 20.512445. Entropy: 0.434930.\n",
      "Iteration 7145: Policy loss: -0.177612. Value loss: 12.946455. Entropy: 0.438126.\n",
      "Iteration 7146: Policy loss: 0.070268. Value loss: 12.396427. Entropy: 0.438081.\n",
      "episode: 3186   score: 260.0  epsilon: 1.0    steps: 531  evaluation reward: 251.95\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7147: Policy loss: -0.360301. Value loss: 26.256973. Entropy: 0.378327.\n",
      "Iteration 7148: Policy loss: -0.615025. Value loss: 19.767397. Entropy: 0.383595.\n",
      "Iteration 7149: Policy loss: -0.413093. Value loss: 14.987650. Entropy: 0.385035.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7150: Policy loss: 0.912132. Value loss: 27.646921. Entropy: 0.264446.\n",
      "Iteration 7151: Policy loss: 1.028031. Value loss: 17.599728. Entropy: 0.317904.\n",
      "Iteration 7152: Policy loss: 0.901919. Value loss: 13.481317. Entropy: 0.313407.\n",
      "episode: 3187   score: 240.0  epsilon: 1.0    steps: 786  evaluation reward: 253.1\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7153: Policy loss: -0.912140. Value loss: 35.677193. Entropy: 0.201294.\n",
      "Iteration 7154: Policy loss: -0.889894. Value loss: 20.005106. Entropy: 0.210645.\n",
      "Iteration 7155: Policy loss: -0.996250. Value loss: 17.858610. Entropy: 0.198675.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7156: Policy loss: -0.885186. Value loss: 212.289948. Entropy: 0.405229.\n",
      "Iteration 7157: Policy loss: -0.963195. Value loss: 101.195030. Entropy: 0.419203.\n",
      "Iteration 7158: Policy loss: -1.085734. Value loss: 102.973854. Entropy: 0.446996.\n",
      "episode: 3188   score: 275.0  epsilon: 1.0    steps: 30  evaluation reward: 253.0\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7159: Policy loss: -0.116072. Value loss: 23.217171. Entropy: 0.454469.\n",
      "Iteration 7160: Policy loss: 0.324959. Value loss: 17.142282. Entropy: 0.439015.\n",
      "Iteration 7161: Policy loss: 0.168552. Value loss: 13.139723. Entropy: 0.505131.\n",
      "episode: 3189   score: 260.0  epsilon: 1.0    steps: 417  evaluation reward: 254.55\n",
      "episode: 3190   score: 105.0  epsilon: 1.0    steps: 795  evaluation reward: 251.05\n",
      "episode: 3191   score: 285.0  epsilon: 1.0    steps: 897  evaluation reward: 251.3\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7162: Policy loss: -0.894011. Value loss: 37.186550. Entropy: 0.544923.\n",
      "Iteration 7163: Policy loss: -0.877699. Value loss: 19.766672. Entropy: 0.566129.\n",
      "Iteration 7164: Policy loss: -1.081134. Value loss: 16.391317. Entropy: 0.553573.\n",
      "episode: 3192   score: 380.0  epsilon: 1.0    steps: 139  evaluation reward: 252.2\n",
      "episode: 3193   score: 345.0  epsilon: 1.0    steps: 321  evaluation reward: 254.3\n",
      "episode: 3194   score: 285.0  epsilon: 1.0    steps: 725  evaluation reward: 256.5\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7165: Policy loss: 2.219322. Value loss: 26.618141. Entropy: 0.577002.\n",
      "Iteration 7166: Policy loss: 2.446260. Value loss: 17.550833. Entropy: 0.528381.\n",
      "Iteration 7167: Policy loss: 2.523227. Value loss: 15.014308. Entropy: 0.526242.\n",
      "episode: 3195   score: 540.0  epsilon: 1.0    steps: 632  evaluation reward: 259.3\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7168: Policy loss: 1.403235. Value loss: 35.395779. Entropy: 0.578807.\n",
      "Iteration 7169: Policy loss: 1.363997. Value loss: 17.802023. Entropy: 0.565671.\n",
      "Iteration 7170: Policy loss: 1.374352. Value loss: 16.011646. Entropy: 0.558233.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7171: Policy loss: 1.369127. Value loss: 34.586540. Entropy: 0.317278.\n",
      "Iteration 7172: Policy loss: 1.529678. Value loss: 17.799561. Entropy: 0.291176.\n",
      "Iteration 7173: Policy loss: 1.307435. Value loss: 13.652160. Entropy: 0.289229.\n",
      "episode: 3196   score: 190.0  epsilon: 1.0    steps: 1007  evaluation reward: 259.1\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7174: Policy loss: 2.553610. Value loss: 30.787737. Entropy: 0.232055.\n",
      "Iteration 7175: Policy loss: 2.248094. Value loss: 19.217653. Entropy: 0.207602.\n",
      "Iteration 7176: Policy loss: 2.483380. Value loss: 16.946602. Entropy: 0.219686.\n",
      "episode: 3197   score: 310.0  epsilon: 1.0    steps: 43  evaluation reward: 259.95\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7177: Policy loss: 2.341998. Value loss: 30.293839. Entropy: 0.243985.\n",
      "Iteration 7178: Policy loss: 2.355085. Value loss: 18.187735. Entropy: 0.221422.\n",
      "Iteration 7179: Policy loss: 2.070085. Value loss: 14.108044. Entropy: 0.239858.\n",
      "episode: 3198   score: 180.0  epsilon: 1.0    steps: 669  evaluation reward: 259.4\n",
      "episode: 3199   score: 260.0  epsilon: 1.0    steps: 772  evaluation reward: 259.9\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7180: Policy loss: 3.276853. Value loss: 35.152653. Entropy: 0.518251.\n",
      "Iteration 7181: Policy loss: 3.177066. Value loss: 17.161650. Entropy: 0.532382.\n",
      "Iteration 7182: Policy loss: 3.329360. Value loss: 19.247889. Entropy: 0.561907.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7183: Policy loss: -0.416649. Value loss: 30.904268. Entropy: 0.621214.\n",
      "Iteration 7184: Policy loss: -0.244203. Value loss: 17.007130. Entropy: 0.633162.\n",
      "Iteration 7185: Policy loss: -0.363045. Value loss: 14.988638. Entropy: 0.639462.\n",
      "episode: 3200   score: 365.0  epsilon: 1.0    steps: 208  evaluation reward: 261.15\n",
      "now time :  2019-02-25 20:54:21.762480\n",
      "episode: 3201   score: 255.0  epsilon: 1.0    steps: 277  evaluation reward: 259.95\n",
      "episode: 3202   score: 260.0  epsilon: 1.0    steps: 606  evaluation reward: 259.95\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7186: Policy loss: 0.923185. Value loss: 16.910917. Entropy: 0.247679.\n",
      "Iteration 7187: Policy loss: 0.860188. Value loss: 9.735008. Entropy: 0.245355.\n",
      "Iteration 7188: Policy loss: 0.882132. Value loss: 8.597839. Entropy: 0.263408.\n",
      "episode: 3203   score: 155.0  epsilon: 1.0    steps: 940  evaluation reward: 261.0\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7189: Policy loss: -1.138805. Value loss: 34.747952. Entropy: 0.329485.\n",
      "Iteration 7190: Policy loss: -0.886986. Value loss: 17.918510. Entropy: 0.366815.\n",
      "Iteration 7191: Policy loss: -1.008318. Value loss: 13.033777. Entropy: 0.364807.\n",
      "episode: 3204   score: 165.0  epsilon: 1.0    steps: 73  evaluation reward: 259.0\n",
      "episode: 3205   score: 490.0  epsilon: 1.0    steps: 438  evaluation reward: 261.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7192: Policy loss: 2.419033. Value loss: 31.792269. Entropy: 0.501449.\n",
      "Iteration 7193: Policy loss: 2.623880. Value loss: 18.068054. Entropy: 0.490874.\n",
      "Iteration 7194: Policy loss: 2.469742. Value loss: 14.933040. Entropy: 0.496331.\n",
      "episode: 3206   score: 240.0  epsilon: 1.0    steps: 858  evaluation reward: 263.15\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7195: Policy loss: 1.499198. Value loss: 28.078077. Entropy: 0.401900.\n",
      "Iteration 7196: Policy loss: 1.561404. Value loss: 13.437133. Entropy: 0.423787.\n",
      "Iteration 7197: Policy loss: 1.466037. Value loss: 12.797100. Entropy: 0.428548.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7198: Policy loss: 3.098836. Value loss: 23.256384. Entropy: 0.315042.\n",
      "Iteration 7199: Policy loss: 3.041380. Value loss: 14.829965. Entropy: 0.308659.\n",
      "Iteration 7200: Policy loss: 3.059209. Value loss: 13.694926. Entropy: 0.380083.\n",
      "episode: 3207   score: 180.0  epsilon: 1.0    steps: 231  evaluation reward: 262.85\n",
      "episode: 3208   score: 180.0  epsilon: 1.0    steps: 1011  evaluation reward: 263.55\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7201: Policy loss: 1.431788. Value loss: 36.854485. Entropy: 0.411705.\n",
      "Iteration 7202: Policy loss: 1.497091. Value loss: 20.652403. Entropy: 0.408872.\n",
      "Iteration 7203: Policy loss: 1.552546. Value loss: 15.831980. Entropy: 0.406730.\n",
      "episode: 3209   score: 110.0  epsilon: 1.0    steps: 97  evaluation reward: 262.25\n",
      "episode: 3210   score: 260.0  epsilon: 1.0    steps: 299  evaluation reward: 263.3\n",
      "episode: 3211   score: 180.0  epsilon: 1.0    steps: 760  evaluation reward: 263.3\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7204: Policy loss: 1.605875. Value loss: 26.207256. Entropy: 0.576035.\n",
      "Iteration 7205: Policy loss: 1.570689. Value loss: 11.695548. Entropy: 0.592362.\n",
      "Iteration 7206: Policy loss: 1.517233. Value loss: 9.263098. Entropy: 0.599396.\n",
      "episode: 3212   score: 335.0  epsilon: 1.0    steps: 567  evaluation reward: 264.85\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7207: Policy loss: 1.538008. Value loss: 20.972563. Entropy: 0.790812.\n",
      "Iteration 7208: Policy loss: 1.446246. Value loss: 13.727901. Entropy: 0.797358.\n",
      "Iteration 7209: Policy loss: 1.869078. Value loss: 8.775807. Entropy: 0.802771.\n",
      "episode: 3213   score: 270.0  epsilon: 1.0    steps: 505  evaluation reward: 266.2\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7210: Policy loss: -3.430316. Value loss: 34.723221. Entropy: 0.569809.\n",
      "Iteration 7211: Policy loss: -3.553887. Value loss: 19.341537. Entropy: 0.583085.\n",
      "Iteration 7212: Policy loss: -3.508884. Value loss: 14.228348. Entropy: 0.548728.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7213: Policy loss: 0.042524. Value loss: 27.230507. Entropy: 0.430009.\n",
      "Iteration 7214: Policy loss: -0.016271. Value loss: 16.345161. Entropy: 0.447962.\n",
      "Iteration 7215: Policy loss: 0.096335. Value loss: 14.506577. Entropy: 0.428613.\n",
      "episode: 3214   score: 210.0  epsilon: 1.0    steps: 977  evaluation reward: 265.7\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7216: Policy loss: -0.450438. Value loss: 27.016592. Entropy: 0.473091.\n",
      "Iteration 7217: Policy loss: -0.460961. Value loss: 16.556990. Entropy: 0.448249.\n",
      "Iteration 7218: Policy loss: -0.672879. Value loss: 12.616721. Entropy: 0.452826.\n",
      "episode: 3215   score: 285.0  epsilon: 1.0    steps: 218  evaluation reward: 264.95\n",
      "episode: 3216   score: 315.0  epsilon: 1.0    steps: 860  evaluation reward: 267.35\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7219: Policy loss: 0.118849. Value loss: 28.109249. Entropy: 0.588765.\n",
      "Iteration 7220: Policy loss: 0.434699. Value loss: 18.553720. Entropy: 0.616761.\n",
      "Iteration 7221: Policy loss: 0.136752. Value loss: 13.184579. Entropy: 0.613997.\n",
      "episode: 3217   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 267.35\n",
      "episode: 3218   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 266.85\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7222: Policy loss: 0.182399. Value loss: 22.092216. Entropy: 0.603446.\n",
      "Iteration 7223: Policy loss: 0.112596. Value loss: 14.043832. Entropy: 0.649773.\n",
      "Iteration 7224: Policy loss: -0.065936. Value loss: 10.709260. Entropy: 0.644824.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7225: Policy loss: 0.166106. Value loss: 23.982050. Entropy: 0.690914.\n",
      "Iteration 7226: Policy loss: 0.342685. Value loss: 14.874453. Entropy: 0.725702.\n",
      "Iteration 7227: Policy loss: 0.326259. Value loss: 11.627052. Entropy: 0.705646.\n",
      "episode: 3219   score: 285.0  epsilon: 1.0    steps: 731  evaluation reward: 267.6\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7228: Policy loss: -0.583111. Value loss: 33.745617. Entropy: 0.613929.\n",
      "Iteration 7229: Policy loss: -0.466276. Value loss: 15.782929. Entropy: 0.629960.\n",
      "Iteration 7230: Policy loss: -0.768778. Value loss: 12.842120. Entropy: 0.662294.\n",
      "episode: 3220   score: 90.0  epsilon: 1.0    steps: 179  evaluation reward: 265.65\n",
      "episode: 3221   score: 245.0  epsilon: 1.0    steps: 462  evaluation reward: 264.45\n",
      "episode: 3222   score: 90.0  epsilon: 1.0    steps: 784  evaluation reward: 263.25\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7231: Policy loss: 0.976931. Value loss: 35.656822. Entropy: 0.515748.\n",
      "Iteration 7232: Policy loss: 0.847616. Value loss: 19.906527. Entropy: 0.532209.\n",
      "Iteration 7233: Policy loss: 0.632246. Value loss: 17.536825. Entropy: 0.517278.\n",
      "episode: 3223   score: 285.0  epsilon: 1.0    steps: 970  evaluation reward: 263.4\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7234: Policy loss: -0.340800. Value loss: 22.255323. Entropy: 0.699367.\n",
      "Iteration 7235: Policy loss: -0.070899. Value loss: 12.232585. Entropy: 0.699944.\n",
      "Iteration 7236: Policy loss: -0.311811. Value loss: 10.552534. Entropy: 0.731044.\n",
      "episode: 3224   score: 495.0  epsilon: 1.0    steps: 4  evaluation reward: 267.55\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7237: Policy loss: 1.060431. Value loss: 22.938065. Entropy: 0.631562.\n",
      "Iteration 7238: Policy loss: 1.206255. Value loss: 14.733398. Entropy: 0.617653.\n",
      "Iteration 7239: Policy loss: 0.936811. Value loss: 10.407759. Entropy: 0.640660.\n",
      "episode: 3225   score: 45.0  epsilon: 1.0    steps: 875  evaluation reward: 265.85\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7240: Policy loss: -0.086105. Value loss: 39.362217. Entropy: 0.524253.\n",
      "Iteration 7241: Policy loss: 0.012804. Value loss: 19.701654. Entropy: 0.540123.\n",
      "Iteration 7242: Policy loss: -0.070732. Value loss: 19.283232. Entropy: 0.516877.\n",
      "episode: 3226   score: 370.0  epsilon: 1.0    steps: 323  evaluation reward: 266.95\n",
      "episode: 3227   score: 290.0  epsilon: 1.0    steps: 525  evaluation reward: 267.1\n",
      "episode: 3228   score: 210.0  epsilon: 1.0    steps: 746  evaluation reward: 266.6\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7243: Policy loss: 0.839496. Value loss: 14.959448. Entropy: 0.742892.\n",
      "Iteration 7244: Policy loss: 0.909922. Value loss: 11.118793. Entropy: 0.744516.\n",
      "Iteration 7245: Policy loss: 0.951171. Value loss: 9.295544. Entropy: 0.744692.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7246: Policy loss: -0.435963. Value loss: 23.237391. Entropy: 0.763750.\n",
      "Iteration 7247: Policy loss: -0.578504. Value loss: 12.475760. Entropy: 0.743868.\n",
      "Iteration 7248: Policy loss: -0.381896. Value loss: 9.523700. Entropy: 0.756006.\n",
      "episode: 3229   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 267.9\n",
      "episode: 3230   score: 240.0  epsilon: 1.0    steps: 164  evaluation reward: 267.55\n",
      "episode: 3231   score: 240.0  epsilon: 1.0    steps: 420  evaluation reward: 267.7\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7249: Policy loss: -0.085073. Value loss: 13.838102. Entropy: 0.550340.\n",
      "Iteration 7250: Policy loss: -0.096353. Value loss: 10.349220. Entropy: 0.552888.\n",
      "Iteration 7251: Policy loss: 0.043796. Value loss: 6.410823. Entropy: 0.563372.\n",
      "episode: 3232   score: 260.0  epsilon: 1.0    steps: 916  evaluation reward: 267.6\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7252: Policy loss: -1.196849. Value loss: 31.005295. Entropy: 0.769922.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7253: Policy loss: -0.665847. Value loss: 14.142336. Entropy: 0.754052.\n",
      "Iteration 7254: Policy loss: -1.119935. Value loss: 14.371737. Entropy: 0.782355.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7255: Policy loss: 2.335570. Value loss: 26.938019. Entropy: 0.753332.\n",
      "Iteration 7256: Policy loss: 2.487636. Value loss: 18.761889. Entropy: 0.748217.\n",
      "Iteration 7257: Policy loss: 2.287062. Value loss: 14.593274. Entropy: 0.745838.\n",
      "episode: 3233   score: 105.0  epsilon: 1.0    steps: 122  evaluation reward: 266.35\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7258: Policy loss: 0.054890. Value loss: 16.722067. Entropy: 0.587322.\n",
      "Iteration 7259: Policy loss: -0.015175. Value loss: 8.469484. Entropy: 0.590602.\n",
      "Iteration 7260: Policy loss: 0.086465. Value loss: 7.543274. Entropy: 0.524240.\n",
      "episode: 3234   score: 210.0  epsilon: 1.0    steps: 258  evaluation reward: 264.75\n",
      "episode: 3235   score: 285.0  epsilon: 1.0    steps: 725  evaluation reward: 263.35\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7261: Policy loss: -1.353651. Value loss: 23.642586. Entropy: 0.611118.\n",
      "Iteration 7262: Policy loss: -1.482021. Value loss: 12.069342. Entropy: 0.615431.\n",
      "Iteration 7263: Policy loss: -1.428380. Value loss: 10.895869. Entropy: 0.616644.\n",
      "episode: 3236   score: 240.0  epsilon: 1.0    steps: 510  evaluation reward: 262.6\n",
      "episode: 3237   score: 240.0  epsilon: 1.0    steps: 778  evaluation reward: 262.7\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7264: Policy loss: -2.441968. Value loss: 272.216248. Entropy: 0.737723.\n",
      "Iteration 7265: Policy loss: -2.079384. Value loss: 94.276062. Entropy: 0.700848.\n",
      "Iteration 7266: Policy loss: -2.487536. Value loss: 93.822327. Entropy: 0.662225.\n",
      "episode: 3238   score: 260.0  epsilon: 1.0    steps: 161  evaluation reward: 262.85\n",
      "episode: 3239   score: 625.0  epsilon: 1.0    steps: 556  evaluation reward: 265.5\n",
      "episode: 3240   score: 210.0  epsilon: 1.0    steps: 927  evaluation reward: 265.05\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7267: Policy loss: 0.409509. Value loss: 14.388247. Entropy: 0.739222.\n",
      "Iteration 7268: Policy loss: 0.261335. Value loss: 10.217319. Entropy: 0.731663.\n",
      "Iteration 7269: Policy loss: 0.284234. Value loss: 8.523478. Entropy: 0.732828.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7270: Policy loss: 1.125494. Value loss: 21.580950. Entropy: 0.632500.\n",
      "Iteration 7271: Policy loss: 1.212532. Value loss: 16.711172. Entropy: 0.623092.\n",
      "Iteration 7272: Policy loss: 1.303567. Value loss: 13.006783. Entropy: 0.654429.\n",
      "episode: 3241   score: 210.0  epsilon: 1.0    steps: 108  evaluation reward: 266.15\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7273: Policy loss: 0.006024. Value loss: 29.141123. Entropy: 0.516013.\n",
      "Iteration 7274: Policy loss: 0.337276. Value loss: 18.761606. Entropy: 0.516736.\n",
      "Iteration 7275: Policy loss: 0.143800. Value loss: 15.473783. Entropy: 0.522340.\n",
      "episode: 3242   score: 180.0  epsilon: 1.0    steps: 301  evaluation reward: 266.3\n",
      "episode: 3243   score: 180.0  epsilon: 1.0    steps: 730  evaluation reward: 266.3\n",
      "episode: 3244   score: 210.0  epsilon: 1.0    steps: 849  evaluation reward: 263.35\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7276: Policy loss: 2.067497. Value loss: 16.834396. Entropy: 0.465374.\n",
      "Iteration 7277: Policy loss: 2.067206. Value loss: 8.415804. Entropy: 0.487006.\n",
      "Iteration 7278: Policy loss: 2.060806. Value loss: 7.650004. Entropy: 0.516569.\n",
      "episode: 3245   score: 155.0  epsilon: 1.0    steps: 256  evaluation reward: 262.3\n",
      "episode: 3246   score: 180.0  epsilon: 1.0    steps: 1018  evaluation reward: 261.7\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7279: Policy loss: 2.086020. Value loss: 28.275620. Entropy: 0.570850.\n",
      "Iteration 7280: Policy loss: 1.961009. Value loss: 20.996811. Entropy: 0.527272.\n",
      "Iteration 7281: Policy loss: 2.132397. Value loss: 16.134647. Entropy: 0.531133.\n",
      "episode: 3247   score: 210.0  epsilon: 1.0    steps: 391  evaluation reward: 256.85\n",
      "episode: 3248   score: 210.0  epsilon: 1.0    steps: 529  evaluation reward: 256.35\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7282: Policy loss: 1.062021. Value loss: 9.582224. Entropy: 0.669108.\n",
      "Iteration 7283: Policy loss: 0.972260. Value loss: 6.483090. Entropy: 0.670217.\n",
      "Iteration 7284: Policy loss: 0.993382. Value loss: 8.066889. Entropy: 0.667260.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7285: Policy loss: -4.404698. Value loss: 295.699585. Entropy: 0.623640.\n",
      "Iteration 7286: Policy loss: -4.925067. Value loss: 157.612656. Entropy: 0.561641.\n",
      "Iteration 7287: Policy loss: -4.236749. Value loss: 115.078255. Entropy: 0.561716.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7288: Policy loss: 2.007334. Value loss: 28.627399. Entropy: 0.348625.\n",
      "Iteration 7289: Policy loss: 1.869233. Value loss: 17.274834. Entropy: 0.358314.\n",
      "Iteration 7290: Policy loss: 1.872980. Value loss: 13.265549. Entropy: 0.378592.\n",
      "episode: 3249   score: 210.0  epsilon: 1.0    steps: 3  evaluation reward: 257.1\n",
      "episode: 3250   score: 180.0  epsilon: 1.0    steps: 349  evaluation reward: 255.75\n",
      "now time :  2019-02-25 20:56:20.271715\n",
      "episode: 3251   score: 155.0  epsilon: 1.0    steps: 694  evaluation reward: 256.1\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7291: Policy loss: 0.792325. Value loss: 29.065916. Entropy: 0.413325.\n",
      "Iteration 7292: Policy loss: 0.593218. Value loss: 14.543147. Entropy: 0.404757.\n",
      "Iteration 7293: Policy loss: 0.684820. Value loss: 14.408302. Entropy: 0.407016.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7294: Policy loss: 1.088751. Value loss: 30.992487. Entropy: 0.647713.\n",
      "Iteration 7295: Policy loss: 1.023907. Value loss: 23.344244. Entropy: 0.650742.\n",
      "Iteration 7296: Policy loss: 1.222632. Value loss: 15.806467. Entropy: 0.639015.\n",
      "episode: 3252   score: 210.0  epsilon: 1.0    steps: 405  evaluation reward: 256.05\n",
      "episode: 3253   score: 210.0  epsilon: 1.0    steps: 905  evaluation reward: 255.75\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7297: Policy loss: -0.247527. Value loss: 30.702820. Entropy: 0.651278.\n",
      "Iteration 7298: Policy loss: -0.205573. Value loss: 16.513493. Entropy: 0.629545.\n",
      "Iteration 7299: Policy loss: -0.142334. Value loss: 14.059878. Entropy: 0.657649.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7300: Policy loss: 0.370691. Value loss: 47.436020. Entropy: 0.640114.\n",
      "Iteration 7301: Policy loss: -0.518217. Value loss: 27.023285. Entropy: 0.657033.\n",
      "Iteration 7302: Policy loss: -0.174200. Value loss: 18.412073. Entropy: 0.668633.\n",
      "episode: 3254   score: 155.0  epsilon: 1.0    steps: 39  evaluation reward: 254.15\n",
      "episode: 3255   score: 255.0  epsilon: 1.0    steps: 580  evaluation reward: 256.35\n",
      "episode: 3256   score: 135.0  epsilon: 1.0    steps: 717  evaluation reward: 255.1\n",
      "episode: 3257   score: 520.0  epsilon: 1.0    steps: 874  evaluation reward: 257.7\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7303: Policy loss: 4.615032. Value loss: 56.996002. Entropy: 0.425995.\n",
      "Iteration 7304: Policy loss: 4.683746. Value loss: 21.976828. Entropy: 0.472365.\n",
      "Iteration 7305: Policy loss: 4.539087. Value loss: 16.351463. Entropy: 0.484286.\n",
      "episode: 3258   score: 415.0  epsilon: 1.0    steps: 233  evaluation reward: 258.0\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7306: Policy loss: 0.516451. Value loss: 37.131180. Entropy: 0.868044.\n",
      "Iteration 7307: Policy loss: 0.558000. Value loss: 21.034866. Entropy: 0.842944.\n",
      "Iteration 7308: Policy loss: 0.672413. Value loss: 19.340191. Entropy: 0.887721.\n",
      "episode: 3259   score: 135.0  epsilon: 1.0    steps: 902  evaluation reward: 256.95\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7309: Policy loss: -0.951926. Value loss: 40.414276. Entropy: 0.759284.\n",
      "Iteration 7310: Policy loss: -0.992074. Value loss: 22.536417. Entropy: 0.807020.\n",
      "Iteration 7311: Policy loss: -0.899040. Value loss: 20.792400. Entropy: 0.781438.\n",
      "episode: 3260   score: 215.0  epsilon: 1.0    steps: 432  evaluation reward: 252.0\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7312: Policy loss: 1.127682. Value loss: 26.929981. Entropy: 0.693829.\n",
      "Iteration 7313: Policy loss: 1.340814. Value loss: 13.809944. Entropy: 0.684650.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7314: Policy loss: 1.058828. Value loss: 14.831835. Entropy: 0.693349.\n",
      "episode: 3261   score: 285.0  epsilon: 1.0    steps: 344  evaluation reward: 252.0\n",
      "episode: 3262   score: 140.0  epsilon: 1.0    steps: 706  evaluation reward: 251.25\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7315: Policy loss: 0.561555. Value loss: 33.512131. Entropy: 0.678018.\n",
      "Iteration 7316: Policy loss: 0.943487. Value loss: 15.746131. Entropy: 0.689581.\n",
      "Iteration 7317: Policy loss: 0.490943. Value loss: 11.584265. Entropy: 0.680948.\n",
      "episode: 3263   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 251.4\n",
      "episode: 3264   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 248.6\n",
      "episode: 3265   score: 210.0  epsilon: 1.0    steps: 540  evaluation reward: 248.1\n",
      "episode: 3266   score: 155.0  epsilon: 1.0    steps: 777  evaluation reward: 247.55\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7318: Policy loss: 0.928706. Value loss: 24.686419. Entropy: 0.859104.\n",
      "Iteration 7319: Policy loss: 0.772982. Value loss: 14.137619. Entropy: 0.860986.\n",
      "Iteration 7320: Policy loss: 0.873367. Value loss: 12.329351. Entropy: 0.858377.\n",
      "episode: 3267   score: 110.0  epsilon: 1.0    steps: 1017  evaluation reward: 244.6\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7321: Policy loss: -0.554613. Value loss: 19.209496. Entropy: 0.878704.\n",
      "Iteration 7322: Policy loss: -0.599205. Value loss: 11.003610. Entropy: 0.881906.\n",
      "Iteration 7323: Policy loss: -0.573088. Value loss: 9.260849. Entropy: 0.886374.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7324: Policy loss: 0.970405. Value loss: 31.320576. Entropy: 0.637398.\n",
      "Iteration 7325: Policy loss: 0.953458. Value loss: 16.388014. Entropy: 0.632896.\n",
      "Iteration 7326: Policy loss: 1.084641. Value loss: 12.625684. Entropy: 0.646017.\n",
      "episode: 3268   score: 230.0  epsilon: 1.0    steps: 433  evaluation reward: 244.75\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7327: Policy loss: 1.866491. Value loss: 16.611563. Entropy: 0.651840.\n",
      "Iteration 7328: Policy loss: 1.659828. Value loss: 12.388933. Entropy: 0.660805.\n",
      "Iteration 7329: Policy loss: 1.791463. Value loss: 8.946495. Entropy: 0.656410.\n",
      "episode: 3269   score: 210.0  epsilon: 1.0    steps: 333  evaluation reward: 244.0\n",
      "episode: 3270   score: 140.0  epsilon: 1.0    steps: 691  evaluation reward: 243.0\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7330: Policy loss: -1.278341. Value loss: 23.616871. Entropy: 0.678851.\n",
      "Iteration 7331: Policy loss: -1.094045. Value loss: 15.094265. Entropy: 0.728706.\n",
      "Iteration 7332: Policy loss: -1.081784. Value loss: 13.277500. Entropy: 0.670309.\n",
      "episode: 3271   score: 210.0  epsilon: 1.0    steps: 253  evaluation reward: 242.35\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7333: Policy loss: 1.075487. Value loss: 28.218227. Entropy: 0.831161.\n",
      "Iteration 7334: Policy loss: 1.080343. Value loss: 13.759216. Entropy: 0.836493.\n",
      "Iteration 7335: Policy loss: 0.936816. Value loss: 11.217680. Entropy: 0.833922.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7336: Policy loss: -0.231049. Value loss: 37.515671. Entropy: 0.886785.\n",
      "Iteration 7337: Policy loss: -0.285230. Value loss: 14.846387. Entropy: 0.924012.\n",
      "Iteration 7338: Policy loss: -0.041369. Value loss: 12.872602. Entropy: 0.899993.\n",
      "episode: 3272   score: 345.0  epsilon: 1.0    steps: 565  evaluation reward: 242.15\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7339: Policy loss: -0.281494. Value loss: 43.334450. Entropy: 0.765170.\n",
      "Iteration 7340: Policy loss: -0.041656. Value loss: 21.019405. Entropy: 0.777135.\n",
      "Iteration 7341: Policy loss: -0.155157. Value loss: 16.981421. Entropy: 0.798421.\n",
      "episode: 3273   score: 390.0  epsilon: 1.0    steps: 770  evaluation reward: 242.05\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7342: Policy loss: 0.588304. Value loss: 30.089397. Entropy: 0.884806.\n",
      "Iteration 7343: Policy loss: 0.731247. Value loss: 20.672752. Entropy: 0.892868.\n",
      "Iteration 7344: Policy loss: 0.636308. Value loss: 14.016913. Entropy: 0.900735.\n",
      "episode: 3274   score: 295.0  epsilon: 1.0    steps: 79  evaluation reward: 242.75\n",
      "episode: 3275   score: 275.0  epsilon: 1.0    steps: 511  evaluation reward: 242.3\n",
      "episode: 3276   score: 240.0  epsilon: 1.0    steps: 932  evaluation reward: 241.85\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7345: Policy loss: 1.390509. Value loss: 26.788885. Entropy: 0.764732.\n",
      "Iteration 7346: Policy loss: 1.410331. Value loss: 11.282973. Entropy: 0.780172.\n",
      "Iteration 7347: Policy loss: 1.341890. Value loss: 8.728235. Entropy: 0.761787.\n",
      "episode: 3277   score: 270.0  epsilon: 1.0    steps: 712  evaluation reward: 243.5\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7348: Policy loss: 1.846415. Value loss: 22.530718. Entropy: 0.891138.\n",
      "Iteration 7349: Policy loss: 1.573455. Value loss: 9.372122. Entropy: 0.876406.\n",
      "Iteration 7350: Policy loss: 1.748714. Value loss: 8.772794. Entropy: 0.904194.\n",
      "episode: 3278   score: 210.0  epsilon: 1.0    steps: 172  evaluation reward: 243.5\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7351: Policy loss: 3.574922. Value loss: 30.197355. Entropy: 0.819850.\n",
      "Iteration 7352: Policy loss: 3.497159. Value loss: 15.797984. Entropy: 0.808775.\n",
      "Iteration 7353: Policy loss: 3.610719. Value loss: 11.864143. Entropy: 0.820494.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7354: Policy loss: 0.094766. Value loss: 29.406034. Entropy: 0.865192.\n",
      "Iteration 7355: Policy loss: 0.134241. Value loss: 12.911939. Entropy: 0.859135.\n",
      "Iteration 7356: Policy loss: -0.024071. Value loss: 9.940618. Entropy: 0.860238.\n",
      "episode: 3279   score: 370.0  epsilon: 1.0    steps: 356  evaluation reward: 244.6\n",
      "episode: 3280   score: 180.0  epsilon: 1.0    steps: 873  evaluation reward: 244.15\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7357: Policy loss: -0.330631. Value loss: 40.442047. Entropy: 0.692507.\n",
      "Iteration 7358: Policy loss: -0.650403. Value loss: 21.177126. Entropy: 0.678042.\n",
      "Iteration 7359: Policy loss: -0.333285. Value loss: 15.631410. Entropy: 0.675293.\n",
      "episode: 3281   score: 215.0  epsilon: 1.0    steps: 512  evaluation reward: 244.2\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7360: Policy loss: -1.805457. Value loss: 34.460175. Entropy: 0.955112.\n",
      "Iteration 7361: Policy loss: -1.666378. Value loss: 15.774362. Entropy: 0.957340.\n",
      "Iteration 7362: Policy loss: -1.673339. Value loss: 12.712877. Entropy: 0.946585.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7363: Policy loss: 2.313747. Value loss: 21.186932. Entropy: 0.983742.\n",
      "Iteration 7364: Policy loss: 2.416126. Value loss: 11.555943. Entropy: 0.971250.\n",
      "Iteration 7365: Policy loss: 2.321146. Value loss: 9.068927. Entropy: 0.980953.\n",
      "episode: 3282   score: 215.0  epsilon: 1.0    steps: 82  evaluation reward: 244.25\n",
      "episode: 3283   score: 270.0  epsilon: 1.0    steps: 523  evaluation reward: 245.6\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7366: Policy loss: 1.248920. Value loss: 27.523481. Entropy: 0.672593.\n",
      "Iteration 7367: Policy loss: 1.361924. Value loss: 16.028429. Entropy: 0.668344.\n",
      "Iteration 7368: Policy loss: 1.391007. Value loss: 13.815077. Entropy: 0.666730.\n",
      "episode: 3284   score: 240.0  epsilon: 1.0    steps: 221  evaluation reward: 245.9\n",
      "episode: 3285   score: 180.0  epsilon: 1.0    steps: 681  evaluation reward: 242.75\n",
      "episode: 3286   score: 390.0  epsilon: 1.0    steps: 1005  evaluation reward: 244.05\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7369: Policy loss: 1.634508. Value loss: 22.844818. Entropy: 0.721286.\n",
      "Iteration 7370: Policy loss: 1.163640. Value loss: 15.534141. Entropy: 0.718169.\n",
      "Iteration 7371: Policy loss: 1.479311. Value loss: 11.316819. Entropy: 0.732308.\n",
      "episode: 3287   score: 210.0  epsilon: 1.0    steps: 327  evaluation reward: 243.75\n",
      "episode: 3288   score: 225.0  epsilon: 1.0    steps: 849  evaluation reward: 243.25\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7372: Policy loss: -1.084540. Value loss: 22.185951. Entropy: 0.747255.\n",
      "Iteration 7373: Policy loss: -1.060950. Value loss: 13.570689. Entropy: 0.756392.\n",
      "Iteration 7374: Policy loss: -1.051008. Value loss: 15.255840. Entropy: 0.752865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3289   score: 105.0  epsilon: 1.0    steps: 618  evaluation reward: 241.7\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7375: Policy loss: -0.582948. Value loss: 29.945196. Entropy: 0.822206.\n",
      "Iteration 7376: Policy loss: -0.595992. Value loss: 18.382549. Entropy: 0.813117.\n",
      "Iteration 7377: Policy loss: -0.388830. Value loss: 15.200196. Entropy: 0.822024.\n",
      "episode: 3290   score: 160.0  epsilon: 1.0    steps: 89  evaluation reward: 242.25\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7378: Policy loss: -0.436624. Value loss: 28.286169. Entropy: 0.726455.\n",
      "Iteration 7379: Policy loss: -0.313932. Value loss: 12.011748. Entropy: 0.738245.\n",
      "Iteration 7380: Policy loss: -0.518565. Value loss: 9.410004. Entropy: 0.731285.\n",
      "episode: 3291   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 240.45\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7381: Policy loss: -0.719723. Value loss: 31.815842. Entropy: 0.608360.\n",
      "Iteration 7382: Policy loss: -0.769350. Value loss: 16.469763. Entropy: 0.597090.\n",
      "Iteration 7383: Policy loss: -0.875261. Value loss: 12.704720. Entropy: 0.586170.\n",
      "episode: 3292   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 238.45\n",
      "episode: 3293   score: 320.0  epsilon: 1.0    steps: 407  evaluation reward: 238.2\n",
      "episode: 3294   score: 220.0  epsilon: 1.0    steps: 752  evaluation reward: 237.55\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7384: Policy loss: -0.681566. Value loss: 19.031618. Entropy: 0.689270.\n",
      "Iteration 7385: Policy loss: -0.736781. Value loss: 11.042732. Entropy: 0.682831.\n",
      "Iteration 7386: Policy loss: -0.517455. Value loss: 9.334710. Entropy: 0.668609.\n",
      "episode: 3295   score: 120.0  epsilon: 1.0    steps: 117  evaluation reward: 233.35\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7387: Policy loss: -2.663556. Value loss: 22.187803. Entropy: 0.834499.\n",
      "Iteration 7388: Policy loss: -2.821901. Value loss: 10.473932. Entropy: 0.821479.\n",
      "Iteration 7389: Policy loss: -2.654304. Value loss: 8.086232. Entropy: 0.818719.\n",
      "episode: 3296   score: 180.0  epsilon: 1.0    steps: 540  evaluation reward: 233.25\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7390: Policy loss: 0.399331. Value loss: 160.073059. Entropy: 0.736825.\n",
      "Iteration 7391: Policy loss: 0.259306. Value loss: 132.885574. Entropy: 0.702826.\n",
      "Iteration 7392: Policy loss: 0.760056. Value loss: 67.192116. Entropy: 0.676657.\n",
      "episode: 3297   score: 550.0  epsilon: 1.0    steps: 984  evaluation reward: 235.65\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7393: Policy loss: 0.258895. Value loss: 29.148399. Entropy: 0.625893.\n",
      "Iteration 7394: Policy loss: 0.624877. Value loss: 15.174097. Entropy: 0.607800.\n",
      "Iteration 7395: Policy loss: 0.541192. Value loss: 12.102335. Entropy: 0.620277.\n",
      "episode: 3298   score: 180.0  epsilon: 1.0    steps: 460  evaluation reward: 235.65\n",
      "episode: 3299   score: 330.0  epsilon: 1.0    steps: 810  evaluation reward: 236.35\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7396: Policy loss: -0.274402. Value loss: 19.737957. Entropy: 0.512345.\n",
      "Iteration 7397: Policy loss: -0.350232. Value loss: 10.868992. Entropy: 0.505978.\n",
      "Iteration 7398: Policy loss: -0.290982. Value loss: 9.394867. Entropy: 0.498326.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7399: Policy loss: 1.251561. Value loss: 19.407646. Entropy: 0.655155.\n",
      "Iteration 7400: Policy loss: 1.421351. Value loss: 11.703863. Entropy: 0.658426.\n",
      "Iteration 7401: Policy loss: 1.281833. Value loss: 9.787583. Entropy: 0.649462.\n",
      "episode: 3300   score: 210.0  epsilon: 1.0    steps: 92  evaluation reward: 234.8\n",
      "now time :  2019-02-25 20:58:24.600410\n",
      "episode: 3301   score: 210.0  epsilon: 1.0    steps: 597  evaluation reward: 234.35\n",
      "episode: 3302   score: 215.0  epsilon: 1.0    steps: 655  evaluation reward: 233.9\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7402: Policy loss: -1.180169. Value loss: 18.995865. Entropy: 0.467031.\n",
      "Iteration 7403: Policy loss: -1.101621. Value loss: 12.746360. Entropy: 0.473103.\n",
      "Iteration 7404: Policy loss: -0.992616. Value loss: 8.629682. Entropy: 0.473583.\n",
      "episode: 3303   score: 280.0  epsilon: 1.0    steps: 327  evaluation reward: 235.15\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7405: Policy loss: 0.661221. Value loss: 16.576881. Entropy: 0.546212.\n",
      "Iteration 7406: Policy loss: 0.449656. Value loss: 12.600397. Entropy: 0.552484.\n",
      "Iteration 7407: Policy loss: 0.588572. Value loss: 10.724430. Entropy: 0.570576.\n",
      "episode: 3304   score: 210.0  epsilon: 1.0    steps: 972  evaluation reward: 235.6\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7408: Policy loss: 0.943036. Value loss: 19.050068. Entropy: 0.633054.\n",
      "Iteration 7409: Policy loss: 1.162578. Value loss: 10.320463. Entropy: 0.606991.\n",
      "Iteration 7410: Policy loss: 1.013753. Value loss: 8.710876. Entropy: 0.639892.\n",
      "episode: 3305   score: 180.0  epsilon: 1.0    steps: 775  evaluation reward: 232.5\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7411: Policy loss: -1.930043. Value loss: 21.811277. Entropy: 0.658383.\n",
      "Iteration 7412: Policy loss: -1.910170. Value loss: 13.205021. Entropy: 0.657465.\n",
      "Iteration 7413: Policy loss: -1.911536. Value loss: 10.532948. Entropy: 0.647250.\n",
      "episode: 3306   score: 320.0  epsilon: 1.0    steps: 184  evaluation reward: 233.3\n",
      "episode: 3307   score: 155.0  epsilon: 1.0    steps: 720  evaluation reward: 233.05\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7414: Policy loss: 1.784539. Value loss: 20.230368. Entropy: 0.474591.\n",
      "Iteration 7415: Policy loss: 1.839185. Value loss: 10.912341. Entropy: 0.487039.\n",
      "Iteration 7416: Policy loss: 1.716065. Value loss: 11.803557. Entropy: 0.481539.\n",
      "episode: 3308   score: 180.0  epsilon: 1.0    steps: 45  evaluation reward: 233.05\n",
      "episode: 3309   score: 155.0  epsilon: 1.0    steps: 548  evaluation reward: 233.5\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7417: Policy loss: -0.598775. Value loss: 21.245722. Entropy: 0.593457.\n",
      "Iteration 7418: Policy loss: -0.679183. Value loss: 12.329518. Entropy: 0.570298.\n",
      "Iteration 7419: Policy loss: -0.622561. Value loss: 9.060744. Entropy: 0.586792.\n",
      "episode: 3310   score: 210.0  epsilon: 1.0    steps: 317  evaluation reward: 233.0\n",
      "episode: 3311   score: 275.0  epsilon: 1.0    steps: 401  evaluation reward: 233.95\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7420: Policy loss: -0.212860. Value loss: 18.783379. Entropy: 0.576299.\n",
      "Iteration 7421: Policy loss: -0.033353. Value loss: 11.087626. Entropy: 0.556320.\n",
      "Iteration 7422: Policy loss: 0.045838. Value loss: 9.186152. Entropy: 0.564965.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7423: Policy loss: -1.096259. Value loss: 15.753394. Entropy: 0.660543.\n",
      "Iteration 7424: Policy loss: -1.082737. Value loss: 8.628435. Entropy: 0.631811.\n",
      "Iteration 7425: Policy loss: -1.059161. Value loss: 5.926307. Entropy: 0.638015.\n",
      "episode: 3312   score: 260.0  epsilon: 1.0    steps: 899  evaluation reward: 233.2\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7426: Policy loss: 0.420466. Value loss: 17.025770. Entropy: 0.435800.\n",
      "Iteration 7427: Policy loss: 0.405339. Value loss: 11.266128. Entropy: 0.439701.\n",
      "Iteration 7428: Policy loss: 0.317965. Value loss: 10.387514. Entropy: 0.447324.\n",
      "episode: 3313   score: 240.0  epsilon: 1.0    steps: 216  evaluation reward: 232.9\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7429: Policy loss: 1.749228. Value loss: 18.205256. Entropy: 0.500538.\n",
      "Iteration 7430: Policy loss: 1.916887. Value loss: 9.704142. Entropy: 0.515254.\n",
      "Iteration 7431: Policy loss: 1.684781. Value loss: 8.020635. Entropy: 0.525414.\n",
      "episode: 3314   score: 210.0  epsilon: 1.0    steps: 12  evaluation reward: 232.9\n",
      "episode: 3315   score: 260.0  epsilon: 1.0    steps: 600  evaluation reward: 232.65\n",
      "episode: 3316   score: 220.0  epsilon: 1.0    steps: 826  evaluation reward: 231.7\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7432: Policy loss: 2.012279. Value loss: 16.629187. Entropy: 0.566939.\n",
      "Iteration 7433: Policy loss: 1.796004. Value loss: 10.138744. Entropy: 0.560222.\n",
      "Iteration 7434: Policy loss: 1.966703. Value loss: 7.985530. Entropy: 0.573754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3317   score: 265.0  epsilon: 1.0    steps: 362  evaluation reward: 231.75\n",
      "episode: 3318   score: 235.0  epsilon: 1.0    steps: 693  evaluation reward: 232.3\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7435: Policy loss: -0.058241. Value loss: 20.512720. Entropy: 0.547845.\n",
      "Iteration 7436: Policy loss: 0.011023. Value loss: 13.360107. Entropy: 0.560677.\n",
      "Iteration 7437: Policy loss: 0.021101. Value loss: 10.119625. Entropy: 0.542074.\n",
      "episode: 3319   score: 240.0  epsilon: 1.0    steps: 424  evaluation reward: 231.85\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7438: Policy loss: 0.563295. Value loss: 17.136786. Entropy: 0.582237.\n",
      "Iteration 7439: Policy loss: 0.601855. Value loss: 12.503965. Entropy: 0.575265.\n",
      "Iteration 7440: Policy loss: 0.652857. Value loss: 10.243402. Entropy: 0.567068.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7441: Policy loss: -0.096946. Value loss: 23.367626. Entropy: 0.524913.\n",
      "Iteration 7442: Policy loss: 0.050570. Value loss: 14.453397. Entropy: 0.517391.\n",
      "Iteration 7443: Policy loss: -0.247250. Value loss: 12.128805. Entropy: 0.527105.\n",
      "episode: 3320   score: 180.0  epsilon: 1.0    steps: 209  evaluation reward: 232.75\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7444: Policy loss: 2.877055. Value loss: 26.200005. Entropy: 0.423858.\n",
      "Iteration 7445: Policy loss: 2.892434. Value loss: 16.326750. Entropy: 0.417914.\n",
      "Iteration 7446: Policy loss: 2.998357. Value loss: 13.508782. Entropy: 0.440473.\n",
      "episode: 3321   score: 210.0  epsilon: 1.0    steps: 1  evaluation reward: 232.4\n",
      "episode: 3322   score: 160.0  epsilon: 1.0    steps: 577  evaluation reward: 233.1\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7447: Policy loss: 2.002955. Value loss: 21.313503. Entropy: 0.510861.\n",
      "Iteration 7448: Policy loss: 1.812827. Value loss: 12.908628. Entropy: 0.451516.\n",
      "Iteration 7449: Policy loss: 1.903107. Value loss: 10.674765. Entropy: 0.467436.\n",
      "episode: 3323   score: 180.0  epsilon: 1.0    steps: 455  evaluation reward: 232.05\n",
      "episode: 3324   score: 180.0  epsilon: 1.0    steps: 671  evaluation reward: 228.9\n",
      "episode: 3325   score: 240.0  epsilon: 1.0    steps: 796  evaluation reward: 230.85\n",
      "episode: 3326   score: 270.0  epsilon: 1.0    steps: 916  evaluation reward: 229.85\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7450: Policy loss: 0.900697. Value loss: 20.056263. Entropy: 0.651169.\n",
      "Iteration 7451: Policy loss: 0.751776. Value loss: 13.331979. Entropy: 0.639148.\n",
      "Iteration 7452: Policy loss: 0.907417. Value loss: 13.006499. Entropy: 0.665899.\n",
      "episode: 3327   score: 135.0  epsilon: 1.0    steps: 287  evaluation reward: 228.3\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7453: Policy loss: 1.173113. Value loss: 17.206589. Entropy: 0.640923.\n",
      "Iteration 7454: Policy loss: 1.198350. Value loss: 11.876539. Entropy: 0.661173.\n",
      "Iteration 7455: Policy loss: 1.240066. Value loss: 10.590558. Entropy: 0.657937.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7456: Policy loss: -0.638849. Value loss: 17.618711. Entropy: 0.457682.\n",
      "Iteration 7457: Policy loss: -0.760989. Value loss: 11.000163. Entropy: 0.445368.\n",
      "Iteration 7458: Policy loss: -0.670814. Value loss: 8.994189. Entropy: 0.462718.\n",
      "episode: 3328   score: 240.0  epsilon: 1.0    steps: 248  evaluation reward: 228.6\n",
      "episode: 3329   score: 160.0  epsilon: 1.0    steps: 574  evaluation reward: 228.1\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7459: Policy loss: -1.078376. Value loss: 21.690624. Entropy: 0.348932.\n",
      "Iteration 7460: Policy loss: -1.163343. Value loss: 11.839245. Entropy: 0.354606.\n",
      "Iteration 7461: Policy loss: -0.981061. Value loss: 9.667610. Entropy: 0.341444.\n",
      "episode: 3330   score: 225.0  epsilon: 1.0    steps: 67  evaluation reward: 227.95\n",
      "episode: 3331   score: 105.0  epsilon: 1.0    steps: 795  evaluation reward: 226.6\n",
      "episode: 3332   score: 180.0  epsilon: 1.0    steps: 969  evaluation reward: 225.8\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7462: Policy loss: -0.283062. Value loss: 13.900626. Entropy: 0.505275.\n",
      "Iteration 7463: Policy loss: -0.349842. Value loss: 9.713347. Entropy: 0.547130.\n",
      "Iteration 7464: Policy loss: -0.728076. Value loss: 7.622581. Entropy: 0.558477.\n",
      "episode: 3333   score: 155.0  epsilon: 1.0    steps: 380  evaluation reward: 226.3\n",
      "episode: 3334   score: 210.0  epsilon: 1.0    steps: 474  evaluation reward: 226.3\n",
      "episode: 3335   score: 210.0  epsilon: 1.0    steps: 668  evaluation reward: 225.55\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7465: Policy loss: -0.658943. Value loss: 24.793325. Entropy: 0.645804.\n",
      "Iteration 7466: Policy loss: -0.406924. Value loss: 17.160990. Entropy: 0.627987.\n",
      "Iteration 7467: Policy loss: -0.504310. Value loss: 12.054401. Entropy: 0.638807.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7468: Policy loss: -0.196526. Value loss: 18.996378. Entropy: 0.643726.\n",
      "Iteration 7469: Policy loss: -0.165435. Value loss: 12.127309. Entropy: 0.657955.\n",
      "Iteration 7470: Policy loss: -0.140663. Value loss: 10.105757. Entropy: 0.661929.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7471: Policy loss: 0.589635. Value loss: 20.882046. Entropy: 0.593298.\n",
      "Iteration 7472: Policy loss: 0.676274. Value loss: 11.548432. Entropy: 0.592556.\n",
      "Iteration 7473: Policy loss: 0.832991. Value loss: 9.145479. Entropy: 0.584240.\n",
      "episode: 3336   score: 180.0  epsilon: 1.0    steps: 193  evaluation reward: 224.95\n",
      "episode: 3337   score: 180.0  epsilon: 1.0    steps: 585  evaluation reward: 224.35\n",
      "episode: 3338   score: 110.0  epsilon: 1.0    steps: 824  evaluation reward: 222.85\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7474: Policy loss: -0.406671. Value loss: 36.217236. Entropy: 0.460121.\n",
      "Iteration 7475: Policy loss: -0.119307. Value loss: 21.088337. Entropy: 0.442434.\n",
      "Iteration 7476: Policy loss: -0.299946. Value loss: 16.388241. Entropy: 0.448155.\n",
      "episode: 3339   score: 210.0  epsilon: 1.0    steps: 970  evaluation reward: 218.7\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7477: Policy loss: -0.464989. Value loss: 25.777891. Entropy: 0.709960.\n",
      "Iteration 7478: Policy loss: -0.712933. Value loss: 16.089235. Entropy: 0.709534.\n",
      "Iteration 7479: Policy loss: -0.531971. Value loss: 12.393992. Entropy: 0.707052.\n",
      "episode: 3340   score: 270.0  epsilon: 1.0    steps: 54  evaluation reward: 219.3\n",
      "episode: 3341   score: 185.0  epsilon: 1.0    steps: 347  evaluation reward: 219.05\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7480: Policy loss: 1.490359. Value loss: 17.104782. Entropy: 0.587081.\n",
      "Iteration 7481: Policy loss: 1.316099. Value loss: 11.416551. Entropy: 0.617085.\n",
      "Iteration 7482: Policy loss: 1.426044. Value loss: 8.174837. Entropy: 0.622654.\n",
      "episode: 3342   score: 210.0  epsilon: 1.0    steps: 413  evaluation reward: 219.35\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7483: Policy loss: -3.424718. Value loss: 259.817871. Entropy: 0.700495.\n",
      "Iteration 7484: Policy loss: -3.720191. Value loss: 208.423615. Entropy: 0.709258.\n",
      "Iteration 7485: Policy loss: -3.297754. Value loss: 162.961777. Entropy: 0.645772.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7486: Policy loss: 0.231410. Value loss: 37.930683. Entropy: 0.529265.\n",
      "Iteration 7487: Policy loss: 0.265520. Value loss: 18.921005. Entropy: 0.540189.\n",
      "Iteration 7488: Policy loss: 0.144532. Value loss: 14.224016. Entropy: 0.506454.\n",
      "episode: 3343   score: 390.0  epsilon: 1.0    steps: 216  evaluation reward: 221.45\n",
      "episode: 3344   score: 180.0  epsilon: 1.0    steps: 805  evaluation reward: 221.15\n",
      "episode: 3345   score: 120.0  epsilon: 1.0    steps: 979  evaluation reward: 220.8\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7489: Policy loss: 0.661890. Value loss: 35.080112. Entropy: 0.489393.\n",
      "Iteration 7490: Policy loss: 0.807337. Value loss: 21.444489. Entropy: 0.512055.\n",
      "Iteration 7491: Policy loss: 0.617590. Value loss: 17.404762. Entropy: 0.524442.\n",
      "episode: 3346   score: 475.0  epsilon: 1.0    steps: 734  evaluation reward: 223.75\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7492: Policy loss: -2.230622. Value loss: 29.067677. Entropy: 0.494340.\n",
      "Iteration 7493: Policy loss: -1.983996. Value loss: 17.745792. Entropy: 0.489991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7494: Policy loss: -2.134095. Value loss: 13.468282. Entropy: 0.497597.\n",
      "episode: 3347   score: 210.0  epsilon: 1.0    steps: 34  evaluation reward: 223.75\n",
      "episode: 3348   score: 210.0  epsilon: 1.0    steps: 365  evaluation reward: 223.75\n",
      "episode: 3349   score: 270.0  epsilon: 1.0    steps: 577  evaluation reward: 224.35\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7495: Policy loss: 0.544065. Value loss: 15.679067. Entropy: 0.552961.\n",
      "Iteration 7496: Policy loss: 0.354107. Value loss: 9.863743. Entropy: 0.552242.\n",
      "Iteration 7497: Policy loss: 0.459617. Value loss: 8.179467. Entropy: 0.554013.\n",
      "episode: 3350   score: 215.0  epsilon: 1.0    steps: 388  evaluation reward: 224.7\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7498: Policy loss: 0.975711. Value loss: 24.728661. Entropy: 0.635242.\n",
      "Iteration 7499: Policy loss: 0.811499. Value loss: 17.997820. Entropy: 0.647776.\n",
      "Iteration 7500: Policy loss: 0.906181. Value loss: 14.323285. Entropy: 0.625984.\n",
      "now time :  2019-02-25 21:00:17.285804\n",
      "episode: 3351   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 225.25\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7501: Policy loss: -0.437618. Value loss: 29.042315. Entropy: 0.619570.\n",
      "Iteration 7502: Policy loss: -0.417940. Value loss: 16.135201. Entropy: 0.638743.\n",
      "Iteration 7503: Policy loss: -0.602956. Value loss: 12.208072. Entropy: 0.639916.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7504: Policy loss: -2.444936. Value loss: 31.205318. Entropy: 0.612222.\n",
      "Iteration 7505: Policy loss: -2.694520. Value loss: 18.678469. Entropy: 0.578442.\n",
      "Iteration 7506: Policy loss: -2.337170. Value loss: 12.544021. Entropy: 0.584470.\n",
      "episode: 3352   score: 210.0  epsilon: 1.0    steps: 123  evaluation reward: 225.25\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7507: Policy loss: -4.467574. Value loss: 249.803375. Entropy: 0.518573.\n",
      "Iteration 7508: Policy loss: -4.896249. Value loss: 147.807343. Entropy: 0.497901.\n",
      "Iteration 7509: Policy loss: -4.461100. Value loss: 103.416519. Entropy: 0.425797.\n",
      "episode: 3353   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 225.25\n",
      "episode: 3354   score: 470.0  epsilon: 1.0    steps: 1013  evaluation reward: 228.4\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7510: Policy loss: 3.371542. Value loss: 45.950134. Entropy: 0.555363.\n",
      "Iteration 7511: Policy loss: 3.316107. Value loss: 19.851082. Entropy: 0.543808.\n",
      "Iteration 7512: Policy loss: 3.611422. Value loss: 15.031647. Entropy: 0.563634.\n",
      "episode: 3355   score: 265.0  epsilon: 1.0    steps: 505  evaluation reward: 228.5\n",
      "episode: 3356   score: 240.0  epsilon: 1.0    steps: 584  evaluation reward: 229.55\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7513: Policy loss: 1.545277. Value loss: 27.134003. Entropy: 0.725051.\n",
      "Iteration 7514: Policy loss: 1.381703. Value loss: 16.969437. Entropy: 0.716003.\n",
      "Iteration 7515: Policy loss: 1.518197. Value loss: 10.947534. Entropy: 0.727717.\n",
      "episode: 3357   score: 405.0  epsilon: 1.0    steps: 196  evaluation reward: 228.4\n",
      "episode: 3358   score: 255.0  epsilon: 1.0    steps: 704  evaluation reward: 226.8\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7516: Policy loss: 2.517971. Value loss: 26.824593. Entropy: 0.594398.\n",
      "Iteration 7517: Policy loss: 2.761766. Value loss: 16.365530. Entropy: 0.543934.\n",
      "Iteration 7518: Policy loss: 2.714843. Value loss: 15.478506. Entropy: 0.560611.\n",
      "episode: 3359   score: 210.0  epsilon: 1.0    steps: 807  evaluation reward: 227.55\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7519: Policy loss: 1.160170. Value loss: 23.656172. Entropy: 0.481669.\n",
      "Iteration 7520: Policy loss: 1.366744. Value loss: 17.216484. Entropy: 0.488337.\n",
      "Iteration 7521: Policy loss: 1.356463. Value loss: 13.932895. Entropy: 0.493765.\n",
      "episode: 3360   score: 155.0  epsilon: 1.0    steps: 97  evaluation reward: 226.95\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7522: Policy loss: 1.923840. Value loss: 15.117476. Entropy: 0.480776.\n",
      "Iteration 7523: Policy loss: 1.623936. Value loss: 11.588713. Entropy: 0.476036.\n",
      "Iteration 7524: Policy loss: 1.853061. Value loss: 8.478360. Entropy: 0.501503.\n",
      "episode: 3361   score: 155.0  epsilon: 1.0    steps: 278  evaluation reward: 225.65\n",
      "episode: 3362   score: 105.0  epsilon: 1.0    steps: 565  evaluation reward: 225.3\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7525: Policy loss: 0.260752. Value loss: 11.548007. Entropy: 0.445484.\n",
      "Iteration 7526: Policy loss: 0.348619. Value loss: 7.919168. Entropy: 0.446146.\n",
      "Iteration 7527: Policy loss: 0.234375. Value loss: 7.156318. Entropy: 0.465307.\n",
      "episode: 3363   score: 110.0  epsilon: 1.0    steps: 190  evaluation reward: 224.3\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7528: Policy loss: 2.686158. Value loss: 20.272446. Entropy: 0.557772.\n",
      "Iteration 7529: Policy loss: 2.647873. Value loss: 10.456506. Entropy: 0.583900.\n",
      "Iteration 7530: Policy loss: 2.561103. Value loss: 8.483324. Entropy: 0.627182.\n",
      "episode: 3364   score: 155.0  epsilon: 1.0    steps: 872  evaluation reward: 223.75\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7531: Policy loss: 0.327913. Value loss: 25.112385. Entropy: 0.571598.\n",
      "Iteration 7532: Policy loss: 0.237030. Value loss: 16.166559. Entropy: 0.590410.\n",
      "Iteration 7533: Policy loss: 0.556763. Value loss: 11.914627. Entropy: 0.569068.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7534: Policy loss: 1.000058. Value loss: 26.842735. Entropy: 0.642619.\n",
      "Iteration 7535: Policy loss: 1.229687. Value loss: 13.592729. Entropy: 0.648785.\n",
      "Iteration 7536: Policy loss: 0.976675. Value loss: 11.814449. Entropy: 0.636247.\n",
      "episode: 3365   score: 180.0  epsilon: 1.0    steps: 111  evaluation reward: 223.45\n",
      "episode: 3366   score: 180.0  epsilon: 1.0    steps: 626  evaluation reward: 223.7\n",
      "episode: 3367   score: 280.0  epsilon: 1.0    steps: 658  evaluation reward: 225.4\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7537: Policy loss: 0.318684. Value loss: 30.819084. Entropy: 0.572054.\n",
      "Iteration 7538: Policy loss: -0.014532. Value loss: 17.655878. Entropy: 0.571391.\n",
      "Iteration 7539: Policy loss: 0.231836. Value loss: 16.111834. Entropy: 0.577334.\n",
      "episode: 3368   score: 125.0  epsilon: 1.0    steps: 314  evaluation reward: 224.35\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7540: Policy loss: 0.088582. Value loss: 20.789278. Entropy: 0.613620.\n",
      "Iteration 7541: Policy loss: -0.064027. Value loss: 10.389814. Entropy: 0.594325.\n",
      "Iteration 7542: Policy loss: -0.013138. Value loss: 7.759519. Entropy: 0.605244.\n",
      "episode: 3369   score: 180.0  epsilon: 1.0    steps: 180  evaluation reward: 224.05\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7543: Policy loss: -2.349902. Value loss: 33.342670. Entropy: 0.638996.\n",
      "Iteration 7544: Policy loss: -2.594648. Value loss: 18.211618. Entropy: 0.641406.\n",
      "Iteration 7545: Policy loss: -2.385661. Value loss: 13.525898. Entropy: 0.655788.\n",
      "episode: 3370   score: 425.0  epsilon: 1.0    steps: 418  evaluation reward: 226.9\n",
      "episode: 3371   score: 210.0  epsilon: 1.0    steps: 847  evaluation reward: 226.9\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7546: Policy loss: -2.507562. Value loss: 120.043617. Entropy: 0.375069.\n",
      "Iteration 7547: Policy loss: -2.659949. Value loss: 134.537476. Entropy: 0.412637.\n",
      "Iteration 7548: Policy loss: -2.557532. Value loss: 77.836182. Entropy: 0.408008.\n",
      "episode: 3372   score: 610.0  epsilon: 1.0    steps: 918  evaluation reward: 229.55\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7549: Policy loss: -0.474491. Value loss: 26.253185. Entropy: 0.480723.\n",
      "Iteration 7550: Policy loss: -0.337716. Value loss: 16.006399. Entropy: 0.497818.\n",
      "Iteration 7551: Policy loss: -0.345194. Value loss: 12.686440. Entropy: 0.509708.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7552: Policy loss: 1.584778. Value loss: 27.660906. Entropy: 0.570271.\n",
      "Iteration 7553: Policy loss: 1.538409. Value loss: 15.132697. Entropy: 0.551484.\n",
      "Iteration 7554: Policy loss: 1.574861. Value loss: 12.851249. Entropy: 0.560603.\n",
      "episode: 3373   score: 240.0  epsilon: 1.0    steps: 550  evaluation reward: 228.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7555: Policy loss: -1.682774. Value loss: 23.005930. Entropy: 0.543969.\n",
      "Iteration 7556: Policy loss: -1.573440. Value loss: 10.800488. Entropy: 0.564825.\n",
      "Iteration 7557: Policy loss: -1.608432. Value loss: 9.159486. Entropy: 0.552501.\n",
      "episode: 3374   score: 180.0  epsilon: 1.0    steps: 201  evaluation reward: 226.9\n",
      "episode: 3375   score: 240.0  epsilon: 1.0    steps: 304  evaluation reward: 226.55\n",
      "episode: 3376   score: 295.0  epsilon: 1.0    steps: 646  evaluation reward: 227.1\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7558: Policy loss: 0.387628. Value loss: 33.680740. Entropy: 0.578231.\n",
      "Iteration 7559: Policy loss: 0.454079. Value loss: 16.258190. Entropy: 0.564505.\n",
      "Iteration 7560: Policy loss: 0.433494. Value loss: 13.623474. Entropy: 0.584449.\n",
      "episode: 3377   score: 375.0  epsilon: 1.0    steps: 122  evaluation reward: 228.15\n",
      "episode: 3378   score: 210.0  epsilon: 1.0    steps: 798  evaluation reward: 228.15\n",
      "episode: 3379   score: 110.0  epsilon: 1.0    steps: 983  evaluation reward: 225.55\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7561: Policy loss: 2.230588. Value loss: 32.565437. Entropy: 0.593234.\n",
      "Iteration 7562: Policy loss: 2.138680. Value loss: 17.062050. Entropy: 0.600976.\n",
      "Iteration 7563: Policy loss: 2.390385. Value loss: 13.158935. Entropy: 0.593008.\n",
      "episode: 3380   score: 290.0  epsilon: 1.0    steps: 464  evaluation reward: 226.65\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7564: Policy loss: 1.679082. Value loss: 21.832672. Entropy: 0.652302.\n",
      "Iteration 7565: Policy loss: 1.555426. Value loss: 14.563290. Entropy: 0.689676.\n",
      "Iteration 7566: Policy loss: 1.673333. Value loss: 11.365880. Entropy: 0.716447.\n",
      "episode: 3381   score: 155.0  epsilon: 1.0    steps: 635  evaluation reward: 226.05\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7567: Policy loss: 1.745449. Value loss: 21.415962. Entropy: 0.648031.\n",
      "Iteration 7568: Policy loss: 2.031183. Value loss: 11.244431. Entropy: 0.607222.\n",
      "Iteration 7569: Policy loss: 1.825536. Value loss: 11.451047. Entropy: 0.612176.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7570: Policy loss: -0.007772. Value loss: 19.035784. Entropy: 0.594739.\n",
      "Iteration 7571: Policy loss: -0.275975. Value loss: 11.436471. Entropy: 0.600299.\n",
      "Iteration 7572: Policy loss: -0.065537. Value loss: 7.922388. Entropy: 0.618718.\n",
      "episode: 3382   score: 155.0  epsilon: 1.0    steps: 195  evaluation reward: 225.45\n",
      "episode: 3383   score: 210.0  epsilon: 1.0    steps: 327  evaluation reward: 224.85\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7573: Policy loss: 0.188202. Value loss: 14.818855. Entropy: 0.578475.\n",
      "Iteration 7574: Policy loss: 0.128705. Value loss: 9.296340. Entropy: 0.579939.\n",
      "Iteration 7575: Policy loss: 0.148148. Value loss: 7.407513. Entropy: 0.588699.\n",
      "episode: 3384   score: 135.0  epsilon: 1.0    steps: 498  evaluation reward: 223.8\n",
      "episode: 3385   score: 210.0  epsilon: 1.0    steps: 815  evaluation reward: 224.1\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7576: Policy loss: 0.545044. Value loss: 18.586449. Entropy: 0.685295.\n",
      "Iteration 7577: Policy loss: 0.405117. Value loss: 9.427998. Entropy: 0.678588.\n",
      "Iteration 7578: Policy loss: 0.375423. Value loss: 6.839552. Entropy: 0.657746.\n",
      "episode: 3386   score: 230.0  epsilon: 1.0    steps: 7  evaluation reward: 222.5\n",
      "episode: 3387   score: 190.0  epsilon: 1.0    steps: 669  evaluation reward: 222.3\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7579: Policy loss: 0.667464. Value loss: 15.039725. Entropy: 0.747160.\n",
      "Iteration 7580: Policy loss: 1.210612. Value loss: 9.194316. Entropy: 0.746436.\n",
      "Iteration 7581: Policy loss: 0.480384. Value loss: 6.317909. Entropy: 0.760536.\n",
      "episode: 3388   score: 180.0  epsilon: 1.0    steps: 618  evaluation reward: 221.85\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7582: Policy loss: 0.091758. Value loss: 21.680664. Entropy: 0.773103.\n",
      "Iteration 7583: Policy loss: -0.540937. Value loss: 15.154814. Entropy: 0.767362.\n",
      "Iteration 7584: Policy loss: -0.063595. Value loss: 11.814609. Entropy: 0.771703.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7585: Policy loss: -0.877259. Value loss: 23.358141. Entropy: 0.649359.\n",
      "Iteration 7586: Policy loss: -0.300322. Value loss: 14.315782. Entropy: 0.640420.\n",
      "Iteration 7587: Policy loss: -0.595795. Value loss: 9.898889. Entropy: 0.655201.\n",
      "episode: 3389   score: 210.0  epsilon: 1.0    steps: 134  evaluation reward: 222.9\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7588: Policy loss: -2.491443. Value loss: 27.473856. Entropy: 0.603250.\n",
      "Iteration 7589: Policy loss: -2.500551. Value loss: 16.230083. Entropy: 0.606918.\n",
      "Iteration 7590: Policy loss: -2.489088. Value loss: 12.920572. Entropy: 0.587282.\n",
      "episode: 3390   score: 245.0  epsilon: 1.0    steps: 881  evaluation reward: 223.75\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7591: Policy loss: 0.392322. Value loss: 27.947384. Entropy: 0.714246.\n",
      "Iteration 7592: Policy loss: 0.461054. Value loss: 14.127511. Entropy: 0.714103.\n",
      "Iteration 7593: Policy loss: 0.236814. Value loss: 10.680191. Entropy: 0.732289.\n",
      "episode: 3391   score: 240.0  epsilon: 1.0    steps: 114  evaluation reward: 225.1\n",
      "episode: 3392   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 225.4\n",
      "episode: 3393   score: 290.0  epsilon: 1.0    steps: 931  evaluation reward: 225.1\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7594: Policy loss: 0.793237. Value loss: 16.148500. Entropy: 0.646991.\n",
      "Iteration 7595: Policy loss: 1.160654. Value loss: 8.385498. Entropy: 0.642418.\n",
      "Iteration 7596: Policy loss: 0.977348. Value loss: 6.334542. Entropy: 0.655452.\n",
      "episode: 3394   score: 290.0  epsilon: 1.0    steps: 439  evaluation reward: 225.8\n",
      "episode: 3395   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 226.4\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7597: Policy loss: 1.335936. Value loss: 30.111990. Entropy: 0.629918.\n",
      "Iteration 7598: Policy loss: 1.312164. Value loss: 16.083817. Entropy: 0.634506.\n",
      "Iteration 7599: Policy loss: 1.453747. Value loss: 14.716763. Entropy: 0.634677.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7600: Policy loss: -0.156416. Value loss: 26.608128. Entropy: 0.628323.\n",
      "Iteration 7601: Policy loss: -0.182431. Value loss: 19.148314. Entropy: 0.645598.\n",
      "Iteration 7602: Policy loss: -0.421459. Value loss: 16.348949. Entropy: 0.649850.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7603: Policy loss: -1.048635. Value loss: 33.236198. Entropy: 0.551521.\n",
      "Iteration 7604: Policy loss: -1.145392. Value loss: 18.644630. Entropy: 0.541637.\n",
      "Iteration 7605: Policy loss: -1.057616. Value loss: 13.561298. Entropy: 0.543789.\n",
      "episode: 3396   score: 275.0  epsilon: 1.0    steps: 193  evaluation reward: 227.35\n",
      "episode: 3397   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 223.95\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7606: Policy loss: -0.220571. Value loss: 24.205652. Entropy: 0.607934.\n",
      "Iteration 7607: Policy loss: -0.322874. Value loss: 15.068855. Entropy: 0.615472.\n",
      "Iteration 7608: Policy loss: -0.346431. Value loss: 12.444150. Entropy: 0.598186.\n",
      "episode: 3398   score: 180.0  epsilon: 1.0    steps: 918  evaluation reward: 223.95\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7609: Policy loss: 2.195967. Value loss: 30.899134. Entropy: 0.771084.\n",
      "Iteration 7610: Policy loss: 1.957894. Value loss: 18.623175. Entropy: 0.798548.\n",
      "Iteration 7611: Policy loss: 2.002343. Value loss: 12.031212. Entropy: 0.784977.\n",
      "episode: 3399   score: 585.0  epsilon: 1.0    steps: 300  evaluation reward: 226.5\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7612: Policy loss: -2.951540. Value loss: 220.079346. Entropy: 0.721716.\n",
      "Iteration 7613: Policy loss: -3.945641. Value loss: 208.833298. Entropy: 0.699303.\n",
      "Iteration 7614: Policy loss: -2.486359. Value loss: 59.639336. Entropy: 0.675988.\n",
      "episode: 3400   score: 225.0  epsilon: 1.0    steps: 609  evaluation reward: 226.65\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7615: Policy loss: 1.174209. Value loss: 42.684254. Entropy: 0.591891.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7616: Policy loss: 1.738201. Value loss: 20.752251. Entropy: 0.616647.\n",
      "Iteration 7617: Policy loss: 1.632161. Value loss: 14.452641. Entropy: 0.593627.\n",
      "now time :  2019-02-25 21:02:26.821956\n",
      "episode: 3401   score: 485.0  epsilon: 1.0    steps: 68  evaluation reward: 229.4\n",
      "episode: 3402   score: 280.0  epsilon: 1.0    steps: 478  evaluation reward: 230.05\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7618: Policy loss: 0.448295. Value loss: 29.964386. Entropy: 0.509013.\n",
      "Iteration 7619: Policy loss: 0.364306. Value loss: 17.147083. Entropy: 0.517136.\n",
      "Iteration 7620: Policy loss: 0.344801. Value loss: 12.710340. Entropy: 0.527677.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7621: Policy loss: 0.953908. Value loss: 36.200886. Entropy: 0.708599.\n",
      "Iteration 7622: Policy loss: 0.732986. Value loss: 17.490658. Entropy: 0.728476.\n",
      "Iteration 7623: Policy loss: 0.944093. Value loss: 11.616475. Entropy: 0.716892.\n",
      "episode: 3403   score: 225.0  epsilon: 1.0    steps: 130  evaluation reward: 229.5\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7624: Policy loss: 2.276320. Value loss: 34.069859. Entropy: 0.570911.\n",
      "Iteration 7625: Policy loss: 2.470661. Value loss: 15.995628. Entropy: 0.573149.\n",
      "Iteration 7626: Policy loss: 2.481977. Value loss: 14.263408. Entropy: 0.573116.\n",
      "episode: 3404   score: 180.0  epsilon: 1.0    steps: 311  evaluation reward: 229.2\n",
      "episode: 3405   score: 700.0  epsilon: 1.0    steps: 729  evaluation reward: 234.4\n",
      "episode: 3406   score: 270.0  epsilon: 1.0    steps: 861  evaluation reward: 233.9\n",
      "episode: 3407   score: 200.0  epsilon: 1.0    steps: 905  evaluation reward: 234.35\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7627: Policy loss: 1.391662. Value loss: 38.978554. Entropy: 0.489007.\n",
      "Iteration 7628: Policy loss: 1.302865. Value loss: 21.306875. Entropy: 0.531798.\n",
      "Iteration 7629: Policy loss: 1.539043. Value loss: 17.603899. Entropy: 0.517798.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7630: Policy loss: 0.989683. Value loss: 23.759058. Entropy: 0.707158.\n",
      "Iteration 7631: Policy loss: 0.845001. Value loss: 15.317945. Entropy: 0.721933.\n",
      "Iteration 7632: Policy loss: 0.922235. Value loss: 12.632208. Entropy: 0.725354.\n",
      "episode: 3408   score: 185.0  epsilon: 1.0    steps: 23  evaluation reward: 234.4\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7633: Policy loss: -0.620602. Value loss: 24.033361. Entropy: 0.629551.\n",
      "Iteration 7634: Policy loss: -0.359010. Value loss: 14.624922. Entropy: 0.614250.\n",
      "Iteration 7635: Policy loss: -0.456831. Value loss: 11.520616. Entropy: 0.638237.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7636: Policy loss: 0.788175. Value loss: 34.310863. Entropy: 0.564580.\n",
      "Iteration 7637: Policy loss: 0.880974. Value loss: 18.057659. Entropy: 0.577923.\n",
      "Iteration 7638: Policy loss: 0.456762. Value loss: 14.902174. Entropy: 0.587576.\n",
      "episode: 3409   score: 225.0  epsilon: 1.0    steps: 173  evaluation reward: 235.1\n",
      "episode: 3410   score: 260.0  epsilon: 1.0    steps: 420  evaluation reward: 235.6\n",
      "episode: 3411   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 234.65\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7639: Policy loss: 1.226021. Value loss: 17.217163. Entropy: 0.641457.\n",
      "Iteration 7640: Policy loss: 1.365836. Value loss: 9.926862. Entropy: 0.634392.\n",
      "Iteration 7641: Policy loss: 1.217097. Value loss: 7.630220. Entropy: 0.638219.\n",
      "episode: 3412   score: 345.0  epsilon: 1.0    steps: 614  evaluation reward: 235.5\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7642: Policy loss: 1.155274. Value loss: 38.754646. Entropy: 0.782645.\n",
      "Iteration 7643: Policy loss: 1.313749. Value loss: 21.673342. Entropy: 0.733645.\n",
      "Iteration 7644: Policy loss: 0.969189. Value loss: 15.240403. Entropy: 0.755591.\n",
      "episode: 3413   score: 260.0  epsilon: 1.0    steps: 287  evaluation reward: 235.7\n",
      "episode: 3414   score: 210.0  epsilon: 1.0    steps: 945  evaluation reward: 235.7\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7645: Policy loss: 0.930206. Value loss: 36.731785. Entropy: 0.541019.\n",
      "Iteration 7646: Policy loss: 0.955722. Value loss: 20.869984. Entropy: 0.538954.\n",
      "Iteration 7647: Policy loss: 0.947089. Value loss: 17.175770. Entropy: 0.545262.\n",
      "episode: 3415   score: 210.0  epsilon: 1.0    steps: 26  evaluation reward: 235.2\n",
      "episode: 3416   score: 75.0  epsilon: 1.0    steps: 431  evaluation reward: 233.75\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7648: Policy loss: 3.004709. Value loss: 25.230709. Entropy: 0.660930.\n",
      "Iteration 7649: Policy loss: 2.810177. Value loss: 11.309522. Entropy: 0.680874.\n",
      "Iteration 7650: Policy loss: 2.868035. Value loss: 12.177919. Entropy: 0.694829.\n",
      "episode: 3417   score: 105.0  epsilon: 1.0    steps: 142  evaluation reward: 232.15\n",
      "episode: 3418   score: 355.0  epsilon: 1.0    steps: 861  evaluation reward: 233.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7651: Policy loss: 1.359147. Value loss: 25.692381. Entropy: 0.682332.\n",
      "Iteration 7652: Policy loss: 1.208178. Value loss: 17.108236. Entropy: 0.695269.\n",
      "Iteration 7653: Policy loss: 1.280762. Value loss: 13.650935. Entropy: 0.703198.\n",
      "episode: 3419   score: 105.0  epsilon: 1.0    steps: 274  evaluation reward: 232.0\n",
      "episode: 3420   score: 135.0  epsilon: 1.0    steps: 605  evaluation reward: 231.55\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7654: Policy loss: 0.164190. Value loss: 25.921122. Entropy: 0.649082.\n",
      "Iteration 7655: Policy loss: 0.243953. Value loss: 13.117771. Entropy: 0.626963.\n",
      "Iteration 7656: Policy loss: 0.317692. Value loss: 10.850776. Entropy: 0.642156.\n",
      "episode: 3421   score: 265.0  epsilon: 1.0    steps: 737  evaluation reward: 232.1\n",
      "episode: 3422   score: 120.0  epsilon: 1.0    steps: 926  evaluation reward: 231.7\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7657: Policy loss: 0.188453. Value loss: 30.970547. Entropy: 0.646323.\n",
      "Iteration 7658: Policy loss: 0.044634. Value loss: 17.562683. Entropy: 0.609223.\n",
      "Iteration 7659: Policy loss: 0.792631. Value loss: 11.430672. Entropy: 0.631473.\n",
      "episode: 3423   score: 60.0  epsilon: 1.0    steps: 219  evaluation reward: 230.5\n",
      "episode: 3424   score: 65.0  epsilon: 1.0    steps: 844  evaluation reward: 229.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7660: Policy loss: 0.118551. Value loss: 36.457439. Entropy: 0.584834.\n",
      "Iteration 7661: Policy loss: 0.589878. Value loss: 18.463543. Entropy: 0.558429.\n",
      "Iteration 7662: Policy loss: 0.015794. Value loss: 14.608965. Entropy: 0.610356.\n",
      "episode: 3425   score: 285.0  epsilon: 1.0    steps: 111  evaluation reward: 229.8\n",
      "episode: 3426   score: 215.0  epsilon: 1.0    steps: 438  evaluation reward: 229.25\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7663: Policy loss: -1.094460. Value loss: 25.304974. Entropy: 0.638879.\n",
      "Iteration 7664: Policy loss: -1.147852. Value loss: 15.265600. Entropy: 0.641410.\n",
      "Iteration 7665: Policy loss: -1.035150. Value loss: 12.531085. Entropy: 0.627139.\n",
      "episode: 3427   score: 125.0  epsilon: 1.0    steps: 588  evaluation reward: 229.15\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7666: Policy loss: -1.095169. Value loss: 18.279297. Entropy: 0.635600.\n",
      "Iteration 7667: Policy loss: -0.929189. Value loss: 10.367658. Entropy: 0.633703.\n",
      "Iteration 7668: Policy loss: -1.124249. Value loss: 8.555098. Entropy: 0.644491.\n",
      "episode: 3428   score: 120.0  epsilon: 1.0    steps: 915  evaluation reward: 227.95\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7669: Policy loss: 1.379833. Value loss: 21.846687. Entropy: 0.815077.\n",
      "Iteration 7670: Policy loss: 1.276020. Value loss: 11.903511. Entropy: 0.826369.\n",
      "Iteration 7671: Policy loss: 1.550719. Value loss: 9.249027. Entropy: 0.811927.\n",
      "episode: 3429   score: 300.0  epsilon: 1.0    steps: 352  evaluation reward: 229.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7672: Policy loss: 0.634934. Value loss: 27.985952. Entropy: 0.735766.\n",
      "Iteration 7673: Policy loss: 0.565999. Value loss: 18.805319. Entropy: 0.719281.\n",
      "Iteration 7674: Policy loss: 0.513252. Value loss: 14.547966. Entropy: 0.739457.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7675: Policy loss: -1.696202. Value loss: 34.377735. Entropy: 0.599925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7676: Policy loss: -1.840669. Value loss: 17.348156. Entropy: 0.632912.\n",
      "Iteration 7677: Policy loss: -1.918804. Value loss: 12.986900. Entropy: 0.639218.\n",
      "episode: 3430   score: 260.0  epsilon: 1.0    steps: 201  evaluation reward: 229.7\n",
      "episode: 3431   score: 180.0  epsilon: 1.0    steps: 437  evaluation reward: 230.45\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7678: Policy loss: -2.155740. Value loss: 30.919617. Entropy: 0.586443.\n",
      "Iteration 7679: Policy loss: -1.864506. Value loss: 17.989641. Entropy: 0.600915.\n",
      "Iteration 7680: Policy loss: -1.943805. Value loss: 13.189551. Entropy: 0.582050.\n",
      "episode: 3432   score: 260.0  epsilon: 1.0    steps: 67  evaluation reward: 231.25\n",
      "episode: 3433   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 231.8\n",
      "episode: 3434   score: 365.0  epsilon: 1.0    steps: 888  evaluation reward: 233.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7681: Policy loss: 0.258305. Value loss: 29.944201. Entropy: 0.720180.\n",
      "Iteration 7682: Policy loss: 0.239004. Value loss: 16.727715. Entropy: 0.718911.\n",
      "Iteration 7683: Policy loss: 0.338817. Value loss: 13.581923. Entropy: 0.733922.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7684: Policy loss: 2.537225. Value loss: 17.679943. Entropy: 0.690208.\n",
      "Iteration 7685: Policy loss: 2.250492. Value loss: 12.263114. Entropy: 0.695916.\n",
      "Iteration 7686: Policy loss: 2.394374. Value loss: 8.719162. Entropy: 0.696436.\n",
      "episode: 3435   score: 75.0  epsilon: 1.0    steps: 426  evaluation reward: 232.0\n",
      "episode: 3436   score: 260.0  epsilon: 1.0    steps: 899  evaluation reward: 232.8\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7687: Policy loss: 0.227174. Value loss: 21.991514. Entropy: 0.645632.\n",
      "Iteration 7688: Policy loss: 0.660545. Value loss: 14.243614. Entropy: 0.665409.\n",
      "Iteration 7689: Policy loss: 0.113874. Value loss: 13.816563. Entropy: 0.629422.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7690: Policy loss: 0.601609. Value loss: 28.626268. Entropy: 0.684268.\n",
      "Iteration 7691: Policy loss: 0.634453. Value loss: 15.686009. Entropy: 0.636644.\n",
      "Iteration 7692: Policy loss: 0.646818. Value loss: 11.404583. Entropy: 0.633759.\n",
      "episode: 3437   score: 155.0  epsilon: 1.0    steps: 159  evaluation reward: 232.55\n",
      "episode: 3438   score: 275.0  epsilon: 1.0    steps: 283  evaluation reward: 234.2\n",
      "episode: 3439   score: 105.0  epsilon: 1.0    steps: 589  evaluation reward: 233.15\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7693: Policy loss: 0.509009. Value loss: 35.385319. Entropy: 0.558055.\n",
      "Iteration 7694: Policy loss: 0.652813. Value loss: 17.883720. Entropy: 0.570350.\n",
      "Iteration 7695: Policy loss: 0.554187. Value loss: 13.826564. Entropy: 0.562971.\n",
      "episode: 3440   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 232.55\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7696: Policy loss: 0.676612. Value loss: 27.659639. Entropy: 0.601847.\n",
      "Iteration 7697: Policy loss: 0.586315. Value loss: 15.756433. Entropy: 0.647878.\n",
      "Iteration 7698: Policy loss: 0.515295. Value loss: 12.891911. Entropy: 0.659851.\n",
      "episode: 3441   score: 110.0  epsilon: 1.0    steps: 463  evaluation reward: 231.8\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7699: Policy loss: 2.693292. Value loss: 17.647518. Entropy: 0.774655.\n",
      "Iteration 7700: Policy loss: 2.551234. Value loss: 9.350169. Entropy: 0.806054.\n",
      "Iteration 7701: Policy loss: 2.966892. Value loss: 5.256748. Entropy: 0.788576.\n",
      "episode: 3442   score: 545.0  epsilon: 1.0    steps: 768  evaluation reward: 235.15\n",
      "episode: 3443   score: 210.0  epsilon: 1.0    steps: 1009  evaluation reward: 233.35\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7702: Policy loss: 0.166849. Value loss: 34.893433. Entropy: 0.799132.\n",
      "Iteration 7703: Policy loss: -0.098605. Value loss: 20.661055. Entropy: 0.817892.\n",
      "Iteration 7704: Policy loss: 0.279906. Value loss: 12.246624. Entropy: 0.823126.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7705: Policy loss: 0.069696. Value loss: 21.455753. Entropy: 0.855242.\n",
      "Iteration 7706: Policy loss: -0.262175. Value loss: 13.839998. Entropy: 0.834873.\n",
      "Iteration 7707: Policy loss: -0.074223. Value loss: 10.463554. Entropy: 0.859462.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7708: Policy loss: 0.071691. Value loss: 21.515354. Entropy: 0.669299.\n",
      "Iteration 7709: Policy loss: 0.179410. Value loss: 10.994716. Entropy: 0.680641.\n",
      "Iteration 7710: Policy loss: 0.070302. Value loss: 10.239100. Entropy: 0.683014.\n",
      "episode: 3444   score: 150.0  epsilon: 1.0    steps: 52  evaluation reward: 233.05\n",
      "episode: 3445   score: 225.0  epsilon: 1.0    steps: 147  evaluation reward: 234.1\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7711: Policy loss: -1.173505. Value loss: 285.487396. Entropy: 0.867159.\n",
      "Iteration 7712: Policy loss: -0.705637. Value loss: 208.395721. Entropy: 0.842175.\n",
      "Iteration 7713: Policy loss: -0.624520. Value loss: 148.649551. Entropy: 0.784783.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7714: Policy loss: 0.971781. Value loss: 33.141628. Entropy: 0.732502.\n",
      "Iteration 7715: Policy loss: 0.459916. Value loss: 17.012089. Entropy: 0.711370.\n",
      "Iteration 7716: Policy loss: 0.964171. Value loss: 11.524871. Entropy: 0.727240.\n",
      "episode: 3446   score: 260.0  epsilon: 1.0    steps: 408  evaluation reward: 231.95\n",
      "episode: 3447   score: 415.0  epsilon: 1.0    steps: 562  evaluation reward: 234.0\n",
      "episode: 3448   score: 180.0  epsilon: 1.0    steps: 747  evaluation reward: 233.7\n",
      "episode: 3449   score: 210.0  epsilon: 1.0    steps: 1001  evaluation reward: 233.1\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7717: Policy loss: 1.037968. Value loss: 43.259731. Entropy: 0.792490.\n",
      "Iteration 7718: Policy loss: 1.592219. Value loss: 18.596176. Entropy: 0.765967.\n",
      "Iteration 7719: Policy loss: 1.161081. Value loss: 16.086283. Entropy: 0.772167.\n",
      "episode: 3450   score: 425.0  epsilon: 1.0    steps: 799  evaluation reward: 235.2\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7720: Policy loss: -2.743837. Value loss: 280.781738. Entropy: 0.538889.\n",
      "Iteration 7721: Policy loss: -2.283135. Value loss: 126.744011. Entropy: 0.523939.\n",
      "Iteration 7722: Policy loss: -2.033900. Value loss: 115.354568. Entropy: 0.500372.\n",
      "now time :  2019-02-25 21:04:25.535096\n",
      "episode: 3451   score: 525.0  epsilon: 1.0    steps: 294  evaluation reward: 238.35\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7723: Policy loss: -1.543685. Value loss: 29.776112. Entropy: 0.643461.\n",
      "Iteration 7724: Policy loss: -1.502477. Value loss: 21.135448. Entropy: 0.658405.\n",
      "Iteration 7725: Policy loss: -1.430880. Value loss: 18.675386. Entropy: 0.644217.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7726: Policy loss: 0.318983. Value loss: 37.907536. Entropy: 0.751871.\n",
      "Iteration 7727: Policy loss: 0.245282. Value loss: 19.423994. Entropy: 0.750531.\n",
      "Iteration 7728: Policy loss: 0.275454. Value loss: 14.967838. Entropy: 0.735159.\n",
      "episode: 3452   score: 285.0  epsilon: 1.0    steps: 8  evaluation reward: 239.1\n",
      "episode: 3453   score: 125.0  epsilon: 1.0    steps: 593  evaluation reward: 238.25\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7729: Policy loss: 1.330986. Value loss: 32.675774. Entropy: 0.688887.\n",
      "Iteration 7730: Policy loss: 1.910703. Value loss: 16.894793. Entropy: 0.712389.\n",
      "Iteration 7731: Policy loss: 1.565458. Value loss: 11.956988. Entropy: 0.706457.\n",
      "episode: 3454   score: 185.0  epsilon: 1.0    steps: 396  evaluation reward: 235.4\n",
      "episode: 3455   score: 210.0  epsilon: 1.0    steps: 972  evaluation reward: 234.85\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7732: Policy loss: 0.786569. Value loss: 30.635748. Entropy: 0.710054.\n",
      "Iteration 7733: Policy loss: 1.249656. Value loss: 15.540352. Entropy: 0.698854.\n",
      "Iteration 7734: Policy loss: 0.914762. Value loss: 13.450387. Entropy: 0.733370.\n",
      "episode: 3456   score: 260.0  epsilon: 1.0    steps: 861  evaluation reward: 235.05\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7735: Policy loss: -0.381677. Value loss: 34.519012. Entropy: 0.621839.\n",
      "Iteration 7736: Policy loss: -0.308674. Value loss: 21.196505. Entropy: 0.622414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7737: Policy loss: -0.650453. Value loss: 14.469857. Entropy: 0.612769.\n",
      "episode: 3457   score: 180.0  epsilon: 1.0    steps: 291  evaluation reward: 232.8\n",
      "episode: 3458   score: 255.0  epsilon: 1.0    steps: 646  evaluation reward: 232.8\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7738: Policy loss: -0.361140. Value loss: 35.214649. Entropy: 0.666427.\n",
      "Iteration 7739: Policy loss: -0.521871. Value loss: 23.233587. Entropy: 0.654345.\n",
      "Iteration 7740: Policy loss: -0.517421. Value loss: 16.862852. Entropy: 0.672717.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7741: Policy loss: 0.676019. Value loss: 30.849482. Entropy: 0.595653.\n",
      "Iteration 7742: Policy loss: 0.645422. Value loss: 16.397230. Entropy: 0.603082.\n",
      "Iteration 7743: Policy loss: 0.480783. Value loss: 13.013117. Entropy: 0.606335.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7744: Policy loss: 1.868326. Value loss: 36.228668. Entropy: 0.636752.\n",
      "Iteration 7745: Policy loss: 1.771827. Value loss: 16.887194. Entropy: 0.642963.\n",
      "Iteration 7746: Policy loss: 1.916368. Value loss: 12.857951. Entropy: 0.651652.\n",
      "episode: 3459   score: 500.0  epsilon: 1.0    steps: 130  evaluation reward: 235.7\n",
      "episode: 3460   score: 180.0  epsilon: 1.0    steps: 387  evaluation reward: 235.95\n",
      "episode: 3461   score: 80.0  epsilon: 1.0    steps: 871  evaluation reward: 235.2\n",
      "episode: 3462   score: 210.0  epsilon: 1.0    steps: 993  evaluation reward: 236.25\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7747: Policy loss: -0.060403. Value loss: 35.370663. Entropy: 0.645846.\n",
      "Iteration 7748: Policy loss: -0.005361. Value loss: 20.638685. Entropy: 0.642933.\n",
      "Iteration 7749: Policy loss: -0.291926. Value loss: 17.288292. Entropy: 0.625650.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7750: Policy loss: 1.748632. Value loss: 31.936171. Entropy: 0.645254.\n",
      "Iteration 7751: Policy loss: 1.400420. Value loss: 19.413103. Entropy: 0.685502.\n",
      "Iteration 7752: Policy loss: 1.496453. Value loss: 13.899774. Entropy: 0.675355.\n",
      "episode: 3463   score: 280.0  epsilon: 1.0    steps: 57  evaluation reward: 237.95\n",
      "episode: 3464   score: 385.0  epsilon: 1.0    steps: 603  evaluation reward: 240.25\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7753: Policy loss: 2.948237. Value loss: 32.124630. Entropy: 0.681286.\n",
      "Iteration 7754: Policy loss: 2.842740. Value loss: 19.300880. Entropy: 0.710521.\n",
      "Iteration 7755: Policy loss: 2.742988. Value loss: 14.885629. Entropy: 0.717703.\n",
      "episode: 3465   score: 240.0  epsilon: 1.0    steps: 281  evaluation reward: 240.85\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7756: Policy loss: 0.451466. Value loss: 27.136889. Entropy: 0.807143.\n",
      "Iteration 7757: Policy loss: 0.480880. Value loss: 13.304610. Entropy: 0.819479.\n",
      "Iteration 7758: Policy loss: 0.417952. Value loss: 11.117267. Entropy: 0.795214.\n",
      "episode: 3466   score: 45.0  epsilon: 1.0    steps: 402  evaluation reward: 239.5\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7759: Policy loss: 1.432535. Value loss: 21.374439. Entropy: 0.650096.\n",
      "Iteration 7760: Policy loss: 1.458089. Value loss: 11.906044. Entropy: 0.651247.\n",
      "Iteration 7761: Policy loss: 1.404518. Value loss: 9.535233. Entropy: 0.651459.\n",
      "episode: 3467   score: 275.0  epsilon: 1.0    steps: 671  evaluation reward: 239.45\n",
      "episode: 3468   score: 135.0  epsilon: 1.0    steps: 1000  evaluation reward: 239.55\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7762: Policy loss: 1.865887. Value loss: 39.962727. Entropy: 0.683108.\n",
      "Iteration 7763: Policy loss: 1.801057. Value loss: 15.766323. Entropy: 0.697328.\n",
      "Iteration 7764: Policy loss: 1.363333. Value loss: 14.276970. Entropy: 0.696461.\n",
      "episode: 3469   score: 135.0  epsilon: 1.0    steps: 775  evaluation reward: 239.1\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7765: Policy loss: 0.298433. Value loss: 22.149508. Entropy: 0.673805.\n",
      "Iteration 7766: Policy loss: 0.055620. Value loss: 13.984739. Entropy: 0.700423.\n",
      "Iteration 7767: Policy loss: 0.350866. Value loss: 11.381961. Entropy: 0.699704.\n",
      "episode: 3470   score: 275.0  epsilon: 1.0    steps: 216  evaluation reward: 237.6\n",
      "episode: 3471   score: 165.0  epsilon: 1.0    steps: 359  evaluation reward: 237.15\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7768: Policy loss: 2.982724. Value loss: 18.909813. Entropy: 0.685555.\n",
      "Iteration 7769: Policy loss: 2.981197. Value loss: 11.428061. Entropy: 0.724515.\n",
      "Iteration 7770: Policy loss: 2.934774. Value loss: 9.226047. Entropy: 0.711653.\n",
      "episode: 3472   score: 155.0  epsilon: 1.0    steps: 463  evaluation reward: 232.6\n",
      "episode: 3473   score: 210.0  epsilon: 1.0    steps: 628  evaluation reward: 232.3\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7771: Policy loss: 1.333413. Value loss: 22.240915. Entropy: 0.821379.\n",
      "Iteration 7772: Policy loss: 1.311583. Value loss: 11.260681. Entropy: 0.826744.\n",
      "Iteration 7773: Policy loss: 1.252373. Value loss: 8.090764. Entropy: 0.819584.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7774: Policy loss: -0.504554. Value loss: 17.152002. Entropy: 0.652268.\n",
      "Iteration 7775: Policy loss: -0.458550. Value loss: 11.400361. Entropy: 0.678185.\n",
      "Iteration 7776: Policy loss: -0.496319. Value loss: 8.511775. Entropy: 0.680272.\n",
      "episode: 3474   score: 230.0  epsilon: 1.0    steps: 2  evaluation reward: 232.8\n",
      "episode: 3475   score: 135.0  epsilon: 1.0    steps: 780  evaluation reward: 231.75\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7777: Policy loss: -0.242943. Value loss: 14.086872. Entropy: 0.863006.\n",
      "Iteration 7778: Policy loss: -0.233162. Value loss: 7.492694. Entropy: 0.878628.\n",
      "Iteration 7779: Policy loss: -0.397045. Value loss: 6.942192. Entropy: 0.863976.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7780: Policy loss: -2.757527. Value loss: 229.285233. Entropy: 0.764577.\n",
      "Iteration 7781: Policy loss: -2.837295. Value loss: 176.685959. Entropy: 0.753971.\n",
      "Iteration 7782: Policy loss: -2.958482. Value loss: 140.291718. Entropy: 0.721530.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7783: Policy loss: -0.538139. Value loss: 20.434341. Entropy: 0.658476.\n",
      "Iteration 7784: Policy loss: -0.756630. Value loss: 11.778412. Entropy: 0.680256.\n",
      "Iteration 7785: Policy loss: -0.652532. Value loss: 8.723668. Entropy: 0.679530.\n",
      "episode: 3476   score: 285.0  epsilon: 1.0    steps: 237  evaluation reward: 231.65\n",
      "episode: 3477   score: 180.0  epsilon: 1.0    steps: 420  evaluation reward: 229.7\n",
      "episode: 3478   score: 210.0  epsilon: 1.0    steps: 598  evaluation reward: 229.7\n",
      "episode: 3479   score: 430.0  epsilon: 1.0    steps: 993  evaluation reward: 232.9\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7786: Policy loss: -1.404004. Value loss: 43.087269. Entropy: 0.607527.\n",
      "Iteration 7787: Policy loss: -1.745759. Value loss: 24.215996. Entropy: 0.622081.\n",
      "Iteration 7788: Policy loss: -1.263669. Value loss: 20.025513. Entropy: 0.606580.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7789: Policy loss: 1.110843. Value loss: 20.288105. Entropy: 0.762755.\n",
      "Iteration 7790: Policy loss: 1.286656. Value loss: 10.902412. Entropy: 0.771822.\n",
      "Iteration 7791: Policy loss: 1.185166. Value loss: 8.500687. Entropy: 0.771365.\n",
      "episode: 3480   score: 390.0  epsilon: 1.0    steps: 682  evaluation reward: 233.9\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7792: Policy loss: 0.266729. Value loss: 24.586084. Entropy: 0.675621.\n",
      "Iteration 7793: Policy loss: 0.448772. Value loss: 16.397789. Entropy: 0.641646.\n",
      "Iteration 7794: Policy loss: 0.607334. Value loss: 12.465522. Entropy: 0.660830.\n",
      "episode: 3481   score: 260.0  epsilon: 1.0    steps: 37  evaluation reward: 234.95\n",
      "episode: 3482   score: 350.0  epsilon: 1.0    steps: 292  evaluation reward: 236.9\n",
      "episode: 3483   score: 260.0  epsilon: 1.0    steps: 785  evaluation reward: 237.4\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7795: Policy loss: 1.003489. Value loss: 17.959097. Entropy: 0.663058.\n",
      "Iteration 7796: Policy loss: 1.075059. Value loss: 11.482320. Entropy: 0.639060.\n",
      "Iteration 7797: Policy loss: 1.156742. Value loss: 7.984663. Entropy: 0.632770.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7798: Policy loss: 0.367302. Value loss: 27.152136. Entropy: 0.614990.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7799: Policy loss: 0.374598. Value loss: 18.254538. Entropy: 0.612487.\n",
      "Iteration 7800: Policy loss: 0.381249. Value loss: 15.233129. Entropy: 0.618693.\n",
      "episode: 3484   score: 185.0  epsilon: 1.0    steps: 471  evaluation reward: 237.9\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7801: Policy loss: -0.512126. Value loss: 21.580370. Entropy: 0.603303.\n",
      "Iteration 7802: Policy loss: -0.574197. Value loss: 14.012331. Entropy: 0.604647.\n",
      "Iteration 7803: Policy loss: -0.455140. Value loss: 12.659803. Entropy: 0.636175.\n",
      "episode: 3485   score: 210.0  epsilon: 1.0    steps: 163  evaluation reward: 237.9\n",
      "episode: 3486   score: 260.0  epsilon: 1.0    steps: 558  evaluation reward: 238.2\n",
      "episode: 3487   score: 210.0  epsilon: 1.0    steps: 898  evaluation reward: 238.4\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7804: Policy loss: 1.689678. Value loss: 29.177568. Entropy: 0.563674.\n",
      "Iteration 7805: Policy loss: 1.984245. Value loss: 12.813570. Entropy: 0.570893.\n",
      "Iteration 7806: Policy loss: 1.794510. Value loss: 11.198438. Entropy: 0.550179.\n",
      "episode: 3488   score: 210.0  epsilon: 1.0    steps: 677  evaluation reward: 238.7\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7807: Policy loss: 0.152919. Value loss: 18.258043. Entropy: 0.783438.\n",
      "Iteration 7808: Policy loss: 0.200596. Value loss: 12.494190. Entropy: 0.745298.\n",
      "Iteration 7809: Policy loss: 0.224883. Value loss: 6.013182. Entropy: 0.751224.\n",
      "episode: 3489   score: 185.0  epsilon: 1.0    steps: 365  evaluation reward: 238.45\n",
      "episode: 3490   score: 135.0  epsilon: 1.0    steps: 824  evaluation reward: 237.35\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7810: Policy loss: -0.135918. Value loss: 16.133673. Entropy: 0.681747.\n",
      "Iteration 7811: Policy loss: 0.006800. Value loss: 9.437927. Entropy: 0.699847.\n",
      "Iteration 7812: Policy loss: 0.089084. Value loss: 8.005942. Entropy: 0.713025.\n",
      "episode: 3491   score: 260.0  epsilon: 1.0    steps: 51  evaluation reward: 237.55\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7813: Policy loss: -0.752152. Value loss: 12.655427. Entropy: 0.547780.\n",
      "Iteration 7814: Policy loss: -0.835395. Value loss: 9.381226. Entropy: 0.554236.\n",
      "Iteration 7815: Policy loss: -0.782029. Value loss: 7.136118. Entropy: 0.531562.\n",
      "episode: 3492   score: 135.0  epsilon: 1.0    steps: 573  evaluation reward: 236.8\n",
      "episode: 3493   score: 155.0  epsilon: 1.0    steps: 937  evaluation reward: 235.45\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7816: Policy loss: -0.316401. Value loss: 21.887690. Entropy: 0.577624.\n",
      "Iteration 7817: Policy loss: -0.349778. Value loss: 13.646786. Entropy: 0.586942.\n",
      "Iteration 7818: Policy loss: -0.272612. Value loss: 9.480382. Entropy: 0.578055.\n",
      "episode: 3494   score: 180.0  epsilon: 1.0    steps: 130  evaluation reward: 234.35\n",
      "episode: 3495   score: 210.0  epsilon: 1.0    steps: 397  evaluation reward: 234.65\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7819: Policy loss: 1.355121. Value loss: 18.951416. Entropy: 0.719340.\n",
      "Iteration 7820: Policy loss: 1.513780. Value loss: 9.542095. Entropy: 0.702384.\n",
      "Iteration 7821: Policy loss: 1.456409. Value loss: 8.667806. Entropy: 0.721622.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7822: Policy loss: -0.607899. Value loss: 19.141266. Entropy: 0.687161.\n",
      "Iteration 7823: Policy loss: -0.571900. Value loss: 11.639372. Entropy: 0.700763.\n",
      "Iteration 7824: Policy loss: -0.436574. Value loss: 9.052118. Entropy: 0.688850.\n",
      "episode: 3496   score: 215.0  epsilon: 1.0    steps: 716  evaluation reward: 234.05\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7825: Policy loss: -1.592465. Value loss: 13.300107. Entropy: 0.531451.\n",
      "Iteration 7826: Policy loss: -1.847500. Value loss: 8.312203. Entropy: 0.537377.\n",
      "Iteration 7827: Policy loss: -1.826102. Value loss: 6.963426. Entropy: 0.523053.\n",
      "episode: 3497   score: 270.0  epsilon: 1.0    steps: 842  evaluation reward: 234.65\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7828: Policy loss: -1.101396. Value loss: 32.213272. Entropy: 0.523578.\n",
      "Iteration 7829: Policy loss: -1.075894. Value loss: 15.285332. Entropy: 0.529690.\n",
      "Iteration 7830: Policy loss: -1.323133. Value loss: 12.718060. Entropy: 0.549417.\n",
      "episode: 3498   score: 325.0  epsilon: 1.0    steps: 376  evaluation reward: 236.1\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7831: Policy loss: -0.841967. Value loss: 29.643795. Entropy: 0.579542.\n",
      "Iteration 7832: Policy loss: -0.920534. Value loss: 12.847154. Entropy: 0.585379.\n",
      "Iteration 7833: Policy loss: -1.033219. Value loss: 10.541351. Entropy: 0.573289.\n",
      "episode: 3499   score: 210.0  epsilon: 1.0    steps: 443  evaluation reward: 232.35\n",
      "episode: 3500   score: 240.0  epsilon: 1.0    steps: 536  evaluation reward: 232.5\n",
      "now time :  2019-02-25 21:06:32.163890\n",
      "episode: 3501   score: 225.0  epsilon: 1.0    steps: 902  evaluation reward: 229.9\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7834: Policy loss: -0.177252. Value loss: 16.830402. Entropy: 0.718711.\n",
      "Iteration 7835: Policy loss: -0.337956. Value loss: 9.234509. Entropy: 0.703945.\n",
      "Iteration 7836: Policy loss: -0.210325. Value loss: 7.962518. Entropy: 0.697869.\n",
      "episode: 3502   score: 260.0  epsilon: 1.0    steps: 163  evaluation reward: 229.7\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7837: Policy loss: 0.445202. Value loss: 33.124020. Entropy: 0.639990.\n",
      "Iteration 7838: Policy loss: 0.253045. Value loss: 24.378063. Entropy: 0.624080.\n",
      "Iteration 7839: Policy loss: 0.090833. Value loss: 17.512243. Entropy: 0.648105.\n",
      "episode: 3503   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 229.55\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7840: Policy loss: 1.947054. Value loss: 34.238712. Entropy: 0.755011.\n",
      "Iteration 7841: Policy loss: 2.126394. Value loss: 19.550707. Entropy: 0.770746.\n",
      "Iteration 7842: Policy loss: 1.770494. Value loss: 12.236476. Entropy: 0.748946.\n",
      "episode: 3504   score: 445.0  epsilon: 1.0    steps: 39  evaluation reward: 232.2\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7843: Policy loss: 0.129397. Value loss: 21.543589. Entropy: 0.582493.\n",
      "Iteration 7844: Policy loss: 0.065332. Value loss: 11.564763. Entropy: 0.632298.\n",
      "Iteration 7845: Policy loss: 0.206104. Value loss: 10.197755. Entropy: 0.638280.\n",
      "episode: 3505   score: 210.0  epsilon: 1.0    steps: 639  evaluation reward: 227.3\n",
      "episode: 3506   score: 245.0  epsilon: 1.0    steps: 813  evaluation reward: 227.05\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7846: Policy loss: -0.803725. Value loss: 22.370268. Entropy: 0.653234.\n",
      "Iteration 7847: Policy loss: -0.667336. Value loss: 12.272882. Entropy: 0.658812.\n",
      "Iteration 7848: Policy loss: -0.760326. Value loss: 11.322906. Entropy: 0.654530.\n",
      "episode: 3507   score: 190.0  epsilon: 1.0    steps: 325  evaluation reward: 226.95\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7849: Policy loss: 0.850587. Value loss: 27.973763. Entropy: 0.648366.\n",
      "Iteration 7850: Policy loss: 1.028993. Value loss: 14.094106. Entropy: 0.641337.\n",
      "Iteration 7851: Policy loss: 0.774653. Value loss: 10.707191. Entropy: 0.637041.\n",
      "episode: 3508   score: 290.0  epsilon: 1.0    steps: 484  evaluation reward: 228.0\n",
      "episode: 3509   score: 260.0  epsilon: 1.0    steps: 903  evaluation reward: 228.35\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7852: Policy loss: -0.096002. Value loss: 25.997187. Entropy: 0.661050.\n",
      "Iteration 7853: Policy loss: -0.265556. Value loss: 15.728956. Entropy: 0.663025.\n",
      "Iteration 7854: Policy loss: -0.342002. Value loss: 10.664570. Entropy: 0.679674.\n",
      "episode: 3510   score: 260.0  epsilon: 1.0    steps: 140  evaluation reward: 228.35\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7855: Policy loss: 0.749207. Value loss: 24.780140. Entropy: 0.693851.\n",
      "Iteration 7856: Policy loss: 0.503255. Value loss: 13.304473. Entropy: 0.689299.\n",
      "Iteration 7857: Policy loss: 0.639377. Value loss: 9.508698. Entropy: 0.684005.\n",
      "episode: 3511   score: 105.0  epsilon: 1.0    steps: 60  evaluation reward: 227.6\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7858: Policy loss: 0.256738. Value loss: 21.384581. Entropy: 0.708361.\n",
      "Iteration 7859: Policy loss: 0.199274. Value loss: 10.846623. Entropy: 0.692133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7860: Policy loss: -0.021478. Value loss: 9.218896. Entropy: 0.702461.\n",
      "episode: 3512   score: 275.0  epsilon: 1.0    steps: 763  evaluation reward: 226.9\n",
      "episode: 3513   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 226.4\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7861: Policy loss: -2.166748. Value loss: 28.306669. Entropy: 0.655091.\n",
      "Iteration 7862: Policy loss: -2.315167. Value loss: 12.836143. Entropy: 0.632906.\n",
      "Iteration 7863: Policy loss: -2.348294. Value loss: 9.339906. Entropy: 0.637464.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7864: Policy loss: 0.399484. Value loss: 17.960016. Entropy: 0.534363.\n",
      "Iteration 7865: Policy loss: 0.280058. Value loss: 11.173434. Entropy: 0.511396.\n",
      "Iteration 7866: Policy loss: 0.096582. Value loss: 8.225745. Entropy: 0.511342.\n",
      "episode: 3514   score: 270.0  epsilon: 1.0    steps: 997  evaluation reward: 227.0\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7867: Policy loss: -0.934566. Value loss: 21.699772. Entropy: 0.515231.\n",
      "Iteration 7868: Policy loss: -0.748755. Value loss: 11.226764. Entropy: 0.554355.\n",
      "Iteration 7869: Policy loss: -0.808642. Value loss: 8.447089. Entropy: 0.542401.\n",
      "episode: 3515   score: 335.0  epsilon: 1.0    steps: 364  evaluation reward: 228.25\n",
      "episode: 3516   score: 260.0  epsilon: 1.0    steps: 478  evaluation reward: 230.1\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7870: Policy loss: 0.542975. Value loss: 31.302481. Entropy: 0.481980.\n",
      "Iteration 7871: Policy loss: 0.694826. Value loss: 14.451447. Entropy: 0.494750.\n",
      "Iteration 7872: Policy loss: 0.761615. Value loss: 11.620663. Entropy: 0.477522.\n",
      "episode: 3517   score: 335.0  epsilon: 1.0    steps: 199  evaluation reward: 232.4\n",
      "episode: 3518   score: 460.0  epsilon: 1.0    steps: 606  evaluation reward: 233.45\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7873: Policy loss: -0.006874. Value loss: 18.602634. Entropy: 0.581560.\n",
      "Iteration 7874: Policy loss: -0.049759. Value loss: 12.478421. Entropy: 0.565802.\n",
      "Iteration 7875: Policy loss: -0.083486. Value loss: 10.395568. Entropy: 0.575562.\n",
      "episode: 3519   score: 270.0  epsilon: 1.0    steps: 52  evaluation reward: 235.1\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7876: Policy loss: 2.946427. Value loss: 29.318386. Entropy: 0.520690.\n",
      "Iteration 7877: Policy loss: 3.119488. Value loss: 18.686710. Entropy: 0.537674.\n",
      "Iteration 7878: Policy loss: 2.872348. Value loss: 14.545736. Entropy: 0.542699.\n",
      "episode: 3520   score: 260.0  epsilon: 1.0    steps: 785  evaluation reward: 236.35\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7879: Policy loss: 0.477530. Value loss: 29.798506. Entropy: 0.558203.\n",
      "Iteration 7880: Policy loss: 0.382957. Value loss: 21.420601. Entropy: 0.550570.\n",
      "Iteration 7881: Policy loss: 0.561800. Value loss: 16.252398. Entropy: 0.517332.\n",
      "episode: 3521   score: 210.0  epsilon: 1.0    steps: 677  evaluation reward: 235.8\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7882: Policy loss: 0.098886. Value loss: 21.428068. Entropy: 0.469310.\n",
      "Iteration 7883: Policy loss: 0.191962. Value loss: 11.467741. Entropy: 0.468731.\n",
      "Iteration 7884: Policy loss: 0.255906. Value loss: 9.246181. Entropy: 0.465680.\n",
      "episode: 3522   score: 210.0  epsilon: 1.0    steps: 341  evaluation reward: 236.7\n",
      "episode: 3523   score: 210.0  epsilon: 1.0    steps: 491  evaluation reward: 238.2\n",
      "episode: 3524   score: 240.0  epsilon: 1.0    steps: 990  evaluation reward: 239.95\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7885: Policy loss: 0.121853. Value loss: 22.853876. Entropy: 0.499981.\n",
      "Iteration 7886: Policy loss: 0.338094. Value loss: 15.895085. Entropy: 0.494946.\n",
      "Iteration 7887: Policy loss: 0.088505. Value loss: 13.168126. Entropy: 0.499174.\n",
      "episode: 3525   score: 210.0  epsilon: 1.0    steps: 170  evaluation reward: 239.2\n",
      "episode: 3526   score: 210.0  epsilon: 1.0    steps: 638  evaluation reward: 239.15\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7888: Policy loss: -0.779622. Value loss: 15.670531. Entropy: 0.515847.\n",
      "Iteration 7889: Policy loss: -0.788526. Value loss: 9.824055. Entropy: 0.492036.\n",
      "Iteration 7890: Policy loss: -0.969822. Value loss: 8.838096. Entropy: 0.517581.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7891: Policy loss: -0.887360. Value loss: 25.101063. Entropy: 0.609116.\n",
      "Iteration 7892: Policy loss: -0.916156. Value loss: 18.781652. Entropy: 0.626834.\n",
      "Iteration 7893: Policy loss: -0.867801. Value loss: 14.806063. Entropy: 0.596923.\n",
      "episode: 3527   score: 260.0  epsilon: 1.0    steps: 866  evaluation reward: 240.5\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7894: Policy loss: -2.348127. Value loss: 31.270302. Entropy: 0.515931.\n",
      "Iteration 7895: Policy loss: -2.521401. Value loss: 19.479376. Entropy: 0.510220.\n",
      "Iteration 7896: Policy loss: -2.516686. Value loss: 14.020275. Entropy: 0.509862.\n",
      "episode: 3528   score: 330.0  epsilon: 1.0    steps: 39  evaluation reward: 242.6\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7897: Policy loss: -1.384039. Value loss: 236.100082. Entropy: 0.686308.\n",
      "Iteration 7898: Policy loss: -1.943350. Value loss: 160.533905. Entropy: 0.620553.\n",
      "Iteration 7899: Policy loss: -2.242561. Value loss: 139.039581. Entropy: 0.599909.\n",
      "episode: 3529   score: 275.0  epsilon: 1.0    steps: 698  evaluation reward: 242.35\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7900: Policy loss: 1.978060. Value loss: 36.883492. Entropy: 0.512403.\n",
      "Iteration 7901: Policy loss: 1.958322. Value loss: 20.415785. Entropy: 0.520921.\n",
      "Iteration 7902: Policy loss: 2.253163. Value loss: 17.166481. Entropy: 0.529772.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7903: Policy loss: 2.802528. Value loss: 28.973269. Entropy: 0.622582.\n",
      "Iteration 7904: Policy loss: 3.087451. Value loss: 15.153557. Entropy: 0.627621.\n",
      "Iteration 7905: Policy loss: 3.027368. Value loss: 12.632331. Entropy: 0.646580.\n",
      "episode: 3530   score: 155.0  epsilon: 1.0    steps: 100  evaluation reward: 241.3\n",
      "episode: 3531   score: 230.0  epsilon: 1.0    steps: 287  evaluation reward: 241.8\n",
      "episode: 3532   score: 240.0  epsilon: 1.0    steps: 441  evaluation reward: 241.6\n",
      "episode: 3533   score: 270.0  epsilon: 1.0    steps: 907  evaluation reward: 242.2\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7906: Policy loss: 1.588624. Value loss: 30.120905. Entropy: 0.437501.\n",
      "Iteration 7907: Policy loss: 1.845310. Value loss: 17.795521. Entropy: 0.447059.\n",
      "Iteration 7908: Policy loss: 1.375845. Value loss: 13.780679. Entropy: 0.482215.\n",
      "episode: 3534   score: 345.0  epsilon: 1.0    steps: 592  evaluation reward: 242.0\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7909: Policy loss: 0.147712. Value loss: 32.648216. Entropy: 0.468147.\n",
      "Iteration 7910: Policy loss: 0.059107. Value loss: 16.487728. Entropy: 0.459365.\n",
      "Iteration 7911: Policy loss: -0.108298. Value loss: 14.844123. Entropy: 0.448541.\n",
      "episode: 3535   score: 240.0  epsilon: 1.0    steps: 801  evaluation reward: 243.65\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7912: Policy loss: 2.169168. Value loss: 28.335613. Entropy: 0.607900.\n",
      "Iteration 7913: Policy loss: 2.441752. Value loss: 19.306414. Entropy: 0.615680.\n",
      "Iteration 7914: Policy loss: 1.952637. Value loss: 14.130602. Entropy: 0.619564.\n",
      "episode: 3536   score: 65.0  epsilon: 1.0    steps: 419  evaluation reward: 241.7\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7915: Policy loss: -0.584475. Value loss: 226.010376. Entropy: 0.598880.\n",
      "Iteration 7916: Policy loss: 0.137621. Value loss: 122.660568. Entropy: 0.606970.\n",
      "Iteration 7917: Policy loss: -0.079833. Value loss: 78.665108. Entropy: 0.660540.\n",
      "episode: 3537   score: 680.0  epsilon: 1.0    steps: 232  evaluation reward: 246.95\n",
      "episode: 3538   score: 290.0  epsilon: 1.0    steps: 738  evaluation reward: 247.1\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7918: Policy loss: 1.178216. Value loss: 27.980713. Entropy: 0.515156.\n",
      "Iteration 7919: Policy loss: 1.378265. Value loss: 18.880051. Entropy: 0.523364.\n",
      "Iteration 7920: Policy loss: 1.209034. Value loss: 15.300303. Entropy: 0.546481.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7921: Policy loss: 1.530359. Value loss: 36.182945. Entropy: 0.588737.\n",
      "Iteration 7922: Policy loss: 1.487731. Value loss: 19.454988. Entropy: 0.593458.\n",
      "Iteration 7923: Policy loss: 1.560381. Value loss: 14.338184. Entropy: 0.605101.\n",
      "episode: 3539   score: 240.0  epsilon: 1.0    steps: 370  evaluation reward: 248.45\n",
      "episode: 3540   score: 410.0  epsilon: 1.0    steps: 967  evaluation reward: 250.45\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7924: Policy loss: 1.528901. Value loss: 39.082909. Entropy: 0.550786.\n",
      "Iteration 7925: Policy loss: 1.452376. Value loss: 20.054100. Entropy: 0.539016.\n",
      "Iteration 7926: Policy loss: 1.555296. Value loss: 16.362099. Entropy: 0.552701.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7927: Policy loss: -0.268591. Value loss: 46.467258. Entropy: 0.582283.\n",
      "Iteration 7928: Policy loss: -0.343325. Value loss: 26.236799. Entropy: 0.566888.\n",
      "Iteration 7929: Policy loss: -0.365662. Value loss: 20.126282. Entropy: 0.550672.\n",
      "episode: 3541   score: 565.0  epsilon: 1.0    steps: 82  evaluation reward: 255.0\n",
      "episode: 3542   score: 210.0  epsilon: 1.0    steps: 497  evaluation reward: 251.65\n",
      "episode: 3543   score: 265.0  epsilon: 1.0    steps: 836  evaluation reward: 252.2\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7930: Policy loss: -0.559495. Value loss: 34.549229. Entropy: 0.518442.\n",
      "Iteration 7931: Policy loss: -0.262323. Value loss: 20.507292. Entropy: 0.532324.\n",
      "Iteration 7932: Policy loss: -0.247534. Value loss: 15.115582. Entropy: 0.506702.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7933: Policy loss: -0.758280. Value loss: 20.495367. Entropy: 0.480709.\n",
      "Iteration 7934: Policy loss: -0.799373. Value loss: 12.239220. Entropy: 0.464783.\n",
      "Iteration 7935: Policy loss: -0.784184. Value loss: 10.419763. Entropy: 0.472692.\n",
      "episode: 3544   score: 180.0  epsilon: 1.0    steps: 658  evaluation reward: 252.5\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7936: Policy loss: 0.953649. Value loss: 26.804422. Entropy: 0.678776.\n",
      "Iteration 7937: Policy loss: 1.087653. Value loss: 16.315973. Entropy: 0.682600.\n",
      "Iteration 7938: Policy loss: 0.898077. Value loss: 12.844455. Entropy: 0.690764.\n",
      "episode: 3545   score: 215.0  epsilon: 1.0    steps: 157  evaluation reward: 252.4\n",
      "episode: 3546   score: 400.0  epsilon: 1.0    steps: 601  evaluation reward: 253.8\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7939: Policy loss: 1.333020. Value loss: 16.871883. Entropy: 0.653050.\n",
      "Iteration 7940: Policy loss: 1.246414. Value loss: 9.616834. Entropy: 0.645972.\n",
      "Iteration 7941: Policy loss: 1.130762. Value loss: 8.390721. Entropy: 0.642833.\n",
      "episode: 3547   score: 280.0  epsilon: 1.0    steps: 1008  evaluation reward: 252.45\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7942: Policy loss: -0.130710. Value loss: 22.906227. Entropy: 0.522966.\n",
      "Iteration 7943: Policy loss: -0.083216. Value loss: 16.113388. Entropy: 0.518833.\n",
      "Iteration 7944: Policy loss: 0.200202. Value loss: 11.552112. Entropy: 0.531463.\n",
      "episode: 3548   score: 255.0  epsilon: 1.0    steps: 295  evaluation reward: 253.2\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7945: Policy loss: -0.296194. Value loss: 25.120647. Entropy: 0.539815.\n",
      "Iteration 7946: Policy loss: -0.205255. Value loss: 15.997317. Entropy: 0.561342.\n",
      "Iteration 7947: Policy loss: -0.254379. Value loss: 12.101510. Entropy: 0.542363.\n",
      "episode: 3549   score: 210.0  epsilon: 1.0    steps: 409  evaluation reward: 253.2\n",
      "episode: 3550   score: 260.0  epsilon: 1.0    steps: 841  evaluation reward: 251.55\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7948: Policy loss: -0.538544. Value loss: 21.719391. Entropy: 0.505461.\n",
      "Iteration 7949: Policy loss: -0.368361. Value loss: 13.098788. Entropy: 0.496072.\n",
      "Iteration 7950: Policy loss: -0.578277. Value loss: 11.176615. Entropy: 0.497884.\n",
      "now time :  2019-02-25 21:08:41.250573\n",
      "episode: 3551   score: 290.0  epsilon: 1.0    steps: 29  evaluation reward: 249.2\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7951: Policy loss: -0.659060. Value loss: 29.389410. Entropy: 0.646127.\n",
      "Iteration 7952: Policy loss: -0.541424. Value loss: 20.581680. Entropy: 0.647822.\n",
      "Iteration 7953: Policy loss: -0.486703. Value loss: 14.403125. Entropy: 0.658830.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7954: Policy loss: 0.211907. Value loss: 19.982824. Entropy: 0.560751.\n",
      "Iteration 7955: Policy loss: 0.773480. Value loss: 11.961004. Entropy: 0.578727.\n",
      "Iteration 7956: Policy loss: 0.382978. Value loss: 11.255329. Entropy: 0.558981.\n",
      "episode: 3552   score: 290.0  epsilon: 1.0    steps: 187  evaluation reward: 249.25\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7957: Policy loss: 0.994556. Value loss: 24.419069. Entropy: 0.520379.\n",
      "Iteration 7958: Policy loss: 0.745764. Value loss: 12.677271. Entropy: 0.538522.\n",
      "Iteration 7959: Policy loss: 0.881496. Value loss: 11.694028. Entropy: 0.533780.\n",
      "episode: 3553   score: 240.0  epsilon: 1.0    steps: 564  evaluation reward: 250.4\n",
      "episode: 3554   score: 315.0  epsilon: 1.0    steps: 661  evaluation reward: 251.7\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7960: Policy loss: -0.037266. Value loss: 16.839041. Entropy: 0.595405.\n",
      "Iteration 7961: Policy loss: 0.011815. Value loss: 8.028465. Entropy: 0.601547.\n",
      "Iteration 7962: Policy loss: 0.004959. Value loss: 7.644872. Entropy: 0.598581.\n",
      "episode: 3555   score: 295.0  epsilon: 1.0    steps: 1011  evaluation reward: 252.55\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7963: Policy loss: 1.131193. Value loss: 39.437252. Entropy: 0.655365.\n",
      "Iteration 7964: Policy loss: 1.238094. Value loss: 20.448790. Entropy: 0.622599.\n",
      "Iteration 7965: Policy loss: 0.944595. Value loss: 14.854328. Entropy: 0.642085.\n",
      "episode: 3556   score: 330.0  epsilon: 1.0    steps: 264  evaluation reward: 253.25\n",
      "episode: 3557   score: 225.0  epsilon: 1.0    steps: 503  evaluation reward: 253.7\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7966: Policy loss: 0.080070. Value loss: 26.985888. Entropy: 0.498683.\n",
      "Iteration 7967: Policy loss: 0.127325. Value loss: 12.824576. Entropy: 0.515924.\n",
      "Iteration 7968: Policy loss: -0.234767. Value loss: 10.385269. Entropy: 0.520610.\n",
      "episode: 3558   score: 285.0  epsilon: 1.0    steps: 62  evaluation reward: 254.0\n",
      "episode: 3559   score: 360.0  epsilon: 1.0    steps: 875  evaluation reward: 252.6\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7969: Policy loss: 0.705711. Value loss: 26.377554. Entropy: 0.619874.\n",
      "Iteration 7970: Policy loss: 0.827516. Value loss: 13.107280. Entropy: 0.625678.\n",
      "Iteration 7971: Policy loss: 0.971019. Value loss: 10.484275. Entropy: 0.634048.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7972: Policy loss: 0.505966. Value loss: 23.385715. Entropy: 0.698479.\n",
      "Iteration 7973: Policy loss: 0.797301. Value loss: 12.414991. Entropy: 0.662170.\n",
      "Iteration 7974: Policy loss: 0.459012. Value loss: 8.725770. Entropy: 0.705519.\n",
      "episode: 3560   score: 240.0  epsilon: 1.0    steps: 222  evaluation reward: 253.2\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7975: Policy loss: -0.331960. Value loss: 32.452644. Entropy: 0.586806.\n",
      "Iteration 7976: Policy loss: -0.087686. Value loss: 15.010073. Entropy: 0.581510.\n",
      "Iteration 7977: Policy loss: -0.094180. Value loss: 10.483826. Entropy: 0.587624.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7978: Policy loss: 0.114469. Value loss: 27.060890. Entropy: 0.506901.\n",
      "Iteration 7979: Policy loss: 0.188040. Value loss: 12.308084. Entropy: 0.531322.\n",
      "Iteration 7980: Policy loss: 0.237171. Value loss: 10.656826. Entropy: 0.529869.\n",
      "episode: 3561   score: 425.0  epsilon: 1.0    steps: 758  evaluation reward: 256.65\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7981: Policy loss: 1.286381. Value loss: 28.158251. Entropy: 0.555467.\n",
      "Iteration 7982: Policy loss: 1.371965. Value loss: 14.756166. Entropy: 0.559839.\n",
      "Iteration 7983: Policy loss: 1.555463. Value loss: 10.901797. Entropy: 0.564981.\n",
      "episode: 3562   score: 235.0  epsilon: 1.0    steps: 352  evaluation reward: 256.9\n",
      "episode: 3563   score: 210.0  epsilon: 1.0    steps: 931  evaluation reward: 256.2\n",
      "Training network. lr: 0.000189. clip: 0.075587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7984: Policy loss: 0.550768. Value loss: 26.699940. Entropy: 0.564290.\n",
      "Iteration 7985: Policy loss: 0.738125. Value loss: 14.147029. Entropy: 0.581439.\n",
      "Iteration 7986: Policy loss: 0.359977. Value loss: 11.035158. Entropy: 0.566740.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7987: Policy loss: 2.294861. Value loss: 31.217573. Entropy: 0.635634.\n",
      "Iteration 7988: Policy loss: 1.827954. Value loss: 16.562092. Entropy: 0.609194.\n",
      "Iteration 7989: Policy loss: 2.093875. Value loss: 12.694000. Entropy: 0.623300.\n",
      "episode: 3564   score: 360.0  epsilon: 1.0    steps: 84  evaluation reward: 255.95\n",
      "episode: 3565   score: 210.0  epsilon: 1.0    steps: 205  evaluation reward: 255.65\n",
      "episode: 3566   score: 430.0  epsilon: 1.0    steps: 550  evaluation reward: 259.5\n",
      "episode: 3567   score: 280.0  epsilon: 1.0    steps: 776  evaluation reward: 259.55\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7990: Policy loss: -0.468745. Value loss: 20.094006. Entropy: 0.678986.\n",
      "Iteration 7991: Policy loss: -0.485978. Value loss: 11.353365. Entropy: 0.703495.\n",
      "Iteration 7992: Policy loss: -0.714722. Value loss: 10.290545. Entropy: 0.689020.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7993: Policy loss: 0.072642. Value loss: 22.991810. Entropy: 0.454147.\n",
      "Iteration 7994: Policy loss: 0.153334. Value loss: 13.309868. Entropy: 0.436844.\n",
      "Iteration 7995: Policy loss: -0.037089. Value loss: 11.779082. Entropy: 0.451110.\n",
      "episode: 3568   score: 370.0  epsilon: 1.0    steps: 420  evaluation reward: 261.9\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7996: Policy loss: -1.211657. Value loss: 20.455170. Entropy: 0.657076.\n",
      "Iteration 7997: Policy loss: -1.310371. Value loss: 11.164136. Entropy: 0.669415.\n",
      "Iteration 7998: Policy loss: -1.222915. Value loss: 9.337645. Entropy: 0.659929.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7999: Policy loss: -2.460650. Value loss: 217.548859. Entropy: 0.645796.\n",
      "Iteration 8000: Policy loss: -1.162549. Value loss: 51.663593. Entropy: 0.632030.\n",
      "Iteration 8001: Policy loss: -2.952749. Value loss: 77.921738. Entropy: 0.650836.\n",
      "episode: 3569   score: 210.0  epsilon: 1.0    steps: 274  evaluation reward: 262.65\n",
      "episode: 3570   score: 425.0  epsilon: 1.0    steps: 1017  evaluation reward: 264.15\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8002: Policy loss: -2.147885. Value loss: 303.778198. Entropy: 0.719461.\n",
      "Iteration 8003: Policy loss: -1.182545. Value loss: 193.123932. Entropy: 0.705553.\n",
      "Iteration 8004: Policy loss: -1.314816. Value loss: 113.568802. Entropy: 0.764721.\n",
      "episode: 3571   score: 515.0  epsilon: 1.0    steps: 673  evaluation reward: 267.65\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8005: Policy loss: 1.993194. Value loss: 31.681597. Entropy: 0.614380.\n",
      "Iteration 8006: Policy loss: 2.003272. Value loss: 17.525238. Entropy: 0.543592.\n",
      "Iteration 8007: Policy loss: 2.284200. Value loss: 14.836246. Entropy: 0.573787.\n",
      "episode: 3572   score: 260.0  epsilon: 1.0    steps: 110  evaluation reward: 268.7\n",
      "episode: 3573   score: 520.0  epsilon: 1.0    steps: 864  evaluation reward: 271.8\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8008: Policy loss: -0.578321. Value loss: 37.431099. Entropy: 0.551229.\n",
      "Iteration 8009: Policy loss: -0.751137. Value loss: 22.866045. Entropy: 0.537604.\n",
      "Iteration 8010: Policy loss: -0.733522. Value loss: 16.282822. Entropy: 0.528464.\n",
      "episode: 3574   score: 320.0  epsilon: 1.0    steps: 156  evaluation reward: 272.7\n",
      "episode: 3575   score: 260.0  epsilon: 1.0    steps: 557  evaluation reward: 273.95\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8011: Policy loss: -0.671951. Value loss: 23.474699. Entropy: 0.597392.\n",
      "Iteration 8012: Policy loss: -0.689408. Value loss: 16.959391. Entropy: 0.583179.\n",
      "Iteration 8013: Policy loss: -0.812099. Value loss: 13.867931. Entropy: 0.577551.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8014: Policy loss: 1.205719. Value loss: 39.433735. Entropy: 0.615852.\n",
      "Iteration 8015: Policy loss: 0.786494. Value loss: 19.319609. Entropy: 0.641655.\n",
      "Iteration 8016: Policy loss: 0.946332. Value loss: 15.092872. Entropy: 0.662841.\n",
      "episode: 3576   score: 315.0  epsilon: 1.0    steps: 392  evaluation reward: 274.25\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8017: Policy loss: 0.140185. Value loss: 25.755884. Entropy: 0.652148.\n",
      "Iteration 8018: Policy loss: 0.127811. Value loss: 17.366640. Entropy: 0.678749.\n",
      "Iteration 8019: Policy loss: 0.057531. Value loss: 16.758551. Entropy: 0.681130.\n",
      "episode: 3577   score: 275.0  epsilon: 1.0    steps: 313  evaluation reward: 275.2\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8020: Policy loss: 2.646461. Value loss: 44.880249. Entropy: 0.771840.\n",
      "Iteration 8021: Policy loss: 2.184345. Value loss: 18.792891. Entropy: 0.733199.\n",
      "Iteration 8022: Policy loss: 2.428137. Value loss: 18.135508. Entropy: 0.735399.\n",
      "episode: 3578   score: 135.0  epsilon: 1.0    steps: 562  evaluation reward: 274.45\n",
      "episode: 3579   score: 260.0  epsilon: 1.0    steps: 763  evaluation reward: 272.75\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8023: Policy loss: 2.546026. Value loss: 40.158558. Entropy: 0.725766.\n",
      "Iteration 8024: Policy loss: 2.608246. Value loss: 22.629183. Entropy: 0.730287.\n",
      "Iteration 8025: Policy loss: 2.562390. Value loss: 16.875206. Entropy: 0.735097.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8026: Policy loss: 1.374979. Value loss: 39.324673. Entropy: 0.613474.\n",
      "Iteration 8027: Policy loss: 1.263292. Value loss: 20.858252. Entropy: 0.628136.\n",
      "Iteration 8028: Policy loss: 1.470216. Value loss: 15.211195. Entropy: 0.612497.\n",
      "episode: 3580   score: 345.0  epsilon: 1.0    steps: 76  evaluation reward: 272.3\n",
      "episode: 3581   score: 250.0  epsilon: 1.0    steps: 829  evaluation reward: 272.2\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8029: Policy loss: 0.295166. Value loss: 24.687653. Entropy: 0.694193.\n",
      "Iteration 8030: Policy loss: 0.691452. Value loss: 13.951392. Entropy: 0.707578.\n",
      "Iteration 8031: Policy loss: 0.237288. Value loss: 9.995899. Entropy: 0.699234.\n",
      "episode: 3582   score: 180.0  epsilon: 1.0    steps: 447  evaluation reward: 270.5\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8032: Policy loss: -0.686303. Value loss: 31.019503. Entropy: 0.696673.\n",
      "Iteration 8033: Policy loss: -0.864406. Value loss: 16.517271. Entropy: 0.669339.\n",
      "Iteration 8034: Policy loss: -0.311012. Value loss: 12.590945. Entropy: 0.692596.\n",
      "episode: 3583   score: 330.0  epsilon: 1.0    steps: 205  evaluation reward: 271.2\n",
      "episode: 3584   score: 400.0  epsilon: 1.0    steps: 986  evaluation reward: 273.35\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8035: Policy loss: -1.538530. Value loss: 249.867981. Entropy: 0.696776.\n",
      "Iteration 8036: Policy loss: -1.255931. Value loss: 131.024918. Entropy: 0.667306.\n",
      "Iteration 8037: Policy loss: -0.721408. Value loss: 98.640503. Entropy: 0.656496.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8038: Policy loss: -0.265954. Value loss: 35.543175. Entropy: 0.591165.\n",
      "Iteration 8039: Policy loss: -0.026205. Value loss: 22.330204. Entropy: 0.592720.\n",
      "Iteration 8040: Policy loss: -0.154373. Value loss: 17.130299. Entropy: 0.584183.\n",
      "episode: 3585   score: 335.0  epsilon: 1.0    steps: 593  evaluation reward: 274.6\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8041: Policy loss: 2.473958. Value loss: 40.135078. Entropy: 0.547748.\n",
      "Iteration 8042: Policy loss: 2.536844. Value loss: 22.788795. Entropy: 0.568686.\n",
      "Iteration 8043: Policy loss: 2.465440. Value loss: 17.315132. Entropy: 0.548751.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8044: Policy loss: 1.696172. Value loss: 35.235901. Entropy: 0.531556.\n",
      "Iteration 8045: Policy loss: 1.752478. Value loss: 16.483664. Entropy: 0.499333.\n",
      "Iteration 8046: Policy loss: 1.585910. Value loss: 13.383850. Entropy: 0.542074.\n",
      "episode: 3586   score: 620.0  epsilon: 1.0    steps: 338  evaluation reward: 278.2\n",
      "episode: 3587   score: 310.0  epsilon: 1.0    steps: 846  evaluation reward: 279.2\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8047: Policy loss: 0.832656. Value loss: 45.236565. Entropy: 0.577701.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8048: Policy loss: 0.963183. Value loss: 24.028994. Entropy: 0.591606.\n",
      "Iteration 8049: Policy loss: 0.686377. Value loss: 20.464241. Entropy: 0.562295.\n",
      "episode: 3588   score: 225.0  epsilon: 1.0    steps: 444  evaluation reward: 279.35\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8050: Policy loss: 1.405582. Value loss: 41.792137. Entropy: 0.672179.\n",
      "Iteration 8051: Policy loss: 1.194467. Value loss: 19.721859. Entropy: 0.695465.\n",
      "Iteration 8052: Policy loss: 1.208019. Value loss: 14.979302. Entropy: 0.686717.\n",
      "episode: 3589   score: 325.0  epsilon: 1.0    steps: 53  evaluation reward: 280.75\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8053: Policy loss: -0.094556. Value loss: 45.803627. Entropy: 0.720939.\n",
      "Iteration 8054: Policy loss: -0.482263. Value loss: 29.797934. Entropy: 0.731287.\n",
      "Iteration 8055: Policy loss: -0.434755. Value loss: 24.158140. Entropy: 0.717279.\n",
      "episode: 3590   score: 210.0  epsilon: 1.0    steps: 635  evaluation reward: 281.5\n",
      "episode: 3591   score: 605.0  epsilon: 1.0    steps: 719  evaluation reward: 284.95\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8056: Policy loss: 1.898072. Value loss: 36.891670. Entropy: 0.585085.\n",
      "Iteration 8057: Policy loss: 1.720080. Value loss: 17.348272. Entropy: 0.567168.\n",
      "Iteration 8058: Policy loss: 1.951341. Value loss: 16.116766. Entropy: 0.562630.\n",
      "episode: 3592   score: 285.0  epsilon: 1.0    steps: 188  evaluation reward: 286.45\n",
      "episode: 3593   score: 380.0  epsilon: 1.0    steps: 1003  evaluation reward: 288.7\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8059: Policy loss: 1.900056. Value loss: 21.508852. Entropy: 0.595042.\n",
      "Iteration 8060: Policy loss: 2.134496. Value loss: 10.752161. Entropy: 0.621779.\n",
      "Iteration 8061: Policy loss: 1.943346. Value loss: 8.588432. Entropy: 0.610854.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8062: Policy loss: 2.437487. Value loss: 30.821276. Entropy: 0.585768.\n",
      "Iteration 8063: Policy loss: 2.328762. Value loss: 16.931606. Entropy: 0.637313.\n",
      "Iteration 8064: Policy loss: 2.238320. Value loss: 12.424225. Entropy: 0.632771.\n",
      "episode: 3594   score: 260.0  epsilon: 1.0    steps: 333  evaluation reward: 289.5\n",
      "episode: 3595   score: 260.0  epsilon: 1.0    steps: 511  evaluation reward: 290.0\n",
      "episode: 3596   score: 225.0  epsilon: 1.0    steps: 829  evaluation reward: 290.1\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8065: Policy loss: 1.132614. Value loss: 19.615791. Entropy: 0.642534.\n",
      "Iteration 8066: Policy loss: 0.983590. Value loss: 11.057977. Entropy: 0.597778.\n",
      "Iteration 8067: Policy loss: 1.124806. Value loss: 9.081695. Entropy: 0.635873.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8068: Policy loss: 0.917567. Value loss: 21.678503. Entropy: 0.572411.\n",
      "Iteration 8069: Policy loss: 0.795830. Value loss: 13.457337. Entropy: 0.577535.\n",
      "Iteration 8070: Policy loss: 0.860445. Value loss: 10.033686. Entropy: 0.579132.\n",
      "episode: 3597   score: 310.0  epsilon: 1.0    steps: 54  evaluation reward: 290.5\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8071: Policy loss: 1.461170. Value loss: 34.799583. Entropy: 0.704830.\n",
      "Iteration 8072: Policy loss: 1.396769. Value loss: 22.027458. Entropy: 0.711907.\n",
      "Iteration 8073: Policy loss: 1.529755. Value loss: 17.513254. Entropy: 0.688878.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8074: Policy loss: -0.426295. Value loss: 31.632996. Entropy: 0.779390.\n",
      "Iteration 8075: Policy loss: -0.597077. Value loss: 16.591457. Entropy: 0.778259.\n",
      "Iteration 8076: Policy loss: -0.283712. Value loss: 14.005366. Entropy: 0.774897.\n",
      "episode: 3598   score: 250.0  epsilon: 1.0    steps: 730  evaluation reward: 289.75\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8077: Policy loss: 0.677716. Value loss: 32.161968. Entropy: 0.653451.\n",
      "Iteration 8078: Policy loss: 0.538322. Value loss: 17.710655. Entropy: 0.649447.\n",
      "Iteration 8079: Policy loss: 0.748077. Value loss: 13.053211. Entropy: 0.654318.\n",
      "episode: 3599   score: 310.0  epsilon: 1.0    steps: 249  evaluation reward: 290.75\n",
      "episode: 3600   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 290.45\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8080: Policy loss: 0.078256. Value loss: 19.370766. Entropy: 0.715794.\n",
      "Iteration 8081: Policy loss: 0.266916. Value loss: 9.864699. Entropy: 0.718126.\n",
      "Iteration 8082: Policy loss: 0.047321. Value loss: 7.847329. Entropy: 0.704341.\n",
      "now time :  2019-02-25 21:11:11.046388\n",
      "episode: 3601   score: 210.0  epsilon: 1.0    steps: 441  evaluation reward: 290.3\n",
      "episode: 3602   score: 295.0  epsilon: 1.0    steps: 572  evaluation reward: 290.65\n",
      "episode: 3603   score: 210.0  epsilon: 1.0    steps: 809  evaluation reward: 290.65\n",
      "episode: 3604   score: 445.0  epsilon: 1.0    steps: 996  evaluation reward: 290.65\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8083: Policy loss: 1.382590. Value loss: 17.165440. Entropy: 0.687639.\n",
      "Iteration 8084: Policy loss: 1.517317. Value loss: 9.123158. Entropy: 0.668806.\n",
      "Iteration 8085: Policy loss: 1.329345. Value loss: 8.002045. Entropy: 0.694942.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8086: Policy loss: -1.971636. Value loss: 27.671852. Entropy: 0.642806.\n",
      "Iteration 8087: Policy loss: -2.104608. Value loss: 17.443230. Entropy: 0.634330.\n",
      "Iteration 8088: Policy loss: -1.917407. Value loss: 12.568983. Entropy: 0.645108.\n",
      "episode: 3605   score: 225.0  epsilon: 1.0    steps: 57  evaluation reward: 290.8\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8089: Policy loss: 1.248400. Value loss: 18.511158. Entropy: 0.708729.\n",
      "Iteration 8090: Policy loss: 0.784703. Value loss: 11.867824. Entropy: 0.708729.\n",
      "Iteration 8091: Policy loss: 0.865894. Value loss: 10.449833. Entropy: 0.718172.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8092: Policy loss: -2.720981. Value loss: 300.424225. Entropy: 0.695859.\n",
      "Iteration 8093: Policy loss: -2.174739. Value loss: 161.599640. Entropy: 0.699061.\n",
      "Iteration 8094: Policy loss: -2.030180. Value loss: 113.015656. Entropy: 0.679079.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8095: Policy loss: 1.713882. Value loss: 35.259338. Entropy: 0.706194.\n",
      "Iteration 8096: Policy loss: 1.738649. Value loss: 19.455357. Entropy: 0.711440.\n",
      "Iteration 8097: Policy loss: 1.572103. Value loss: 16.973217. Entropy: 0.691687.\n",
      "episode: 3606   score: 210.0  epsilon: 1.0    steps: 158  evaluation reward: 290.45\n",
      "episode: 3607   score: 240.0  epsilon: 1.0    steps: 362  evaluation reward: 290.95\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8098: Policy loss: 1.154622. Value loss: 33.804092. Entropy: 0.590942.\n",
      "Iteration 8099: Policy loss: 0.978569. Value loss: 16.739750. Entropy: 0.588532.\n",
      "Iteration 8100: Policy loss: 1.151014. Value loss: 13.798884. Entropy: 0.611296.\n",
      "episode: 3608   score: 215.0  epsilon: 1.0    steps: 449  evaluation reward: 290.2\n",
      "episode: 3609   score: 315.0  epsilon: 1.0    steps: 685  evaluation reward: 290.75\n",
      "episode: 3610   score: 245.0  epsilon: 1.0    steps: 923  evaluation reward: 290.6\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8101: Policy loss: -0.162894. Value loss: 24.448395. Entropy: 0.690195.\n",
      "Iteration 8102: Policy loss: -0.295260. Value loss: 15.915506. Entropy: 0.695029.\n",
      "Iteration 8103: Policy loss: 0.014130. Value loss: 10.897161. Entropy: 0.696294.\n",
      "episode: 3611   score: 270.0  epsilon: 1.0    steps: 817  evaluation reward: 292.25\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8104: Policy loss: 1.768445. Value loss: 25.175848. Entropy: 0.761993.\n",
      "Iteration 8105: Policy loss: 1.869863. Value loss: 13.288586. Entropy: 0.787459.\n",
      "Iteration 8106: Policy loss: 1.511379. Value loss: 12.118398. Entropy: 0.766219.\n",
      "episode: 3612   score: 545.0  epsilon: 1.0    steps: 566  evaluation reward: 294.95\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8107: Policy loss: 1.573361. Value loss: 26.011591. Entropy: 0.714806.\n",
      "Iteration 8108: Policy loss: 1.646144. Value loss: 15.670752. Entropy: 0.707137.\n",
      "Iteration 8109: Policy loss: 1.607150. Value loss: 16.271406. Entropy: 0.703668.\n",
      "episode: 3613   score: 275.0  epsilon: 1.0    steps: 121  evaluation reward: 295.6\n",
      "episode: 3614   score: 130.0  epsilon: 1.0    steps: 226  evaluation reward: 294.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8110: Policy loss: 1.304860. Value loss: 27.330627. Entropy: 0.775612.\n",
      "Iteration 8111: Policy loss: 1.249070. Value loss: 19.471500. Entropy: 0.743394.\n",
      "Iteration 8112: Policy loss: 0.895895. Value loss: 13.632736. Entropy: 0.753075.\n",
      "episode: 3615   score: 155.0  epsilon: 1.0    steps: 501  evaluation reward: 292.4\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8113: Policy loss: -1.870504. Value loss: 233.726105. Entropy: 0.734164.\n",
      "Iteration 8114: Policy loss: -2.164112. Value loss: 133.257141. Entropy: 0.718787.\n",
      "Iteration 8115: Policy loss: -2.044286. Value loss: 105.500839. Entropy: 0.712075.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8116: Policy loss: 2.831376. Value loss: 32.502399. Entropy: 0.741576.\n",
      "Iteration 8117: Policy loss: 2.987942. Value loss: 15.916286. Entropy: 0.755838.\n",
      "Iteration 8118: Policy loss: 2.869224. Value loss: 12.536307. Entropy: 0.754489.\n",
      "episode: 3616   score: 420.0  epsilon: 1.0    steps: 295  evaluation reward: 294.0\n",
      "episode: 3617   score: 325.0  epsilon: 1.0    steps: 724  evaluation reward: 293.9\n",
      "episode: 3618   score: 185.0  epsilon: 1.0    steps: 772  evaluation reward: 291.15\n",
      "episode: 3619   score: 235.0  epsilon: 1.0    steps: 920  evaluation reward: 290.8\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8119: Policy loss: 1.116714. Value loss: 28.259533. Entropy: 0.758676.\n",
      "Iteration 8120: Policy loss: 1.338693. Value loss: 16.085693. Entropy: 0.778937.\n",
      "Iteration 8121: Policy loss: 1.309715. Value loss: 12.670053. Entropy: 0.770898.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8122: Policy loss: 0.915884. Value loss: 19.586273. Entropy: 0.700286.\n",
      "Iteration 8123: Policy loss: 1.052992. Value loss: 11.575049. Entropy: 0.677509.\n",
      "Iteration 8124: Policy loss: 1.018038. Value loss: 10.793815. Entropy: 0.692645.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8125: Policy loss: -0.153002. Value loss: 39.557499. Entropy: 0.708523.\n",
      "Iteration 8126: Policy loss: -0.231895. Value loss: 16.869947. Entropy: 0.704446.\n",
      "Iteration 8127: Policy loss: -0.426545. Value loss: 12.580825. Entropy: 0.700487.\n",
      "episode: 3620   score: 215.0  epsilon: 1.0    steps: 15  evaluation reward: 290.35\n",
      "episode: 3621   score: 235.0  epsilon: 1.0    steps: 547  evaluation reward: 290.6\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8128: Policy loss: -1.419437. Value loss: 34.240318. Entropy: 0.834221.\n",
      "Iteration 8129: Policy loss: -1.236374. Value loss: 17.753487. Entropy: 0.841797.\n",
      "Iteration 8130: Policy loss: -1.193494. Value loss: 14.542639. Entropy: 0.841645.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8131: Policy loss: -0.221883. Value loss: 26.322857. Entropy: 0.838713.\n",
      "Iteration 8132: Policy loss: -0.173555. Value loss: 12.859838. Entropy: 0.865250.\n",
      "Iteration 8133: Policy loss: -0.121967. Value loss: 10.128840. Entropy: 0.844636.\n",
      "episode: 3622   score: 265.0  epsilon: 1.0    steps: 452  evaluation reward: 291.15\n",
      "episode: 3623   score: 210.0  epsilon: 1.0    steps: 727  evaluation reward: 291.15\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8134: Policy loss: -1.828122. Value loss: 36.287785. Entropy: 0.891069.\n",
      "Iteration 8135: Policy loss: -2.318779. Value loss: 21.935076. Entropy: 0.889587.\n",
      "Iteration 8136: Policy loss: -1.784029. Value loss: 14.662620. Entropy: 0.884337.\n",
      "episode: 3624   score: 410.0  epsilon: 1.0    steps: 235  evaluation reward: 292.85\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8137: Policy loss: -0.304067. Value loss: 38.780891. Entropy: 0.915004.\n",
      "Iteration 8138: Policy loss: -0.217251. Value loss: 17.699327. Entropy: 0.907341.\n",
      "Iteration 8139: Policy loss: -0.252748. Value loss: 13.252649. Entropy: 0.925439.\n",
      "episode: 3625   score: 330.0  epsilon: 1.0    steps: 352  evaluation reward: 294.05\n",
      "episode: 3626   score: 285.0  epsilon: 1.0    steps: 812  evaluation reward: 294.8\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8140: Policy loss: 0.422204. Value loss: 27.415895. Entropy: 0.913100.\n",
      "Iteration 8141: Policy loss: 0.527486. Value loss: 13.369412. Entropy: 0.900312.\n",
      "Iteration 8142: Policy loss: 0.446701. Value loss: 12.316551. Entropy: 0.889629.\n",
      "episode: 3627   score: 230.0  epsilon: 1.0    steps: 58  evaluation reward: 294.5\n",
      "episode: 3628   score: 395.0  epsilon: 1.0    steps: 955  evaluation reward: 295.15\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8143: Policy loss: 0.195329. Value loss: 20.308372. Entropy: 0.888437.\n",
      "Iteration 8144: Policy loss: 0.020366. Value loss: 14.213774. Entropy: 0.902648.\n",
      "Iteration 8145: Policy loss: 0.121224. Value loss: 11.811977. Entropy: 0.886515.\n",
      "episode: 3629   score: 285.0  epsilon: 1.0    steps: 620  evaluation reward: 295.25\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8146: Policy loss: 3.179238. Value loss: 25.615416. Entropy: 0.844194.\n",
      "Iteration 8147: Policy loss: 3.133587. Value loss: 14.081835. Entropy: 0.846138.\n",
      "Iteration 8148: Policy loss: 3.196559. Value loss: 10.268125. Entropy: 0.858913.\n",
      "episode: 3630   score: 180.0  epsilon: 1.0    steps: 436  evaluation reward: 295.5\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8149: Policy loss: -1.092389. Value loss: 28.814062. Entropy: 0.826274.\n",
      "Iteration 8150: Policy loss: -0.816802. Value loss: 15.744095. Entropy: 0.838143.\n",
      "Iteration 8151: Policy loss: -1.069863. Value loss: 12.171336. Entropy: 0.824982.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8152: Policy loss: 1.104236. Value loss: 17.079779. Entropy: 0.791282.\n",
      "Iteration 8153: Policy loss: 0.896104. Value loss: 10.019421. Entropy: 0.808837.\n",
      "Iteration 8154: Policy loss: 1.092725. Value loss: 6.940225. Entropy: 0.803412.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8155: Policy loss: 3.826169. Value loss: 23.872663. Entropy: 0.906224.\n",
      "Iteration 8156: Policy loss: 4.178319. Value loss: 13.034253. Entropy: 0.932454.\n",
      "Iteration 8157: Policy loss: 4.018959. Value loss: 10.737660. Entropy: 0.910067.\n",
      "episode: 3631   score: 210.0  epsilon: 1.0    steps: 154  evaluation reward: 295.3\n",
      "episode: 3632   score: 230.0  epsilon: 1.0    steps: 381  evaluation reward: 295.2\n",
      "episode: 3633   score: 310.0  epsilon: 1.0    steps: 666  evaluation reward: 295.6\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8158: Policy loss: 1.068425. Value loss: 24.311533. Entropy: 0.759897.\n",
      "Iteration 8159: Policy loss: 0.925966. Value loss: 13.338961. Entropy: 0.776089.\n",
      "Iteration 8160: Policy loss: 1.113077. Value loss: 11.713883. Entropy: 0.795949.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8161: Policy loss: 1.160137. Value loss: 19.129425. Entropy: 0.853186.\n",
      "Iteration 8162: Policy loss: 1.111065. Value loss: 11.507159. Entropy: 0.854762.\n",
      "Iteration 8163: Policy loss: 1.210159. Value loss: 8.391063. Entropy: 0.860028.\n",
      "episode: 3634   score: 285.0  epsilon: 1.0    steps: 988  evaluation reward: 295.0\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8164: Policy loss: 0.766389. Value loss: 31.087770. Entropy: 0.885709.\n",
      "Iteration 8165: Policy loss: 1.321771. Value loss: 14.282893. Entropy: 0.904039.\n",
      "Iteration 8166: Policy loss: 0.847163. Value loss: 11.951438. Entropy: 0.888991.\n",
      "episode: 3635   score: 285.0  epsilon: 1.0    steps: 490  evaluation reward: 295.45\n",
      "episode: 3636   score: 320.0  epsilon: 1.0    steps: 803  evaluation reward: 298.0\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8167: Policy loss: -0.093296. Value loss: 14.335042. Entropy: 0.826248.\n",
      "Iteration 8168: Policy loss: -0.127202. Value loss: 9.104424. Entropy: 0.827174.\n",
      "Iteration 8169: Policy loss: -0.176950. Value loss: 6.775196. Entropy: 0.835752.\n",
      "episode: 3637   score: 320.0  epsilon: 1.0    steps: 49  evaluation reward: 294.4\n",
      "episode: 3638   score: 290.0  epsilon: 1.0    steps: 603  evaluation reward: 294.4\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8170: Policy loss: 1.036765. Value loss: 27.363232. Entropy: 0.798463.\n",
      "Iteration 8171: Policy loss: 1.103652. Value loss: 17.066437. Entropy: 0.825881.\n",
      "Iteration 8172: Policy loss: 0.782278. Value loss: 14.469979. Entropy: 0.818824.\n",
      "episode: 3639   score: 225.0  epsilon: 1.0    steps: 238  evaluation reward: 294.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3640   score: 180.0  epsilon: 1.0    steps: 677  evaluation reward: 291.95\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8173: Policy loss: 1.823640. Value loss: 19.669361. Entropy: 0.878253.\n",
      "Iteration 8174: Policy loss: 1.701594. Value loss: 9.846487. Entropy: 0.900078.\n",
      "Iteration 8175: Policy loss: 1.724483. Value loss: 8.785608. Entropy: 0.893516.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8176: Policy loss: -0.951289. Value loss: 239.895905. Entropy: 1.020871.\n",
      "Iteration 8177: Policy loss: 0.137325. Value loss: 129.832581. Entropy: 0.980994.\n",
      "Iteration 8178: Policy loss: -0.601462. Value loss: 152.710983. Entropy: 0.963795.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8179: Policy loss: 0.560190. Value loss: 29.400097. Entropy: 0.786294.\n",
      "Iteration 8180: Policy loss: 0.756289. Value loss: 16.375431. Entropy: 0.777671.\n",
      "Iteration 8181: Policy loss: 0.458854. Value loss: 11.683267. Entropy: 0.788940.\n",
      "episode: 3641   score: 210.0  epsilon: 1.0    steps: 460  evaluation reward: 288.4\n",
      "episode: 3642   score: 210.0  epsilon: 1.0    steps: 826  evaluation reward: 288.4\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8182: Policy loss: -0.541692. Value loss: 27.193468. Entropy: 0.817726.\n",
      "Iteration 8183: Policy loss: -0.516351. Value loss: 13.485807. Entropy: 0.831569.\n",
      "Iteration 8184: Policy loss: -0.683159. Value loss: 13.000286. Entropy: 0.821780.\n",
      "episode: 3643   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 287.85\n",
      "episode: 3644   score: 290.0  epsilon: 1.0    steps: 264  evaluation reward: 288.95\n",
      "episode: 3645   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 288.9\n",
      "episode: 3646   score: 470.0  epsilon: 1.0    steps: 950  evaluation reward: 289.6\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8185: Policy loss: -0.040619. Value loss: 21.601763. Entropy: 0.842995.\n",
      "Iteration 8186: Policy loss: -0.143924. Value loss: 15.992049. Entropy: 0.877758.\n",
      "Iteration 8187: Policy loss: -0.181611. Value loss: 13.929205. Entropy: 0.853797.\n",
      "episode: 3647   score: 210.0  epsilon: 1.0    steps: 225  evaluation reward: 288.9\n",
      "episode: 3648   score: 155.0  epsilon: 1.0    steps: 670  evaluation reward: 287.9\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8188: Policy loss: -1.920451. Value loss: 26.741798. Entropy: 0.810179.\n",
      "Iteration 8189: Policy loss: -1.713428. Value loss: 18.625999. Entropy: 0.820803.\n",
      "Iteration 8190: Policy loss: -1.840794. Value loss: 17.034319. Entropy: 0.811093.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8191: Policy loss: 0.048660. Value loss: 22.161411. Entropy: 1.027808.\n",
      "Iteration 8192: Policy loss: -0.152069. Value loss: 15.293970. Entropy: 1.016477.\n",
      "Iteration 8193: Policy loss: -0.026203. Value loss: 11.611393. Entropy: 1.010726.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8194: Policy loss: 4.730738. Value loss: 48.519825. Entropy: 1.029622.\n",
      "Iteration 8195: Policy loss: 4.992741. Value loss: 19.098631. Entropy: 1.053808.\n",
      "Iteration 8196: Policy loss: 4.657785. Value loss: 15.075499. Entropy: 1.035645.\n",
      "episode: 3649   score: 130.0  epsilon: 1.0    steps: 495  evaluation reward: 287.1\n",
      "episode: 3650   score: 240.0  epsilon: 1.0    steps: 889  evaluation reward: 286.9\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8197: Policy loss: 4.084026. Value loss: 34.078053. Entropy: 0.893341.\n",
      "Iteration 8198: Policy loss: 3.528478. Value loss: 16.039246. Entropy: 0.861383.\n",
      "Iteration 8199: Policy loss: 3.860180. Value loss: 13.444622. Entropy: 0.868854.\n",
      "now time :  2019-02-25 21:13:22.107209\n",
      "episode: 3651   score: 210.0  epsilon: 1.0    steps: 94  evaluation reward: 286.1\n",
      "episode: 3652   score: 150.0  epsilon: 1.0    steps: 259  evaluation reward: 284.7\n",
      "episode: 3653   score: 230.0  epsilon: 1.0    steps: 584  evaluation reward: 284.6\n",
      "episode: 3654   score: 245.0  epsilon: 1.0    steps: 1006  evaluation reward: 283.9\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8200: Policy loss: -0.149931. Value loss: 12.509145. Entropy: 0.805723.\n",
      "Iteration 8201: Policy loss: -0.002860. Value loss: 10.049146. Entropy: 0.841610.\n",
      "Iteration 8202: Policy loss: -0.165611. Value loss: 9.242222. Entropy: 0.862392.\n",
      "episode: 3655   score: 210.0  epsilon: 1.0    steps: 211  evaluation reward: 283.05\n",
      "episode: 3656   score: 180.0  epsilon: 1.0    steps: 697  evaluation reward: 281.55\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8203: Policy loss: -0.370799. Value loss: 23.193380. Entropy: 0.747994.\n",
      "Iteration 8204: Policy loss: -0.307050. Value loss: 17.026609. Entropy: 0.768697.\n",
      "Iteration 8205: Policy loss: -0.375931. Value loss: 14.565166. Entropy: 0.766414.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8206: Policy loss: 0.769567. Value loss: 21.141966. Entropy: 0.907849.\n",
      "Iteration 8207: Policy loss: 0.764706. Value loss: 14.481459. Entropy: 0.920772.\n",
      "Iteration 8208: Policy loss: 0.637356. Value loss: 12.842471. Entropy: 0.921631.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8209: Policy loss: 1.409015. Value loss: 21.696184. Entropy: 0.873493.\n",
      "Iteration 8210: Policy loss: 1.031920. Value loss: 11.987235. Entropy: 0.883503.\n",
      "Iteration 8211: Policy loss: 1.461988. Value loss: 8.630172. Entropy: 0.857489.\n",
      "episode: 3657   score: 180.0  epsilon: 1.0    steps: 469  evaluation reward: 281.1\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8212: Policy loss: 1.134359. Value loss: 18.946316. Entropy: 0.741897.\n",
      "Iteration 8213: Policy loss: 1.056706. Value loss: 11.960663. Entropy: 0.747558.\n",
      "Iteration 8214: Policy loss: 1.275696. Value loss: 9.139993. Entropy: 0.740329.\n",
      "episode: 3658   score: 225.0  epsilon: 1.0    steps: 804  evaluation reward: 280.5\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8215: Policy loss: -0.011628. Value loss: 26.963800. Entropy: 0.744310.\n",
      "Iteration 8216: Policy loss: -0.243980. Value loss: 14.203225. Entropy: 0.707678.\n",
      "Iteration 8217: Policy loss: -0.087486. Value loss: 12.102430. Entropy: 0.697788.\n",
      "episode: 3659   score: 210.0  epsilon: 1.0    steps: 219  evaluation reward: 279.0\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8218: Policy loss: -3.238799. Value loss: 135.374207. Entropy: 0.777166.\n",
      "Iteration 8219: Policy loss: -3.882821. Value loss: 108.308754. Entropy: 0.743222.\n",
      "Iteration 8220: Policy loss: -4.203036. Value loss: 68.625397. Entropy: 0.728767.\n",
      "episode: 3660   score: 305.0  epsilon: 1.0    steps: 84  evaluation reward: 279.65\n",
      "episode: 3661   score: 180.0  epsilon: 1.0    steps: 266  evaluation reward: 277.2\n",
      "episode: 3662   score: 215.0  epsilon: 1.0    steps: 537  evaluation reward: 277.0\n",
      "episode: 3663   score: 285.0  epsilon: 1.0    steps: 727  evaluation reward: 277.75\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8221: Policy loss: -0.459458. Value loss: 32.610649. Entropy: 0.890603.\n",
      "Iteration 8222: Policy loss: -0.466360. Value loss: 20.601559. Entropy: 0.871575.\n",
      "Iteration 8223: Policy loss: -0.511359. Value loss: 16.689278. Entropy: 0.871754.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8224: Policy loss: 0.119940. Value loss: 18.977203. Entropy: 0.678636.\n",
      "Iteration 8225: Policy loss: -0.345666. Value loss: 13.600538. Entropy: 0.687281.\n",
      "Iteration 8226: Policy loss: -0.151739. Value loss: 10.414887. Entropy: 0.683786.\n",
      "episode: 3664   score: 545.0  epsilon: 1.0    steps: 1001  evaluation reward: 279.6\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8227: Policy loss: 0.133830. Value loss: 39.000149. Entropy: 0.788509.\n",
      "Iteration 8228: Policy loss: 0.307400. Value loss: 20.388805. Entropy: 0.797666.\n",
      "Iteration 8229: Policy loss: 0.066787. Value loss: 17.889675. Entropy: 0.801154.\n",
      "episode: 3665   score: 240.0  epsilon: 1.0    steps: 810  evaluation reward: 279.9\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8230: Policy loss: 1.622440. Value loss: 27.931061. Entropy: 0.830027.\n",
      "Iteration 8231: Policy loss: 1.584087. Value loss: 14.994953. Entropy: 0.842148.\n",
      "Iteration 8232: Policy loss: 1.849000. Value loss: 11.454021. Entropy: 0.853978.\n",
      "episode: 3666   score: 235.0  epsilon: 1.0    steps: 122  evaluation reward: 277.95\n",
      "episode: 3667   score: 190.0  epsilon: 1.0    steps: 238  evaluation reward: 277.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8233: Policy loss: -0.565410. Value loss: 25.862061. Entropy: 0.819463.\n",
      "Iteration 8234: Policy loss: -0.532010. Value loss: 19.309992. Entropy: 0.820505.\n",
      "Iteration 8235: Policy loss: -0.552505. Value loss: 15.867217. Entropy: 0.807700.\n",
      "episode: 3668   score: 155.0  epsilon: 1.0    steps: 286  evaluation reward: 274.9\n",
      "episode: 3669   score: 375.0  epsilon: 1.0    steps: 413  evaluation reward: 276.55\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8236: Policy loss: 0.330941. Value loss: 19.769186. Entropy: 0.912126.\n",
      "Iteration 8237: Policy loss: 0.406905. Value loss: 12.852826. Entropy: 0.916192.\n",
      "Iteration 8238: Policy loss: 0.382259. Value loss: 10.349863. Entropy: 0.921047.\n",
      "episode: 3670   score: 200.0  epsilon: 1.0    steps: 528  evaluation reward: 274.3\n",
      "episode: 3671   score: 225.0  epsilon: 1.0    steps: 663  evaluation reward: 271.4\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8239: Policy loss: -0.780875. Value loss: 234.665115. Entropy: 0.706161.\n",
      "Iteration 8240: Policy loss: -1.569190. Value loss: 139.602554. Entropy: 0.670445.\n",
      "Iteration 8241: Policy loss: -0.624007. Value loss: 60.654480. Entropy: 0.595654.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8242: Policy loss: -2.510067. Value loss: 31.990667. Entropy: 0.727595.\n",
      "Iteration 8243: Policy loss: -2.906457. Value loss: 22.295038. Entropy: 0.733069.\n",
      "Iteration 8244: Policy loss: -2.966163. Value loss: 19.034264. Entropy: 0.735218.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8245: Policy loss: 1.603216. Value loss: 113.550285. Entropy: 0.786340.\n",
      "Iteration 8246: Policy loss: 0.915961. Value loss: 103.641693. Entropy: 0.796453.\n",
      "Iteration 8247: Policy loss: 1.086470. Value loss: 70.913864. Entropy: 0.753001.\n",
      "episode: 3672   score: 225.0  epsilon: 1.0    steps: 252  evaluation reward: 271.05\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8248: Policy loss: 0.928599. Value loss: 46.850899. Entropy: 0.802840.\n",
      "Iteration 8249: Policy loss: 0.646931. Value loss: 23.098198. Entropy: 0.796758.\n",
      "Iteration 8250: Policy loss: 0.834366. Value loss: 18.491051. Entropy: 0.803352.\n",
      "episode: 3673   score: 255.0  epsilon: 1.0    steps: 446  evaluation reward: 268.4\n",
      "episode: 3674   score: 295.0  epsilon: 1.0    steps: 852  evaluation reward: 268.15\n",
      "episode: 3675   score: 545.0  epsilon: 1.0    steps: 980  evaluation reward: 271.0\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8251: Policy loss: -1.457678. Value loss: 39.852680. Entropy: 0.800987.\n",
      "Iteration 8252: Policy loss: -1.180450. Value loss: 23.986921. Entropy: 0.792159.\n",
      "Iteration 8253: Policy loss: -1.277679. Value loss: 18.896589. Entropy: 0.765336.\n",
      "episode: 3676   score: 335.0  epsilon: 1.0    steps: 28  evaluation reward: 271.2\n",
      "episode: 3677   score: 180.0  epsilon: 1.0    steps: 698  evaluation reward: 270.25\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8254: Policy loss: 1.611301. Value loss: 40.171204. Entropy: 0.651191.\n",
      "Iteration 8255: Policy loss: 1.541691. Value loss: 21.956285. Entropy: 0.658427.\n",
      "Iteration 8256: Policy loss: 1.540424. Value loss: 19.547050. Entropy: 0.641152.\n",
      "episode: 3678   score: 285.0  epsilon: 1.0    steps: 532  evaluation reward: 271.75\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8257: Policy loss: 0.839109. Value loss: 16.770407. Entropy: 0.796497.\n",
      "Iteration 8258: Policy loss: 0.747675. Value loss: 7.598825. Entropy: 0.800933.\n",
      "Iteration 8259: Policy loss: 0.790307. Value loss: 6.742012. Entropy: 0.769500.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8260: Policy loss: 0.390006. Value loss: 20.610474. Entropy: 0.790674.\n",
      "Iteration 8261: Policy loss: 0.282608. Value loss: 9.869608. Entropy: 0.799009.\n",
      "Iteration 8262: Policy loss: 0.187466. Value loss: 9.428949. Entropy: 0.773278.\n",
      "episode: 3679   score: 620.0  epsilon: 1.0    steps: 283  evaluation reward: 275.35\n",
      "episode: 3680   score: 110.0  epsilon: 1.0    steps: 827  evaluation reward: 273.0\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8263: Policy loss: 1.758578. Value loss: 27.699360. Entropy: 0.775430.\n",
      "Iteration 8264: Policy loss: 1.695554. Value loss: 15.287827. Entropy: 0.773647.\n",
      "Iteration 8265: Policy loss: 1.524813. Value loss: 13.131651. Entropy: 0.764824.\n",
      "episode: 3681   score: 155.0  epsilon: 1.0    steps: 69  evaluation reward: 272.05\n",
      "episode: 3682   score: 180.0  epsilon: 1.0    steps: 909  evaluation reward: 272.05\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8266: Policy loss: 2.499933. Value loss: 40.764267. Entropy: 0.684183.\n",
      "Iteration 8267: Policy loss: 2.616681. Value loss: 23.833109. Entropy: 0.694925.\n",
      "Iteration 8268: Policy loss: 2.655077. Value loss: 17.048012. Entropy: 0.716416.\n",
      "episode: 3683   score: 180.0  epsilon: 1.0    steps: 704  evaluation reward: 270.55\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8269: Policy loss: 1.213990. Value loss: 28.886303. Entropy: 0.811418.\n",
      "Iteration 8270: Policy loss: 1.567353. Value loss: 13.527067. Entropy: 0.826172.\n",
      "Iteration 8271: Policy loss: 1.169595. Value loss: 11.124445. Entropy: 0.806040.\n",
      "episode: 3684   score: 285.0  epsilon: 1.0    steps: 209  evaluation reward: 269.4\n",
      "episode: 3685   score: 280.0  epsilon: 1.0    steps: 439  evaluation reward: 268.85\n",
      "episode: 3686   score: 210.0  epsilon: 1.0    steps: 611  evaluation reward: 264.75\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8272: Policy loss: 1.125577. Value loss: 35.544178. Entropy: 0.875692.\n",
      "Iteration 8273: Policy loss: 0.835824. Value loss: 16.682016. Entropy: 0.855696.\n",
      "Iteration 8274: Policy loss: 0.717172. Value loss: 14.580633. Entropy: 0.867539.\n",
      "episode: 3687   score: 125.0  epsilon: 1.0    steps: 300  evaluation reward: 262.9\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8275: Policy loss: 1.503138. Value loss: 22.323298. Entropy: 0.804958.\n",
      "Iteration 8276: Policy loss: 1.287123. Value loss: 11.676695. Entropy: 0.823226.\n",
      "Iteration 8277: Policy loss: 1.574515. Value loss: 12.323679. Entropy: 0.831518.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8278: Policy loss: 0.796129. Value loss: 20.713240. Entropy: 0.729061.\n",
      "Iteration 8279: Policy loss: 0.949581. Value loss: 11.879714. Entropy: 0.738619.\n",
      "Iteration 8280: Policy loss: 0.756892. Value loss: 9.792892. Entropy: 0.734631.\n",
      "episode: 3688   score: 290.0  epsilon: 1.0    steps: 896  evaluation reward: 263.55\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8281: Policy loss: -0.136158. Value loss: 16.027225. Entropy: 0.817433.\n",
      "Iteration 8282: Policy loss: -0.021766. Value loss: 9.400622. Entropy: 0.808540.\n",
      "Iteration 8283: Policy loss: -0.137049. Value loss: 6.656325. Entropy: 0.818205.\n",
      "episode: 3689   score: 210.0  epsilon: 1.0    steps: 95  evaluation reward: 262.4\n",
      "episode: 3690   score: 105.0  epsilon: 1.0    steps: 586  evaluation reward: 261.35\n",
      "episode: 3691   score: 235.0  epsilon: 1.0    steps: 972  evaluation reward: 257.65\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8284: Policy loss: -2.365904. Value loss: 245.283020. Entropy: 0.773262.\n",
      "Iteration 8285: Policy loss: -2.331878. Value loss: 126.352562. Entropy: 0.738356.\n",
      "Iteration 8286: Policy loss: -1.722999. Value loss: 93.931320. Entropy: 0.710038.\n",
      "episode: 3692   score: 180.0  epsilon: 1.0    steps: 370  evaluation reward: 256.6\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8287: Policy loss: -0.454507. Value loss: 52.009380. Entropy: 0.703008.\n",
      "Iteration 8288: Policy loss: -0.658458. Value loss: 24.876842. Entropy: 0.698832.\n",
      "Iteration 8289: Policy loss: -0.594596. Value loss: 17.902946. Entropy: 0.684524.\n",
      "episode: 3693   score: 460.0  epsilon: 1.0    steps: 418  evaluation reward: 257.4\n",
      "episode: 3694   score: 470.0  epsilon: 1.0    steps: 723  evaluation reward: 259.5\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8290: Policy loss: 1.480590. Value loss: 22.980597. Entropy: 0.822016.\n",
      "Iteration 8291: Policy loss: 1.525996. Value loss: 13.098085. Entropy: 0.817658.\n",
      "Iteration 8292: Policy loss: 1.788594. Value loss: 11.221729. Entropy: 0.795230.\n",
      "episode: 3695   score: 105.0  epsilon: 1.0    steps: 871  evaluation reward: 257.95\n",
      "Training network. lr: 0.000187. clip: 0.074665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8293: Policy loss: 1.404238. Value loss: 28.224007. Entropy: 0.810076.\n",
      "Iteration 8294: Policy loss: 1.409024. Value loss: 16.636559. Entropy: 0.823671.\n",
      "Iteration 8295: Policy loss: 1.592064. Value loss: 12.006332. Entropy: 0.799807.\n",
      "episode: 3696   score: 320.0  epsilon: 1.0    steps: 202  evaluation reward: 258.9\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8296: Policy loss: 3.215753. Value loss: 41.026943. Entropy: 0.744653.\n",
      "Iteration 8297: Policy loss: 3.670957. Value loss: 19.535645. Entropy: 0.754733.\n",
      "Iteration 8298: Policy loss: 3.169511. Value loss: 15.072751. Entropy: 0.760329.\n",
      "episode: 3697   score: 185.0  epsilon: 1.0    steps: 96  evaluation reward: 257.65\n",
      "episode: 3698   score: 150.0  epsilon: 1.0    steps: 593  evaluation reward: 256.65\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8299: Policy loss: 1.511134. Value loss: 20.411165. Entropy: 0.791360.\n",
      "Iteration 8300: Policy loss: 1.596914. Value loss: 11.949772. Entropy: 0.778640.\n",
      "Iteration 8301: Policy loss: 1.299407. Value loss: 7.907803. Entropy: 0.773208.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8302: Policy loss: -0.685719. Value loss: 24.120302. Entropy: 0.763449.\n",
      "Iteration 8303: Policy loss: -0.749667. Value loss: 12.883389. Entropy: 0.777057.\n",
      "Iteration 8304: Policy loss: -0.744330. Value loss: 8.365204. Entropy: 0.755789.\n",
      "episode: 3699   score: 280.0  epsilon: 1.0    steps: 345  evaluation reward: 256.35\n",
      "episode: 3700   score: 180.0  epsilon: 1.0    steps: 692  evaluation reward: 256.05\n",
      "now time :  2019-02-25 21:15:22.715212\n",
      "episode: 3701   score: 285.0  epsilon: 1.0    steps: 1017  evaluation reward: 256.8\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8305: Policy loss: -1.060608. Value loss: 286.074402. Entropy: 0.701099.\n",
      "Iteration 8306: Policy loss: -0.178072. Value loss: 157.449448. Entropy: 0.691226.\n",
      "Iteration 8307: Policy loss: -0.337278. Value loss: 112.153275. Entropy: 0.725437.\n",
      "episode: 3702   score: 400.0  epsilon: 1.0    steps: 452  evaluation reward: 257.85\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8308: Policy loss: 2.600839. Value loss: 29.720060. Entropy: 0.658000.\n",
      "Iteration 8309: Policy loss: 2.350516. Value loss: 16.904217. Entropy: 0.658540.\n",
      "Iteration 8310: Policy loss: 2.433731. Value loss: 14.000420. Entropy: 0.667621.\n",
      "episode: 3703   score: 155.0  epsilon: 1.0    steps: 790  evaluation reward: 257.3\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8311: Policy loss: 0.615547. Value loss: 25.752132. Entropy: 0.623067.\n",
      "Iteration 8312: Policy loss: 0.559424. Value loss: 13.151176. Entropy: 0.605330.\n",
      "Iteration 8313: Policy loss: 0.599456. Value loss: 12.655727. Entropy: 0.609422.\n",
      "episode: 3704   score: 180.0  epsilon: 1.0    steps: 556  evaluation reward: 254.65\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8314: Policy loss: -1.691916. Value loss: 25.400089. Entropy: 0.757142.\n",
      "Iteration 8315: Policy loss: -1.976329. Value loss: 17.014313. Entropy: 0.748162.\n",
      "Iteration 8316: Policy loss: -1.609718. Value loss: 12.592953. Entropy: 0.743165.\n",
      "episode: 3705   score: 135.0  epsilon: 1.0    steps: 753  evaluation reward: 253.75\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8317: Policy loss: -1.653758. Value loss: 42.943935. Entropy: 0.624917.\n",
      "Iteration 8318: Policy loss: -1.319427. Value loss: 19.226727. Entropy: 0.619263.\n",
      "Iteration 8319: Policy loss: -1.117069. Value loss: 13.707396. Entropy: 0.611031.\n",
      "episode: 3706   score: 280.0  epsilon: 1.0    steps: 61  evaluation reward: 254.45\n",
      "episode: 3707   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 254.15\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8320: Policy loss: 1.275055. Value loss: 43.912708. Entropy: 0.657871.\n",
      "Iteration 8321: Policy loss: 0.782365. Value loss: 23.161785. Entropy: 0.655702.\n",
      "Iteration 8322: Policy loss: 0.952441. Value loss: 16.556082. Entropy: 0.653983.\n",
      "episode: 3708   score: 265.0  epsilon: 1.0    steps: 348  evaluation reward: 254.65\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8323: Policy loss: -1.721932. Value loss: 29.913340. Entropy: 0.601318.\n",
      "Iteration 8324: Policy loss: -1.610016. Value loss: 17.717642. Entropy: 0.596212.\n",
      "Iteration 8325: Policy loss: -1.607531. Value loss: 14.116271. Entropy: 0.586606.\n",
      "episode: 3709   score: 450.0  epsilon: 1.0    steps: 243  evaluation reward: 256.0\n",
      "episode: 3710   score: 240.0  epsilon: 1.0    steps: 430  evaluation reward: 255.95\n",
      "episode: 3711   score: 210.0  epsilon: 1.0    steps: 771  evaluation reward: 255.35\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8326: Policy loss: -1.922138. Value loss: 205.756561. Entropy: 0.862186.\n",
      "Iteration 8327: Policy loss: -1.271260. Value loss: 96.090424. Entropy: 0.795291.\n",
      "Iteration 8328: Policy loss: -1.545151. Value loss: 79.144127. Entropy: 0.858021.\n",
      "episode: 3712   score: 215.0  epsilon: 1.0    steps: 595  evaluation reward: 252.05\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8329: Policy loss: 3.535609. Value loss: 91.262512. Entropy: 0.722366.\n",
      "Iteration 8330: Policy loss: 2.977535. Value loss: 23.208881. Entropy: 0.679005.\n",
      "Iteration 8331: Policy loss: 3.538358. Value loss: 20.508902. Entropy: 0.675544.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8332: Policy loss: 2.958271. Value loss: 30.805895. Entropy: 0.684152.\n",
      "Iteration 8333: Policy loss: 2.198239. Value loss: 15.150328. Entropy: 0.686483.\n",
      "Iteration 8334: Policy loss: 2.702189. Value loss: 13.717552. Entropy: 0.668597.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8335: Policy loss: 1.884861. Value loss: 43.891518. Entropy: 0.515503.\n",
      "Iteration 8336: Policy loss: 2.106714. Value loss: 22.571236. Entropy: 0.506652.\n",
      "Iteration 8337: Policy loss: 1.873346. Value loss: 15.731009. Entropy: 0.507328.\n",
      "episode: 3713   score: 565.0  epsilon: 1.0    steps: 755  evaluation reward: 254.95\n",
      "episode: 3714   score: 225.0  epsilon: 1.0    steps: 937  evaluation reward: 255.9\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8338: Policy loss: 1.337044. Value loss: 33.596081. Entropy: 0.563539.\n",
      "Iteration 8339: Policy loss: 1.067031. Value loss: 19.054575. Entropy: 0.558859.\n",
      "Iteration 8340: Policy loss: 1.140234. Value loss: 14.947655. Entropy: 0.563412.\n",
      "episode: 3715   score: 260.0  epsilon: 1.0    steps: 860  evaluation reward: 256.95\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8341: Policy loss: -7.001185. Value loss: 403.231354. Entropy: 0.536413.\n",
      "Iteration 8342: Policy loss: -6.777638. Value loss: 215.950546. Entropy: 0.543224.\n",
      "Iteration 8343: Policy loss: -6.521077. Value loss: 168.379684. Entropy: 0.567484.\n",
      "episode: 3716   score: 425.0  epsilon: 1.0    steps: 242  evaluation reward: 257.0\n",
      "episode: 3717   score: 485.0  epsilon: 1.0    steps: 301  evaluation reward: 258.6\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8344: Policy loss: 4.256891. Value loss: 33.369354. Entropy: 0.698361.\n",
      "Iteration 8345: Policy loss: 4.398925. Value loss: 18.111593. Entropy: 0.689854.\n",
      "Iteration 8346: Policy loss: 4.210628. Value loss: 13.554923. Entropy: 0.691378.\n",
      "episode: 3718   score: 365.0  epsilon: 1.0    steps: 50  evaluation reward: 260.4\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8347: Policy loss: 1.466151. Value loss: 37.371429. Entropy: 0.655511.\n",
      "Iteration 8348: Policy loss: 1.488279. Value loss: 18.385893. Entropy: 0.655507.\n",
      "Iteration 8349: Policy loss: 1.288040. Value loss: 16.850620. Entropy: 0.661918.\n",
      "episode: 3719   score: 230.0  epsilon: 1.0    steps: 524  evaluation reward: 260.35\n",
      "episode: 3720   score: 150.0  epsilon: 1.0    steps: 880  evaluation reward: 259.7\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8350: Policy loss: 1.231203. Value loss: 27.682827. Entropy: 0.690206.\n",
      "Iteration 8351: Policy loss: 1.439558. Value loss: 12.904829. Entropy: 0.728131.\n",
      "Iteration 8352: Policy loss: 1.404546. Value loss: 11.127985. Entropy: 0.734840.\n",
      "episode: 3721   score: 225.0  epsilon: 1.0    steps: 393  evaluation reward: 259.6\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8353: Policy loss: -0.885915. Value loss: 28.296614. Entropy: 0.665330.\n",
      "Iteration 8354: Policy loss: -1.074939. Value loss: 16.753492. Entropy: 0.648405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8355: Policy loss: -1.029521. Value loss: 14.206028. Entropy: 0.652856.\n",
      "episode: 3722   score: 150.0  epsilon: 1.0    steps: 276  evaluation reward: 258.45\n",
      "episode: 3723   score: 170.0  epsilon: 1.0    steps: 701  evaluation reward: 258.05\n",
      "episode: 3724   score: 285.0  epsilon: 1.0    steps: 936  evaluation reward: 256.8\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8356: Policy loss: 2.155053. Value loss: 32.553715. Entropy: 0.809334.\n",
      "Iteration 8357: Policy loss: 2.173289. Value loss: 17.831619. Entropy: 0.805320.\n",
      "Iteration 8358: Policy loss: 2.045562. Value loss: 15.224674. Entropy: 0.836442.\n",
      "episode: 3725   score: 185.0  epsilon: 1.0    steps: 187  evaluation reward: 255.35\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8359: Policy loss: -1.754115. Value loss: 32.148415. Entropy: 0.709439.\n",
      "Iteration 8360: Policy loss: -1.667053. Value loss: 19.894617. Entropy: 0.714691.\n",
      "Iteration 8361: Policy loss: -1.789657. Value loss: 15.788326. Entropy: 0.693784.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8362: Policy loss: -2.976702. Value loss: 221.710724. Entropy: 0.647863.\n",
      "Iteration 8363: Policy loss: -2.904842. Value loss: 177.103897. Entropy: 0.642900.\n",
      "Iteration 8364: Policy loss: -3.095364. Value loss: 164.938660. Entropy: 0.608335.\n",
      "episode: 3726   score: 195.0  epsilon: 1.0    steps: 539  evaluation reward: 254.45\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8365: Policy loss: 1.205642. Value loss: 48.833115. Entropy: 0.618255.\n",
      "Iteration 8366: Policy loss: 1.387722. Value loss: 19.891603. Entropy: 0.601074.\n",
      "Iteration 8367: Policy loss: 1.192873. Value loss: 13.111996. Entropy: 0.634301.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8368: Policy loss: -1.371891. Value loss: 285.717194. Entropy: 0.657876.\n",
      "Iteration 8369: Policy loss: -0.769928. Value loss: 200.059280. Entropy: 0.647062.\n",
      "Iteration 8370: Policy loss: -0.773259. Value loss: 108.709076. Entropy: 0.604271.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8371: Policy loss: 2.538404. Value loss: 53.141270. Entropy: 0.629679.\n",
      "Iteration 8372: Policy loss: 2.329166. Value loss: 23.927427. Entropy: 0.637915.\n",
      "Iteration 8373: Policy loss: 2.223966. Value loss: 18.099352. Entropy: 0.653520.\n",
      "episode: 3727   score: 290.0  epsilon: 1.0    steps: 944  evaluation reward: 255.05\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8374: Policy loss: -1.324648. Value loss: 38.568569. Entropy: 0.665350.\n",
      "Iteration 8375: Policy loss: -1.152259. Value loss: 22.734442. Entropy: 0.675617.\n",
      "Iteration 8376: Policy loss: -1.102935. Value loss: 16.246504. Entropy: 0.685996.\n",
      "episode: 3728   score: 620.0  epsilon: 1.0    steps: 508  evaluation reward: 257.3\n",
      "episode: 3729   score: 290.0  epsilon: 1.0    steps: 838  evaluation reward: 257.35\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8377: Policy loss: -2.605524. Value loss: 32.020840. Entropy: 0.657268.\n",
      "Iteration 8378: Policy loss: -2.181881. Value loss: 19.677799. Entropy: 0.648530.\n",
      "Iteration 8379: Policy loss: -2.375710. Value loss: 16.485523. Entropy: 0.665791.\n",
      "episode: 3730   score: 460.0  epsilon: 1.0    steps: 380  evaluation reward: 260.15\n",
      "episode: 3731   score: 320.0  epsilon: 1.0    steps: 655  evaluation reward: 261.25\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8380: Policy loss: -1.627714. Value loss: 55.854946. Entropy: 0.860428.\n",
      "Iteration 8381: Policy loss: -1.356515. Value loss: 23.274908. Entropy: 0.857201.\n",
      "Iteration 8382: Policy loss: -1.726694. Value loss: 16.211916. Entropy: 0.855570.\n",
      "episode: 3732   score: 245.0  epsilon: 1.0    steps: 113  evaluation reward: 261.4\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8383: Policy loss: 1.853800. Value loss: 45.595463. Entropy: 0.679478.\n",
      "Iteration 8384: Policy loss: 1.913171. Value loss: 23.147612. Entropy: 0.698674.\n",
      "Iteration 8385: Policy loss: 1.851131. Value loss: 18.746616. Entropy: 0.693884.\n",
      "episode: 3733   score: 680.0  epsilon: 1.0    steps: 215  evaluation reward: 265.1\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8386: Policy loss: 0.210295. Value loss: 22.410513. Entropy: 0.769098.\n",
      "Iteration 8387: Policy loss: 0.174627. Value loss: 13.942808. Entropy: 0.772245.\n",
      "Iteration 8388: Policy loss: 0.276766. Value loss: 11.504154. Entropy: 0.757460.\n",
      "episode: 3734   score: 265.0  epsilon: 1.0    steps: 516  evaluation reward: 264.9\n",
      "episode: 3735   score: 120.0  epsilon: 1.0    steps: 724  evaluation reward: 263.25\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8389: Policy loss: 0.816289. Value loss: 17.438257. Entropy: 0.699908.\n",
      "Iteration 8390: Policy loss: 0.820734. Value loss: 9.894142. Entropy: 0.698705.\n",
      "Iteration 8391: Policy loss: 0.791269. Value loss: 8.555368. Entropy: 0.692584.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8392: Policy loss: 2.644044. Value loss: 20.010998. Entropy: 0.609548.\n",
      "Iteration 8393: Policy loss: 2.529744. Value loss: 10.992277. Entropy: 0.647381.\n",
      "Iteration 8394: Policy loss: 2.610684. Value loss: 9.337515. Entropy: 0.630889.\n",
      "episode: 3736   score: 225.0  epsilon: 1.0    steps: 455  evaluation reward: 262.3\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8395: Policy loss: 0.332013. Value loss: 25.441366. Entropy: 0.636862.\n",
      "Iteration 8396: Policy loss: 0.376862. Value loss: 13.603968. Entropy: 0.613714.\n",
      "Iteration 8397: Policy loss: 0.241535. Value loss: 12.074080. Entropy: 0.631454.\n",
      "episode: 3737   score: 210.0  epsilon: 1.0    steps: 893  evaluation reward: 261.2\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8398: Policy loss: 0.586170. Value loss: 27.446615. Entropy: 0.558239.\n",
      "Iteration 8399: Policy loss: 0.596709. Value loss: 17.394735. Entropy: 0.567251.\n",
      "Iteration 8400: Policy loss: 0.629190. Value loss: 14.855639. Entropy: 0.561723.\n",
      "episode: 3738   score: 210.0  epsilon: 1.0    steps: 134  evaluation reward: 260.4\n",
      "episode: 3739   score: 205.0  epsilon: 1.0    steps: 279  evaluation reward: 260.2\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8401: Policy loss: -3.194238. Value loss: 231.578522. Entropy: 0.680474.\n",
      "Iteration 8402: Policy loss: -3.102362. Value loss: 158.248367. Entropy: 0.681699.\n",
      "Iteration 8403: Policy loss: -3.514148. Value loss: 140.602905. Entropy: 0.630580.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8404: Policy loss: -0.332163. Value loss: 31.958775. Entropy: 0.528259.\n",
      "Iteration 8405: Policy loss: -0.717140. Value loss: 16.841888. Entropy: 0.498035.\n",
      "Iteration 8406: Policy loss: -0.453582. Value loss: 14.541265. Entropy: 0.490420.\n",
      "episode: 3740   score: 245.0  epsilon: 1.0    steps: 541  evaluation reward: 260.85\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8407: Policy loss: -1.405877. Value loss: 188.423370. Entropy: 0.819174.\n",
      "Iteration 8408: Policy loss: -1.518751. Value loss: 90.031631. Entropy: 0.754789.\n",
      "Iteration 8409: Policy loss: -0.906666. Value loss: 61.348545. Entropy: 0.693894.\n",
      "episode: 3741   score: 560.0  epsilon: 1.0    steps: 997  evaluation reward: 264.35\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8410: Policy loss: 2.429308. Value loss: 51.363892. Entropy: 0.474070.\n",
      "Iteration 8411: Policy loss: 2.274244. Value loss: 20.371675. Entropy: 0.464476.\n",
      "Iteration 8412: Policy loss: 2.678532. Value loss: 17.037006. Entropy: 0.438340.\n",
      "episode: 3742   score: 260.0  epsilon: 1.0    steps: 443  evaluation reward: 264.85\n",
      "episode: 3743   score: 345.0  epsilon: 1.0    steps: 699  evaluation reward: 266.2\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8413: Policy loss: 1.686067. Value loss: 32.211288. Entropy: 0.535665.\n",
      "Iteration 8414: Policy loss: 1.839092. Value loss: 17.728834. Entropy: 0.549469.\n",
      "Iteration 8415: Policy loss: 1.732543. Value loss: 15.945146. Entropy: 0.540411.\n",
      "episode: 3744   score: 225.0  epsilon: 1.0    steps: 846  evaluation reward: 265.55\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8416: Policy loss: 3.629832. Value loss: 27.951069. Entropy: 0.530336.\n",
      "Iteration 8417: Policy loss: 3.709461. Value loss: 17.076637. Entropy: 0.539127.\n",
      "Iteration 8418: Policy loss: 3.805100. Value loss: 12.935917. Entropy: 0.546382.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8419: Policy loss: 0.575359. Value loss: 27.442684. Entropy: 0.701117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8420: Policy loss: 0.345297. Value loss: 13.688594. Entropy: 0.722639.\n",
      "Iteration 8421: Policy loss: 0.452234. Value loss: 10.877520. Entropy: 0.703229.\n",
      "episode: 3745   score: 435.0  epsilon: 1.0    steps: 58  evaluation reward: 267.8\n",
      "episode: 3746   score: 285.0  epsilon: 1.0    steps: 382  evaluation reward: 265.95\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8422: Policy loss: -0.291779. Value loss: 38.656403. Entropy: 0.546464.\n",
      "Iteration 8423: Policy loss: -0.477453. Value loss: 18.296604. Entropy: 0.534585.\n",
      "Iteration 8424: Policy loss: -0.411683. Value loss: 13.540089. Entropy: 0.531050.\n",
      "episode: 3747   score: 340.0  epsilon: 1.0    steps: 143  evaluation reward: 267.25\n",
      "episode: 3748   score: 180.0  epsilon: 1.0    steps: 472  evaluation reward: 267.5\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8425: Policy loss: 1.426024. Value loss: 31.506277. Entropy: 0.615977.\n",
      "Iteration 8426: Policy loss: 1.836607. Value loss: 20.226072. Entropy: 0.585717.\n",
      "Iteration 8427: Policy loss: 1.655639. Value loss: 14.248897. Entropy: 0.599283.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8428: Policy loss: -0.326024. Value loss: 30.927519. Entropy: 0.492139.\n",
      "Iteration 8429: Policy loss: -0.277619. Value loss: 17.976999. Entropy: 0.510398.\n",
      "Iteration 8430: Policy loss: -0.233568. Value loss: 13.806252. Entropy: 0.491172.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8431: Policy loss: 1.565348. Value loss: 32.829014. Entropy: 0.563148.\n",
      "Iteration 8432: Policy loss: 1.570790. Value loss: 21.070240. Entropy: 0.578413.\n",
      "Iteration 8433: Policy loss: 1.607017. Value loss: 16.747345. Entropy: 0.572522.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8434: Policy loss: 0.156130. Value loss: 26.636572. Entropy: 0.572683.\n",
      "Iteration 8435: Policy loss: 0.229817. Value loss: 12.735328. Entropy: 0.563206.\n",
      "Iteration 8436: Policy loss: 0.195109. Value loss: 10.631562. Entropy: 0.575877.\n",
      "episode: 3749   score: 320.0  epsilon: 1.0    steps: 532  evaluation reward: 269.4\n",
      "episode: 3750   score: 285.0  epsilon: 1.0    steps: 658  evaluation reward: 269.85\n",
      "now time :  2019-02-25 21:17:50.229058\n",
      "episode: 3751   score: 430.0  epsilon: 1.0    steps: 974  evaluation reward: 272.05\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8437: Policy loss: -2.188191. Value loss: 34.353893. Entropy: 0.570462.\n",
      "Iteration 8438: Policy loss: -1.963470. Value loss: 22.421139. Entropy: 0.524707.\n",
      "Iteration 8439: Policy loss: -2.104549. Value loss: 19.655403. Entropy: 0.562403.\n",
      "episode: 3752   score: 240.0  epsilon: 1.0    steps: 313  evaluation reward: 272.95\n",
      "episode: 3753   score: 210.0  epsilon: 1.0    steps: 405  evaluation reward: 272.75\n",
      "episode: 3754   score: 315.0  epsilon: 1.0    steps: 839  evaluation reward: 273.45\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8440: Policy loss: -0.359039. Value loss: 23.376726. Entropy: 0.576430.\n",
      "Iteration 8441: Policy loss: -0.318427. Value loss: 17.904730. Entropy: 0.590376.\n",
      "Iteration 8442: Policy loss: -0.394313. Value loss: 12.873461. Entropy: 0.587928.\n",
      "episode: 3755   score: 255.0  epsilon: 1.0    steps: 184  evaluation reward: 273.9\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8443: Policy loss: -0.594265. Value loss: 23.940311. Entropy: 0.734062.\n",
      "Iteration 8444: Policy loss: -0.562919. Value loss: 16.222179. Entropy: 0.738246.\n",
      "Iteration 8445: Policy loss: -0.487646. Value loss: 12.980970. Entropy: 0.734431.\n",
      "episode: 3756   score: 285.0  epsilon: 1.0    steps: 44  evaluation reward: 274.95\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8446: Policy loss: -1.646091. Value loss: 184.794388. Entropy: 0.795652.\n",
      "Iteration 8447: Policy loss: -1.448321. Value loss: 64.227753. Entropy: 0.782777.\n",
      "Iteration 8448: Policy loss: -1.517432. Value loss: 42.433918. Entropy: 0.806612.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8449: Policy loss: 0.738226. Value loss: 75.799843. Entropy: 0.607807.\n",
      "Iteration 8450: Policy loss: 0.931131. Value loss: 28.767565. Entropy: 0.613024.\n",
      "Iteration 8451: Policy loss: 0.709946. Value loss: 19.647959. Entropy: 0.584861.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8452: Policy loss: -0.110921. Value loss: 27.024151. Entropy: 0.411113.\n",
      "Iteration 8453: Policy loss: 0.381754. Value loss: 13.607450. Entropy: 0.409411.\n",
      "Iteration 8454: Policy loss: -0.221943. Value loss: 11.119546. Entropy: 0.387775.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8455: Policy loss: -0.733515. Value loss: 52.943058. Entropy: 0.541448.\n",
      "Iteration 8456: Policy loss: -0.556961. Value loss: 24.402246. Entropy: 0.525294.\n",
      "Iteration 8457: Policy loss: -0.959412. Value loss: 16.933918. Entropy: 0.532104.\n",
      "episode: 3757   score: 335.0  epsilon: 1.0    steps: 763  evaluation reward: 276.5\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8458: Policy loss: -1.601433. Value loss: 35.665310. Entropy: 0.512466.\n",
      "Iteration 8459: Policy loss: -1.204473. Value loss: 17.956661. Entropy: 0.519964.\n",
      "Iteration 8460: Policy loss: -1.216357. Value loss: 13.420074. Entropy: 0.515039.\n",
      "episode: 3758   score: 335.0  epsilon: 1.0    steps: 268  evaluation reward: 277.6\n",
      "episode: 3759   score: 390.0  epsilon: 1.0    steps: 597  evaluation reward: 279.4\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8461: Policy loss: 1.239945. Value loss: 32.973248. Entropy: 0.567901.\n",
      "Iteration 8462: Policy loss: 1.227100. Value loss: 19.209085. Entropy: 0.581287.\n",
      "Iteration 8463: Policy loss: 1.053140. Value loss: 16.717747. Entropy: 0.581564.\n",
      "episode: 3760   score: 500.0  epsilon: 1.0    steps: 171  evaluation reward: 281.35\n",
      "episode: 3761   score: 345.0  epsilon: 1.0    steps: 408  evaluation reward: 283.0\n",
      "episode: 3762   score: 345.0  epsilon: 1.0    steps: 814  evaluation reward: 284.3\n",
      "episode: 3763   score: 515.0  epsilon: 1.0    steps: 901  evaluation reward: 286.6\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8464: Policy loss: -2.285986. Value loss: 271.790558. Entropy: 0.652224.\n",
      "Iteration 8465: Policy loss: -0.363212. Value loss: 110.515503. Entropy: 0.559417.\n",
      "Iteration 8466: Policy loss: -0.923425. Value loss: 108.682854. Entropy: 0.501961.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8467: Policy loss: 1.259937. Value loss: 21.516590. Entropy: 0.542311.\n",
      "Iteration 8468: Policy loss: 1.193225. Value loss: 19.858305. Entropy: 0.542151.\n",
      "Iteration 8469: Policy loss: 1.304581. Value loss: 16.263823. Entropy: 0.538205.\n",
      "episode: 3764   score: 300.0  epsilon: 1.0    steps: 11  evaluation reward: 284.15\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8470: Policy loss: 0.891706. Value loss: 26.591061. Entropy: 0.479210.\n",
      "Iteration 8471: Policy loss: 0.927742. Value loss: 22.971367. Entropy: 0.478786.\n",
      "Iteration 8472: Policy loss: 0.707703. Value loss: 18.333849. Entropy: 0.476800.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8473: Policy loss: 1.133784. Value loss: 26.614254. Entropy: 0.384155.\n",
      "Iteration 8474: Policy loss: 1.121142. Value loss: 17.502588. Entropy: 0.370912.\n",
      "Iteration 8475: Policy loss: 1.054117. Value loss: 13.274457. Entropy: 0.382069.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8476: Policy loss: -0.386707. Value loss: 46.559948. Entropy: 0.407616.\n",
      "Iteration 8477: Policy loss: -0.489827. Value loss: 19.223541. Entropy: 0.412241.\n",
      "Iteration 8478: Policy loss: -0.599515. Value loss: 12.898708. Entropy: 0.413862.\n",
      "episode: 3765   score: 260.0  epsilon: 1.0    steps: 307  evaluation reward: 284.35\n",
      "episode: 3766   score: 260.0  epsilon: 1.0    steps: 511  evaluation reward: 284.6\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8479: Policy loss: 6.979679. Value loss: 49.453159. Entropy: 0.607940.\n",
      "Iteration 8480: Policy loss: 6.777797. Value loss: 25.576494. Entropy: 0.613320.\n",
      "Iteration 8481: Policy loss: 6.803222. Value loss: 20.688038. Entropy: 0.608843.\n",
      "episode: 3767   score: 315.0  epsilon: 1.0    steps: 589  evaluation reward: 285.85\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8482: Policy loss: 1.833842. Value loss: 29.209225. Entropy: 0.504237.\n",
      "Iteration 8483: Policy loss: 1.744426. Value loss: 16.213655. Entropy: 0.529980.\n",
      "Iteration 8484: Policy loss: 1.577949. Value loss: 11.723898. Entropy: 0.521421.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3768   score: 240.0  epsilon: 1.0    steps: 711  evaluation reward: 286.7\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8485: Policy loss: 0.444534. Value loss: 27.191387. Entropy: 0.566938.\n",
      "Iteration 8486: Policy loss: 0.378957. Value loss: 15.362684. Entropy: 0.575878.\n",
      "Iteration 8487: Policy loss: 0.405681. Value loss: 11.613672. Entropy: 0.550197.\n",
      "episode: 3769   score: 260.0  epsilon: 1.0    steps: 205  evaluation reward: 285.55\n",
      "episode: 3770   score: 355.0  epsilon: 1.0    steps: 823  evaluation reward: 287.1\n",
      "episode: 3771   score: 270.0  epsilon: 1.0    steps: 926  evaluation reward: 287.55\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8488: Policy loss: 1.646034. Value loss: 29.648720. Entropy: 0.720250.\n",
      "Iteration 8489: Policy loss: 1.768682. Value loss: 19.012781. Entropy: 0.706126.\n",
      "Iteration 8490: Policy loss: 1.594553. Value loss: 14.070521. Entropy: 0.715415.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8491: Policy loss: 1.072656. Value loss: 20.939611. Entropy: 0.646710.\n",
      "Iteration 8492: Policy loss: 0.894742. Value loss: 15.085973. Entropy: 0.618820.\n",
      "Iteration 8493: Policy loss: 0.941988. Value loss: 10.420192. Entropy: 0.638319.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8494: Policy loss: 1.091200. Value loss: 32.599609. Entropy: 0.609104.\n",
      "Iteration 8495: Policy loss: 1.153377. Value loss: 13.798546. Entropy: 0.604900.\n",
      "Iteration 8496: Policy loss: 1.235660. Value loss: 10.752597. Entropy: 0.601671.\n",
      "episode: 3772   score: 485.0  epsilon: 1.0    steps: 40  evaluation reward: 290.15\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8497: Policy loss: -2.196415. Value loss: 262.043243. Entropy: 0.425090.\n",
      "Iteration 8498: Policy loss: -1.187261. Value loss: 87.102997. Entropy: 0.505145.\n",
      "Iteration 8499: Policy loss: -1.393970. Value loss: 44.966797. Entropy: 0.432149.\n",
      "episode: 3773   score: 285.0  epsilon: 1.0    steps: 351  evaluation reward: 290.45\n",
      "episode: 3774   score: 285.0  epsilon: 1.0    steps: 618  evaluation reward: 290.35\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8500: Policy loss: 3.058780. Value loss: 41.169559. Entropy: 0.473293.\n",
      "Iteration 8501: Policy loss: 2.873482. Value loss: 21.884794. Entropy: 0.487037.\n",
      "Iteration 8502: Policy loss: 3.040741. Value loss: 17.450434. Entropy: 0.448671.\n",
      "episode: 3775   score: 230.0  epsilon: 1.0    steps: 716  evaluation reward: 287.2\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8503: Policy loss: 3.092093. Value loss: 30.845127. Entropy: 0.661671.\n",
      "Iteration 8504: Policy loss: 3.116062. Value loss: 19.532745. Entropy: 0.660322.\n",
      "Iteration 8505: Policy loss: 2.941630. Value loss: 15.980659. Entropy: 0.657117.\n",
      "episode: 3776   score: 260.0  epsilon: 1.0    steps: 228  evaluation reward: 286.45\n",
      "episode: 3777   score: 485.0  epsilon: 1.0    steps: 394  evaluation reward: 289.5\n",
      "episode: 3778   score: 260.0  epsilon: 1.0    steps: 994  evaluation reward: 289.25\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8506: Policy loss: -0.804619. Value loss: 33.568069. Entropy: 0.681221.\n",
      "Iteration 8507: Policy loss: -0.840126. Value loss: 18.369562. Entropy: 0.673295.\n",
      "Iteration 8508: Policy loss: -1.144162. Value loss: 16.757847. Entropy: 0.665688.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8509: Policy loss: 0.127909. Value loss: 20.690069. Entropy: 0.775815.\n",
      "Iteration 8510: Policy loss: 0.039330. Value loss: 11.725769. Entropy: 0.785967.\n",
      "Iteration 8511: Policy loss: -0.060151. Value loss: 8.539489. Entropy: 0.794044.\n",
      "episode: 3779   score: 240.0  epsilon: 1.0    steps: 852  evaluation reward: 285.45\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8512: Policy loss: 2.588962. Value loss: 20.686087. Entropy: 0.657291.\n",
      "Iteration 8513: Policy loss: 2.291495. Value loss: 13.479989. Entropy: 0.656433.\n",
      "Iteration 8514: Policy loss: 2.504307. Value loss: 11.226342. Entropy: 0.647169.\n",
      "episode: 3780   score: 255.0  epsilon: 1.0    steps: 81  evaluation reward: 286.9\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8515: Policy loss: 1.825115. Value loss: 20.789927. Entropy: 0.675424.\n",
      "Iteration 8516: Policy loss: 2.025896. Value loss: 11.897247. Entropy: 0.671732.\n",
      "Iteration 8517: Policy loss: 1.770180. Value loss: 9.832197. Entropy: 0.655028.\n",
      "episode: 3781   score: 195.0  epsilon: 1.0    steps: 320  evaluation reward: 287.3\n",
      "episode: 3782   score: 180.0  epsilon: 1.0    steps: 722  evaluation reward: 287.3\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8518: Policy loss: 1.765195. Value loss: 31.006277. Entropy: 0.530295.\n",
      "Iteration 8519: Policy loss: 1.974523. Value loss: 15.347809. Entropy: 0.525998.\n",
      "Iteration 8520: Policy loss: 2.089020. Value loss: 12.085005. Entropy: 0.532091.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8521: Policy loss: 0.042161. Value loss: 27.955492. Entropy: 0.772355.\n",
      "Iteration 8522: Policy loss: 0.242219. Value loss: 13.221630. Entropy: 0.768632.\n",
      "Iteration 8523: Policy loss: 0.191824. Value loss: 11.128512. Entropy: 0.773012.\n",
      "episode: 3783   score: 285.0  epsilon: 1.0    steps: 519  evaluation reward: 288.35\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8524: Policy loss: 1.277572. Value loss: 35.507999. Entropy: 0.708692.\n",
      "Iteration 8525: Policy loss: 1.142960. Value loss: 15.060100. Entropy: 0.699710.\n",
      "Iteration 8526: Policy loss: 1.325184. Value loss: 11.579580. Entropy: 0.750069.\n",
      "episode: 3784   score: 260.0  epsilon: 1.0    steps: 169  evaluation reward: 288.1\n",
      "episode: 3785   score: 370.0  epsilon: 1.0    steps: 402  evaluation reward: 289.0\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8527: Policy loss: -0.828532. Value loss: 17.758686. Entropy: 0.669010.\n",
      "Iteration 8528: Policy loss: -0.807529. Value loss: 10.388388. Entropy: 0.669203.\n",
      "Iteration 8529: Policy loss: -0.996135. Value loss: 9.852612. Entropy: 0.668025.\n",
      "episode: 3786   score: 155.0  epsilon: 1.0    steps: 92  evaluation reward: 288.45\n",
      "episode: 3787   score: 260.0  epsilon: 1.0    steps: 943  evaluation reward: 289.8\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8530: Policy loss: 0.529385. Value loss: 27.529905. Entropy: 0.547101.\n",
      "Iteration 8531: Policy loss: 0.453845. Value loss: 18.453665. Entropy: 0.555557.\n",
      "Iteration 8532: Policy loss: 0.462039. Value loss: 12.758160. Entropy: 0.540632.\n",
      "episode: 3788   score: 155.0  epsilon: 1.0    steps: 689  evaluation reward: 288.45\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8533: Policy loss: 0.417576. Value loss: 20.108337. Entropy: 0.638699.\n",
      "Iteration 8534: Policy loss: 0.390172. Value loss: 10.129290. Entropy: 0.627465.\n",
      "Iteration 8535: Policy loss: 0.254983. Value loss: 9.074015. Entropy: 0.630132.\n",
      "episode: 3789   score: 255.0  epsilon: 1.0    steps: 353  evaluation reward: 288.9\n",
      "episode: 3790   score: 345.0  epsilon: 1.0    steps: 883  evaluation reward: 291.3\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8536: Policy loss: 1.138298. Value loss: 20.209269. Entropy: 0.602742.\n",
      "Iteration 8537: Policy loss: 1.525938. Value loss: 13.389030. Entropy: 0.608821.\n",
      "Iteration 8538: Policy loss: 1.093906. Value loss: 10.775247. Entropy: 0.614860.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8539: Policy loss: 0.687085. Value loss: 15.899706. Entropy: 0.659296.\n",
      "Iteration 8540: Policy loss: 0.689886. Value loss: 9.212158. Entropy: 0.670912.\n",
      "Iteration 8541: Policy loss: 0.522219. Value loss: 6.763589. Entropy: 0.664036.\n",
      "episode: 3791   score: 105.0  epsilon: 1.0    steps: 710  evaluation reward: 290.0\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8542: Policy loss: 1.795380. Value loss: 21.771101. Entropy: 0.704676.\n",
      "Iteration 8543: Policy loss: 2.011483. Value loss: 11.444690. Entropy: 0.703398.\n",
      "Iteration 8544: Policy loss: 1.622096. Value loss: 9.033324. Entropy: 0.693007.\n",
      "episode: 3792   score: 105.0  epsilon: 1.0    steps: 378  evaluation reward: 289.25\n",
      "episode: 3793   score: 335.0  epsilon: 1.0    steps: 483  evaluation reward: 288.0\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8545: Policy loss: 0.235687. Value loss: 27.572845. Entropy: 0.699106.\n",
      "Iteration 8546: Policy loss: 0.597491. Value loss: 13.944584. Entropy: 0.718990.\n",
      "Iteration 8547: Policy loss: 0.500966. Value loss: 11.667855. Entropy: 0.695973.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3794   score: 230.0  epsilon: 1.0    steps: 555  evaluation reward: 285.6\n",
      "episode: 3795   score: 235.0  epsilon: 1.0    steps: 934  evaluation reward: 286.9\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8548: Policy loss: 1.130177. Value loss: 12.268158. Entropy: 0.623906.\n",
      "Iteration 8549: Policy loss: 1.137463. Value loss: 7.448082. Entropy: 0.654360.\n",
      "Iteration 8550: Policy loss: 1.160329. Value loss: 7.479221. Entropy: 0.667136.\n",
      "episode: 3796   score: 245.0  epsilon: 1.0    steps: 225  evaluation reward: 286.15\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8551: Policy loss: -0.232166. Value loss: 24.166203. Entropy: 0.758709.\n",
      "Iteration 8552: Policy loss: -0.233009. Value loss: 17.057377. Entropy: 0.752294.\n",
      "Iteration 8553: Policy loss: -0.032881. Value loss: 14.031604. Entropy: 0.739407.\n",
      "episode: 3797   score: 220.0  epsilon: 1.0    steps: 49  evaluation reward: 286.5\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8554: Policy loss: -0.251610. Value loss: 17.568554. Entropy: 0.532866.\n",
      "Iteration 8555: Policy loss: -0.072069. Value loss: 11.138579. Entropy: 0.579733.\n",
      "Iteration 8556: Policy loss: -0.043825. Value loss: 9.807459. Entropy: 0.580387.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8557: Policy loss: -0.320598. Value loss: 31.878111. Entropy: 0.714305.\n",
      "Iteration 8558: Policy loss: -0.737023. Value loss: 16.200636. Entropy: 0.706508.\n",
      "Iteration 8559: Policy loss: -0.739491. Value loss: 12.385765. Entropy: 0.732591.\n",
      "episode: 3798   score: 55.0  epsilon: 1.0    steps: 525  evaluation reward: 285.55\n",
      "episode: 3799   score: 185.0  epsilon: 1.0    steps: 713  evaluation reward: 284.6\n",
      "episode: 3800   score: 340.0  epsilon: 1.0    steps: 803  evaluation reward: 286.2\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8560: Policy loss: 1.086461. Value loss: 11.825240. Entropy: 0.555561.\n",
      "Iteration 8561: Policy loss: 1.310435. Value loss: 6.498273. Entropy: 0.599553.\n",
      "Iteration 8562: Policy loss: 0.983778. Value loss: 6.799134. Entropy: 0.626359.\n",
      "now time :  2019-02-25 21:20:10.313729\n",
      "episode: 3801   score: 255.0  epsilon: 1.0    steps: 348  evaluation reward: 285.9\n",
      "episode: 3802   score: 220.0  epsilon: 1.0    steps: 509  evaluation reward: 284.1\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8563: Policy loss: 1.260237. Value loss: 28.816425. Entropy: 0.739256.\n",
      "Iteration 8564: Policy loss: 1.233489. Value loss: 16.634432. Entropy: 0.743424.\n",
      "Iteration 8565: Policy loss: 1.140722. Value loss: 14.275620. Entropy: 0.761876.\n",
      "episode: 3803   score: 200.0  epsilon: 1.0    steps: 923  evaluation reward: 284.55\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8566: Policy loss: -0.218033. Value loss: 19.923319. Entropy: 0.658342.\n",
      "Iteration 8567: Policy loss: -0.191195. Value loss: 11.588661. Entropy: 0.620117.\n",
      "Iteration 8568: Policy loss: -0.248467. Value loss: 9.993770. Entropy: 0.655271.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8569: Policy loss: -1.569894. Value loss: 26.190460. Entropy: 0.776718.\n",
      "Iteration 8570: Policy loss: -1.895649. Value loss: 14.142864. Entropy: 0.774161.\n",
      "Iteration 8571: Policy loss: -1.894498. Value loss: 11.005165. Entropy: 0.787009.\n",
      "episode: 3804   score: 285.0  epsilon: 1.0    steps: 115  evaluation reward: 285.6\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8572: Policy loss: -0.303705. Value loss: 27.025076. Entropy: 0.395429.\n",
      "Iteration 8573: Policy loss: -0.649224. Value loss: 13.129678. Entropy: 0.418903.\n",
      "Iteration 8574: Policy loss: -0.399039. Value loss: 8.902763. Entropy: 0.397062.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8575: Policy loss: -0.337296. Value loss: 13.757612. Entropy: 0.392045.\n",
      "Iteration 8576: Policy loss: -0.316690. Value loss: 7.760110. Entropy: 0.405585.\n",
      "Iteration 8577: Policy loss: -0.443674. Value loss: 6.374317. Entropy: 0.403602.\n",
      "episode: 3805   score: 290.0  epsilon: 1.0    steps: 717  evaluation reward: 287.15\n",
      "episode: 3806   score: 285.0  epsilon: 1.0    steps: 819  evaluation reward: 287.2\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8578: Policy loss: -0.006183. Value loss: 21.575706. Entropy: 0.463565.\n",
      "Iteration 8579: Policy loss: -0.144797. Value loss: 12.113242. Entropy: 0.473543.\n",
      "Iteration 8580: Policy loss: -0.002685. Value loss: 9.828044. Entropy: 0.464014.\n",
      "episode: 3807   score: 305.0  epsilon: 1.0    steps: 163  evaluation reward: 288.15\n",
      "episode: 3808   score: 155.0  epsilon: 1.0    steps: 900  evaluation reward: 287.05\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8581: Policy loss: -0.316522. Value loss: 24.507336. Entropy: 0.698893.\n",
      "Iteration 8582: Policy loss: -0.412928. Value loss: 16.475113. Entropy: 0.690570.\n",
      "Iteration 8583: Policy loss: -0.554831. Value loss: 13.395284. Entropy: 0.682518.\n",
      "episode: 3809   score: 150.0  epsilon: 1.0    steps: 89  evaluation reward: 284.05\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8584: Policy loss: -1.180781. Value loss: 33.414738. Entropy: 0.709762.\n",
      "Iteration 8585: Policy loss: -1.383528. Value loss: 15.970057. Entropy: 0.716649.\n",
      "Iteration 8586: Policy loss: -1.192484. Value loss: 11.663446. Entropy: 0.701516.\n",
      "episode: 3810   score: 305.0  epsilon: 1.0    steps: 329  evaluation reward: 284.7\n",
      "episode: 3811   score: 410.0  epsilon: 1.0    steps: 532  evaluation reward: 286.7\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8587: Policy loss: -0.738955. Value loss: 23.662066. Entropy: 0.814223.\n",
      "Iteration 8588: Policy loss: -0.639580. Value loss: 12.842038. Entropy: 0.793350.\n",
      "Iteration 8589: Policy loss: -0.733341. Value loss: 12.301240. Entropy: 0.782307.\n",
      "episode: 3812   score: 375.0  epsilon: 1.0    steps: 509  evaluation reward: 288.3\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8590: Policy loss: 0.957117. Value loss: 33.577995. Entropy: 0.555609.\n",
      "Iteration 8591: Policy loss: 0.809275. Value loss: 16.193213. Entropy: 0.526239.\n",
      "Iteration 8592: Policy loss: 0.700422. Value loss: 13.883836. Entropy: 0.525778.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8593: Policy loss: 1.138561. Value loss: 16.030239. Entropy: 0.607496.\n",
      "Iteration 8594: Policy loss: 1.124724. Value loss: 7.859443. Entropy: 0.599094.\n",
      "Iteration 8595: Policy loss: 1.105756. Value loss: 6.142781. Entropy: 0.611628.\n",
      "episode: 3813   score: 80.0  epsilon: 1.0    steps: 290  evaluation reward: 283.45\n",
      "episode: 3814   score: 260.0  epsilon: 1.0    steps: 773  evaluation reward: 283.8\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8596: Policy loss: 1.056461. Value loss: 21.136547. Entropy: 0.544673.\n",
      "Iteration 8597: Policy loss: 1.049469. Value loss: 10.638318. Entropy: 0.562031.\n",
      "Iteration 8598: Policy loss: 1.099134. Value loss: 8.475444. Entropy: 0.535813.\n",
      "episode: 3815   score: 185.0  epsilon: 1.0    steps: 147  evaluation reward: 283.05\n",
      "episode: 3816   score: 115.0  epsilon: 1.0    steps: 542  evaluation reward: 279.95\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8599: Policy loss: 0.406114. Value loss: 34.857128. Entropy: 0.516426.\n",
      "Iteration 8600: Policy loss: 0.823760. Value loss: 17.259529. Entropy: 0.561976.\n",
      "Iteration 8601: Policy loss: 0.372361. Value loss: 13.302032. Entropy: 0.526047.\n",
      "episode: 3817   score: 120.0  epsilon: 1.0    steps: 504  evaluation reward: 276.3\n",
      "episode: 3818   score: 295.0  epsilon: 1.0    steps: 724  evaluation reward: 275.6\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8602: Policy loss: 2.290399. Value loss: 18.904415. Entropy: 0.513334.\n",
      "Iteration 8603: Policy loss: 2.015224. Value loss: 11.158956. Entropy: 0.527556.\n",
      "Iteration 8604: Policy loss: 2.245827. Value loss: 7.894053. Entropy: 0.539612.\n",
      "episode: 3819   score: 205.0  epsilon: 1.0    steps: 982  evaluation reward: 275.35\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8605: Policy loss: 1.725961. Value loss: 17.467306. Entropy: 0.698022.\n",
      "Iteration 8606: Policy loss: 1.690840. Value loss: 12.333603. Entropy: 0.684592.\n",
      "Iteration 8607: Policy loss: 1.851044. Value loss: 8.672467. Entropy: 0.680146.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8608: Policy loss: 0.292996. Value loss: 14.171908. Entropy: 0.546901.\n",
      "Iteration 8609: Policy loss: 0.260171. Value loss: 8.844551. Entropy: 0.540021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8610: Policy loss: 0.418580. Value loss: 7.441772. Entropy: 0.543411.\n",
      "episode: 3820   score: 285.0  epsilon: 1.0    steps: 118  evaluation reward: 276.7\n",
      "episode: 3821   score: 100.0  epsilon: 1.0    steps: 176  evaluation reward: 275.45\n",
      "episode: 3822   score: 105.0  epsilon: 1.0    steps: 749  evaluation reward: 275.0\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8611: Policy loss: -0.334067. Value loss: 23.759796. Entropy: 0.547155.\n",
      "Iteration 8612: Policy loss: -0.478961. Value loss: 13.598880. Entropy: 0.514568.\n",
      "Iteration 8613: Policy loss: -0.267292. Value loss: 10.978965. Entropy: 0.531684.\n",
      "episode: 3823   score: 240.0  epsilon: 1.0    steps: 263  evaluation reward: 275.7\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8614: Policy loss: -1.011752. Value loss: 13.128079. Entropy: 0.737655.\n",
      "Iteration 8615: Policy loss: -1.093558. Value loss: 7.643767. Entropy: 0.733006.\n",
      "Iteration 8616: Policy loss: -1.069473. Value loss: 6.057052. Entropy: 0.716868.\n",
      "episode: 3824   score: 155.0  epsilon: 1.0    steps: 495  evaluation reward: 274.4\n",
      "episode: 3825   score: 225.0  epsilon: 1.0    steps: 617  evaluation reward: 274.8\n",
      "episode: 3826   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 275.45\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8617: Policy loss: 0.518067. Value loss: 22.569828. Entropy: 0.451979.\n",
      "Iteration 8618: Policy loss: 0.549379. Value loss: 14.858133. Entropy: 0.464113.\n",
      "Iteration 8619: Policy loss: 0.401934. Value loss: 10.930031. Entropy: 0.456935.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8620: Policy loss: -2.150049. Value loss: 15.870998. Entropy: 0.680524.\n",
      "Iteration 8621: Policy loss: -1.895366. Value loss: 8.298438. Entropy: 0.672916.\n",
      "Iteration 8622: Policy loss: -2.020114. Value loss: 6.577800. Entropy: 0.651714.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8623: Policy loss: -1.710384. Value loss: 22.909388. Entropy: 0.550285.\n",
      "Iteration 8624: Policy loss: -1.598092. Value loss: 15.375561. Entropy: 0.533250.\n",
      "Iteration 8625: Policy loss: -1.590608. Value loss: 11.912563. Entropy: 0.533294.\n",
      "episode: 3827   score: 285.0  epsilon: 1.0    steps: 914  evaluation reward: 275.4\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8626: Policy loss: 0.462336. Value loss: 20.558542. Entropy: 0.510812.\n",
      "Iteration 8627: Policy loss: 0.198472. Value loss: 11.514490. Entropy: 0.532749.\n",
      "Iteration 8628: Policy loss: 0.276644. Value loss: 9.594233. Entropy: 0.516155.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8629: Policy loss: -1.117894. Value loss: 17.745996. Entropy: 0.497348.\n",
      "Iteration 8630: Policy loss: -1.046033. Value loss: 11.111938. Entropy: 0.519700.\n",
      "Iteration 8631: Policy loss: -1.233711. Value loss: 8.911763. Entropy: 0.521165.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8632: Policy loss: -3.953781. Value loss: 56.723293. Entropy: 0.815700.\n",
      "Iteration 8633: Policy loss: -3.969549. Value loss: 30.113646. Entropy: 0.818056.\n",
      "Iteration 8634: Policy loss: -4.074183. Value loss: 17.991501. Entropy: 0.800582.\n",
      "episode: 3828   score: 420.0  epsilon: 1.0    steps: 37  evaluation reward: 273.4\n",
      "episode: 3829   score: 455.0  epsilon: 1.0    steps: 382  evaluation reward: 275.05\n",
      "episode: 3830   score: 185.0  epsilon: 1.0    steps: 488  evaluation reward: 272.3\n",
      "episode: 3831   score: 220.0  epsilon: 1.0    steps: 581  evaluation reward: 271.3\n",
      "episode: 3832   score: 390.0  epsilon: 1.0    steps: 766  evaluation reward: 272.75\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8635: Policy loss: -0.047218. Value loss: 38.740120. Entropy: 0.771961.\n",
      "Iteration 8636: Policy loss: -0.061040. Value loss: 18.666428. Entropy: 0.766383.\n",
      "Iteration 8637: Policy loss: -0.074302. Value loss: 14.476778. Entropy: 0.755018.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8638: Policy loss: -2.572234. Value loss: 21.586367. Entropy: 0.690344.\n",
      "Iteration 8639: Policy loss: -2.426476. Value loss: 12.432228. Entropy: 0.687215.\n",
      "Iteration 8640: Policy loss: -2.534559. Value loss: 11.692175. Entropy: 0.679529.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8641: Policy loss: -0.946500. Value loss: 28.478622. Entropy: 0.506251.\n",
      "Iteration 8642: Policy loss: -0.961326. Value loss: 15.660402. Entropy: 0.500680.\n",
      "Iteration 8643: Policy loss: -1.019867. Value loss: 11.859119. Entropy: 0.511847.\n",
      "episode: 3833   score: 485.0  epsilon: 1.0    steps: 245  evaluation reward: 270.8\n",
      "episode: 3834   score: 265.0  epsilon: 1.0    steps: 790  evaluation reward: 270.8\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8644: Policy loss: -1.073337. Value loss: 28.617857. Entropy: 0.611813.\n",
      "Iteration 8645: Policy loss: -0.640165. Value loss: 14.175692. Entropy: 0.592647.\n",
      "Iteration 8646: Policy loss: -0.788519. Value loss: 11.330068. Entropy: 0.619105.\n",
      "episode: 3835   score: 105.0  epsilon: 1.0    steps: 465  evaluation reward: 270.65\n",
      "episode: 3836   score: 330.0  epsilon: 1.0    steps: 1019  evaluation reward: 271.7\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8647: Policy loss: 0.903131. Value loss: 16.678440. Entropy: 0.525207.\n",
      "Iteration 8648: Policy loss: 0.842374. Value loss: 9.664895. Entropy: 0.517184.\n",
      "Iteration 8649: Policy loss: 1.053662. Value loss: 7.710607. Entropy: 0.519603.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8650: Policy loss: -0.260950. Value loss: 23.168589. Entropy: 0.721703.\n",
      "Iteration 8651: Policy loss: 0.003206. Value loss: 15.171772. Entropy: 0.729983.\n",
      "Iteration 8652: Policy loss: -0.258257. Value loss: 9.926937. Entropy: 0.708159.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8653: Policy loss: -1.169827. Value loss: 33.369614. Entropy: 0.746493.\n",
      "Iteration 8654: Policy loss: -1.234971. Value loss: 16.673120. Entropy: 0.745117.\n",
      "Iteration 8655: Policy loss: -1.393829. Value loss: 14.232504. Entropy: 0.753288.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8656: Policy loss: -1.415527. Value loss: 44.337025. Entropy: 0.805280.\n",
      "Iteration 8657: Policy loss: -1.401215. Value loss: 23.048445. Entropy: 0.803361.\n",
      "Iteration 8658: Policy loss: -1.305737. Value loss: 17.183687. Entropy: 0.789608.\n",
      "episode: 3837   score: 210.0  epsilon: 1.0    steps: 509  evaluation reward: 271.7\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8659: Policy loss: 0.971533. Value loss: 34.756111. Entropy: 0.564708.\n",
      "Iteration 8660: Policy loss: 1.175933. Value loss: 18.772785. Entropy: 0.545104.\n",
      "Iteration 8661: Policy loss: 1.237902. Value loss: 14.618828. Entropy: 0.537059.\n",
      "episode: 3838   score: 365.0  epsilon: 1.0    steps: 45  evaluation reward: 273.25\n",
      "episode: 3839   score: 280.0  epsilon: 1.0    steps: 327  evaluation reward: 274.0\n",
      "episode: 3840   score: 210.0  epsilon: 1.0    steps: 1007  evaluation reward: 273.65\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8662: Policy loss: 0.337421. Value loss: 26.218021. Entropy: 0.371349.\n",
      "Iteration 8663: Policy loss: 0.864519. Value loss: 13.033468. Entropy: 0.376278.\n",
      "Iteration 8664: Policy loss: 0.225010. Value loss: 12.238597. Entropy: 0.377270.\n",
      "episode: 3841   score: 225.0  epsilon: 1.0    steps: 250  evaluation reward: 270.3\n",
      "episode: 3842   score: 435.0  epsilon: 1.0    steps: 628  evaluation reward: 272.05\n",
      "episode: 3843   score: 525.0  epsilon: 1.0    steps: 677  evaluation reward: 273.85\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8665: Policy loss: 1.032332. Value loss: 32.526691. Entropy: 0.716544.\n",
      "Iteration 8666: Policy loss: 0.878203. Value loss: 14.358544. Entropy: 0.720859.\n",
      "Iteration 8667: Policy loss: 0.948123. Value loss: 10.893682. Entropy: 0.719051.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8668: Policy loss: -0.378942. Value loss: 24.314709. Entropy: 0.501098.\n",
      "Iteration 8669: Policy loss: -0.357651. Value loss: 17.773865. Entropy: 0.481206.\n",
      "Iteration 8670: Policy loss: -0.431804. Value loss: 13.167721. Entropy: 0.477583.\n",
      "episode: 3844   score: 335.0  epsilon: 1.0    steps: 845  evaluation reward: 274.95\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8671: Policy loss: 0.536132. Value loss: 19.522385. Entropy: 0.508505.\n",
      "Iteration 8672: Policy loss: 0.418473. Value loss: 12.816377. Entropy: 0.527848.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8673: Policy loss: 0.577786. Value loss: 9.458999. Entropy: 0.506646.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8674: Policy loss: 2.080467. Value loss: 25.858215. Entropy: 0.500580.\n",
      "Iteration 8675: Policy loss: 2.220991. Value loss: 15.769832. Entropy: 0.485534.\n",
      "Iteration 8676: Policy loss: 2.140052. Value loss: 12.330439. Entropy: 0.471513.\n",
      "episode: 3845   score: 180.0  epsilon: 1.0    steps: 322  evaluation reward: 272.4\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8677: Policy loss: -0.934700. Value loss: 226.269516. Entropy: 0.234767.\n",
      "Iteration 8678: Policy loss: -0.751271. Value loss: 120.866821. Entropy: 0.233244.\n",
      "Iteration 8679: Policy loss: -2.146240. Value loss: 125.473732. Entropy: 0.229941.\n",
      "episode: 3846   score: 225.0  epsilon: 1.0    steps: 747  evaluation reward: 271.8\n",
      "episode: 3847   score: 95.0  epsilon: 1.0    steps: 859  evaluation reward: 269.35\n",
      "episode: 3848   score: 210.0  epsilon: 1.0    steps: 990  evaluation reward: 269.65\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8680: Policy loss: 2.368910. Value loss: 32.075794. Entropy: 0.455098.\n",
      "Iteration 8681: Policy loss: 2.796736. Value loss: 15.738334. Entropy: 0.457012.\n",
      "Iteration 8682: Policy loss: 2.337235. Value loss: 12.751751. Entropy: 0.468790.\n",
      "episode: 3849   score: 205.0  epsilon: 1.0    steps: 89  evaluation reward: 268.5\n",
      "episode: 3850   score: 180.0  epsilon: 1.0    steps: 564  evaluation reward: 267.45\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8683: Policy loss: 3.598731. Value loss: 43.775490. Entropy: 0.514619.\n",
      "Iteration 8684: Policy loss: 3.436292. Value loss: 14.977184. Entropy: 0.548561.\n",
      "Iteration 8685: Policy loss: 3.296123. Value loss: 12.049313. Entropy: 0.554043.\n",
      "now time :  2019-02-25 21:22:28.239139\n",
      "episode: 3851   score: 90.0  epsilon: 1.0    steps: 353  evaluation reward: 264.05\n",
      "episode: 3852   score: 530.0  epsilon: 1.0    steps: 454  evaluation reward: 266.95\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8686: Policy loss: 2.226424. Value loss: 33.929008. Entropy: 0.720063.\n",
      "Iteration 8687: Policy loss: 2.248873. Value loss: 19.677916. Entropy: 0.745678.\n",
      "Iteration 8688: Policy loss: 2.237543. Value loss: 16.187841. Entropy: 0.743423.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8689: Policy loss: 0.924879. Value loss: 39.763893. Entropy: 0.631816.\n",
      "Iteration 8690: Policy loss: 0.917699. Value loss: 19.725580. Entropy: 0.639762.\n",
      "Iteration 8691: Policy loss: 1.164122. Value loss: 15.463374. Entropy: 0.644444.\n",
      "episode: 3853   score: 345.0  epsilon: 1.0    steps: 231  evaluation reward: 268.3\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8692: Policy loss: 0.091326. Value loss: 30.304651. Entropy: 0.589666.\n",
      "Iteration 8693: Policy loss: 0.132150. Value loss: 17.786150. Entropy: 0.593282.\n",
      "Iteration 8694: Policy loss: -0.156067. Value loss: 13.910874. Entropy: 0.606313.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8695: Policy loss: 0.825388. Value loss: 21.852806. Entropy: 0.471423.\n",
      "Iteration 8696: Policy loss: 0.795275. Value loss: 13.304871. Entropy: 0.509519.\n",
      "Iteration 8697: Policy loss: 0.818369. Value loss: 9.908120. Entropy: 0.490095.\n",
      "episode: 3854   score: 210.0  epsilon: 1.0    steps: 68  evaluation reward: 267.25\n",
      "episode: 3855   score: 185.0  epsilon: 1.0    steps: 567  evaluation reward: 266.55\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8698: Policy loss: 2.939645. Value loss: 29.658558. Entropy: 0.376404.\n",
      "Iteration 8699: Policy loss: 3.009980. Value loss: 15.891259. Entropy: 0.393837.\n",
      "Iteration 8700: Policy loss: 3.051973. Value loss: 12.791036. Entropy: 0.419011.\n",
      "episode: 3856   score: 250.0  epsilon: 1.0    steps: 788  evaluation reward: 266.2\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8701: Policy loss: 0.753029. Value loss: 31.239792. Entropy: 0.751847.\n",
      "Iteration 8702: Policy loss: 0.579803. Value loss: 15.526362. Entropy: 0.768915.\n",
      "Iteration 8703: Policy loss: 0.532706. Value loss: 11.421151. Entropy: 0.756278.\n",
      "episode: 3857   score: 260.0  epsilon: 1.0    steps: 375  evaluation reward: 265.45\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8704: Policy loss: 0.768865. Value loss: 24.474073. Entropy: 0.615326.\n",
      "Iteration 8705: Policy loss: 0.445203. Value loss: 12.693171. Entropy: 0.625271.\n",
      "Iteration 8706: Policy loss: 0.562954. Value loss: 10.308435. Entropy: 0.623415.\n",
      "episode: 3858   score: 80.0  epsilon: 1.0    steps: 576  evaluation reward: 262.9\n",
      "episode: 3859   score: 265.0  epsilon: 1.0    steps: 959  evaluation reward: 261.65\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8707: Policy loss: 0.692484. Value loss: 21.674677. Entropy: 0.660936.\n",
      "Iteration 8708: Policy loss: 0.720783. Value loss: 10.053847. Entropy: 0.698270.\n",
      "Iteration 8709: Policy loss: 0.522156. Value loss: 7.720756. Entropy: 0.696040.\n",
      "episode: 3860   score: 260.0  epsilon: 1.0    steps: 211  evaluation reward: 259.25\n",
      "episode: 3861   score: 425.0  epsilon: 1.0    steps: 477  evaluation reward: 260.05\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8710: Policy loss: 0.031638. Value loss: 25.055674. Entropy: 0.716824.\n",
      "Iteration 8711: Policy loss: -0.144980. Value loss: 15.310708. Entropy: 0.702893.\n",
      "Iteration 8712: Policy loss: -0.048628. Value loss: 9.451385. Entropy: 0.710527.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8713: Policy loss: 1.089055. Value loss: 21.754688. Entropy: 0.777788.\n",
      "Iteration 8714: Policy loss: 0.981979. Value loss: 12.672921. Entropy: 0.821927.\n",
      "Iteration 8715: Policy loss: 1.019483. Value loss: 8.874868. Entropy: 0.798930.\n",
      "episode: 3862   score: 55.0  epsilon: 1.0    steps: 282  evaluation reward: 257.15\n",
      "episode: 3863   score: 180.0  epsilon: 1.0    steps: 790  evaluation reward: 253.8\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8716: Policy loss: -0.115784. Value loss: 24.084091. Entropy: 0.768443.\n",
      "Iteration 8717: Policy loss: -0.018667. Value loss: 13.715927. Entropy: 0.781330.\n",
      "Iteration 8718: Policy loss: -0.228275. Value loss: 9.805693. Entropy: 0.764310.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8719: Policy loss: 0.167947. Value loss: 23.537346. Entropy: 0.655498.\n",
      "Iteration 8720: Policy loss: 0.071127. Value loss: 13.465292. Entropy: 0.673706.\n",
      "Iteration 8721: Policy loss: 0.158983. Value loss: 10.295716. Entropy: 0.682416.\n",
      "episode: 3864   score: 255.0  epsilon: 1.0    steps: 43  evaluation reward: 253.35\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8722: Policy loss: -1.410372. Value loss: 52.003380. Entropy: 0.679289.\n",
      "Iteration 8723: Policy loss: -1.641840. Value loss: 27.131020. Entropy: 0.727869.\n",
      "Iteration 8724: Policy loss: -1.044875. Value loss: 16.413475. Entropy: 0.683696.\n",
      "episode: 3865   score: 575.0  epsilon: 1.0    steps: 721  evaluation reward: 256.5\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8725: Policy loss: 0.412663. Value loss: 39.489513. Entropy: 0.614187.\n",
      "Iteration 8726: Policy loss: 0.498049. Value loss: 17.811312. Entropy: 0.623392.\n",
      "Iteration 8727: Policy loss: 0.416145. Value loss: 11.090137. Entropy: 0.651298.\n",
      "episode: 3866   score: 265.0  epsilon: 1.0    steps: 946  evaluation reward: 256.55\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8728: Policy loss: 0.873379. Value loss: 30.635872. Entropy: 0.614118.\n",
      "Iteration 8729: Policy loss: 0.934200. Value loss: 15.432124. Entropy: 0.621416.\n",
      "Iteration 8730: Policy loss: 0.771483. Value loss: 12.869664. Entropy: 0.631425.\n",
      "episode: 3867   score: 185.0  epsilon: 1.0    steps: 283  evaluation reward: 255.25\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8731: Policy loss: 1.120199. Value loss: 32.139256. Entropy: 0.665154.\n",
      "Iteration 8732: Policy loss: 1.150936. Value loss: 14.822976. Entropy: 0.654809.\n",
      "Iteration 8733: Policy loss: 1.005294. Value loss: 11.515514. Entropy: 0.662583.\n",
      "episode: 3868   score: 385.0  epsilon: 1.0    steps: 534  evaluation reward: 256.7\n",
      "episode: 3869   score: 265.0  epsilon: 1.0    steps: 771  evaluation reward: 256.75\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8734: Policy loss: -1.326991. Value loss: 230.316101. Entropy: 0.779292.\n",
      "Iteration 8735: Policy loss: -0.446959. Value loss: 79.831299. Entropy: 0.785954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8736: Policy loss: -0.779848. Value loss: 65.782356. Entropy: 0.785376.\n",
      "episode: 3870   score: 315.0  epsilon: 1.0    steps: 137  evaluation reward: 256.35\n",
      "episode: 3871   score: 355.0  epsilon: 1.0    steps: 429  evaluation reward: 257.2\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8737: Policy loss: 0.131615. Value loss: 34.590454. Entropy: 0.602328.\n",
      "Iteration 8738: Policy loss: 0.346305. Value loss: 22.040794. Entropy: 0.624731.\n",
      "Iteration 8739: Policy loss: -0.149909. Value loss: 16.310228. Entropy: 0.621516.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8740: Policy loss: 1.528782. Value loss: 35.350132. Entropy: 0.688280.\n",
      "Iteration 8741: Policy loss: 1.386966. Value loss: 21.191610. Entropy: 0.691281.\n",
      "Iteration 8742: Policy loss: 1.774326. Value loss: 14.583620. Entropy: 0.667026.\n",
      "episode: 3872   score: 550.0  epsilon: 1.0    steps: 115  evaluation reward: 257.85\n",
      "episode: 3873   score: 80.0  epsilon: 1.0    steps: 531  evaluation reward: 255.8\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8743: Policy loss: 3.757005. Value loss: 25.953247. Entropy: 0.732823.\n",
      "Iteration 8744: Policy loss: 3.703064. Value loss: 11.230492. Entropy: 0.738333.\n",
      "Iteration 8745: Policy loss: 3.541867. Value loss: 8.401305. Entropy: 0.753498.\n",
      "episode: 3874   score: 275.0  epsilon: 1.0    steps: 340  evaluation reward: 255.7\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8746: Policy loss: 1.546105. Value loss: 27.286816. Entropy: 0.852899.\n",
      "Iteration 8747: Policy loss: 1.746320. Value loss: 14.311430. Entropy: 0.850624.\n",
      "Iteration 8748: Policy loss: 1.405397. Value loss: 9.785528. Entropy: 0.834821.\n",
      "episode: 3875   score: 225.0  epsilon: 1.0    steps: 688  evaluation reward: 255.65\n",
      "episode: 3876   score: 135.0  epsilon: 1.0    steps: 777  evaluation reward: 254.4\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8749: Policy loss: -0.350557. Value loss: 21.199757. Entropy: 0.801403.\n",
      "Iteration 8750: Policy loss: -0.223150. Value loss: 11.957897. Entropy: 0.826711.\n",
      "Iteration 8751: Policy loss: -0.252784. Value loss: 8.340364. Entropy: 0.820502.\n",
      "episode: 3877   score: 210.0  epsilon: 1.0    steps: 450  evaluation reward: 251.65\n",
      "episode: 3878   score: 200.0  epsilon: 1.0    steps: 946  evaluation reward: 251.05\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8752: Policy loss: 1.453164. Value loss: 39.199528. Entropy: 0.783608.\n",
      "Iteration 8753: Policy loss: 1.398831. Value loss: 20.907406. Entropy: 0.806257.\n",
      "Iteration 8754: Policy loss: 1.026889. Value loss: 16.185717. Entropy: 0.773176.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8755: Policy loss: 1.903818. Value loss: 43.606705. Entropy: 0.781033.\n",
      "Iteration 8756: Policy loss: 1.984278. Value loss: 18.493183. Entropy: 0.771743.\n",
      "Iteration 8757: Policy loss: 1.779718. Value loss: 17.772879. Entropy: 0.791865.\n",
      "episode: 3879   score: 345.0  epsilon: 1.0    steps: 239  evaluation reward: 252.1\n",
      "episode: 3880   score: 65.0  epsilon: 1.0    steps: 649  evaluation reward: 250.2\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8758: Policy loss: 1.736219. Value loss: 36.498951. Entropy: 0.792632.\n",
      "Iteration 8759: Policy loss: 1.738891. Value loss: 20.580273. Entropy: 0.780790.\n",
      "Iteration 8760: Policy loss: 1.707604. Value loss: 15.407665. Entropy: 0.785211.\n",
      "episode: 3881   score: 310.0  epsilon: 1.0    steps: 514  evaluation reward: 251.35\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8761: Policy loss: -0.836959. Value loss: 32.033165. Entropy: 0.738611.\n",
      "Iteration 8762: Policy loss: -0.701521. Value loss: 15.972486. Entropy: 0.724315.\n",
      "Iteration 8763: Policy loss: -1.008783. Value loss: 12.005341. Entropy: 0.719093.\n",
      "episode: 3882   score: 290.0  epsilon: 1.0    steps: 8  evaluation reward: 252.45\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8764: Policy loss: -0.246735. Value loss: 30.791142. Entropy: 0.705377.\n",
      "Iteration 8765: Policy loss: -0.327219. Value loss: 13.147608. Entropy: 0.708894.\n",
      "Iteration 8766: Policy loss: 0.016921. Value loss: 9.693445. Entropy: 0.705058.\n",
      "episode: 3883   score: 225.0  epsilon: 1.0    steps: 846  evaluation reward: 251.85\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8767: Policy loss: 2.268700. Value loss: 28.397415. Entropy: 0.658414.\n",
      "Iteration 8768: Policy loss: 2.437588. Value loss: 14.431890. Entropy: 0.670120.\n",
      "Iteration 8769: Policy loss: 2.302832. Value loss: 12.730296. Entropy: 0.656817.\n",
      "episode: 3884   score: 260.0  epsilon: 1.0    steps: 468  evaluation reward: 251.85\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8770: Policy loss: -4.113931. Value loss: 283.215179. Entropy: 0.761606.\n",
      "Iteration 8771: Policy loss: -4.076286. Value loss: 165.637634. Entropy: 0.692443.\n",
      "Iteration 8772: Policy loss: -3.953609. Value loss: 119.058281. Entropy: 0.654385.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8773: Policy loss: 1.531362. Value loss: 42.056686. Entropy: 0.756869.\n",
      "Iteration 8774: Policy loss: 1.065779. Value loss: 24.044697. Entropy: 0.735786.\n",
      "Iteration 8775: Policy loss: 1.227472. Value loss: 16.826227. Entropy: 0.746104.\n",
      "episode: 3885   score: 685.0  epsilon: 1.0    steps: 301  evaluation reward: 255.0\n",
      "episode: 3886   score: 295.0  epsilon: 1.0    steps: 679  evaluation reward: 256.4\n",
      "episode: 3887   score: 400.0  epsilon: 1.0    steps: 988  evaluation reward: 257.8\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8776: Policy loss: 0.075718. Value loss: 29.389809. Entropy: 0.684160.\n",
      "Iteration 8777: Policy loss: 0.078646. Value loss: 17.476273. Entropy: 0.699672.\n",
      "Iteration 8778: Policy loss: -0.037602. Value loss: 15.332831. Entropy: 0.691414.\n",
      "episode: 3888   score: 270.0  epsilon: 1.0    steps: 238  evaluation reward: 258.95\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8779: Policy loss: 2.857480. Value loss: 24.331556. Entropy: 0.769370.\n",
      "Iteration 8780: Policy loss: 2.883877. Value loss: 15.528052. Entropy: 0.757181.\n",
      "Iteration 8781: Policy loss: 2.864120. Value loss: 12.706199. Entropy: 0.775417.\n",
      "episode: 3889   score: 170.0  epsilon: 1.0    steps: 21  evaluation reward: 258.1\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8782: Policy loss: 2.951626. Value loss: 25.965364. Entropy: 0.669323.\n",
      "Iteration 8783: Policy loss: 2.659081. Value loss: 13.458062. Entropy: 0.678721.\n",
      "Iteration 8784: Policy loss: 2.742664. Value loss: 10.086895. Entropy: 0.694052.\n",
      "episode: 3890   score: 260.0  epsilon: 1.0    steps: 875  evaluation reward: 257.25\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8785: Policy loss: 0.683886. Value loss: 25.286188. Entropy: 0.763101.\n",
      "Iteration 8786: Policy loss: 0.375516. Value loss: 19.497179. Entropy: 0.742976.\n",
      "Iteration 8787: Policy loss: 0.309119. Value loss: 14.589238. Entropy: 0.745867.\n",
      "episode: 3891   score: 290.0  epsilon: 1.0    steps: 439  evaluation reward: 259.1\n",
      "episode: 3892   score: 385.0  epsilon: 1.0    steps: 518  evaluation reward: 261.9\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8788: Policy loss: -2.469922. Value loss: 176.004425. Entropy: 0.598464.\n",
      "Iteration 8789: Policy loss: -3.294313. Value loss: 208.948364. Entropy: 0.580181.\n",
      "Iteration 8790: Policy loss: -2.850074. Value loss: 126.661453. Entropy: 0.613027.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8791: Policy loss: 1.504083. Value loss: 22.102415. Entropy: 0.638764.\n",
      "Iteration 8792: Policy loss: 1.588940. Value loss: 10.892937. Entropy: 0.648394.\n",
      "Iteration 8793: Policy loss: 1.502175. Value loss: 9.141924. Entropy: 0.650332.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8794: Policy loss: -0.947648. Value loss: 31.879524. Entropy: 0.725021.\n",
      "Iteration 8795: Policy loss: -1.102428. Value loss: 15.448617. Entropy: 0.718965.\n",
      "Iteration 8796: Policy loss: -0.872279. Value loss: 11.060068. Entropy: 0.707654.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8797: Policy loss: 3.336103. Value loss: 38.584938. Entropy: 0.647849.\n",
      "Iteration 8798: Policy loss: 3.494194. Value loss: 20.822151. Entropy: 0.645748.\n",
      "Iteration 8799: Policy loss: 3.095423. Value loss: 17.604839. Entropy: 0.646295.\n",
      "episode: 3893   score: 475.0  epsilon: 1.0    steps: 222  evaluation reward: 263.3\n",
      "episode: 3894   score: 255.0  epsilon: 1.0    steps: 897  evaluation reward: 263.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8800: Policy loss: 0.746754. Value loss: 28.667652. Entropy: 0.677263.\n",
      "Iteration 8801: Policy loss: 0.954711. Value loss: 16.797670. Entropy: 0.674007.\n",
      "Iteration 8802: Policy loss: 0.708120. Value loss: 12.232882. Entropy: 0.671864.\n",
      "episode: 3895   score: 335.0  epsilon: 1.0    steps: 8  evaluation reward: 264.55\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8803: Policy loss: 0.885150. Value loss: 26.800968. Entropy: 0.718401.\n",
      "Iteration 8804: Policy loss: 1.192332. Value loss: 12.796166. Entropy: 0.719557.\n",
      "Iteration 8805: Policy loss: 0.984854. Value loss: 9.972797. Entropy: 0.706746.\n",
      "episode: 3896   score: 190.0  epsilon: 1.0    steps: 617  evaluation reward: 264.0\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8806: Policy loss: -2.854349. Value loss: 352.083099. Entropy: 0.694854.\n",
      "Iteration 8807: Policy loss: -1.664058. Value loss: 119.387756. Entropy: 0.679231.\n",
      "Iteration 8808: Policy loss: -2.217090. Value loss: 111.199326. Entropy: 0.675065.\n",
      "episode: 3897   score: 465.0  epsilon: 1.0    steps: 271  evaluation reward: 266.45\n",
      "episode: 3898   score: 325.0  epsilon: 1.0    steps: 466  evaluation reward: 269.15\n",
      "episode: 3899   score: 470.0  epsilon: 1.0    steps: 716  evaluation reward: 272.0\n",
      "episode: 3900   score: 285.0  epsilon: 1.0    steps: 886  evaluation reward: 271.45\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8809: Policy loss: 2.850623. Value loss: 109.759087. Entropy: 0.609902.\n",
      "Iteration 8810: Policy loss: 2.332426. Value loss: 49.339794. Entropy: 0.598204.\n",
      "Iteration 8811: Policy loss: 2.841966. Value loss: 37.225029. Entropy: 0.631618.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8812: Policy loss: 2.607757. Value loss: 20.353346. Entropy: 0.515763.\n",
      "Iteration 8813: Policy loss: 2.594136. Value loss: 13.571622. Entropy: 0.535463.\n",
      "Iteration 8814: Policy loss: 2.467889. Value loss: 10.680711. Entropy: 0.516885.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8815: Policy loss: 1.372878. Value loss: 28.671570. Entropy: 0.748070.\n",
      "Iteration 8816: Policy loss: 1.692087. Value loss: 18.363453. Entropy: 0.726971.\n",
      "Iteration 8817: Policy loss: 1.526664. Value loss: 14.286839. Entropy: 0.771936.\n",
      "now time :  2019-02-25 21:24:56.697035\n",
      "episode: 3901   score: 285.0  epsilon: 1.0    steps: 242  evaluation reward: 271.75\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8818: Policy loss: 1.001799. Value loss: 50.042927. Entropy: 0.528011.\n",
      "Iteration 8819: Policy loss: 1.035695. Value loss: 26.084940. Entropy: 0.517257.\n",
      "Iteration 8820: Policy loss: 0.849280. Value loss: 20.816351. Entropy: 0.526846.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8821: Policy loss: 1.384370. Value loss: 32.981056. Entropy: 0.548088.\n",
      "Iteration 8822: Policy loss: 1.353339. Value loss: 20.785660. Entropy: 0.579449.\n",
      "Iteration 8823: Policy loss: 1.308220. Value loss: 18.591324. Entropy: 0.579156.\n",
      "episode: 3902   score: 260.0  epsilon: 1.0    steps: 608  evaluation reward: 272.15\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8824: Policy loss: -0.760253. Value loss: 31.862055. Entropy: 0.689339.\n",
      "Iteration 8825: Policy loss: -0.533211. Value loss: 14.772726. Entropy: 0.709526.\n",
      "Iteration 8826: Policy loss: -0.575646. Value loss: 11.834358. Entropy: 0.699945.\n",
      "episode: 3903   score: 395.0  epsilon: 1.0    steps: 55  evaluation reward: 274.1\n",
      "episode: 3904   score: 230.0  epsilon: 1.0    steps: 765  evaluation reward: 273.55\n",
      "episode: 3905   score: 205.0  epsilon: 1.0    steps: 984  evaluation reward: 272.7\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8827: Policy loss: 0.128106. Value loss: 282.005615. Entropy: 0.700783.\n",
      "Iteration 8828: Policy loss: 1.260178. Value loss: 115.055077. Entropy: 0.694024.\n",
      "Iteration 8829: Policy loss: 1.081562. Value loss: 70.436989. Entropy: 0.671214.\n",
      "episode: 3906   score: 205.0  epsilon: 1.0    steps: 415  evaluation reward: 271.9\n",
      "episode: 3907   score: 455.0  epsilon: 1.0    steps: 865  evaluation reward: 273.4\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8830: Policy loss: -0.459389. Value loss: 16.086061. Entropy: 0.648315.\n",
      "Iteration 8831: Policy loss: -0.551399. Value loss: 12.512560. Entropy: 0.659269.\n",
      "Iteration 8832: Policy loss: -0.266559. Value loss: 9.689855. Entropy: 0.671407.\n",
      "episode: 3908   score: 360.0  epsilon: 1.0    steps: 328  evaluation reward: 275.45\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8833: Policy loss: -1.806637. Value loss: 27.922514. Entropy: 0.735441.\n",
      "Iteration 8834: Policy loss: -1.894606. Value loss: 20.208462. Entropy: 0.718561.\n",
      "Iteration 8835: Policy loss: -1.996829. Value loss: 17.491594. Entropy: 0.700690.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8836: Policy loss: 0.115263. Value loss: 52.079399. Entropy: 0.630271.\n",
      "Iteration 8837: Policy loss: 0.191953. Value loss: 31.464003. Entropy: 0.647474.\n",
      "Iteration 8838: Policy loss: 0.290347. Value loss: 21.205473. Entropy: 0.638232.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8839: Policy loss: -1.850296. Value loss: 199.000504. Entropy: 0.754813.\n",
      "Iteration 8840: Policy loss: -1.438241. Value loss: 114.249687. Entropy: 0.758726.\n",
      "Iteration 8841: Policy loss: -1.819773. Value loss: 82.994987. Entropy: 0.733347.\n",
      "episode: 3909   score: 250.0  epsilon: 1.0    steps: 143  evaluation reward: 276.45\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8842: Policy loss: 1.221076. Value loss: 21.117170. Entropy: 0.635767.\n",
      "Iteration 8843: Policy loss: 1.552995. Value loss: 16.752926. Entropy: 0.668692.\n",
      "Iteration 8844: Policy loss: 1.308242. Value loss: 12.749344. Entropy: 0.642769.\n",
      "episode: 3910   score: 285.0  epsilon: 1.0    steps: 40  evaluation reward: 276.25\n",
      "episode: 3911   score: 275.0  epsilon: 1.0    steps: 563  evaluation reward: 274.9\n",
      "episode: 3912   score: 520.0  epsilon: 1.0    steps: 995  evaluation reward: 276.35\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8845: Policy loss: 1.437764. Value loss: 31.677904. Entropy: 0.677836.\n",
      "Iteration 8846: Policy loss: 1.724414. Value loss: 19.039322. Entropy: 0.705720.\n",
      "Iteration 8847: Policy loss: 1.602686. Value loss: 11.079899. Entropy: 0.687633.\n",
      "episode: 3913   score: 260.0  epsilon: 1.0    steps: 394  evaluation reward: 278.15\n",
      "episode: 3914   score: 260.0  epsilon: 1.0    steps: 879  evaluation reward: 278.15\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8848: Policy loss: -0.418735. Value loss: 37.070370. Entropy: 0.668135.\n",
      "Iteration 8849: Policy loss: -0.547138. Value loss: 17.819374. Entropy: 0.656999.\n",
      "Iteration 8850: Policy loss: -0.503045. Value loss: 13.881610. Entropy: 0.668497.\n",
      "episode: 3915   score: 260.0  epsilon: 1.0    steps: 699  evaluation reward: 278.9\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8851: Policy loss: -1.713880. Value loss: 202.987305. Entropy: 0.682789.\n",
      "Iteration 8852: Policy loss: -1.899145. Value loss: 117.576584. Entropy: 0.705339.\n",
      "Iteration 8853: Policy loss: -1.821752. Value loss: 99.115128. Entropy: 0.702762.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8854: Policy loss: -0.117921. Value loss: 112.994011. Entropy: 0.612856.\n",
      "Iteration 8855: Policy loss: 0.376050. Value loss: 48.897522. Entropy: 0.612699.\n",
      "Iteration 8856: Policy loss: -0.241952. Value loss: 25.995865. Entropy: 0.593221.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8857: Policy loss: 2.421155. Value loss: 56.880024. Entropy: 0.586707.\n",
      "Iteration 8858: Policy loss: 2.573584. Value loss: 26.808720. Entropy: 0.581872.\n",
      "Iteration 8859: Policy loss: 2.703667. Value loss: 23.407274. Entropy: 0.576045.\n",
      "episode: 3916   score: 455.0  epsilon: 1.0    steps: 249  evaluation reward: 282.3\n",
      "episode: 3917   score: 470.0  epsilon: 1.0    steps: 272  evaluation reward: 285.8\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8860: Policy loss: 2.401341. Value loss: 51.460110. Entropy: 0.669290.\n",
      "Iteration 8861: Policy loss: 2.283026. Value loss: 35.904079. Entropy: 0.662234.\n",
      "Iteration 8862: Policy loss: 2.340107. Value loss: 26.693146. Entropy: 0.689779.\n",
      "episode: 3918   score: 260.0  epsilon: 1.0    steps: 447  evaluation reward: 285.45\n",
      "episode: 3919   score: 285.0  epsilon: 1.0    steps: 554  evaluation reward: 286.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8863: Policy loss: 0.998643. Value loss: 32.631325. Entropy: 0.688653.\n",
      "Iteration 8864: Policy loss: 1.341176. Value loss: 15.060885. Entropy: 0.663825.\n",
      "Iteration 8865: Policy loss: 1.006575. Value loss: 11.732555. Entropy: 0.693201.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8866: Policy loss: -0.588970. Value loss: 32.587265. Entropy: 0.745340.\n",
      "Iteration 8867: Policy loss: -0.518496. Value loss: 17.147516. Entropy: 0.761659.\n",
      "Iteration 8868: Policy loss: -0.334998. Value loss: 12.820733. Entropy: 0.749003.\n",
      "episode: 3920   score: 280.0  epsilon: 1.0    steps: 904  evaluation reward: 286.2\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8869: Policy loss: -2.750774. Value loss: 256.889008. Entropy: 0.642282.\n",
      "Iteration 8870: Policy loss: -2.837662. Value loss: 172.323715. Entropy: 0.654903.\n",
      "Iteration 8871: Policy loss: -3.305166. Value loss: 211.710556. Entropy: 0.626282.\n",
      "episode: 3921   score: 440.0  epsilon: 1.0    steps: 126  evaluation reward: 289.6\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8872: Policy loss: -0.697934. Value loss: 233.641006. Entropy: 0.556440.\n",
      "Iteration 8873: Policy loss: -0.931612. Value loss: 209.146225. Entropy: 0.580841.\n",
      "Iteration 8874: Policy loss: -0.129170. Value loss: 95.349037. Entropy: 0.564514.\n",
      "episode: 3922   score: 445.0  epsilon: 1.0    steps: 366  evaluation reward: 293.0\n",
      "episode: 3923   score: 300.0  epsilon: 1.0    steps: 701  evaluation reward: 293.6\n",
      "episode: 3924   score: 660.0  epsilon: 1.0    steps: 836  evaluation reward: 298.65\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8875: Policy loss: 0.446443. Value loss: 39.268703. Entropy: 0.546087.\n",
      "Iteration 8876: Policy loss: 0.377007. Value loss: 22.860071. Entropy: 0.557852.\n",
      "Iteration 8877: Policy loss: 0.320136. Value loss: 19.014149. Entropy: 0.560918.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8878: Policy loss: 1.077748. Value loss: 40.228653. Entropy: 0.593100.\n",
      "Iteration 8879: Policy loss: 1.166489. Value loss: 25.072241. Entropy: 0.599052.\n",
      "Iteration 8880: Policy loss: 1.098014. Value loss: 21.988419. Entropy: 0.602605.\n",
      "episode: 3925   score: 290.0  epsilon: 1.0    steps: 471  evaluation reward: 299.3\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8881: Policy loss: 2.473018. Value loss: 35.760967. Entropy: 0.710707.\n",
      "Iteration 8882: Policy loss: 2.466579. Value loss: 21.273155. Entropy: 0.699759.\n",
      "Iteration 8883: Policy loss: 2.352095. Value loss: 17.076706. Entropy: 0.685892.\n",
      "episode: 3926   score: 295.0  epsilon: 1.0    steps: 164  evaluation reward: 299.65\n",
      "episode: 3927   score: 455.0  epsilon: 1.0    steps: 635  evaluation reward: 301.35\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8884: Policy loss: 0.601734. Value loss: 33.345604. Entropy: 0.703487.\n",
      "Iteration 8885: Policy loss: 0.344832. Value loss: 21.269167. Entropy: 0.709682.\n",
      "Iteration 8886: Policy loss: 0.238414. Value loss: 14.109820. Entropy: 0.685869.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8887: Policy loss: 0.732202. Value loss: 45.350777. Entropy: 0.818437.\n",
      "Iteration 8888: Policy loss: 0.654688. Value loss: 20.520586. Entropy: 0.798021.\n",
      "Iteration 8889: Policy loss: 0.706667. Value loss: 15.376745. Entropy: 0.788092.\n",
      "episode: 3928   score: 330.0  epsilon: 1.0    steps: 996  evaluation reward: 300.45\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8890: Policy loss: 1.574120. Value loss: 50.022789. Entropy: 0.626142.\n",
      "Iteration 8891: Policy loss: 1.471382. Value loss: 25.489639. Entropy: 0.641141.\n",
      "Iteration 8892: Policy loss: 1.529912. Value loss: 19.308367. Entropy: 0.632789.\n",
      "episode: 3929   score: 320.0  epsilon: 1.0    steps: 743  evaluation reward: 299.1\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8893: Policy loss: 2.617263. Value loss: 30.611391. Entropy: 0.564879.\n",
      "Iteration 8894: Policy loss: 2.657314. Value loss: 17.306099. Entropy: 0.542420.\n",
      "Iteration 8895: Policy loss: 2.553596. Value loss: 13.457222. Entropy: 0.557041.\n",
      "episode: 3930   score: 350.0  epsilon: 1.0    steps: 115  evaluation reward: 300.75\n",
      "episode: 3931   score: 225.0  epsilon: 1.0    steps: 775  evaluation reward: 300.8\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8896: Policy loss: 0.417451. Value loss: 39.887535. Entropy: 0.509211.\n",
      "Iteration 8897: Policy loss: 0.562054. Value loss: 22.115604. Entropy: 0.512095.\n",
      "Iteration 8898: Policy loss: 0.413254. Value loss: 18.034647. Entropy: 0.514280.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8899: Policy loss: 1.030081. Value loss: 40.345207. Entropy: 0.604811.\n",
      "Iteration 8900: Policy loss: 1.278316. Value loss: 25.081060. Entropy: 0.621028.\n",
      "Iteration 8901: Policy loss: 1.238677. Value loss: 15.961528. Entropy: 0.627014.\n",
      "episode: 3932   score: 230.0  epsilon: 1.0    steps: 263  evaluation reward: 299.2\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8902: Policy loss: 0.277039. Value loss: 22.355648. Entropy: 0.848264.\n",
      "Iteration 8903: Policy loss: 0.326492. Value loss: 15.769560. Entropy: 0.819139.\n",
      "Iteration 8904: Policy loss: 0.388887. Value loss: 11.212623. Entropy: 0.829367.\n",
      "episode: 3933   score: 350.0  epsilon: 1.0    steps: 176  evaluation reward: 297.85\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8905: Policy loss: 1.090327. Value loss: 26.578392. Entropy: 0.756732.\n",
      "Iteration 8906: Policy loss: 1.108158. Value loss: 15.394588. Entropy: 0.712366.\n",
      "Iteration 8907: Policy loss: 0.828840. Value loss: 12.871539. Entropy: 0.744480.\n",
      "episode: 3934   score: 285.0  epsilon: 1.0    steps: 597  evaluation reward: 298.05\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8908: Policy loss: 0.910016. Value loss: 25.631613. Entropy: 0.804256.\n",
      "Iteration 8909: Policy loss: 0.799685. Value loss: 13.199418. Entropy: 0.811029.\n",
      "Iteration 8910: Policy loss: 0.794996. Value loss: 11.129135. Entropy: 0.792586.\n",
      "episode: 3935   score: 155.0  epsilon: 1.0    steps: 376  evaluation reward: 298.55\n",
      "episode: 3936   score: 555.0  epsilon: 1.0    steps: 462  evaluation reward: 300.8\n",
      "episode: 3937   score: 260.0  epsilon: 1.0    steps: 709  evaluation reward: 301.3\n",
      "episode: 3938   score: 260.0  epsilon: 1.0    steps: 879  evaluation reward: 300.25\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8911: Policy loss: 2.085306. Value loss: 42.046310. Entropy: 0.740927.\n",
      "Iteration 8912: Policy loss: 1.983014. Value loss: 22.134529. Entropy: 0.763686.\n",
      "Iteration 8913: Policy loss: 2.087267. Value loss: 14.749845. Entropy: 0.739919.\n",
      "episode: 3939   score: 205.0  epsilon: 1.0    steps: 65  evaluation reward: 299.5\n",
      "episode: 3940   score: 315.0  epsilon: 1.0    steps: 953  evaluation reward: 300.55\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8914: Policy loss: 1.440794. Value loss: 26.331528. Entropy: 0.780281.\n",
      "Iteration 8915: Policy loss: 1.526848. Value loss: 18.226559. Entropy: 0.791286.\n",
      "Iteration 8916: Policy loss: 1.534236. Value loss: 13.389933. Entropy: 0.786161.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8917: Policy loss: -0.962416. Value loss: 31.122278. Entropy: 0.877958.\n",
      "Iteration 8918: Policy loss: -0.920962. Value loss: 23.089323. Entropy: 0.866973.\n",
      "Iteration 8919: Policy loss: -0.752670. Value loss: 19.535978. Entropy: 0.872214.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8920: Policy loss: -0.108611. Value loss: 34.837173. Entropy: 0.916368.\n",
      "Iteration 8921: Policy loss: -0.087849. Value loss: 19.559887. Entropy: 0.887432.\n",
      "Iteration 8922: Policy loss: 0.012664. Value loss: 16.281118. Entropy: 0.919032.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8923: Policy loss: -0.371986. Value loss: 38.077068. Entropy: 0.697713.\n",
      "Iteration 8924: Policy loss: -0.457350. Value loss: 18.818573. Entropy: 0.720004.\n",
      "Iteration 8925: Policy loss: -0.448083. Value loss: 14.479377. Entropy: 0.697795.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8926: Policy loss: -0.357323. Value loss: 32.523376. Entropy: 0.541041.\n",
      "Iteration 8927: Policy loss: -0.194861. Value loss: 15.047560. Entropy: 0.535814.\n",
      "Iteration 8928: Policy loss: -0.140846. Value loss: 12.015366. Entropy: 0.535358.\n",
      "episode: 3941   score: 380.0  epsilon: 1.0    steps: 166  evaluation reward: 302.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8929: Policy loss: 0.371774. Value loss: 36.830704. Entropy: 0.516407.\n",
      "Iteration 8930: Policy loss: 0.586180. Value loss: 20.610235. Entropy: 0.502874.\n",
      "Iteration 8931: Policy loss: 0.242646. Value loss: 14.798834. Entropy: 0.499160.\n",
      "episode: 3942   score: 270.0  epsilon: 1.0    steps: 264  evaluation reward: 300.45\n",
      "episode: 3943   score: 455.0  epsilon: 1.0    steps: 616  evaluation reward: 299.75\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8932: Policy loss: -0.401930. Value loss: 52.623158. Entropy: 0.570642.\n",
      "Iteration 8933: Policy loss: -0.478637. Value loss: 26.137793. Entropy: 0.568770.\n",
      "Iteration 8934: Policy loss: -0.080016. Value loss: 21.367651. Entropy: 0.584755.\n",
      "episode: 3944   score: 200.0  epsilon: 1.0    steps: 660  evaluation reward: 298.4\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8935: Policy loss: -1.685388. Value loss: 212.108536. Entropy: 0.789417.\n",
      "Iteration 8936: Policy loss: -2.576203. Value loss: 186.111206. Entropy: 0.742749.\n",
      "Iteration 8937: Policy loss: -3.008534. Value loss: 136.205948. Entropy: 0.702203.\n",
      "episode: 3945   score: 420.0  epsilon: 1.0    steps: 809  evaluation reward: 300.8\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8938: Policy loss: 0.746783. Value loss: 35.122314. Entropy: 0.719903.\n",
      "Iteration 8939: Policy loss: 0.539906. Value loss: 17.241005. Entropy: 0.746344.\n",
      "Iteration 8940: Policy loss: 0.667466. Value loss: 13.273768. Entropy: 0.727004.\n",
      "episode: 3946   score: 425.0  epsilon: 1.0    steps: 78  evaluation reward: 302.8\n",
      "episode: 3947   score: 660.0  epsilon: 1.0    steps: 429  evaluation reward: 308.45\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8941: Policy loss: 0.176961. Value loss: 33.925083. Entropy: 0.604129.\n",
      "Iteration 8942: Policy loss: 0.308827. Value loss: 21.415396. Entropy: 0.618216.\n",
      "Iteration 8943: Policy loss: 0.309792. Value loss: 17.850092. Entropy: 0.600971.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8944: Policy loss: 1.652881. Value loss: 28.175535. Entropy: 0.787636.\n",
      "Iteration 8945: Policy loss: 1.717322. Value loss: 14.937236. Entropy: 0.776952.\n",
      "Iteration 8946: Policy loss: 1.722733. Value loss: 11.562606. Entropy: 0.779369.\n",
      "episode: 3948   score: 260.0  epsilon: 1.0    steps: 178  evaluation reward: 308.95\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8947: Policy loss: -1.199223. Value loss: 32.128799. Entropy: 0.745796.\n",
      "Iteration 8948: Policy loss: -1.367077. Value loss: 17.950212. Entropy: 0.767324.\n",
      "Iteration 8949: Policy loss: -1.124859. Value loss: 16.201389. Entropy: 0.739883.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8950: Policy loss: 2.818730. Value loss: 29.073496. Entropy: 0.666088.\n",
      "Iteration 8951: Policy loss: 2.871122. Value loss: 16.341125. Entropy: 0.678699.\n",
      "Iteration 8952: Policy loss: 2.847381. Value loss: 13.703667. Entropy: 0.681732.\n",
      "episode: 3949   score: 570.0  epsilon: 1.0    steps: 996  evaluation reward: 312.6\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8953: Policy loss: 0.925559. Value loss: 38.131271. Entropy: 0.611170.\n",
      "Iteration 8954: Policy loss: 1.000846. Value loss: 23.893299. Entropy: 0.600833.\n",
      "Iteration 8955: Policy loss: 0.921469. Value loss: 19.202415. Entropy: 0.607611.\n",
      "episode: 3950   score: 265.0  epsilon: 1.0    steps: 847  evaluation reward: 313.45\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8956: Policy loss: 1.282188. Value loss: 35.973629. Entropy: 0.620486.\n",
      "Iteration 8957: Policy loss: 1.211501. Value loss: 23.232769. Entropy: 0.645499.\n",
      "Iteration 8958: Policy loss: 1.159144. Value loss: 14.971876. Entropy: 0.626265.\n",
      "now time :  2019-02-25 21:27:33.831332\n",
      "episode: 3951   score: 285.0  epsilon: 1.0    steps: 90  evaluation reward: 315.4\n",
      "episode: 3952   score: 455.0  epsilon: 1.0    steps: 517  evaluation reward: 314.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8959: Policy loss: 0.975786. Value loss: 28.782099. Entropy: 0.648423.\n",
      "Iteration 8960: Policy loss: 1.042454. Value loss: 14.958372. Entropy: 0.643909.\n",
      "Iteration 8961: Policy loss: 0.929610. Value loss: 9.611959. Entropy: 0.630326.\n",
      "episode: 3953   score: 345.0  epsilon: 1.0    steps: 717  evaluation reward: 314.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8962: Policy loss: -3.200865. Value loss: 235.524826. Entropy: 0.594132.\n",
      "Iteration 8963: Policy loss: -3.069880. Value loss: 117.227798. Entropy: 0.541248.\n",
      "Iteration 8964: Policy loss: -3.487799. Value loss: 68.816833. Entropy: 0.607523.\n",
      "episode: 3954   score: 435.0  epsilon: 1.0    steps: 329  evaluation reward: 316.9\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8965: Policy loss: 3.071964. Value loss: 46.330730. Entropy: 0.714759.\n",
      "Iteration 8966: Policy loss: 2.533494. Value loss: 21.017183. Entropy: 0.690126.\n",
      "Iteration 8967: Policy loss: 3.149651. Value loss: 15.858661. Entropy: 0.716766.\n",
      "episode: 3955   score: 525.0  epsilon: 1.0    steps: 137  evaluation reward: 320.3\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8968: Policy loss: 2.095780. Value loss: 31.908689. Entropy: 0.648524.\n",
      "Iteration 8969: Policy loss: 2.183001. Value loss: 17.294367. Entropy: 0.675695.\n",
      "Iteration 8970: Policy loss: 1.931768. Value loss: 13.717261. Entropy: 0.689692.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8971: Policy loss: 0.528512. Value loss: 31.588293. Entropy: 0.554983.\n",
      "Iteration 8972: Policy loss: 0.563087. Value loss: 16.361250. Entropy: 0.538544.\n",
      "Iteration 8973: Policy loss: 0.805063. Value loss: 13.848765. Entropy: 0.577414.\n",
      "episode: 3956   score: 410.0  epsilon: 1.0    steps: 475  evaluation reward: 321.9\n",
      "episode: 3957   score: 355.0  epsilon: 1.0    steps: 991  evaluation reward: 322.85\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8974: Policy loss: 1.501338. Value loss: 52.070904. Entropy: 0.662927.\n",
      "Iteration 8975: Policy loss: 1.854977. Value loss: 24.091377. Entropy: 0.566444.\n",
      "Iteration 8976: Policy loss: 1.135639. Value loss: 18.888924. Entropy: 0.607542.\n",
      "episode: 3958   score: 255.0  epsilon: 1.0    steps: 781  evaluation reward: 324.6\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8977: Policy loss: -3.407409. Value loss: 340.717407. Entropy: 0.655202.\n",
      "Iteration 8978: Policy loss: -1.904662. Value loss: 158.147827. Entropy: 0.639779.\n",
      "Iteration 8979: Policy loss: -1.967940. Value loss: 90.428444. Entropy: 0.660727.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8980: Policy loss: -1.883690. Value loss: 279.923523. Entropy: 0.659060.\n",
      "Iteration 8981: Policy loss: -1.573840. Value loss: 133.281921. Entropy: 0.638429.\n",
      "Iteration 8982: Policy loss: -1.754780. Value loss: 102.058624. Entropy: 0.659285.\n",
      "episode: 3959   score: 425.0  epsilon: 1.0    steps: 30  evaluation reward: 326.2\n",
      "episode: 3960   score: 390.0  epsilon: 1.0    steps: 518  evaluation reward: 327.5\n",
      "episode: 3961   score: 305.0  epsilon: 1.0    steps: 649  evaluation reward: 326.3\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8983: Policy loss: 0.445632. Value loss: 57.398075. Entropy: 0.650515.\n",
      "Iteration 8984: Policy loss: 0.420868. Value loss: 35.309212. Entropy: 0.629608.\n",
      "Iteration 8985: Policy loss: 0.551278. Value loss: 27.722864. Entropy: 0.626448.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8986: Policy loss: 1.657114. Value loss: 255.140991. Entropy: 0.569112.\n",
      "Iteration 8987: Policy loss: 1.927474. Value loss: 199.604736. Entropy: 0.581251.\n",
      "Iteration 8988: Policy loss: 1.866038. Value loss: 103.643898. Entropy: 0.545624.\n",
      "episode: 3962   score: 565.0  epsilon: 1.0    steps: 262  evaluation reward: 331.4\n",
      "episode: 3963   score: 210.0  epsilon: 1.0    steps: 495  evaluation reward: 331.7\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8989: Policy loss: 1.702111. Value loss: 44.413624. Entropy: 0.725109.\n",
      "Iteration 8990: Policy loss: 1.484644. Value loss: 25.144699. Entropy: 0.726795.\n",
      "Iteration 8991: Policy loss: 1.511043. Value loss: 20.559109. Entropy: 0.738040.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8992: Policy loss: -1.756047. Value loss: 50.628826. Entropy: 0.526879.\n",
      "Iteration 8993: Policy loss: -1.097221. Value loss: 24.649475. Entropy: 0.530579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8994: Policy loss: -1.700767. Value loss: 21.899963. Entropy: 0.523251.\n",
      "episode: 3964   score: 550.0  epsilon: 1.0    steps: 998  evaluation reward: 334.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8995: Policy loss: 2.353090. Value loss: 50.598362. Entropy: 0.555237.\n",
      "Iteration 8996: Policy loss: 2.101239. Value loss: 27.606026. Entropy: 0.582397.\n",
      "Iteration 8997: Policy loss: 2.183805. Value loss: 19.885807. Entropy: 0.558221.\n",
      "episode: 3965   score: 185.0  epsilon: 1.0    steps: 79  evaluation reward: 330.75\n",
      "episode: 3966   score: 740.0  epsilon: 1.0    steps: 194  evaluation reward: 335.5\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8998: Policy loss: 3.845306. Value loss: 62.456875. Entropy: 0.635810.\n",
      "Iteration 8999: Policy loss: 3.993078. Value loss: 28.504065. Entropy: 0.635572.\n",
      "Iteration 9000: Policy loss: 3.712785. Value loss: 24.680571. Entropy: 0.627586.\n",
      "episode: 3967   score: 260.0  epsilon: 1.0    steps: 539  evaluation reward: 336.25\n",
      "episode: 3968   score: 260.0  epsilon: 1.0    steps: 669  evaluation reward: 335.0\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9001: Policy loss: 0.192923. Value loss: 40.166027. Entropy: 0.486587.\n",
      "Iteration 9002: Policy loss: 0.185751. Value loss: 25.575865. Entropy: 0.488533.\n",
      "Iteration 9003: Policy loss: 0.185303. Value loss: 17.469149. Entropy: 0.493511.\n",
      "episode: 3969   score: 210.0  epsilon: 1.0    steps: 343  evaluation reward: 334.45\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9004: Policy loss: -2.804847. Value loss: 283.154694. Entropy: 0.611802.\n",
      "Iteration 9005: Policy loss: -1.235736. Value loss: 167.686905. Entropy: 0.638443.\n",
      "Iteration 9006: Policy loss: -1.813939. Value loss: 127.554367. Entropy: 0.581803.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9007: Policy loss: 1.123120. Value loss: 30.817295. Entropy: 0.578565.\n",
      "Iteration 9008: Policy loss: 0.549022. Value loss: 19.535370. Entropy: 0.594440.\n",
      "Iteration 9009: Policy loss: 0.696224. Value loss: 14.867102. Entropy: 0.594381.\n",
      "episode: 3970   score: 485.0  epsilon: 1.0    steps: 456  evaluation reward: 336.15\n",
      "episode: 3971   score: 605.0  epsilon: 1.0    steps: 868  evaluation reward: 338.65\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9010: Policy loss: 2.143971. Value loss: 224.000793. Entropy: 0.485479.\n",
      "Iteration 9011: Policy loss: 2.066143. Value loss: 147.396118. Entropy: 0.438346.\n",
      "Iteration 9012: Policy loss: 2.756391. Value loss: 110.039368. Entropy: 0.441012.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9013: Policy loss: -1.193005. Value loss: 45.383034. Entropy: 0.392965.\n",
      "Iteration 9014: Policy loss: -1.405919. Value loss: 19.328321. Entropy: 0.387450.\n",
      "Iteration 9015: Policy loss: -1.321828. Value loss: 15.100597. Entropy: 0.388449.\n",
      "episode: 3972   score: 285.0  epsilon: 1.0    steps: 63  evaluation reward: 336.0\n",
      "episode: 3973   score: 240.0  epsilon: 1.0    steps: 223  evaluation reward: 337.6\n",
      "episode: 3974   score: 180.0  epsilon: 1.0    steps: 591  evaluation reward: 336.65\n",
      "episode: 3975   score: 515.0  epsilon: 1.0    steps: 1004  evaluation reward: 339.55\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9016: Policy loss: 3.195210. Value loss: 75.541351. Entropy: 0.489022.\n",
      "Iteration 9017: Policy loss: 2.933301. Value loss: 35.929508. Entropy: 0.490745.\n",
      "Iteration 9018: Policy loss: 3.274769. Value loss: 29.333385. Entropy: 0.482901.\n",
      "episode: 3976   score: 325.0  epsilon: 1.0    steps: 748  evaluation reward: 341.45\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9019: Policy loss: 0.911531. Value loss: 37.927921. Entropy: 0.373115.\n",
      "Iteration 9020: Policy loss: 0.982247. Value loss: 23.092659. Entropy: 0.392934.\n",
      "Iteration 9021: Policy loss: 1.058483. Value loss: 16.235161. Entropy: 0.385311.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9022: Policy loss: 4.148160. Value loss: 48.860374. Entropy: 0.399508.\n",
      "Iteration 9023: Policy loss: 4.564361. Value loss: 21.954702. Entropy: 0.437813.\n",
      "Iteration 9024: Policy loss: 4.365147. Value loss: 18.100725. Entropy: 0.424350.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9025: Policy loss: 0.244575. Value loss: 41.724110. Entropy: 0.531171.\n",
      "Iteration 9026: Policy loss: 0.645557. Value loss: 22.734381. Entropy: 0.515465.\n",
      "Iteration 9027: Policy loss: 0.383639. Value loss: 17.519911. Entropy: 0.500960.\n",
      "episode: 3977   score: 290.0  epsilon: 1.0    steps: 328  evaluation reward: 342.25\n",
      "episode: 3978   score: 260.0  epsilon: 1.0    steps: 841  evaluation reward: 342.85\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9028: Policy loss: 2.854756. Value loss: 48.733669. Entropy: 0.358030.\n",
      "Iteration 9029: Policy loss: 2.263288. Value loss: 25.746723. Entropy: 0.339037.\n",
      "Iteration 9030: Policy loss: 2.471355. Value loss: 19.015459. Entropy: 0.346842.\n",
      "episode: 3979   score: 125.0  epsilon: 1.0    steps: 713  evaluation reward: 340.65\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9031: Policy loss: -0.863673. Value loss: 211.012650. Entropy: 0.481646.\n",
      "Iteration 9032: Policy loss: -0.970885. Value loss: 104.478287. Entropy: 0.480511.\n",
      "Iteration 9033: Policy loss: -0.783604. Value loss: 82.763809. Entropy: 0.496630.\n",
      "episode: 3980   score: 360.0  epsilon: 1.0    steps: 413  evaluation reward: 343.6\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9034: Policy loss: -2.239477. Value loss: 41.667667. Entropy: 0.543958.\n",
      "Iteration 9035: Policy loss: -1.859710. Value loss: 23.594099. Entropy: 0.513732.\n",
      "Iteration 9036: Policy loss: -2.414405. Value loss: 22.562792. Entropy: 0.512134.\n",
      "episode: 3981   score: 440.0  epsilon: 1.0    steps: 116  evaluation reward: 344.9\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9037: Policy loss: 0.747563. Value loss: 39.297226. Entropy: 0.474885.\n",
      "Iteration 9038: Policy loss: 0.425863. Value loss: 18.999519. Entropy: 0.457642.\n",
      "Iteration 9039: Policy loss: 0.906284. Value loss: 14.188243. Entropy: 0.461384.\n",
      "episode: 3982   score: 335.0  epsilon: 1.0    steps: 514  evaluation reward: 345.35\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9040: Policy loss: -2.870937. Value loss: 43.712429. Entropy: 0.358413.\n",
      "Iteration 9041: Policy loss: -3.060602. Value loss: 29.361979. Entropy: 0.342514.\n",
      "Iteration 9042: Policy loss: -3.015560. Value loss: 20.474928. Entropy: 0.349704.\n",
      "episode: 3983   score: 310.0  epsilon: 1.0    steps: 146  evaluation reward: 346.2\n",
      "episode: 3984   score: 470.0  epsilon: 1.0    steps: 1010  evaluation reward: 348.3\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9043: Policy loss: 2.360561. Value loss: 44.270771. Entropy: 0.325765.\n",
      "Iteration 9044: Policy loss: 2.584916. Value loss: 24.499531. Entropy: 0.344238.\n",
      "Iteration 9045: Policy loss: 2.430663. Value loss: 17.155342. Entropy: 0.352546.\n",
      "episode: 3985   score: 260.0  epsilon: 1.0    steps: 296  evaluation reward: 344.05\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9046: Policy loss: 0.754006. Value loss: 35.511875. Entropy: 0.592590.\n",
      "Iteration 9047: Policy loss: 0.792338. Value loss: 19.621275. Entropy: 0.589108.\n",
      "Iteration 9048: Policy loss: 1.109810. Value loss: 12.847786. Entropy: 0.579180.\n",
      "episode: 3986   score: 230.0  epsilon: 1.0    steps: 775  evaluation reward: 343.4\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9049: Policy loss: 0.528673. Value loss: 40.404148. Entropy: 0.406577.\n",
      "Iteration 9050: Policy loss: 0.460363. Value loss: 25.594074. Entropy: 0.425490.\n",
      "Iteration 9051: Policy loss: 0.404126. Value loss: 16.687849. Entropy: 0.413698.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9052: Policy loss: 1.075159. Value loss: 27.417809. Entropy: 0.519047.\n",
      "Iteration 9053: Policy loss: 0.991813. Value loss: 16.240442. Entropy: 0.488881.\n",
      "Iteration 9054: Policy loss: 1.132097. Value loss: 13.286852. Entropy: 0.485577.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9055: Policy loss: -2.901641. Value loss: 192.850250. Entropy: 0.410412.\n",
      "Iteration 9056: Policy loss: -2.294764. Value loss: 77.498627. Entropy: 0.428305.\n",
      "Iteration 9057: Policy loss: -2.843037. Value loss: 96.850784. Entropy: 0.417306.\n",
      "episode: 3987   score: 285.0  epsilon: 1.0    steps: 36  evaluation reward: 342.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3988   score: 285.0  epsilon: 1.0    steps: 589  evaluation reward: 342.4\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9058: Policy loss: 1.988192. Value loss: 28.781448. Entropy: 0.583007.\n",
      "Iteration 9059: Policy loss: 2.055382. Value loss: 15.401326. Entropy: 0.596556.\n",
      "Iteration 9060: Policy loss: 2.035673. Value loss: 12.502118. Entropy: 0.609525.\n",
      "episode: 3989   score: 270.0  epsilon: 1.0    steps: 167  evaluation reward: 343.4\n",
      "episode: 3990   score: 420.0  epsilon: 1.0    steps: 686  evaluation reward: 345.0\n",
      "episode: 3991   score: 240.0  epsilon: 1.0    steps: 992  evaluation reward: 344.5\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9061: Policy loss: 0.591462. Value loss: 211.009506. Entropy: 0.497091.\n",
      "Iteration 9062: Policy loss: 0.325799. Value loss: 120.813499. Entropy: 0.482274.\n",
      "Iteration 9063: Policy loss: 1.065930. Value loss: 83.439880. Entropy: 0.479694.\n",
      "episode: 3992   score: 460.0  epsilon: 1.0    steps: 363  evaluation reward: 345.25\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9064: Policy loss: 1.929203. Value loss: 31.966427. Entropy: 0.543855.\n",
      "Iteration 9065: Policy loss: 1.419490. Value loss: 22.949677. Entropy: 0.544628.\n",
      "Iteration 9066: Policy loss: 1.891139. Value loss: 18.147312. Entropy: 0.552455.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9067: Policy loss: -1.030297. Value loss: 35.492336. Entropy: 0.740912.\n",
      "Iteration 9068: Policy loss: -1.234719. Value loss: 22.489634. Entropy: 0.703977.\n",
      "Iteration 9069: Policy loss: -0.911974. Value loss: 15.710962. Entropy: 0.699396.\n",
      "episode: 3993   score: 610.0  epsilon: 1.0    steps: 426  evaluation reward: 346.6\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9070: Policy loss: 0.464346. Value loss: 35.340343. Entropy: 0.514564.\n",
      "Iteration 9071: Policy loss: 0.112294. Value loss: 22.661142. Entropy: 0.507511.\n",
      "Iteration 9072: Policy loss: 0.385387. Value loss: 14.907140. Entropy: 0.500550.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9073: Policy loss: -1.346341. Value loss: 187.586365. Entropy: 0.655973.\n",
      "Iteration 9074: Policy loss: -1.329847. Value loss: 77.360992. Entropy: 0.664167.\n",
      "Iteration 9075: Policy loss: -1.305976. Value loss: 69.064308. Entropy: 0.648065.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9076: Policy loss: -1.109223. Value loss: 33.606865. Entropy: 0.422305.\n",
      "Iteration 9077: Policy loss: -0.882519. Value loss: 16.832735. Entropy: 0.408785.\n",
      "Iteration 9078: Policy loss: -1.017431. Value loss: 12.835678. Entropy: 0.406824.\n",
      "episode: 3994   score: 230.0  epsilon: 1.0    steps: 69  evaluation reward: 346.35\n",
      "episode: 3995   score: 490.0  epsilon: 1.0    steps: 540  evaluation reward: 347.9\n",
      "episode: 3996   score: 420.0  epsilon: 1.0    steps: 779  evaluation reward: 350.2\n",
      "episode: 3997   score: 260.0  epsilon: 1.0    steps: 978  evaluation reward: 348.15\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9079: Policy loss: 1.090267. Value loss: 217.546570. Entropy: 0.499201.\n",
      "Iteration 9080: Policy loss: 1.110371. Value loss: 102.969872. Entropy: 0.496511.\n",
      "Iteration 9081: Policy loss: 1.513134. Value loss: 78.964127. Entropy: 0.503961.\n",
      "episode: 3998   score: 290.0  epsilon: 1.0    steps: 134  evaluation reward: 347.8\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9082: Policy loss: -0.532161. Value loss: 43.432499. Entropy: 0.652076.\n",
      "Iteration 9083: Policy loss: -0.752447. Value loss: 23.919655. Entropy: 0.656829.\n",
      "Iteration 9084: Policy loss: -0.322352. Value loss: 20.292788. Entropy: 0.680789.\n",
      "episode: 3999   score: 320.0  epsilon: 1.0    steps: 367  evaluation reward: 346.3\n",
      "episode: 4000   score: 370.0  epsilon: 1.0    steps: 673  evaluation reward: 347.15\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9085: Policy loss: 0.073820. Value loss: 41.208736. Entropy: 0.766980.\n",
      "Iteration 9086: Policy loss: -0.043250. Value loss: 26.061983. Entropy: 0.746253.\n",
      "Iteration 9087: Policy loss: 0.077221. Value loss: 21.600286. Entropy: 0.747779.\n",
      "now time :  2019-02-25 21:30:00.219957\n",
      "episode: 4001   score: 465.0  epsilon: 1.0    steps: 499  evaluation reward: 348.95\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9088: Policy loss: 5.777267. Value loss: 102.394356. Entropy: 0.538933.\n",
      "Iteration 9089: Policy loss: 5.759984. Value loss: 42.898003. Entropy: 0.509842.\n",
      "Iteration 9090: Policy loss: 5.780955. Value loss: 25.298098. Entropy: 0.545357.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9091: Policy loss: 2.819723. Value loss: 36.912594. Entropy: 0.460002.\n",
      "Iteration 9092: Policy loss: 2.813188. Value loss: 25.118759. Entropy: 0.470741.\n",
      "Iteration 9093: Policy loss: 2.901443. Value loss: 16.503841. Entropy: 0.474549.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9094: Policy loss: 1.280694. Value loss: 32.992237. Entropy: 0.604121.\n",
      "Iteration 9095: Policy loss: 1.280159. Value loss: 17.793840. Entropy: 0.606200.\n",
      "Iteration 9096: Policy loss: 1.466384. Value loss: 17.347025. Entropy: 0.632363.\n",
      "episode: 4002   score: 240.0  epsilon: 1.0    steps: 81  evaluation reward: 348.75\n",
      "episode: 4003   score: 210.0  epsilon: 1.0    steps: 559  evaluation reward: 346.9\n",
      "episode: 4004   score: 240.0  epsilon: 1.0    steps: 840  evaluation reward: 347.0\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9097: Policy loss: 4.131847. Value loss: 37.989853. Entropy: 0.547876.\n",
      "Iteration 9098: Policy loss: 3.621432. Value loss: 24.033409. Entropy: 0.566330.\n",
      "Iteration 9099: Policy loss: 3.807542. Value loss: 17.013496. Entropy: 0.575926.\n",
      "episode: 4005   score: 240.0  epsilon: 1.0    steps: 143  evaluation reward: 347.35\n",
      "episode: 4006   score: 210.0  epsilon: 1.0    steps: 370  evaluation reward: 347.4\n",
      "episode: 4007   score: 325.0  epsilon: 1.0    steps: 969  evaluation reward: 346.1\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9100: Policy loss: -0.479366. Value loss: 19.151756. Entropy: 0.494758.\n",
      "Iteration 9101: Policy loss: -0.435647. Value loss: 16.285660. Entropy: 0.503222.\n",
      "Iteration 9102: Policy loss: -0.406671. Value loss: 13.692245. Entropy: 0.504220.\n",
      "episode: 4008   score: 285.0  epsilon: 1.0    steps: 766  evaluation reward: 345.35\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9103: Policy loss: 1.818247. Value loss: 29.920574. Entropy: 0.625594.\n",
      "Iteration 9104: Policy loss: 1.525526. Value loss: 17.868240. Entropy: 0.637234.\n",
      "Iteration 9105: Policy loss: 1.633733. Value loss: 13.474836. Entropy: 0.653358.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9106: Policy loss: 0.304017. Value loss: 33.544128. Entropy: 0.574972.\n",
      "Iteration 9107: Policy loss: 0.481606. Value loss: 15.382659. Entropy: 0.567604.\n",
      "Iteration 9108: Policy loss: 0.141725. Value loss: 12.184917. Entropy: 0.571149.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9109: Policy loss: -2.935674. Value loss: 146.702515. Entropy: 0.476708.\n",
      "Iteration 9110: Policy loss: -3.875926. Value loss: 112.965569. Entropy: 0.510922.\n",
      "Iteration 9111: Policy loss: -2.753019. Value loss: 54.171078. Entropy: 0.470293.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9112: Policy loss: -2.986523. Value loss: 111.558922. Entropy: 0.468236.\n",
      "Iteration 9113: Policy loss: -2.605300. Value loss: 60.407730. Entropy: 0.476648.\n",
      "Iteration 9114: Policy loss: -2.812826. Value loss: 54.531673. Entropy: 0.474947.\n",
      "episode: 4009   score: 360.0  epsilon: 1.0    steps: 420  evaluation reward: 346.45\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9115: Policy loss: 0.073068. Value loss: 62.505840. Entropy: 0.495463.\n",
      "Iteration 9116: Policy loss: -0.336335. Value loss: 27.156330. Entropy: 0.481510.\n",
      "Iteration 9117: Policy loss: -0.283638. Value loss: 18.236830. Entropy: 0.488048.\n",
      "episode: 4010   score: 285.0  epsilon: 1.0    steps: 140  evaluation reward: 346.45\n",
      "episode: 4011   score: 335.0  epsilon: 1.0    steps: 544  evaluation reward: 347.05\n",
      "episode: 4012   score: 490.0  epsilon: 1.0    steps: 986  evaluation reward: 346.75\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9118: Policy loss: 1.859878. Value loss: 25.488724. Entropy: 0.528920.\n",
      "Iteration 9119: Policy loss: 1.804965. Value loss: 14.355721. Entropy: 0.498629.\n",
      "Iteration 9120: Policy loss: 1.700959. Value loss: 11.818417. Entropy: 0.523653.\n",
      "episode: 4013   score: 325.0  epsilon: 1.0    steps: 28  evaluation reward: 347.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4014   score: 285.0  epsilon: 1.0    steps: 326  evaluation reward: 347.65\n",
      "episode: 4015   score: 600.0  epsilon: 1.0    steps: 879  evaluation reward: 351.05\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9121: Policy loss: 1.897357. Value loss: 57.850929. Entropy: 0.582639.\n",
      "Iteration 9122: Policy loss: 1.511711. Value loss: 24.092590. Entropy: 0.579345.\n",
      "Iteration 9123: Policy loss: 1.947308. Value loss: 17.287880. Entropy: 0.583522.\n",
      "episode: 4016   score: 265.0  epsilon: 1.0    steps: 702  evaluation reward: 349.15\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9124: Policy loss: 2.447845. Value loss: 40.874184. Entropy: 0.636332.\n",
      "Iteration 9125: Policy loss: 2.298501. Value loss: 20.249191. Entropy: 0.636950.\n",
      "Iteration 9126: Policy loss: 2.452949. Value loss: 15.655236. Entropy: 0.633365.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9127: Policy loss: -0.080454. Value loss: 30.339668. Entropy: 0.449591.\n",
      "Iteration 9128: Policy loss: 0.193421. Value loss: 14.926111. Entropy: 0.459299.\n",
      "Iteration 9129: Policy loss: -0.206592. Value loss: 12.958139. Entropy: 0.470478.\n",
      "episode: 4017   score: 210.0  epsilon: 1.0    steps: 439  evaluation reward: 346.55\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9130: Policy loss: 1.968102. Value loss: 66.468338. Entropy: 0.338458.\n",
      "Iteration 9131: Policy loss: 2.207461. Value loss: 36.346127. Entropy: 0.351058.\n",
      "Iteration 9132: Policy loss: 1.687114. Value loss: 27.003811. Entropy: 0.356409.\n",
      "episode: 4018   score: 90.0  epsilon: 1.0    steps: 38  evaluation reward: 344.85\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9133: Policy loss: -1.505051. Value loss: 233.677063. Entropy: 0.508841.\n",
      "Iteration 9134: Policy loss: -1.494530. Value loss: 121.621483. Entropy: 0.479219.\n",
      "Iteration 9135: Policy loss: -1.566395. Value loss: 108.252884. Entropy: 0.494576.\n",
      "episode: 4019   score: 330.0  epsilon: 1.0    steps: 206  evaluation reward: 345.3\n",
      "episode: 4020   score: 265.0  epsilon: 1.0    steps: 518  evaluation reward: 345.15\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9136: Policy loss: 0.995553. Value loss: 28.195246. Entropy: 0.523765.\n",
      "Iteration 9137: Policy loss: 0.969978. Value loss: 14.782752. Entropy: 0.513054.\n",
      "Iteration 9138: Policy loss: 0.812233. Value loss: 13.971583. Entropy: 0.533808.\n",
      "episode: 4021   score: 260.0  epsilon: 1.0    steps: 347  evaluation reward: 343.35\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9139: Policy loss: -0.154997. Value loss: 39.240395. Entropy: 0.506593.\n",
      "Iteration 9140: Policy loss: -0.376767. Value loss: 22.862602. Entropy: 0.489860.\n",
      "Iteration 9141: Policy loss: -0.179207. Value loss: 20.958305. Entropy: 0.485293.\n",
      "episode: 4022   score: 125.0  epsilon: 1.0    steps: 502  evaluation reward: 340.15\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9142: Policy loss: 1.954238. Value loss: 45.829388. Entropy: 0.485602.\n",
      "Iteration 9143: Policy loss: 2.132967. Value loss: 23.061035. Entropy: 0.518627.\n",
      "Iteration 9144: Policy loss: 2.223704. Value loss: 17.267487. Entropy: 0.527545.\n",
      "episode: 4023   score: 355.0  epsilon: 1.0    steps: 808  evaluation reward: 340.7\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9145: Policy loss: -1.818200. Value loss: 41.847340. Entropy: 0.471301.\n",
      "Iteration 9146: Policy loss: -1.524699. Value loss: 19.784971. Entropy: 0.481151.\n",
      "Iteration 9147: Policy loss: -1.563712. Value loss: 15.259300. Entropy: 0.489983.\n",
      "episode: 4024   score: 540.0  epsilon: 1.0    steps: 983  evaluation reward: 339.5\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9148: Policy loss: 0.733435. Value loss: 37.573528. Entropy: 0.513607.\n",
      "Iteration 9149: Policy loss: 1.292975. Value loss: 16.318895. Entropy: 0.529099.\n",
      "Iteration 9150: Policy loss: 1.037718. Value loss: 14.178396. Entropy: 0.505415.\n",
      "episode: 4025   score: 255.0  epsilon: 1.0    steps: 119  evaluation reward: 339.15\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9151: Policy loss: -0.368487. Value loss: 36.447231. Entropy: 0.561123.\n",
      "Iteration 9152: Policy loss: -0.094568. Value loss: 21.171814. Entropy: 0.575425.\n",
      "Iteration 9153: Policy loss: -0.506759. Value loss: 15.899873. Entropy: 0.569401.\n",
      "episode: 4026   score: 680.0  epsilon: 1.0    steps: 727  evaluation reward: 343.0\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9154: Policy loss: -0.129997. Value loss: 29.673277. Entropy: 0.369338.\n",
      "Iteration 9155: Policy loss: -0.127472. Value loss: 17.781086. Entropy: 0.367740.\n",
      "Iteration 9156: Policy loss: -0.143528. Value loss: 12.720343. Entropy: 0.318769.\n",
      "episode: 4027   score: 230.0  epsilon: 1.0    steps: 531  evaluation reward: 340.75\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9157: Policy loss: 0.617794. Value loss: 31.600039. Entropy: 0.552601.\n",
      "Iteration 9158: Policy loss: 0.356891. Value loss: 18.218868. Entropy: 0.567621.\n",
      "Iteration 9159: Policy loss: 0.737824. Value loss: 11.293661. Entropy: 0.565947.\n",
      "episode: 4028   score: 430.0  epsilon: 1.0    steps: 351  evaluation reward: 341.75\n",
      "episode: 4029   score: 265.0  epsilon: 1.0    steps: 509  evaluation reward: 341.2\n",
      "episode: 4030   score: 180.0  epsilon: 1.0    steps: 972  evaluation reward: 339.5\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9160: Policy loss: -2.469372. Value loss: 199.392075. Entropy: 0.486943.\n",
      "Iteration 9161: Policy loss: -3.614941. Value loss: 173.670486. Entropy: 0.465008.\n",
      "Iteration 9162: Policy loss: -2.464641. Value loss: 104.666962. Entropy: 0.429397.\n",
      "episode: 4031   score: 315.0  epsilon: 1.0    steps: 859  evaluation reward: 340.4\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9163: Policy loss: 1.739071. Value loss: 31.192673. Entropy: 0.335285.\n",
      "Iteration 9164: Policy loss: 1.648571. Value loss: 21.560247. Entropy: 0.352674.\n",
      "Iteration 9165: Policy loss: 1.783298. Value loss: 17.298954. Entropy: 0.335318.\n",
      "episode: 4032   score: 670.0  epsilon: 1.0    steps: 246  evaluation reward: 344.8\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9166: Policy loss: 0.469832. Value loss: 41.111336. Entropy: 0.474492.\n",
      "Iteration 9167: Policy loss: 0.505941. Value loss: 25.529119. Entropy: 0.521887.\n",
      "Iteration 9168: Policy loss: 0.455709. Value loss: 19.199032. Entropy: 0.503526.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9169: Policy loss: 2.791363. Value loss: 30.254084. Entropy: 0.485709.\n",
      "Iteration 9170: Policy loss: 2.662262. Value loss: 16.355373. Entropy: 0.481651.\n",
      "Iteration 9171: Policy loss: 2.957522. Value loss: 15.572594. Entropy: 0.507778.\n",
      "episode: 4033   score: 265.0  epsilon: 1.0    steps: 730  evaluation reward: 343.95\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9172: Policy loss: -0.342560. Value loss: 48.948895. Entropy: 0.385696.\n",
      "Iteration 9173: Policy loss: -0.359128. Value loss: 26.128160. Entropy: 0.409063.\n",
      "Iteration 9174: Policy loss: -0.080034. Value loss: 22.743217. Entropy: 0.398634.\n",
      "episode: 4034   score: 285.0  epsilon: 1.0    steps: 572  evaluation reward: 343.95\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9175: Policy loss: 1.141880. Value loss: 17.793476. Entropy: 0.531026.\n",
      "Iteration 9176: Policy loss: 0.744278. Value loss: 11.494105. Entropy: 0.522224.\n",
      "Iteration 9177: Policy loss: 0.949144. Value loss: 9.089762. Entropy: 0.533750.\n",
      "episode: 4035   score: 260.0  epsilon: 1.0    steps: 316  evaluation reward: 345.0\n",
      "episode: 4036   score: 260.0  epsilon: 1.0    steps: 988  evaluation reward: 342.05\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9178: Policy loss: 0.450832. Value loss: 25.536016. Entropy: 0.440187.\n",
      "Iteration 9179: Policy loss: 0.480689. Value loss: 16.319717. Entropy: 0.416279.\n",
      "Iteration 9180: Policy loss: 0.500989. Value loss: 14.826350. Entropy: 0.427858.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9181: Policy loss: 2.078462. Value loss: 25.374079. Entropy: 0.575499.\n",
      "Iteration 9182: Policy loss: 1.951853. Value loss: 15.601830. Entropy: 0.606311.\n",
      "Iteration 9183: Policy loss: 2.015137. Value loss: 13.890816. Entropy: 0.611439.\n",
      "episode: 4037   score: 260.0  epsilon: 1.0    steps: 255  evaluation reward: 342.05\n",
      "episode: 4038   score: 180.0  epsilon: 1.0    steps: 742  evaluation reward: 341.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9184: Policy loss: -0.997695. Value loss: 25.489801. Entropy: 0.406959.\n",
      "Iteration 9185: Policy loss: -0.823411. Value loss: 15.321301. Entropy: 0.420610.\n",
      "Iteration 9186: Policy loss: -0.787293. Value loss: 12.076485. Entropy: 0.381303.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9187: Policy loss: -1.815477. Value loss: 37.899094. Entropy: 0.433516.\n",
      "Iteration 9188: Policy loss: -1.341140. Value loss: 16.400196. Entropy: 0.454988.\n",
      "Iteration 9189: Policy loss: -1.740634. Value loss: 13.151350. Entropy: 0.462597.\n",
      "episode: 4039   score: 435.0  epsilon: 1.0    steps: 400  evaluation reward: 343.55\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9190: Policy loss: -3.438821. Value loss: 198.333054. Entropy: 0.463947.\n",
      "Iteration 9191: Policy loss: -3.728484. Value loss: 141.434418. Entropy: 0.439931.\n",
      "Iteration 9192: Policy loss: -3.123013. Value loss: 85.450829. Entropy: 0.452480.\n",
      "episode: 4040   score: 290.0  epsilon: 1.0    steps: 623  evaluation reward: 343.3\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9193: Policy loss: -4.242551. Value loss: 271.822754. Entropy: 0.374854.\n",
      "Iteration 9194: Policy loss: -3.894173. Value loss: 152.159088. Entropy: 0.358793.\n",
      "Iteration 9195: Policy loss: -3.681251. Value loss: 109.118340. Entropy: 0.352113.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9196: Policy loss: 1.241197. Value loss: 41.151535. Entropy: 0.355386.\n",
      "Iteration 9197: Policy loss: 1.202114. Value loss: 23.192375. Entropy: 0.318467.\n",
      "Iteration 9198: Policy loss: 1.074461. Value loss: 19.296583. Entropy: 0.311490.\n",
      "episode: 4041   score: 805.0  epsilon: 1.0    steps: 28  evaluation reward: 347.55\n",
      "episode: 4042   score: 315.0  epsilon: 1.0    steps: 923  evaluation reward: 348.0\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9199: Policy loss: 1.676508. Value loss: 30.432066. Entropy: 0.409899.\n",
      "Iteration 9200: Policy loss: 1.562126. Value loss: 13.373507. Entropy: 0.401278.\n",
      "Iteration 9201: Policy loss: 1.780371. Value loss: 13.093100. Entropy: 0.456570.\n",
      "episode: 4043   score: 335.0  epsilon: 1.0    steps: 375  evaluation reward: 346.8\n",
      "episode: 4044   score: 710.0  epsilon: 1.0    steps: 878  evaluation reward: 351.9\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9202: Policy loss: -0.151790. Value loss: 44.403561. Entropy: 0.507021.\n",
      "Iteration 9203: Policy loss: 0.073216. Value loss: 21.334423. Entropy: 0.479713.\n",
      "Iteration 9204: Policy loss: -0.426321. Value loss: 18.377821. Entropy: 0.491334.\n",
      "episode: 4045   score: 350.0  epsilon: 1.0    steps: 675  evaluation reward: 351.2\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9205: Policy loss: 1.611016. Value loss: 20.455969. Entropy: 0.585293.\n",
      "Iteration 9206: Policy loss: 1.645703. Value loss: 10.950853. Entropy: 0.595017.\n",
      "Iteration 9207: Policy loss: 1.522550. Value loss: 8.352539. Entropy: 0.602487.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9208: Policy loss: 0.503553. Value loss: 28.956978. Entropy: 0.377437.\n",
      "Iteration 9209: Policy loss: 0.022128. Value loss: 14.505868. Entropy: 0.387344.\n",
      "Iteration 9210: Policy loss: 0.288224. Value loss: 10.627203. Entropy: 0.381913.\n",
      "episode: 4046   score: 310.0  epsilon: 1.0    steps: 443  evaluation reward: 350.05\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9211: Policy loss: -0.083196. Value loss: 36.604458. Entropy: 0.470742.\n",
      "Iteration 9212: Policy loss: 0.135143. Value loss: 21.688343. Entropy: 0.470698.\n",
      "Iteration 9213: Policy loss: -0.053212. Value loss: 17.202900. Entropy: 0.489829.\n",
      "episode: 4047   score: 330.0  epsilon: 1.0    steps: 569  evaluation reward: 346.75\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9214: Policy loss: 2.225319. Value loss: 38.018978. Entropy: 0.415922.\n",
      "Iteration 9215: Policy loss: 2.329523. Value loss: 20.331200. Entropy: 0.443543.\n",
      "Iteration 9216: Policy loss: 2.041092. Value loss: 13.461858. Entropy: 0.415742.\n",
      "episode: 4048   score: 525.0  epsilon: 1.0    steps: 141  evaluation reward: 349.4\n",
      "episode: 4049   score: 315.0  epsilon: 1.0    steps: 912  evaluation reward: 346.85\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9217: Policy loss: 0.517196. Value loss: 35.699200. Entropy: 0.524898.\n",
      "Iteration 9218: Policy loss: 0.730391. Value loss: 18.002268. Entropy: 0.525587.\n",
      "Iteration 9219: Policy loss: 0.709956. Value loss: 14.262878. Entropy: 0.537478.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9220: Policy loss: 2.795019. Value loss: 43.519779. Entropy: 0.407864.\n",
      "Iteration 9221: Policy loss: 2.839753. Value loss: 20.531733. Entropy: 0.439727.\n",
      "Iteration 9222: Policy loss: 2.699526. Value loss: 18.083103. Entropy: 0.440670.\n",
      "episode: 4050   score: 395.0  epsilon: 1.0    steps: 112  evaluation reward: 348.15\n",
      "now time :  2019-02-25 21:32:30.578080\n",
      "episode: 4051   score: 360.0  epsilon: 1.0    steps: 352  evaluation reward: 348.9\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9223: Policy loss: 1.918042. Value loss: 22.124081. Entropy: 0.364163.\n",
      "Iteration 9224: Policy loss: 1.616941. Value loss: 12.882329. Entropy: 0.359931.\n",
      "Iteration 9225: Policy loss: 1.822637. Value loss: 10.956931. Entropy: 0.375583.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9226: Policy loss: 1.390968. Value loss: 23.596676. Entropy: 0.419698.\n",
      "Iteration 9227: Policy loss: 1.261153. Value loss: 12.667934. Entropy: 0.438669.\n",
      "Iteration 9228: Policy loss: 1.394164. Value loss: 9.541401. Entropy: 0.424144.\n",
      "episode: 4052   score: 285.0  epsilon: 1.0    steps: 466  evaluation reward: 347.2\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9229: Policy loss: 1.593274. Value loss: 33.108078. Entropy: 0.499253.\n",
      "Iteration 9230: Policy loss: 1.466610. Value loss: 19.349861. Entropy: 0.504177.\n",
      "Iteration 9231: Policy loss: 1.811797. Value loss: 17.388325. Entropy: 0.507887.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9232: Policy loss: 0.585028. Value loss: 27.717407. Entropy: 0.479819.\n",
      "Iteration 9233: Policy loss: 0.579525. Value loss: 14.929091. Entropy: 0.483762.\n",
      "Iteration 9234: Policy loss: 0.311838. Value loss: 11.499002. Entropy: 0.485808.\n",
      "episode: 4053   score: 355.0  epsilon: 1.0    steps: 556  evaluation reward: 347.3\n",
      "episode: 4054   score: 415.0  epsilon: 1.0    steps: 747  evaluation reward: 347.1\n",
      "episode: 4055   score: 275.0  epsilon: 1.0    steps: 966  evaluation reward: 344.6\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9235: Policy loss: 2.133651. Value loss: 15.079847. Entropy: 0.589911.\n",
      "Iteration 9236: Policy loss: 2.097793. Value loss: 10.978627. Entropy: 0.569689.\n",
      "Iteration 9237: Policy loss: 1.936495. Value loss: 9.590030. Entropy: 0.581439.\n",
      "episode: 4056   score: 310.0  epsilon: 1.0    steps: 832  evaluation reward: 343.6\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9238: Policy loss: -0.921496. Value loss: 17.385641. Entropy: 0.626128.\n",
      "Iteration 9239: Policy loss: -0.981214. Value loss: 12.153605. Entropy: 0.622555.\n",
      "Iteration 9240: Policy loss: -1.005279. Value loss: 9.404885. Entropy: 0.635977.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9241: Policy loss: 1.607200. Value loss: 37.966091. Entropy: 0.587531.\n",
      "Iteration 9242: Policy loss: 1.757869. Value loss: 21.520859. Entropy: 0.585249.\n",
      "Iteration 9243: Policy loss: 1.579709. Value loss: 15.243022. Entropy: 0.578298.\n",
      "episode: 4057   score: 255.0  epsilon: 1.0    steps: 59  evaluation reward: 342.6\n",
      "episode: 4058   score: 335.0  epsilon: 1.0    steps: 172  evaluation reward: 343.4\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9244: Policy loss: 1.435605. Value loss: 33.056057. Entropy: 0.531115.\n",
      "Iteration 9245: Policy loss: 1.590341. Value loss: 16.195898. Entropy: 0.518085.\n",
      "Iteration 9246: Policy loss: 1.234264. Value loss: 14.986654. Entropy: 0.534530.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9247: Policy loss: -0.554727. Value loss: 31.277302. Entropy: 0.452648.\n",
      "Iteration 9248: Policy loss: -0.545776. Value loss: 17.437860. Entropy: 0.450337.\n",
      "Iteration 9249: Policy loss: -0.224363. Value loss: 13.828828. Entropy: 0.440358.\n",
      "episode: 4059   score: 460.0  epsilon: 1.0    steps: 340  evaluation reward: 343.75\n",
      "episode: 4060   score: 210.0  epsilon: 1.0    steps: 737  evaluation reward: 341.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9250: Policy loss: 0.836454. Value loss: 25.069584. Entropy: 0.495626.\n",
      "Iteration 9251: Policy loss: 1.080726. Value loss: 15.993731. Entropy: 0.479423.\n",
      "Iteration 9252: Policy loss: 0.765661. Value loss: 12.796049. Entropy: 0.489417.\n",
      "episode: 4061   score: 210.0  epsilon: 1.0    steps: 815  evaluation reward: 341.0\n",
      "episode: 4062   score: 285.0  epsilon: 1.0    steps: 953  evaluation reward: 338.2\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9253: Policy loss: -1.700523. Value loss: 21.230589. Entropy: 0.533942.\n",
      "Iteration 9254: Policy loss: -1.760337. Value loss: 12.113299. Entropy: 0.528239.\n",
      "Iteration 9255: Policy loss: -1.793937. Value loss: 10.679745. Entropy: 0.528941.\n",
      "episode: 4063   score: 435.0  epsilon: 1.0    steps: 407  evaluation reward: 340.45\n",
      "episode: 4064   score: 365.0  epsilon: 1.0    steps: 631  evaluation reward: 338.6\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9256: Policy loss: -0.447372. Value loss: 30.146036. Entropy: 0.558705.\n",
      "Iteration 9257: Policy loss: -0.317906. Value loss: 19.642300. Entropy: 0.563300.\n",
      "Iteration 9258: Policy loss: -0.386862. Value loss: 15.677885. Entropy: 0.574471.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9259: Policy loss: 1.039785. Value loss: 29.396454. Entropy: 0.452537.\n",
      "Iteration 9260: Policy loss: 1.266949. Value loss: 14.673630. Entropy: 0.459624.\n",
      "Iteration 9261: Policy loss: 0.895594. Value loss: 9.566919. Entropy: 0.475227.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9262: Policy loss: 0.632452. Value loss: 21.755590. Entropy: 0.448716.\n",
      "Iteration 9263: Policy loss: 0.277510. Value loss: 13.628633. Entropy: 0.444163.\n",
      "Iteration 9264: Policy loss: 0.335561. Value loss: 10.379046. Entropy: 0.444490.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9265: Policy loss: 0.041368. Value loss: 17.181522. Entropy: 0.522127.\n",
      "Iteration 9266: Policy loss: -0.000086. Value loss: 8.431028. Entropy: 0.530484.\n",
      "Iteration 9267: Policy loss: 0.035417. Value loss: 6.003849. Entropy: 0.518224.\n",
      "episode: 4065   score: 210.0  epsilon: 1.0    steps: 758  evaluation reward: 338.85\n",
      "episode: 4066   score: 210.0  epsilon: 1.0    steps: 882  evaluation reward: 333.55\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9268: Policy loss: -0.661306. Value loss: 191.901825. Entropy: 0.555334.\n",
      "Iteration 9269: Policy loss: -1.093620. Value loss: 104.641739. Entropy: 0.542132.\n",
      "Iteration 9270: Policy loss: -1.039580. Value loss: 60.416592. Entropy: 0.545548.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9271: Policy loss: -3.175156. Value loss: 229.360107. Entropy: 0.437713.\n",
      "Iteration 9272: Policy loss: -2.402253. Value loss: 83.203735. Entropy: 0.382381.\n",
      "Iteration 9273: Policy loss: -2.506184. Value loss: 52.489975. Entropy: 0.420718.\n",
      "episode: 4067   score: 275.0  epsilon: 1.0    steps: 432  evaluation reward: 333.7\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9274: Policy loss: 0.881393. Value loss: 21.222012. Entropy: 0.389625.\n",
      "Iteration 9275: Policy loss: 1.121270. Value loss: 14.163513. Entropy: 0.392139.\n",
      "Iteration 9276: Policy loss: 1.389823. Value loss: 12.010301. Entropy: 0.397729.\n",
      "episode: 4068   score: 425.0  epsilon: 1.0    steps: 352  evaluation reward: 335.35\n",
      "episode: 4069   score: 330.0  epsilon: 1.0    steps: 630  evaluation reward: 336.55\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9277: Policy loss: -3.166506. Value loss: 193.057449. Entropy: 0.388260.\n",
      "Iteration 9278: Policy loss: -3.454499. Value loss: 173.981583. Entropy: 0.363589.\n",
      "Iteration 9279: Policy loss: -3.589155. Value loss: 107.316597. Entropy: 0.381432.\n",
      "episode: 4070   score: 540.0  epsilon: 1.0    steps: 187  evaluation reward: 337.1\n",
      "episode: 4071   score: 610.0  epsilon: 1.0    steps: 981  evaluation reward: 337.15\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9280: Policy loss: 2.443232. Value loss: 33.008163. Entropy: 0.410232.\n",
      "Iteration 9281: Policy loss: 2.539761. Value loss: 17.333944. Entropy: 0.403491.\n",
      "Iteration 9282: Policy loss: 2.286099. Value loss: 15.235047. Entropy: 0.419808.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9283: Policy loss: -0.002282. Value loss: 16.547058. Entropy: 0.344956.\n",
      "Iteration 9284: Policy loss: 0.118235. Value loss: 10.724116. Entropy: 0.335639.\n",
      "Iteration 9285: Policy loss: 0.041117. Value loss: 7.983422. Entropy: 0.336221.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9286: Policy loss: -0.299046. Value loss: 30.608130. Entropy: 0.396293.\n",
      "Iteration 9287: Policy loss: -0.401892. Value loss: 18.810734. Entropy: 0.372313.\n",
      "Iteration 9288: Policy loss: -0.341728. Value loss: 15.053673. Entropy: 0.398102.\n",
      "episode: 4072   score: 820.0  epsilon: 1.0    steps: 37  evaluation reward: 342.5\n",
      "episode: 4073   score: 365.0  epsilon: 1.0    steps: 869  evaluation reward: 343.75\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9289: Policy loss: 1.977680. Value loss: 34.565769. Entropy: 0.279403.\n",
      "Iteration 9290: Policy loss: 2.236392. Value loss: 15.231587. Entropy: 0.290411.\n",
      "Iteration 9291: Policy loss: 2.053532. Value loss: 11.882281. Entropy: 0.322121.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9292: Policy loss: -4.338821. Value loss: 284.364746. Entropy: 0.559633.\n",
      "Iteration 9293: Policy loss: -3.663322. Value loss: 191.565704. Entropy: 0.539738.\n",
      "Iteration 9294: Policy loss: -4.143205. Value loss: 161.507797. Entropy: 0.520976.\n",
      "episode: 4074   score: 210.0  epsilon: 1.0    steps: 212  evaluation reward: 344.05\n",
      "episode: 4075   score: 255.0  epsilon: 1.0    steps: 652  evaluation reward: 341.45\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9295: Policy loss: 0.129501. Value loss: 50.241447. Entropy: 0.594422.\n",
      "Iteration 9296: Policy loss: -0.021895. Value loss: 27.884771. Entropy: 0.595064.\n",
      "Iteration 9297: Policy loss: 0.211025. Value loss: 19.261288. Entropy: 0.599854.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9298: Policy loss: -1.932451. Value loss: 142.328400. Entropy: 0.256263.\n",
      "Iteration 9299: Policy loss: -2.156850. Value loss: 86.999313. Entropy: 0.259226.\n",
      "Iteration 9300: Policy loss: -2.021943. Value loss: 70.106682. Entropy: 0.271927.\n",
      "episode: 4076   score: 550.0  epsilon: 1.0    steps: 926  evaluation reward: 343.7\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9301: Policy loss: 4.148114. Value loss: 61.525394. Entropy: 0.425622.\n",
      "Iteration 9302: Policy loss: 4.254028. Value loss: 31.580202. Entropy: 0.441967.\n",
      "Iteration 9303: Policy loss: 4.215727. Value loss: 21.495489. Entropy: 0.422845.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9304: Policy loss: 1.095050. Value loss: 26.003424. Entropy: 0.208224.\n",
      "Iteration 9305: Policy loss: 0.918419. Value loss: 16.323317. Entropy: 0.215530.\n",
      "Iteration 9306: Policy loss: 1.298353. Value loss: 11.456669. Entropy: 0.196223.\n",
      "episode: 4077   score: 640.0  epsilon: 1.0    steps: 446  evaluation reward: 347.2\n",
      "episode: 4078   score: 515.0  epsilon: 1.0    steps: 543  evaluation reward: 349.75\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9307: Policy loss: -0.922134. Value loss: 24.377380. Entropy: 0.405841.\n",
      "Iteration 9308: Policy loss: -0.582898. Value loss: 17.826624. Entropy: 0.417723.\n",
      "Iteration 9309: Policy loss: -0.560580. Value loss: 13.103753. Entropy: 0.404924.\n",
      "episode: 4079   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 350.6\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9310: Policy loss: 3.636244. Value loss: 46.557034. Entropy: 0.465438.\n",
      "Iteration 9311: Policy loss: 3.183116. Value loss: 21.594849. Entropy: 0.444577.\n",
      "Iteration 9312: Policy loss: 3.200453. Value loss: 15.658580. Entropy: 0.454167.\n",
      "episode: 4080   score: 395.0  epsilon: 1.0    steps: 88  evaluation reward: 350.95\n",
      "episode: 4081   score: 240.0  epsilon: 1.0    steps: 679  evaluation reward: 348.95\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9313: Policy loss: 0.373981. Value loss: 47.360561. Entropy: 0.488786.\n",
      "Iteration 9314: Policy loss: 0.437161. Value loss: 19.972420. Entropy: 0.481221.\n",
      "Iteration 9315: Policy loss: 0.209211. Value loss: 13.802633. Entropy: 0.492114.\n",
      "episode: 4082   score: 510.0  epsilon: 1.0    steps: 312  evaluation reward: 350.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9316: Policy loss: 1.654623. Value loss: 25.540951. Entropy: 0.540092.\n",
      "Iteration 9317: Policy loss: 1.297182. Value loss: 16.354801. Entropy: 0.544646.\n",
      "Iteration 9318: Policy loss: 1.498840. Value loss: 12.911515. Entropy: 0.522732.\n",
      "episode: 4083   score: 225.0  epsilon: 1.0    steps: 1019  evaluation reward: 349.85\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9319: Policy loss: 0.357562. Value loss: 18.325966. Entropy: 0.461247.\n",
      "Iteration 9320: Policy loss: 0.376131. Value loss: 14.686145. Entropy: 0.465408.\n",
      "Iteration 9321: Policy loss: 0.419796. Value loss: 11.000345. Entropy: 0.465497.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9322: Policy loss: 1.232798. Value loss: 20.931440. Entropy: 0.444378.\n",
      "Iteration 9323: Policy loss: 1.229312. Value loss: 10.301147. Entropy: 0.434308.\n",
      "Iteration 9324: Policy loss: 0.909032. Value loss: 9.957028. Entropy: 0.465986.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9325: Policy loss: 0.293411. Value loss: 26.889181. Entropy: 0.453027.\n",
      "Iteration 9326: Policy loss: 0.621970. Value loss: 19.081848. Entropy: 0.444672.\n",
      "Iteration 9327: Policy loss: 0.250994. Value loss: 14.355108. Entropy: 0.465038.\n",
      "episode: 4084   score: 340.0  epsilon: 1.0    steps: 428  evaluation reward: 348.55\n",
      "episode: 4085   score: 295.0  epsilon: 1.0    steps: 595  evaluation reward: 348.9\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9328: Policy loss: -0.565898. Value loss: 25.230593. Entropy: 0.503721.\n",
      "Iteration 9329: Policy loss: -0.407561. Value loss: 13.305393. Entropy: 0.494399.\n",
      "Iteration 9330: Policy loss: -0.413928. Value loss: 11.050426. Entropy: 0.491034.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9331: Policy loss: -1.810535. Value loss: 37.488007. Entropy: 0.479326.\n",
      "Iteration 9332: Policy loss: -1.706435. Value loss: 22.593153. Entropy: 0.483751.\n",
      "Iteration 9333: Policy loss: -1.918765. Value loss: 16.635323. Entropy: 0.483316.\n",
      "episode: 4086   score: 370.0  epsilon: 1.0    steps: 701  evaluation reward: 350.3\n",
      "episode: 4087   score: 415.0  epsilon: 1.0    steps: 836  evaluation reward: 351.6\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9334: Policy loss: -0.224965. Value loss: 32.063046. Entropy: 0.645493.\n",
      "Iteration 9335: Policy loss: -0.063507. Value loss: 19.794800. Entropy: 0.659341.\n",
      "Iteration 9336: Policy loss: -0.343939. Value loss: 14.829020. Entropy: 0.651614.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9337: Policy loss: 2.349721. Value loss: 34.650578. Entropy: 0.590985.\n",
      "Iteration 9338: Policy loss: 2.039040. Value loss: 18.764114. Entropy: 0.575108.\n",
      "Iteration 9339: Policy loss: 2.141835. Value loss: 14.914193. Entropy: 0.594794.\n",
      "episode: 4088   score: 365.0  epsilon: 1.0    steps: 194  evaluation reward: 352.4\n",
      "episode: 4089   score: 310.0  epsilon: 1.0    steps: 287  evaluation reward: 352.8\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9340: Policy loss: -1.569719. Value loss: 176.618866. Entropy: 0.504681.\n",
      "Iteration 9341: Policy loss: -0.953716. Value loss: 56.705177. Entropy: 0.453494.\n",
      "Iteration 9342: Policy loss: -1.151954. Value loss: 39.428833. Entropy: 0.447405.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9343: Policy loss: 1.466528. Value loss: 36.326374. Entropy: 0.535272.\n",
      "Iteration 9344: Policy loss: 1.360564. Value loss: 20.689838. Entropy: 0.522420.\n",
      "Iteration 9345: Policy loss: 1.440643. Value loss: 17.406227. Entropy: 0.516231.\n",
      "episode: 4090   score: 210.0  epsilon: 1.0    steps: 478  evaluation reward: 350.7\n",
      "episode: 4091   score: 425.0  epsilon: 1.0    steps: 964  evaluation reward: 352.55\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9346: Policy loss: 0.880337. Value loss: 16.977358. Entropy: 0.457275.\n",
      "Iteration 9347: Policy loss: 0.932286. Value loss: 9.588237. Entropy: 0.457253.\n",
      "Iteration 9348: Policy loss: 0.821362. Value loss: 8.676537. Entropy: 0.464986.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9349: Policy loss: 2.798118. Value loss: 27.121786. Entropy: 0.472444.\n",
      "Iteration 9350: Policy loss: 2.685658. Value loss: 15.540471. Entropy: 0.487076.\n",
      "Iteration 9351: Policy loss: 2.719378. Value loss: 12.168022. Entropy: 0.512485.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9352: Policy loss: 0.043449. Value loss: 49.647442. Entropy: 0.515948.\n",
      "Iteration 9353: Policy loss: 0.316684. Value loss: 27.976980. Entropy: 0.557302.\n",
      "Iteration 9354: Policy loss: 0.282816. Value loss: 17.298084. Entropy: 0.548379.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9355: Policy loss: -0.227511. Value loss: 27.138966. Entropy: 0.495222.\n",
      "Iteration 9356: Policy loss: -0.081259. Value loss: 14.979523. Entropy: 0.524010.\n",
      "Iteration 9357: Policy loss: -0.253509. Value loss: 11.138652. Entropy: 0.492626.\n",
      "episode: 4092   score: 580.0  epsilon: 1.0    steps: 19  evaluation reward: 353.75\n",
      "episode: 4093   score: 245.0  epsilon: 1.0    steps: 336  evaluation reward: 350.1\n",
      "episode: 4094   score: 685.0  epsilon: 1.0    steps: 599  evaluation reward: 354.65\n",
      "episode: 4095   score: 270.0  epsilon: 1.0    steps: 718  evaluation reward: 352.45\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9358: Policy loss: -1.411693. Value loss: 183.094116. Entropy: 0.374529.\n",
      "Iteration 9359: Policy loss: -1.187108. Value loss: 132.584000. Entropy: 0.379809.\n",
      "Iteration 9360: Policy loss: -1.085998. Value loss: 81.040604. Entropy: 0.386874.\n",
      "episode: 4096   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 350.35\n",
      "episode: 4097   score: 620.0  epsilon: 1.0    steps: 852  evaluation reward: 353.95\n",
      "episode: 4098   score: 210.0  epsilon: 1.0    steps: 979  evaluation reward: 353.15\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9361: Policy loss: 0.457965. Value loss: 31.325928. Entropy: 0.464404.\n",
      "Iteration 9362: Policy loss: 0.266639. Value loss: 16.242817. Entropy: 0.466373.\n",
      "Iteration 9363: Policy loss: 0.458797. Value loss: 14.071576. Entropy: 0.470279.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9364: Policy loss: -1.446811. Value loss: 23.333466. Entropy: 0.412562.\n",
      "Iteration 9365: Policy loss: -1.067591. Value loss: 14.600875. Entropy: 0.377696.\n",
      "Iteration 9366: Policy loss: -1.229918. Value loss: 12.313014. Entropy: 0.384289.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9367: Policy loss: 0.036162. Value loss: 42.668678. Entropy: 0.334227.\n",
      "Iteration 9368: Policy loss: 0.456467. Value loss: 29.426605. Entropy: 0.333951.\n",
      "Iteration 9369: Policy loss: 0.157805. Value loss: 20.980965. Entropy: 0.327503.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9370: Policy loss: 5.355558. Value loss: 39.534397. Entropy: 0.375329.\n",
      "Iteration 9371: Policy loss: 5.395940. Value loss: 26.024834. Entropy: 0.394097.\n",
      "Iteration 9372: Policy loss: 5.210653. Value loss: 19.776127. Entropy: 0.443664.\n",
      "episode: 4099   score: 190.0  epsilon: 1.0    steps: 13  evaluation reward: 351.85\n",
      "episode: 4100   score: 345.0  epsilon: 1.0    steps: 389  evaluation reward: 351.6\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9373: Policy loss: 0.193633. Value loss: 23.349319. Entropy: 0.508687.\n",
      "Iteration 9374: Policy loss: -0.075364. Value loss: 17.411222. Entropy: 0.513699.\n",
      "Iteration 9375: Policy loss: 0.111412. Value loss: 12.086070. Entropy: 0.502359.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9376: Policy loss: 1.045712. Value loss: 44.092762. Entropy: 0.362148.\n",
      "Iteration 9377: Policy loss: 1.105905. Value loss: 22.220222. Entropy: 0.385855.\n",
      "Iteration 9378: Policy loss: 0.980734. Value loss: 16.876101. Entropy: 0.413043.\n",
      "now time :  2019-02-25 21:35:24.919096\n",
      "episode: 4101   score: 400.0  epsilon: 1.0    steps: 358  evaluation reward: 350.95\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9379: Policy loss: -0.399821. Value loss: 37.517231. Entropy: 0.382459.\n",
      "Iteration 9380: Policy loss: -0.179141. Value loss: 17.043217. Entropy: 0.417044.\n",
      "Iteration 9381: Policy loss: -0.206062. Value loss: 15.351701. Entropy: 0.417942.\n",
      "episode: 4102   score: 395.0  epsilon: 1.0    steps: 526  evaluation reward: 352.5\n",
      "episode: 4103   score: 390.0  epsilon: 1.0    steps: 650  evaluation reward: 354.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4104   score: 310.0  epsilon: 1.0    steps: 775  evaluation reward: 355.0\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9382: Policy loss: -4.395526. Value loss: 125.303772. Entropy: 0.361305.\n",
      "Iteration 9383: Policy loss: -5.550783. Value loss: 166.643784. Entropy: 0.386559.\n",
      "Iteration 9384: Policy loss: -5.174999. Value loss: 71.451416. Entropy: 0.373125.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9385: Policy loss: 2.951899. Value loss: 36.073048. Entropy: 0.609111.\n",
      "Iteration 9386: Policy loss: 3.093097. Value loss: 19.808092. Entropy: 0.643779.\n",
      "Iteration 9387: Policy loss: 2.842817. Value loss: 19.487457. Entropy: 0.614954.\n",
      "episode: 4105   score: 240.0  epsilon: 1.0    steps: 492  evaluation reward: 355.0\n",
      "episode: 4106   score: 320.0  epsilon: 1.0    steps: 978  evaluation reward: 356.1\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9388: Policy loss: 1.100022. Value loss: 21.332989. Entropy: 0.509431.\n",
      "Iteration 9389: Policy loss: 1.007501. Value loss: 15.492350. Entropy: 0.489884.\n",
      "Iteration 9390: Policy loss: 1.013795. Value loss: 12.454872. Entropy: 0.511590.\n",
      "episode: 4107   score: 610.0  epsilon: 1.0    steps: 250  evaluation reward: 358.95\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9391: Policy loss: 0.525206. Value loss: 17.558250. Entropy: 0.485070.\n",
      "Iteration 9392: Policy loss: 0.670228. Value loss: 12.175941. Entropy: 0.497776.\n",
      "Iteration 9393: Policy loss: 0.633199. Value loss: 10.651068. Entropy: 0.483773.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9394: Policy loss: 0.361883. Value loss: 20.256840. Entropy: 0.729056.\n",
      "Iteration 9395: Policy loss: 0.291510. Value loss: 12.843886. Entropy: 0.715632.\n",
      "Iteration 9396: Policy loss: 0.348955. Value loss: 11.086323. Entropy: 0.717119.\n",
      "episode: 4108   score: 210.0  epsilon: 1.0    steps: 819  evaluation reward: 358.2\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9397: Policy loss: 0.015853. Value loss: 23.333380. Entropy: 0.519953.\n",
      "Iteration 9398: Policy loss: -0.071022. Value loss: 14.113454. Entropy: 0.533897.\n",
      "Iteration 9399: Policy loss: -0.203361. Value loss: 10.155652. Entropy: 0.543913.\n",
      "episode: 4109   score: 275.0  epsilon: 1.0    steps: 258  evaluation reward: 357.35\n",
      "episode: 4110   score: 265.0  epsilon: 1.0    steps: 539  evaluation reward: 357.15\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9400: Policy loss: 0.054407. Value loss: 19.979195. Entropy: 0.568042.\n",
      "Iteration 9401: Policy loss: 0.033858. Value loss: 15.708199. Entropy: 0.566814.\n",
      "Iteration 9402: Policy loss: -0.084168. Value loss: 10.484180. Entropy: 0.580127.\n",
      "episode: 4111   score: 165.0  epsilon: 1.0    steps: 511  evaluation reward: 355.45\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9403: Policy loss: 1.579833. Value loss: 23.534229. Entropy: 0.536370.\n",
      "Iteration 9404: Policy loss: 1.615036. Value loss: 12.175938. Entropy: 0.527407.\n",
      "Iteration 9405: Policy loss: 1.548749. Value loss: 10.559694. Entropy: 0.538470.\n",
      "episode: 4112   score: 425.0  epsilon: 1.0    steps: 59  evaluation reward: 354.8\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9406: Policy loss: 0.998563. Value loss: 19.474005. Entropy: 0.576022.\n",
      "Iteration 9407: Policy loss: 1.011885. Value loss: 11.810150. Entropy: 0.591291.\n",
      "Iteration 9408: Policy loss: 1.016800. Value loss: 9.427040. Entropy: 0.601277.\n",
      "episode: 4113   score: 320.0  epsilon: 1.0    steps: 672  evaluation reward: 354.75\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9409: Policy loss: 0.364918. Value loss: 19.604305. Entropy: 0.641612.\n",
      "Iteration 9410: Policy loss: 0.498748. Value loss: 14.064701. Entropy: 0.635309.\n",
      "Iteration 9411: Policy loss: 0.218812. Value loss: 13.978300. Entropy: 0.632752.\n",
      "episode: 4114   score: 260.0  epsilon: 1.0    steps: 137  evaluation reward: 354.5\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9412: Policy loss: -1.903188. Value loss: 199.386581. Entropy: 0.601355.\n",
      "Iteration 9413: Policy loss: -1.617366. Value loss: 53.008228. Entropy: 0.578559.\n",
      "Iteration 9414: Policy loss: -1.295703. Value loss: 24.993324. Entropy: 0.588997.\n",
      "episode: 4115   score: 265.0  epsilon: 1.0    steps: 888  evaluation reward: 351.15\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9415: Policy loss: -2.140674. Value loss: 163.632858. Entropy: 0.463517.\n",
      "Iteration 9416: Policy loss: -1.412554. Value loss: 80.667686. Entropy: 0.489521.\n",
      "Iteration 9417: Policy loss: -1.926131. Value loss: 71.014862. Entropy: 0.451078.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9418: Policy loss: 1.412314. Value loss: 43.644848. Entropy: 0.548504.\n",
      "Iteration 9419: Policy loss: 1.574193. Value loss: 15.980576. Entropy: 0.528026.\n",
      "Iteration 9420: Policy loss: 1.656168. Value loss: 12.225879. Entropy: 0.518543.\n",
      "episode: 4116   score: 390.0  epsilon: 1.0    steps: 262  evaluation reward: 352.4\n",
      "episode: 4117   score: 500.0  epsilon: 1.0    steps: 632  evaluation reward: 355.3\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9421: Policy loss: 2.030788. Value loss: 35.216385. Entropy: 0.459065.\n",
      "Iteration 9422: Policy loss: 1.844863. Value loss: 21.152624. Entropy: 0.449949.\n",
      "Iteration 9423: Policy loss: 1.829863. Value loss: 17.110373. Entropy: 0.451636.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9424: Policy loss: 0.982321. Value loss: 20.507219. Entropy: 0.473328.\n",
      "Iteration 9425: Policy loss: 0.949013. Value loss: 12.916790. Entropy: 0.496899.\n",
      "Iteration 9426: Policy loss: 1.068010. Value loss: 11.138490. Entropy: 0.475817.\n",
      "episode: 4118   score: 630.0  epsilon: 1.0    steps: 475  evaluation reward: 360.7\n",
      "episode: 4119   score: 275.0  epsilon: 1.0    steps: 752  evaluation reward: 360.15\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9427: Policy loss: 0.499326. Value loss: 25.947731. Entropy: 0.538719.\n",
      "Iteration 9428: Policy loss: 0.496659. Value loss: 17.825296. Entropy: 0.511379.\n",
      "Iteration 9429: Policy loss: 0.392826. Value loss: 13.789474. Entropy: 0.544331.\n",
      "episode: 4120   score: 260.0  epsilon: 1.0    steps: 8  evaluation reward: 360.1\n",
      "episode: 4121   score: 260.0  epsilon: 1.0    steps: 162  evaluation reward: 360.1\n",
      "episode: 4122   score: 295.0  epsilon: 1.0    steps: 941  evaluation reward: 361.8\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9430: Policy loss: 1.081784. Value loss: 33.186543. Entropy: 0.583689.\n",
      "Iteration 9431: Policy loss: 0.928741. Value loss: 16.305981. Entropy: 0.560837.\n",
      "Iteration 9432: Policy loss: 1.304843. Value loss: 12.263392. Entropy: 0.570726.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9433: Policy loss: -1.454246. Value loss: 153.985855. Entropy: 0.620616.\n",
      "Iteration 9434: Policy loss: -2.035748. Value loss: 92.694389. Entropy: 0.593156.\n",
      "Iteration 9435: Policy loss: -1.823140. Value loss: 47.434624. Entropy: 0.573352.\n",
      "episode: 4123   score: 105.0  epsilon: 1.0    steps: 498  evaluation reward: 359.3\n",
      "episode: 4124   score: 225.0  epsilon: 1.0    steps: 806  evaluation reward: 356.15\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9436: Policy loss: -1.816008. Value loss: 36.808331. Entropy: 0.638157.\n",
      "Iteration 9437: Policy loss: -1.855528. Value loss: 23.445585. Entropy: 0.640809.\n",
      "Iteration 9438: Policy loss: -2.250896. Value loss: 16.140949. Entropy: 0.639921.\n",
      "episode: 4125   score: 440.0  epsilon: 1.0    steps: 297  evaluation reward: 358.0\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9439: Policy loss: 0.701161. Value loss: 197.677628. Entropy: 0.316697.\n",
      "Iteration 9440: Policy loss: 1.547537. Value loss: 118.680115. Entropy: 0.284844.\n",
      "Iteration 9441: Policy loss: 0.767730. Value loss: 92.088997. Entropy: 0.282277.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9442: Policy loss: -0.122929. Value loss: 23.697552. Entropy: 0.606236.\n",
      "Iteration 9443: Policy loss: 0.091282. Value loss: 11.606600. Entropy: 0.626216.\n",
      "Iteration 9444: Policy loss: -0.034223. Value loss: 10.500108. Entropy: 0.630774.\n",
      "episode: 4126   score: 265.0  epsilon: 1.0    steps: 51  evaluation reward: 353.85\n",
      "episode: 4127   score: 515.0  epsilon: 1.0    steps: 766  evaluation reward: 356.7\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9445: Policy loss: 1.413465. Value loss: 41.413601. Entropy: 0.469804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9446: Policy loss: 1.655988. Value loss: 26.237259. Entropy: 0.479152.\n",
      "Iteration 9447: Policy loss: 1.505047. Value loss: 18.613390. Entropy: 0.440004.\n",
      "episode: 4128   score: 350.0  epsilon: 1.0    steps: 617  evaluation reward: 355.9\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9448: Policy loss: -0.747798. Value loss: 29.769133. Entropy: 0.606711.\n",
      "Iteration 9449: Policy loss: -0.896105. Value loss: 21.620258. Entropy: 0.586727.\n",
      "Iteration 9450: Policy loss: -0.815026. Value loss: 16.550741. Entropy: 0.594605.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9451: Policy loss: -1.453630. Value loss: 30.477999. Entropy: 0.585737.\n",
      "Iteration 9452: Policy loss: -1.462509. Value loss: 16.938412. Entropy: 0.579553.\n",
      "Iteration 9453: Policy loss: -1.644855. Value loss: 12.550988. Entropy: 0.554419.\n",
      "episode: 4129   score: 285.0  epsilon: 1.0    steps: 811  evaluation reward: 356.1\n",
      "episode: 4130   score: 340.0  epsilon: 1.0    steps: 944  evaluation reward: 357.7\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9454: Policy loss: 1.618683. Value loss: 20.243969. Entropy: 0.613949.\n",
      "Iteration 9455: Policy loss: 1.914435. Value loss: 12.923677. Entropy: 0.595856.\n",
      "Iteration 9456: Policy loss: 1.795363. Value loss: 9.901850. Entropy: 0.597828.\n",
      "episode: 4131   score: 385.0  epsilon: 1.0    steps: 152  evaluation reward: 358.4\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9457: Policy loss: -0.443340. Value loss: 20.131044. Entropy: 0.569227.\n",
      "Iteration 9458: Policy loss: -0.526704. Value loss: 12.501619. Entropy: 0.596224.\n",
      "Iteration 9459: Policy loss: -0.434419. Value loss: 10.381548. Entropy: 0.614511.\n",
      "episode: 4132   score: 370.0  epsilon: 1.0    steps: 484  evaluation reward: 355.4\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9460: Policy loss: 0.178324. Value loss: 19.502510. Entropy: 0.690308.\n",
      "Iteration 9461: Policy loss: 0.205228. Value loss: 13.150468. Entropy: 0.664880.\n",
      "Iteration 9462: Policy loss: 0.144751. Value loss: 8.683100. Entropy: 0.688010.\n",
      "episode: 4133   score: 285.0  epsilon: 1.0    steps: 84  evaluation reward: 355.6\n",
      "episode: 4134   score: 295.0  epsilon: 1.0    steps: 740  evaluation reward: 355.7\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9463: Policy loss: 0.525552. Value loss: 25.769693. Entropy: 0.616362.\n",
      "Iteration 9464: Policy loss: 0.758326. Value loss: 12.949364. Entropy: 0.618053.\n",
      "Iteration 9465: Policy loss: 0.764100. Value loss: 10.954230. Entropy: 0.616680.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9466: Policy loss: -0.153603. Value loss: 35.761093. Entropy: 0.452464.\n",
      "Iteration 9467: Policy loss: 0.089918. Value loss: 14.620306. Entropy: 0.459795.\n",
      "Iteration 9468: Policy loss: -0.386316. Value loss: 11.181502. Entropy: 0.461373.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9469: Policy loss: -1.375041. Value loss: 36.673325. Entropy: 0.561616.\n",
      "Iteration 9470: Policy loss: -1.506123. Value loss: 15.423022. Entropy: 0.562667.\n",
      "Iteration 9471: Policy loss: -1.589941. Value loss: 11.342535. Entropy: 0.542455.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9472: Policy loss: -2.137476. Value loss: 180.810196. Entropy: 0.544528.\n",
      "Iteration 9473: Policy loss: -1.263390. Value loss: 94.926849. Entropy: 0.534645.\n",
      "Iteration 9474: Policy loss: -2.202169. Value loss: 65.173393. Entropy: 0.520764.\n",
      "episode: 4135   score: 305.0  epsilon: 1.0    steps: 550  evaluation reward: 356.15\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9475: Policy loss: -2.858730. Value loss: 231.592300. Entropy: 0.743214.\n",
      "Iteration 9476: Policy loss: -2.454868. Value loss: 81.404930. Entropy: 0.743673.\n",
      "Iteration 9477: Policy loss: -2.961762. Value loss: 81.946167. Entropy: 0.729597.\n",
      "episode: 4136   score: 630.0  epsilon: 1.0    steps: 308  evaluation reward: 359.85\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9478: Policy loss: -0.768436. Value loss: 42.158844. Entropy: 0.676632.\n",
      "Iteration 9479: Policy loss: -0.989529. Value loss: 24.616825. Entropy: 0.675509.\n",
      "Iteration 9480: Policy loss: -1.083031. Value loss: 19.414198. Entropy: 0.667398.\n",
      "episode: 4137   score: 295.0  epsilon: 1.0    steps: 125  evaluation reward: 360.2\n",
      "episode: 4138   score: 450.0  epsilon: 1.0    steps: 1021  evaluation reward: 362.9\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9481: Policy loss: -1.624899. Value loss: 274.259216. Entropy: 0.543995.\n",
      "Iteration 9482: Policy loss: -1.730361. Value loss: 164.677689. Entropy: 0.525027.\n",
      "Iteration 9483: Policy loss: -1.283698. Value loss: 85.861794. Entropy: 0.505064.\n",
      "episode: 4139   score: 430.0  epsilon: 1.0    steps: 770  evaluation reward: 362.85\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9484: Policy loss: 1.685744. Value loss: 36.020390. Entropy: 0.530407.\n",
      "Iteration 9485: Policy loss: 1.531223. Value loss: 14.047559. Entropy: 0.546933.\n",
      "Iteration 9486: Policy loss: 1.524330. Value loss: 11.013796. Entropy: 0.529703.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9487: Policy loss: 2.859189. Value loss: 43.887230. Entropy: 0.380238.\n",
      "Iteration 9488: Policy loss: 2.943176. Value loss: 29.367310. Entropy: 0.361511.\n",
      "Iteration 9489: Policy loss: 2.773222. Value loss: 21.721750. Entropy: 0.360317.\n",
      "episode: 4140   score: 665.0  epsilon: 1.0    steps: 146  evaluation reward: 366.6\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9490: Policy loss: 0.396382. Value loss: 28.133543. Entropy: 0.601640.\n",
      "Iteration 9491: Policy loss: 0.522378. Value loss: 12.011640. Entropy: 0.624159.\n",
      "Iteration 9492: Policy loss: 0.517963. Value loss: 10.729114. Entropy: 0.625012.\n",
      "episode: 4141   score: 215.0  epsilon: 1.0    steps: 575  evaluation reward: 360.7\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9493: Policy loss: 3.949718. Value loss: 36.719097. Entropy: 0.534814.\n",
      "Iteration 9494: Policy loss: 3.589584. Value loss: 16.549080. Entropy: 0.546192.\n",
      "Iteration 9495: Policy loss: 3.726293. Value loss: 12.077599. Entropy: 0.544269.\n",
      "episode: 4142   score: 665.0  epsilon: 1.0    steps: 689  evaluation reward: 364.2\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9496: Policy loss: 0.720144. Value loss: 42.369507. Entropy: 0.502312.\n",
      "Iteration 9497: Policy loss: 0.988832. Value loss: 17.168350. Entropy: 0.512620.\n",
      "Iteration 9498: Policy loss: 0.570846. Value loss: 13.197117. Entropy: 0.514838.\n",
      "episode: 4143   score: 210.0  epsilon: 1.0    steps: 783  evaluation reward: 362.95\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9499: Policy loss: 0.701366. Value loss: 23.248533. Entropy: 0.478824.\n",
      "Iteration 9500: Policy loss: 0.944979. Value loss: 18.188730. Entropy: 0.470799.\n",
      "Iteration 9501: Policy loss: 0.928936. Value loss: 12.866407. Entropy: 0.496706.\n",
      "episode: 4144   score: 270.0  epsilon: 1.0    steps: 3  evaluation reward: 358.55\n",
      "episode: 4145   score: 285.0  epsilon: 1.0    steps: 904  evaluation reward: 357.9\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9502: Policy loss: 0.264862. Value loss: 21.622475. Entropy: 0.486569.\n",
      "Iteration 9503: Policy loss: 0.124966. Value loss: 15.006360. Entropy: 0.496754.\n",
      "Iteration 9504: Policy loss: 0.085297. Value loss: 11.279387. Entropy: 0.460928.\n",
      "episode: 4146   score: 180.0  epsilon: 1.0    steps: 144  evaluation reward: 356.6\n",
      "episode: 4147   score: 125.0  epsilon: 1.0    steps: 710  evaluation reward: 354.55\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9505: Policy loss: 0.378325. Value loss: 39.497143. Entropy: 0.466252.\n",
      "Iteration 9506: Policy loss: 0.348830. Value loss: 26.405378. Entropy: 0.461154.\n",
      "Iteration 9507: Policy loss: 0.583745. Value loss: 16.945967. Entropy: 0.462630.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9508: Policy loss: 0.398888. Value loss: 23.948622. Entropy: 0.567549.\n",
      "Iteration 9509: Policy loss: 0.405546. Value loss: 12.028829. Entropy: 0.578655.\n",
      "Iteration 9510: Policy loss: 0.378102. Value loss: 9.800633. Entropy: 0.578956.\n",
      "episode: 4148   score: 705.0  epsilon: 1.0    steps: 457  evaluation reward: 356.35\n",
      "episode: 4149   score: 240.0  epsilon: 1.0    steps: 581  evaluation reward: 355.6\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9511: Policy loss: 1.440120. Value loss: 32.010040. Entropy: 0.651155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9512: Policy loss: 1.511856. Value loss: 18.208586. Entropy: 0.662608.\n",
      "Iteration 9513: Policy loss: 1.638042. Value loss: 17.036697. Entropy: 0.672754.\n",
      "episode: 4150   score: 265.0  epsilon: 1.0    steps: 873  evaluation reward: 354.3\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9514: Policy loss: -0.124290. Value loss: 127.914978. Entropy: 0.574066.\n",
      "Iteration 9515: Policy loss: 0.186041. Value loss: 82.634048. Entropy: 0.587282.\n",
      "Iteration 9516: Policy loss: -0.543470. Value loss: 60.955166. Entropy: 0.578981.\n",
      "now time :  2019-02-25 21:37:58.391612\n",
      "episode: 4151   score: 185.0  epsilon: 1.0    steps: 13  evaluation reward: 352.55\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9517: Policy loss: 0.248072. Value loss: 20.245773. Entropy: 0.639210.\n",
      "Iteration 9518: Policy loss: 0.090627. Value loss: 11.394217. Entropy: 0.639949.\n",
      "Iteration 9519: Policy loss: 0.140106. Value loss: 9.231995. Entropy: 0.639226.\n",
      "episode: 4152   score: 300.0  epsilon: 1.0    steps: 259  evaluation reward: 352.7\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9520: Policy loss: 2.412344. Value loss: 30.966013. Entropy: 0.414376.\n",
      "Iteration 9521: Policy loss: 2.277122. Value loss: 14.158897. Entropy: 0.413997.\n",
      "Iteration 9522: Policy loss: 2.619256. Value loss: 13.628048. Entropy: 0.420001.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9523: Policy loss: -2.318074. Value loss: 186.186859. Entropy: 0.410241.\n",
      "Iteration 9524: Policy loss: -2.613652. Value loss: 118.941757. Entropy: 0.380671.\n",
      "Iteration 9525: Policy loss: -2.865139. Value loss: 77.532356. Entropy: 0.363508.\n",
      "episode: 4153   score: 95.0  epsilon: 1.0    steps: 8  evaluation reward: 350.1\n",
      "episode: 4154   score: 315.0  epsilon: 1.0    steps: 642  evaluation reward: 349.1\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9526: Policy loss: 1.559227. Value loss: 26.629463. Entropy: 0.383652.\n",
      "Iteration 9527: Policy loss: 1.468907. Value loss: 13.293029. Entropy: 0.387087.\n",
      "Iteration 9528: Policy loss: 1.690208. Value loss: 10.350412. Entropy: 0.410488.\n",
      "episode: 4155   score: 240.0  epsilon: 1.0    steps: 498  evaluation reward: 348.75\n",
      "episode: 4156   score: 210.0  epsilon: 1.0    steps: 866  evaluation reward: 347.75\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9529: Policy loss: 0.813523. Value loss: 51.815964. Entropy: 0.321831.\n",
      "Iteration 9530: Policy loss: 1.302908. Value loss: 23.379997. Entropy: 0.331276.\n",
      "Iteration 9531: Policy loss: 0.632067. Value loss: 18.841532. Entropy: 0.340948.\n",
      "episode: 4157   score: 630.0  epsilon: 1.0    steps: 143  evaluation reward: 351.5\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9532: Policy loss: 0.320572. Value loss: 40.570263. Entropy: 0.435460.\n",
      "Iteration 9533: Policy loss: 0.325331. Value loss: 22.163692. Entropy: 0.408992.\n",
      "Iteration 9534: Policy loss: 0.151244. Value loss: 17.231052. Entropy: 0.427337.\n",
      "episode: 4158   score: 285.0  epsilon: 1.0    steps: 376  evaluation reward: 351.0\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9535: Policy loss: 0.920954. Value loss: 29.881763. Entropy: 0.343357.\n",
      "Iteration 9536: Policy loss: 0.891197. Value loss: 15.979141. Entropy: 0.353825.\n",
      "Iteration 9537: Policy loss: 1.204698. Value loss: 12.102826. Entropy: 0.347802.\n",
      "episode: 4159   score: 555.0  epsilon: 1.0    steps: 562  evaluation reward: 351.95\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9538: Policy loss: 0.183090. Value loss: 34.235054. Entropy: 0.464798.\n",
      "Iteration 9539: Policy loss: 0.123984. Value loss: 19.820171. Entropy: 0.478837.\n",
      "Iteration 9540: Policy loss: 0.102425. Value loss: 15.026689. Entropy: 0.464444.\n",
      "episode: 4160   score: 315.0  epsilon: 1.0    steps: 1024  evaluation reward: 353.0\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9541: Policy loss: 0.485086. Value loss: 34.684856. Entropy: 0.462201.\n",
      "Iteration 9542: Policy loss: 0.499142. Value loss: 18.335541. Entropy: 0.468711.\n",
      "Iteration 9543: Policy loss: 0.512972. Value loss: 13.674994. Entropy: 0.471394.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9544: Policy loss: -1.713210. Value loss: 48.419914. Entropy: 0.584314.\n",
      "Iteration 9545: Policy loss: -1.912113. Value loss: 22.162502. Entropy: 0.560937.\n",
      "Iteration 9546: Policy loss: -1.556791. Value loss: 17.953741. Entropy: 0.585934.\n",
      "episode: 4161   score: 345.0  epsilon: 1.0    steps: 27  evaluation reward: 354.35\n",
      "episode: 4162   score: 375.0  epsilon: 1.0    steps: 660  evaluation reward: 355.25\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9547: Policy loss: -2.953483. Value loss: 188.318451. Entropy: 0.571057.\n",
      "Iteration 9548: Policy loss: -2.764916. Value loss: 98.734192. Entropy: 0.507725.\n",
      "Iteration 9549: Policy loss: -3.795880. Value loss: 72.923981. Entropy: 0.510945.\n",
      "episode: 4163   score: 285.0  epsilon: 1.0    steps: 420  evaluation reward: 353.75\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9550: Policy loss: 1.357844. Value loss: 26.344091. Entropy: 0.375571.\n",
      "Iteration 9551: Policy loss: 1.469729. Value loss: 16.527399. Entropy: 0.388113.\n",
      "Iteration 9552: Policy loss: 1.544017. Value loss: 11.035658. Entropy: 0.365893.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9553: Policy loss: -0.344440. Value loss: 46.945511. Entropy: 0.258356.\n",
      "Iteration 9554: Policy loss: -0.308760. Value loss: 22.294416. Entropy: 0.254097.\n",
      "Iteration 9555: Policy loss: -0.050623. Value loss: 15.329108. Entropy: 0.254963.\n",
      "episode: 4164   score: 420.0  epsilon: 1.0    steps: 197  evaluation reward: 354.3\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9556: Policy loss: -0.249088. Value loss: 27.108992. Entropy: 0.201401.\n",
      "Iteration 9557: Policy loss: -0.213528. Value loss: 13.571404. Entropy: 0.181936.\n",
      "Iteration 9558: Policy loss: -0.468295. Value loss: 10.622845. Entropy: 0.190455.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9559: Policy loss: 3.071614. Value loss: 39.728802. Entropy: 0.412034.\n",
      "Iteration 9560: Policy loss: 3.011564. Value loss: 19.398624. Entropy: 0.411367.\n",
      "Iteration 9561: Policy loss: 3.288848. Value loss: 14.434572. Entropy: 0.427446.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9562: Policy loss: 0.965151. Value loss: 41.400093. Entropy: 0.603482.\n",
      "Iteration 9563: Policy loss: 1.184765. Value loss: 24.065882. Entropy: 0.605763.\n",
      "Iteration 9564: Policy loss: 0.801819. Value loss: 18.287012. Entropy: 0.614008.\n",
      "episode: 4165   score: 370.0  epsilon: 1.0    steps: 611  evaluation reward: 355.9\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9565: Policy loss: 1.588251. Value loss: 31.449722. Entropy: 0.495508.\n",
      "Iteration 9566: Policy loss: 1.705388. Value loss: 17.793737. Entropy: 0.476012.\n",
      "Iteration 9567: Policy loss: 1.527717. Value loss: 13.028395. Entropy: 0.488330.\n",
      "episode: 4166   score: 355.0  epsilon: 1.0    steps: 62  evaluation reward: 357.35\n",
      "episode: 4167   score: 290.0  epsilon: 1.0    steps: 732  evaluation reward: 357.5\n",
      "episode: 4168   score: 490.0  epsilon: 1.0    steps: 789  evaluation reward: 358.15\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9568: Policy loss: 0.441134. Value loss: 204.773911. Entropy: 0.533511.\n",
      "Iteration 9569: Policy loss: 0.553893. Value loss: 122.777466. Entropy: 0.518134.\n",
      "Iteration 9570: Policy loss: 0.587251. Value loss: 112.145973. Entropy: 0.480763.\n",
      "episode: 4169   score: 425.0  epsilon: 1.0    steps: 903  evaluation reward: 359.1\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9571: Policy loss: 1.359169. Value loss: 32.043377. Entropy: 0.355288.\n",
      "Iteration 9572: Policy loss: 1.456440. Value loss: 19.654902. Entropy: 0.362062.\n",
      "Iteration 9573: Policy loss: 1.411330. Value loss: 14.308601. Entropy: 0.351769.\n",
      "episode: 4170   score: 295.0  epsilon: 1.0    steps: 215  evaluation reward: 356.65\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9574: Policy loss: -1.237196. Value loss: 33.023445. Entropy: 0.389156.\n",
      "Iteration 9575: Policy loss: -1.312041. Value loss: 20.885580. Entropy: 0.369368.\n",
      "Iteration 9576: Policy loss: -1.390315. Value loss: 17.570398. Entropy: 0.386711.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9577: Policy loss: 1.509937. Value loss: 40.587196. Entropy: 0.312607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9578: Policy loss: 1.246622. Value loss: 19.633371. Entropy: 0.373055.\n",
      "Iteration 9579: Policy loss: 1.012351. Value loss: 18.273321. Entropy: 0.369172.\n",
      "episode: 4171   score: 955.0  epsilon: 1.0    steps: 326  evaluation reward: 360.1\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9580: Policy loss: 1.628498. Value loss: 39.575691. Entropy: 0.307424.\n",
      "Iteration 9581: Policy loss: 1.880805. Value loss: 21.441658. Entropy: 0.308586.\n",
      "Iteration 9582: Policy loss: 1.727700. Value loss: 19.935661. Entropy: 0.330972.\n",
      "episode: 4172   score: 365.0  epsilon: 1.0    steps: 488  evaluation reward: 355.55\n",
      "episode: 4173   score: 210.0  epsilon: 1.0    steps: 772  evaluation reward: 354.0\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9583: Policy loss: 1.133687. Value loss: 22.196987. Entropy: 0.441552.\n",
      "Iteration 9584: Policy loss: 1.223886. Value loss: 13.379522. Entropy: 0.456564.\n",
      "Iteration 9585: Policy loss: 1.175753. Value loss: 12.720636. Entropy: 0.452608.\n",
      "episode: 4174   score: 210.0  epsilon: 1.0    steps: 917  evaluation reward: 354.0\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9586: Policy loss: 0.998674. Value loss: 16.603848. Entropy: 0.417830.\n",
      "Iteration 9587: Policy loss: 0.936867. Value loss: 10.310416. Entropy: 0.424848.\n",
      "Iteration 9588: Policy loss: 1.038114. Value loss: 8.835407. Entropy: 0.427459.\n",
      "episode: 4175   score: 205.0  epsilon: 1.0    steps: 51  evaluation reward: 353.5\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9589: Policy loss: 0.799205. Value loss: 17.933926. Entropy: 0.241429.\n",
      "Iteration 9590: Policy loss: 0.952092. Value loss: 11.826519. Entropy: 0.230219.\n",
      "Iteration 9591: Policy loss: 0.995729. Value loss: 8.765991. Entropy: 0.227377.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9592: Policy loss: 1.281359. Value loss: 26.991505. Entropy: 0.284775.\n",
      "Iteration 9593: Policy loss: 1.518938. Value loss: 14.322886. Entropy: 0.238267.\n",
      "Iteration 9594: Policy loss: 1.201764. Value loss: 10.179565. Entropy: 0.262835.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9595: Policy loss: 0.912157. Value loss: 16.576797. Entropy: 0.167745.\n",
      "Iteration 9596: Policy loss: 0.801812. Value loss: 9.256316. Entropy: 0.164642.\n",
      "Iteration 9597: Policy loss: 0.880418. Value loss: 6.173590. Entropy: 0.171476.\n",
      "episode: 4176   score: 240.0  epsilon: 1.0    steps: 357  evaluation reward: 350.4\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9598: Policy loss: -3.823350. Value loss: 192.991272. Entropy: 0.283171.\n",
      "Iteration 9599: Policy loss: -3.404475. Value loss: 99.423729. Entropy: 0.284544.\n",
      "Iteration 9600: Policy loss: -3.369087. Value loss: 92.337860. Entropy: 0.266588.\n",
      "episode: 4177   score: 210.0  epsilon: 1.0    steps: 385  evaluation reward: 346.1\n",
      "episode: 4178   score: 670.0  epsilon: 1.0    steps: 606  evaluation reward: 347.65\n",
      "episode: 4179   score: 240.0  epsilon: 1.0    steps: 1001  evaluation reward: 347.95\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9601: Policy loss: 2.054640. Value loss: 37.974674. Entropy: 0.183813.\n",
      "Iteration 9602: Policy loss: 2.117887. Value loss: 22.361067. Entropy: 0.209462.\n",
      "Iteration 9603: Policy loss: 2.137963. Value loss: 15.819533. Entropy: 0.199053.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9604: Policy loss: -0.542935. Value loss: 25.505026. Entropy: 0.248715.\n",
      "Iteration 9605: Policy loss: -0.476721. Value loss: 15.401047. Entropy: 0.246739.\n",
      "Iteration 9606: Policy loss: -0.624500. Value loss: 12.095529. Entropy: 0.245467.\n",
      "episode: 4180   score: 260.0  epsilon: 1.0    steps: 671  evaluation reward: 346.6\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9607: Policy loss: -2.589319. Value loss: 167.097168. Entropy: 0.152301.\n",
      "Iteration 9608: Policy loss: -2.652594. Value loss: 137.804459. Entropy: 0.146842.\n",
      "Iteration 9609: Policy loss: -2.700006. Value loss: 99.108116. Entropy: 0.135188.\n",
      "episode: 4181   score: 335.0  epsilon: 1.0    steps: 811  evaluation reward: 347.55\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9610: Policy loss: -0.633723. Value loss: 42.561253. Entropy: 0.126701.\n",
      "Iteration 9611: Policy loss: -0.344572. Value loss: 18.352140. Entropy: 0.135971.\n",
      "Iteration 9612: Policy loss: -0.993870. Value loss: 14.890620. Entropy: 0.137877.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9613: Policy loss: 0.890950. Value loss: 246.274750. Entropy: 0.147787.\n",
      "Iteration 9614: Policy loss: 0.405489. Value loss: 185.028778. Entropy: 0.188372.\n",
      "Iteration 9615: Policy loss: 0.538737. Value loss: 126.725410. Entropy: 0.246312.\n",
      "episode: 4182   score: 490.0  epsilon: 1.0    steps: 199  evaluation reward: 347.35\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9616: Policy loss: -5.114125. Value loss: 251.170578. Entropy: 0.214201.\n",
      "Iteration 9617: Policy loss: -5.360640. Value loss: 175.860550. Entropy: 0.210294.\n",
      "Iteration 9618: Policy loss: -5.282763. Value loss: 106.304855. Entropy: 0.213347.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9619: Policy loss: 0.255755. Value loss: 100.313568. Entropy: 0.207942.\n",
      "Iteration 9620: Policy loss: 0.713360. Value loss: 51.800148. Entropy: 0.201712.\n",
      "Iteration 9621: Policy loss: 0.933407. Value loss: 26.135841. Entropy: 0.203778.\n",
      "episode: 4183   score: 585.0  epsilon: 1.0    steps: 103  evaluation reward: 350.95\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9622: Policy loss: 2.797984. Value loss: 81.314919. Entropy: 0.272923.\n",
      "Iteration 9623: Policy loss: 2.915704. Value loss: 33.013073. Entropy: 0.292240.\n",
      "Iteration 9624: Policy loss: 2.653175. Value loss: 27.189285. Entropy: 0.309610.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9625: Policy loss: 1.263072. Value loss: 226.831421. Entropy: 0.294282.\n",
      "Iteration 9626: Policy loss: 1.787079. Value loss: 68.204140. Entropy: 0.337530.\n",
      "Iteration 9627: Policy loss: 1.765576. Value loss: 57.150566. Entropy: 0.357615.\n",
      "episode: 4184   score: 605.0  epsilon: 1.0    steps: 504  evaluation reward: 353.6\n",
      "episode: 4185   score: 585.0  epsilon: 1.0    steps: 595  evaluation reward: 356.5\n",
      "episode: 4186   score: 590.0  epsilon: 1.0    steps: 746  evaluation reward: 358.7\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9628: Policy loss: 3.345385. Value loss: 50.057632. Entropy: 0.410516.\n",
      "Iteration 9629: Policy loss: 3.077158. Value loss: 27.009445. Entropy: 0.408134.\n",
      "Iteration 9630: Policy loss: 3.264710. Value loss: 21.534023. Entropy: 0.391953.\n",
      "episode: 4187   score: 210.0  epsilon: 1.0    steps: 190  evaluation reward: 356.65\n",
      "episode: 4188   score: 330.0  epsilon: 1.0    steps: 840  evaluation reward: 356.3\n",
      "episode: 4189   score: 390.0  epsilon: 1.0    steps: 905  evaluation reward: 357.1\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9631: Policy loss: 1.722182. Value loss: 21.942944. Entropy: 0.375668.\n",
      "Iteration 9632: Policy loss: 1.714836. Value loss: 15.975384. Entropy: 0.372876.\n",
      "Iteration 9633: Policy loss: 1.710590. Value loss: 13.815885. Entropy: 0.370550.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9634: Policy loss: 2.264891. Value loss: 61.717373. Entropy: 0.350082.\n",
      "Iteration 9635: Policy loss: 2.303578. Value loss: 32.863953. Entropy: 0.361640.\n",
      "Iteration 9636: Policy loss: 2.238483. Value loss: 25.613392. Entropy: 0.343764.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9637: Policy loss: -1.090643. Value loss: 45.911205. Entropy: 0.297566.\n",
      "Iteration 9638: Policy loss: -1.233966. Value loss: 22.247786. Entropy: 0.292226.\n",
      "Iteration 9639: Policy loss: -1.095755. Value loss: 15.979585. Entropy: 0.282152.\n",
      "episode: 4190   score: 490.0  epsilon: 1.0    steps: 341  evaluation reward: 359.9\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9640: Policy loss: 3.439667. Value loss: 33.301838. Entropy: 0.165079.\n",
      "Iteration 9641: Policy loss: 3.278970. Value loss: 16.209808. Entropy: 0.169218.\n",
      "Iteration 9642: Policy loss: 3.514656. Value loss: 13.045164. Entropy: 0.191712.\n",
      "episode: 4191   score: 210.0  epsilon: 1.0    steps: 503  evaluation reward: 357.75\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9643: Policy loss: 2.699697. Value loss: 44.909775. Entropy: 0.333579.\n",
      "Iteration 9644: Policy loss: 2.728904. Value loss: 24.220068. Entropy: 0.356218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9645: Policy loss: 2.736868. Value loss: 18.181646. Entropy: 0.331225.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9646: Policy loss: -1.165094. Value loss: 20.768850. Entropy: 0.273126.\n",
      "Iteration 9647: Policy loss: -1.140035. Value loss: 10.812868. Entropy: 0.253602.\n",
      "Iteration 9648: Policy loss: -1.180030. Value loss: 8.958117. Entropy: 0.272459.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9649: Policy loss: 1.344256. Value loss: 47.973900. Entropy: 0.227818.\n",
      "Iteration 9650: Policy loss: 1.346563. Value loss: 24.747730. Entropy: 0.217706.\n",
      "Iteration 9651: Policy loss: 1.312200. Value loss: 16.902954. Entropy: 0.227995.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9652: Policy loss: 0.765840. Value loss: 30.328762. Entropy: 0.355631.\n",
      "Iteration 9653: Policy loss: 0.846057. Value loss: 17.273521. Entropy: 0.379390.\n",
      "Iteration 9654: Policy loss: 0.803083. Value loss: 11.072812. Entropy: 0.373364.\n",
      "episode: 4192   score: 445.0  epsilon: 1.0    steps: 641  evaluation reward: 356.4\n",
      "episode: 4193   score: 240.0  epsilon: 1.0    steps: 899  evaluation reward: 356.35\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9655: Policy loss: 1.182813. Value loss: 25.172823. Entropy: 0.434103.\n",
      "Iteration 9656: Policy loss: 0.999424. Value loss: 15.482672. Entropy: 0.438823.\n",
      "Iteration 9657: Policy loss: 1.033930. Value loss: 11.831634. Entropy: 0.444854.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9658: Policy loss: -0.100410. Value loss: 32.663731. Entropy: 0.380627.\n",
      "Iteration 9659: Policy loss: -0.029566. Value loss: 15.832467. Entropy: 0.399747.\n",
      "Iteration 9660: Policy loss: -0.052350. Value loss: 12.111119. Entropy: 0.386168.\n",
      "episode: 4194   score: 480.0  epsilon: 1.0    steps: 209  evaluation reward: 354.3\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9661: Policy loss: -3.602232. Value loss: 274.212250. Entropy: 0.453288.\n",
      "Iteration 9662: Policy loss: -2.889280. Value loss: 115.590874. Entropy: 0.437253.\n",
      "Iteration 9663: Policy loss: -3.274185. Value loss: 115.040977. Entropy: 0.435377.\n",
      "episode: 4195   score: 570.0  epsilon: 1.0    steps: 563  evaluation reward: 357.3\n",
      "episode: 4196   score: 360.0  epsilon: 1.0    steps: 797  evaluation reward: 358.8\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9664: Policy loss: 2.242433. Value loss: 44.325302. Entropy: 0.422597.\n",
      "Iteration 9665: Policy loss: 1.957218. Value loss: 18.632444. Entropy: 0.396596.\n",
      "Iteration 9666: Policy loss: 2.113681. Value loss: 14.139597. Entropy: 0.400480.\n",
      "episode: 4197   score: 405.0  epsilon: 1.0    steps: 446  evaluation reward: 356.65\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9667: Policy loss: 1.575137. Value loss: 36.810959. Entropy: 0.405696.\n",
      "Iteration 9668: Policy loss: 2.423877. Value loss: 21.336325. Entropy: 0.427734.\n",
      "Iteration 9669: Policy loss: 2.062588. Value loss: 17.039434. Entropy: 0.422433.\n",
      "episode: 4198   score: 510.0  epsilon: 1.0    steps: 7  evaluation reward: 359.65\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9670: Policy loss: -3.655873. Value loss: 166.908218. Entropy: 0.224466.\n",
      "Iteration 9671: Policy loss: -3.057939. Value loss: 93.254303. Entropy: 0.201055.\n",
      "Iteration 9672: Policy loss: -3.627798. Value loss: 85.644852. Entropy: 0.199125.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9673: Policy loss: 1.759408. Value loss: 40.027309. Entropy: 0.284133.\n",
      "Iteration 9674: Policy loss: 1.562905. Value loss: 20.649868. Entropy: 0.284495.\n",
      "Iteration 9675: Policy loss: 1.816725. Value loss: 16.138756. Entropy: 0.301031.\n",
      "episode: 4199   score: 210.0  epsilon: 1.0    steps: 188  evaluation reward: 359.85\n",
      "episode: 4200   score: 590.0  epsilon: 1.0    steps: 344  evaluation reward: 362.3\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9676: Policy loss: 3.396480. Value loss: 42.278194. Entropy: 0.247318.\n",
      "Iteration 9677: Policy loss: 3.377609. Value loss: 19.537355. Entropy: 0.229883.\n",
      "Iteration 9678: Policy loss: 3.354236. Value loss: 13.887219. Entropy: 0.234142.\n",
      "now time :  2019-02-25 21:40:59.861723\n",
      "episode: 4201   score: 230.0  epsilon: 1.0    steps: 632  evaluation reward: 360.6\n",
      "episode: 4202   score: 330.0  epsilon: 1.0    steps: 732  evaluation reward: 359.95\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9679: Policy loss: 2.096786. Value loss: 29.483683. Entropy: 0.289116.\n",
      "Iteration 9680: Policy loss: 1.801104. Value loss: 17.303703. Entropy: 0.285956.\n",
      "Iteration 9681: Policy loss: 2.219192. Value loss: 13.791129. Entropy: 0.289180.\n",
      "episode: 4203   score: 630.0  epsilon: 1.0    steps: 1012  evaluation reward: 362.35\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9682: Policy loss: 0.286725. Value loss: 41.487106. Entropy: 0.242355.\n",
      "Iteration 9683: Policy loss: 0.446521. Value loss: 17.208715. Entropy: 0.269575.\n",
      "Iteration 9684: Policy loss: 0.204484. Value loss: 13.972046. Entropy: 0.280251.\n",
      "episode: 4204   score: 210.0  epsilon: 1.0    steps: 26  evaluation reward: 361.35\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9685: Policy loss: 1.046522. Value loss: 25.765978. Entropy: 0.247559.\n",
      "Iteration 9686: Policy loss: 0.890552. Value loss: 15.098904. Entropy: 0.255433.\n",
      "Iteration 9687: Policy loss: 1.134718. Value loss: 14.067917. Entropy: 0.250861.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9688: Policy loss: 0.641829. Value loss: 33.310139. Entropy: 0.195359.\n",
      "Iteration 9689: Policy loss: 1.031631. Value loss: 14.968678. Entropy: 0.173166.\n",
      "Iteration 9690: Policy loss: 0.839583. Value loss: 10.465134. Entropy: 0.179159.\n",
      "episode: 4205   score: 210.0  epsilon: 1.0    steps: 345  evaluation reward: 361.05\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9691: Policy loss: 1.350059. Value loss: 19.582716. Entropy: 0.102774.\n",
      "Iteration 9692: Policy loss: 1.226030. Value loss: 10.319947. Entropy: 0.105653.\n",
      "Iteration 9693: Policy loss: 1.315542. Value loss: 7.584237. Entropy: 0.110140.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9694: Policy loss: 0.980522. Value loss: 18.484221. Entropy: 0.174751.\n",
      "Iteration 9695: Policy loss: 1.009372. Value loss: 8.780217. Entropy: 0.173887.\n",
      "Iteration 9696: Policy loss: 0.915691. Value loss: 6.923912. Entropy: 0.163119.\n",
      "episode: 4206   score: 260.0  epsilon: 1.0    steps: 638  evaluation reward: 360.45\n",
      "episode: 4207   score: 260.0  epsilon: 1.0    steps: 708  evaluation reward: 356.95\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9697: Policy loss: 1.910294. Value loss: 27.688892. Entropy: 0.240753.\n",
      "Iteration 9698: Policy loss: 2.213398. Value loss: 17.289871. Entropy: 0.241127.\n",
      "Iteration 9699: Policy loss: 2.153032. Value loss: 12.804920. Entropy: 0.242799.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9700: Policy loss: 0.186016. Value loss: 18.427832. Entropy: 0.125179.\n",
      "Iteration 9701: Policy loss: -0.017080. Value loss: 11.878584. Entropy: 0.115704.\n",
      "Iteration 9702: Policy loss: 0.083815. Value loss: 10.336139. Entropy: 0.113484.\n",
      "episode: 4208   score: 280.0  epsilon: 1.0    steps: 886  evaluation reward: 357.65\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9703: Policy loss: 2.754049. Value loss: 20.407660. Entropy: 0.216304.\n",
      "Iteration 9704: Policy loss: 2.774969. Value loss: 13.672827. Entropy: 0.344375.\n",
      "Iteration 9705: Policy loss: 2.660547. Value loss: 11.804193. Entropy: 0.473574.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9706: Policy loss: 1.581456. Value loss: 19.325264. Entropy: 0.424690.\n",
      "Iteration 9707: Policy loss: 1.610002. Value loss: 10.432826. Entropy: 0.440082.\n",
      "Iteration 9708: Policy loss: 1.370119. Value loss: 9.381501. Entropy: 0.417292.\n",
      "episode: 4209   score: 300.0  epsilon: 1.0    steps: 478  evaluation reward: 357.9\n",
      "episode: 4210   score: 355.0  epsilon: 1.0    steps: 927  evaluation reward: 358.8\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9709: Policy loss: 3.047308. Value loss: 21.278774. Entropy: 0.406881.\n",
      "Iteration 9710: Policy loss: 3.193166. Value loss: 8.885596. Entropy: 0.396157.\n",
      "Iteration 9711: Policy loss: 3.119567. Value loss: 8.151917. Entropy: 0.400079.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9712: Policy loss: -2.003704. Value loss: 23.240547. Entropy: 0.374543.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9713: Policy loss: -2.079514. Value loss: 14.435359. Entropy: 0.350660.\n",
      "Iteration 9714: Policy loss: -2.102995. Value loss: 10.493537. Entropy: 0.361393.\n",
      "episode: 4211   score: 235.0  epsilon: 1.0    steps: 258  evaluation reward: 359.5\n",
      "episode: 4212   score: 210.0  epsilon: 1.0    steps: 599  evaluation reward: 357.35\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9715: Policy loss: 3.779915. Value loss: 25.769709. Entropy: 0.306934.\n",
      "Iteration 9716: Policy loss: 3.518782. Value loss: 12.511119. Entropy: 0.304120.\n",
      "Iteration 9717: Policy loss: 3.453809. Value loss: 9.896291. Entropy: 0.329572.\n",
      "episode: 4213   score: 310.0  epsilon: 1.0    steps: 154  evaluation reward: 357.25\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9718: Policy loss: -0.865182. Value loss: 20.336941. Entropy: 0.344885.\n",
      "Iteration 9719: Policy loss: -1.126791. Value loss: 11.901110. Entropy: 0.340182.\n",
      "Iteration 9720: Policy loss: -1.080341. Value loss: 8.888572. Entropy: 0.339516.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9721: Policy loss: -2.203351. Value loss: 32.301670. Entropy: 0.257761.\n",
      "Iteration 9722: Policy loss: -2.120118. Value loss: 13.514812. Entropy: 0.258351.\n",
      "Iteration 9723: Policy loss: -2.078307. Value loss: 9.401730. Entropy: 0.272684.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9724: Policy loss: -1.705467. Value loss: 29.298468. Entropy: 0.207191.\n",
      "Iteration 9725: Policy loss: -1.646321. Value loss: 15.531015. Entropy: 0.187462.\n",
      "Iteration 9726: Policy loss: -1.387879. Value loss: 11.020240. Entropy: 0.185031.\n",
      "episode: 4214   score: 260.0  epsilon: 1.0    steps: 485  evaluation reward: 357.25\n",
      "episode: 4215   score: 345.0  epsilon: 1.0    steps: 812  evaluation reward: 358.05\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9727: Policy loss: -0.805759. Value loss: 35.854713. Entropy: 0.400697.\n",
      "Iteration 9728: Policy loss: -0.332298. Value loss: 17.035683. Entropy: 0.403779.\n",
      "Iteration 9729: Policy loss: -0.795492. Value loss: 13.405595. Entropy: 0.417541.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9730: Policy loss: -2.024881. Value loss: 33.471596. Entropy: 0.415131.\n",
      "Iteration 9731: Policy loss: -1.565623. Value loss: 14.601930. Entropy: 0.424590.\n",
      "Iteration 9732: Policy loss: -1.724501. Value loss: 10.354549. Entropy: 0.418221.\n",
      "episode: 4216   score: 475.0  epsilon: 1.0    steps: 103  evaluation reward: 358.9\n",
      "episode: 4217   score: 440.0  epsilon: 1.0    steps: 701  evaluation reward: 358.3\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9733: Policy loss: 0.640876. Value loss: 30.602171. Entropy: 0.405941.\n",
      "Iteration 9734: Policy loss: 0.565946. Value loss: 20.173101. Entropy: 0.420373.\n",
      "Iteration 9735: Policy loss: 0.601470. Value loss: 13.453163. Entropy: 0.486185.\n",
      "episode: 4218   score: 300.0  epsilon: 1.0    steps: 996  evaluation reward: 355.0\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9736: Policy loss: 0.161079. Value loss: 22.987875. Entropy: 0.519313.\n",
      "Iteration 9737: Policy loss: 0.168784. Value loss: 12.955102. Entropy: 0.493032.\n",
      "Iteration 9738: Policy loss: 0.214236. Value loss: 9.799360. Entropy: 0.547647.\n",
      "episode: 4219   score: 450.0  epsilon: 1.0    steps: 317  evaluation reward: 356.75\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9739: Policy loss: -1.206553. Value loss: 36.555851. Entropy: 0.532426.\n",
      "Iteration 9740: Policy loss: -1.074374. Value loss: 17.985699. Entropy: 0.498979.\n",
      "Iteration 9741: Policy loss: -1.173151. Value loss: 17.067801. Entropy: 0.489838.\n",
      "episode: 4220   score: 505.0  epsilon: 1.0    steps: 533  evaluation reward: 359.2\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9742: Policy loss: 1.240825. Value loss: 26.096960. Entropy: 0.613055.\n",
      "Iteration 9743: Policy loss: 1.329315. Value loss: 13.224579. Entropy: 0.648660.\n",
      "Iteration 9744: Policy loss: 1.141145. Value loss: 9.318987. Entropy: 0.642620.\n",
      "episode: 4221   score: 330.0  epsilon: 1.0    steps: 132  evaluation reward: 359.9\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9745: Policy loss: 0.505069. Value loss: 14.144813. Entropy: 0.446478.\n",
      "Iteration 9746: Policy loss: 0.385006. Value loss: 11.466626. Entropy: 0.454960.\n",
      "Iteration 9747: Policy loss: 0.394305. Value loss: 9.537542. Entropy: 0.459982.\n",
      "episode: 4222   score: 270.0  epsilon: 1.0    steps: 805  evaluation reward: 359.65\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9748: Policy loss: -1.334730. Value loss: 28.017918. Entropy: 0.431092.\n",
      "Iteration 9749: Policy loss: -1.036537. Value loss: 14.928329. Entropy: 0.440456.\n",
      "Iteration 9750: Policy loss: -1.219854. Value loss: 11.073675. Entropy: 0.430776.\n",
      "episode: 4223   score: 385.0  epsilon: 1.0    steps: 490  evaluation reward: 362.45\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9751: Policy loss: -0.384459. Value loss: 27.659157. Entropy: 0.423710.\n",
      "Iteration 9752: Policy loss: -0.210988. Value loss: 15.442465. Entropy: 0.403578.\n",
      "Iteration 9753: Policy loss: -0.520862. Value loss: 10.601825. Entropy: 0.422549.\n",
      "episode: 4224   score: 310.0  epsilon: 1.0    steps: 1006  evaluation reward: 363.3\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9754: Policy loss: 0.219853. Value loss: 18.511805. Entropy: 0.361344.\n",
      "Iteration 9755: Policy loss: 0.407065. Value loss: 9.106080. Entropy: 0.358179.\n",
      "Iteration 9756: Policy loss: 0.201234. Value loss: 8.903682. Entropy: 0.350144.\n",
      "episode: 4225   score: 240.0  epsilon: 1.0    steps: 302  evaluation reward: 361.3\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9757: Policy loss: 0.991530. Value loss: 18.016649. Entropy: 0.390352.\n",
      "Iteration 9758: Policy loss: 0.825220. Value loss: 10.734893. Entropy: 0.400507.\n",
      "Iteration 9759: Policy loss: 0.956771. Value loss: 8.440580. Entropy: 0.388290.\n",
      "episode: 4226   score: 225.0  epsilon: 1.0    steps: 535  evaluation reward: 360.9\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9760: Policy loss: 0.214476. Value loss: 18.437025. Entropy: 0.401696.\n",
      "Iteration 9761: Policy loss: 0.191900. Value loss: 12.815897. Entropy: 0.407349.\n",
      "Iteration 9762: Policy loss: 0.256481. Value loss: 8.736697. Entropy: 0.409829.\n",
      "episode: 4227   score: 345.0  epsilon: 1.0    steps: 6  evaluation reward: 359.2\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9763: Policy loss: -0.472972. Value loss: 20.229641. Entropy: 0.381565.\n",
      "Iteration 9764: Policy loss: -0.447141. Value loss: 11.545961. Entropy: 0.372091.\n",
      "Iteration 9765: Policy loss: -0.492264. Value loss: 10.072607. Entropy: 0.382625.\n",
      "episode: 4228   score: 345.0  epsilon: 1.0    steps: 245  evaluation reward: 359.15\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9766: Policy loss: -0.472119. Value loss: 20.497215. Entropy: 0.373296.\n",
      "Iteration 9767: Policy loss: -0.555129. Value loss: 10.039495. Entropy: 0.371326.\n",
      "Iteration 9768: Policy loss: -0.471359. Value loss: 7.444020. Entropy: 0.387974.\n",
      "episode: 4229   score: 490.0  epsilon: 1.0    steps: 688  evaluation reward: 361.2\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9769: Policy loss: -3.430238. Value loss: 202.403320. Entropy: 0.474187.\n",
      "Iteration 9770: Policy loss: -3.574830. Value loss: 63.949162. Entropy: 0.459155.\n",
      "Iteration 9771: Policy loss: -4.065043. Value loss: 26.166039. Entropy: 0.466972.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9772: Policy loss: 0.050679. Value loss: 26.483572. Entropy: 0.393804.\n",
      "Iteration 9773: Policy loss: 0.159341. Value loss: 15.592022. Entropy: 0.395037.\n",
      "Iteration 9774: Policy loss: -0.030821. Value loss: 9.673933. Entropy: 0.393101.\n",
      "episode: 4230   score: 275.0  epsilon: 1.0    steps: 373  evaluation reward: 360.55\n",
      "episode: 4231   score: 260.0  epsilon: 1.0    steps: 927  evaluation reward: 359.3\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9775: Policy loss: -0.346695. Value loss: 29.008596. Entropy: 0.478156.\n",
      "Iteration 9776: Policy loss: -0.016538. Value loss: 13.111198. Entropy: 0.470974.\n",
      "Iteration 9777: Policy loss: -0.598235. Value loss: 12.510389. Entropy: 0.509213.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9778: Policy loss: -0.162542. Value loss: 29.058670. Entropy: 0.426722.\n",
      "Iteration 9779: Policy loss: -0.255644. Value loss: 15.062874. Entropy: 0.444459.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9780: Policy loss: -0.121420. Value loss: 11.995272. Entropy: 0.444689.\n",
      "episode: 4232   score: 270.0  epsilon: 1.0    steps: 19  evaluation reward: 358.3\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9781: Policy loss: 0.176040. Value loss: 15.855047. Entropy: 0.497466.\n",
      "Iteration 9782: Policy loss: 0.115550. Value loss: 10.597612. Entropy: 0.523043.\n",
      "Iteration 9783: Policy loss: 0.247902. Value loss: 8.789125. Entropy: 0.549219.\n",
      "episode: 4233   score: 240.0  epsilon: 1.0    steps: 220  evaluation reward: 357.85\n",
      "episode: 4234   score: 600.0  epsilon: 1.0    steps: 862  evaluation reward: 360.9\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9784: Policy loss: 1.463509. Value loss: 32.052402. Entropy: 0.611877.\n",
      "Iteration 9785: Policy loss: 1.395972. Value loss: 19.050501. Entropy: 0.573726.\n",
      "Iteration 9786: Policy loss: 1.623012. Value loss: 15.378757. Entropy: 0.601224.\n",
      "episode: 4235   score: 620.0  epsilon: 1.0    steps: 538  evaluation reward: 364.05\n",
      "episode: 4236   score: 300.0  epsilon: 1.0    steps: 716  evaluation reward: 360.75\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9787: Policy loss: 1.003336. Value loss: 15.005612. Entropy: 0.582031.\n",
      "Iteration 9788: Policy loss: 0.786482. Value loss: 10.508243. Entropy: 0.596986.\n",
      "Iteration 9789: Policy loss: 0.961861. Value loss: 7.819589. Entropy: 0.593914.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9790: Policy loss: 0.573314. Value loss: 20.835928. Entropy: 0.687243.\n",
      "Iteration 9791: Policy loss: 0.773202. Value loss: 12.231821. Entropy: 0.703434.\n",
      "Iteration 9792: Policy loss: 0.508097. Value loss: 10.452439. Entropy: 0.715067.\n",
      "episode: 4237   score: 260.0  epsilon: 1.0    steps: 384  evaluation reward: 360.4\n",
      "episode: 4238   score: 400.0  epsilon: 1.0    steps: 409  evaluation reward: 359.9\n",
      "episode: 4239   score: 285.0  epsilon: 1.0    steps: 917  evaluation reward: 358.45\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9793: Policy loss: 0.500962. Value loss: 20.202112. Entropy: 0.564780.\n",
      "Iteration 9794: Policy loss: 0.577816. Value loss: 12.633651. Entropy: 0.577685.\n",
      "Iteration 9795: Policy loss: 0.505443. Value loss: 11.829031. Entropy: 0.572069.\n",
      "episode: 4240   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 353.9\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9796: Policy loss: 1.256394. Value loss: 18.311668. Entropy: 0.443494.\n",
      "Iteration 9797: Policy loss: 1.183568. Value loss: 11.890371. Entropy: 0.446860.\n",
      "Iteration 9798: Policy loss: 1.200713. Value loss: 9.206051. Entropy: 0.427645.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9799: Policy loss: 0.599259. Value loss: 25.676130. Entropy: 0.625767.\n",
      "Iteration 9800: Policy loss: 0.753195. Value loss: 15.891907. Entropy: 0.663140.\n",
      "Iteration 9801: Policy loss: 0.556286. Value loss: 12.143730. Entropy: 0.642936.\n",
      "episode: 4241   score: 260.0  epsilon: 1.0    steps: 201  evaluation reward: 354.35\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9802: Policy loss: -0.323410. Value loss: 24.548758. Entropy: 0.515714.\n",
      "Iteration 9803: Policy loss: -0.280235. Value loss: 13.325845. Entropy: 0.506838.\n",
      "Iteration 9804: Policy loss: -0.184722. Value loss: 9.541199. Entropy: 0.519606.\n",
      "episode: 4242   score: 260.0  epsilon: 1.0    steps: 787  evaluation reward: 350.3\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9805: Policy loss: -1.900149. Value loss: 28.479837. Entropy: 0.417713.\n",
      "Iteration 9806: Policy loss: -1.778687. Value loss: 18.742065. Entropy: 0.432620.\n",
      "Iteration 9807: Policy loss: -1.891991. Value loss: 11.255425. Entropy: 0.423823.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9808: Policy loss: 0.262640. Value loss: 18.712339. Entropy: 0.531033.\n",
      "Iteration 9809: Policy loss: -0.046507. Value loss: 10.430150. Entropy: 0.512736.\n",
      "Iteration 9810: Policy loss: 0.158666. Value loss: 7.709012. Entropy: 0.523188.\n",
      "episode: 4243   score: 250.0  epsilon: 1.0    steps: 419  evaluation reward: 350.7\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9811: Policy loss: -2.645417. Value loss: 245.909897. Entropy: 0.539620.\n",
      "Iteration 9812: Policy loss: -2.619548. Value loss: 92.077652. Entropy: 0.529888.\n",
      "Iteration 9813: Policy loss: -3.043092. Value loss: 91.096382. Entropy: 0.539819.\n",
      "episode: 4244   score: 260.0  epsilon: 1.0    steps: 265  evaluation reward: 350.6\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9814: Policy loss: -1.151105. Value loss: 34.740231. Entropy: 0.555406.\n",
      "Iteration 9815: Policy loss: -1.272785. Value loss: 20.533428. Entropy: 0.557859.\n",
      "Iteration 9816: Policy loss: -1.408178. Value loss: 14.950408. Entropy: 0.576268.\n",
      "episode: 4245   score: 465.0  epsilon: 1.0    steps: 585  evaluation reward: 352.4\n",
      "episode: 4246   score: 525.0  epsilon: 1.0    steps: 714  evaluation reward: 355.85\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9817: Policy loss: 2.299601. Value loss: 24.666462. Entropy: 0.489613.\n",
      "Iteration 9818: Policy loss: 2.054894. Value loss: 16.386484. Entropy: 0.497271.\n",
      "Iteration 9819: Policy loss: 2.017333. Value loss: 11.377615. Entropy: 0.471620.\n",
      "episode: 4247   score: 400.0  epsilon: 1.0    steps: 100  evaluation reward: 358.6\n",
      "episode: 4248   score: 300.0  epsilon: 1.0    steps: 908  evaluation reward: 354.55\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9820: Policy loss: -2.931566. Value loss: 167.081711. Entropy: 0.509128.\n",
      "Iteration 9821: Policy loss: -2.048972. Value loss: 70.967194. Entropy: 0.488442.\n",
      "Iteration 9822: Policy loss: -3.612043. Value loss: 56.097084. Entropy: 0.496141.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9823: Policy loss: 0.960031. Value loss: 17.231096. Entropy: 0.494599.\n",
      "Iteration 9824: Policy loss: 0.634795. Value loss: 11.109183. Entropy: 0.495525.\n",
      "Iteration 9825: Policy loss: 0.744204. Value loss: 8.473104. Entropy: 0.494429.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9826: Policy loss: -1.298646. Value loss: 35.639713. Entropy: 0.418210.\n",
      "Iteration 9827: Policy loss: -1.166576. Value loss: 18.050182. Entropy: 0.430092.\n",
      "Iteration 9828: Policy loss: -1.428074. Value loss: 11.869011. Entropy: 0.420235.\n",
      "episode: 4249   score: 770.0  epsilon: 1.0    steps: 138  evaluation reward: 359.85\n",
      "episode: 4250   score: 260.0  epsilon: 1.0    steps: 445  evaluation reward: 359.8\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9829: Policy loss: 2.599878. Value loss: 69.396004. Entropy: 0.299095.\n",
      "Iteration 9830: Policy loss: 2.492048. Value loss: 25.375286. Entropy: 0.300415.\n",
      "Iteration 9831: Policy loss: 3.223983. Value loss: 16.493887. Entropy: 0.307683.\n",
      "now time :  2019-02-25 21:43:51.957535\n",
      "episode: 4251   score: 435.0  epsilon: 1.0    steps: 820  evaluation reward: 362.3\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9832: Policy loss: 2.239963. Value loss: 25.968582. Entropy: 0.444330.\n",
      "Iteration 9833: Policy loss: 2.156142. Value loss: 15.628874. Entropy: 0.453406.\n",
      "Iteration 9834: Policy loss: 2.138430. Value loss: 11.673047. Entropy: 0.448515.\n",
      "episode: 4252   score: 310.0  epsilon: 1.0    steps: 376  evaluation reward: 362.4\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9835: Policy loss: -1.171636. Value loss: 48.433117. Entropy: 0.607888.\n",
      "Iteration 9836: Policy loss: -0.623759. Value loss: 27.582661. Entropy: 0.609578.\n",
      "Iteration 9837: Policy loss: -0.673534. Value loss: 19.682024. Entropy: 0.598287.\n",
      "episode: 4253   score: 265.0  epsilon: 1.0    steps: 926  evaluation reward: 364.1\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9838: Policy loss: 2.477631. Value loss: 38.532356. Entropy: 0.639081.\n",
      "Iteration 9839: Policy loss: 2.411414. Value loss: 15.885759. Entropy: 0.630717.\n",
      "Iteration 9840: Policy loss: 2.310023. Value loss: 12.193401. Entropy: 0.634374.\n",
      "episode: 4254   score: 375.0  epsilon: 1.0    steps: 632  evaluation reward: 364.7\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9841: Policy loss: 2.945717. Value loss: 25.598680. Entropy: 0.648234.\n",
      "Iteration 9842: Policy loss: 3.084350. Value loss: 14.217332. Entropy: 0.647168.\n",
      "Iteration 9843: Policy loss: 2.911675. Value loss: 10.087757. Entropy: 0.650135.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9844: Policy loss: -0.099491. Value loss: 25.758072. Entropy: 0.559952.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9845: Policy loss: -0.077838. Value loss: 13.753905. Entropy: 0.565430.\n",
      "Iteration 9846: Policy loss: -0.051078. Value loss: 10.526990. Entropy: 0.557395.\n",
      "episode: 4255   score: 395.0  epsilon: 1.0    steps: 35  evaluation reward: 366.25\n",
      "episode: 4256   score: 365.0  epsilon: 1.0    steps: 683  evaluation reward: 367.8\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9847: Policy loss: -2.267825. Value loss: 138.021103. Entropy: 0.495000.\n",
      "Iteration 9848: Policy loss: -1.777942. Value loss: 66.449150. Entropy: 0.484329.\n",
      "Iteration 9849: Policy loss: -2.948675. Value loss: 42.026020. Entropy: 0.485067.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9850: Policy loss: 1.106516. Value loss: 63.335812. Entropy: 0.598365.\n",
      "Iteration 9851: Policy loss: 0.706650. Value loss: 30.227688. Entropy: 0.564124.\n",
      "Iteration 9852: Policy loss: 0.634025. Value loss: 17.257153. Entropy: 0.578180.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9853: Policy loss: -0.033230. Value loss: 51.565369. Entropy: 0.618667.\n",
      "Iteration 9854: Policy loss: -0.117696. Value loss: 26.644108. Entropy: 0.612065.\n",
      "Iteration 9855: Policy loss: 0.047609. Value loss: 20.997910. Entropy: 0.607842.\n",
      "episode: 4257   score: 400.0  epsilon: 1.0    steps: 152  evaluation reward: 365.5\n",
      "episode: 4258   score: 545.0  epsilon: 1.0    steps: 357  evaluation reward: 368.1\n",
      "episode: 4259   score: 305.0  epsilon: 1.0    steps: 446  evaluation reward: 365.6\n",
      "episode: 4260   score: 340.0  epsilon: 1.0    steps: 784  evaluation reward: 365.85\n",
      "episode: 4261   score: 270.0  epsilon: 1.0    steps: 948  evaluation reward: 365.1\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9856: Policy loss: -2.604574. Value loss: 234.981277. Entropy: 0.374040.\n",
      "Iteration 9857: Policy loss: -2.110591. Value loss: 91.249771. Entropy: 0.398444.\n",
      "Iteration 9858: Policy loss: -2.510375. Value loss: 61.839024. Entropy: 0.375431.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9859: Policy loss: 4.188008. Value loss: 61.261593. Entropy: 0.571141.\n",
      "Iteration 9860: Policy loss: 4.085363. Value loss: 30.857819. Entropy: 0.584100.\n",
      "Iteration 9861: Policy loss: 3.811785. Value loss: 28.456226. Entropy: 0.551251.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9862: Policy loss: 1.172724. Value loss: 33.052330. Entropy: 0.508187.\n",
      "Iteration 9863: Policy loss: 1.328865. Value loss: 17.479860. Entropy: 0.473348.\n",
      "Iteration 9864: Policy loss: 1.234922. Value loss: 17.793905. Entropy: 0.486379.\n",
      "episode: 4262   score: 275.0  epsilon: 1.0    steps: 549  evaluation reward: 364.1\n",
      "episode: 4263   score: 350.0  epsilon: 1.0    steps: 754  evaluation reward: 364.75\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9865: Policy loss: -0.427896. Value loss: 47.096634. Entropy: 0.456985.\n",
      "Iteration 9866: Policy loss: -0.254913. Value loss: 20.322222. Entropy: 0.438765.\n",
      "Iteration 9867: Policy loss: -0.117663. Value loss: 16.607275. Entropy: 0.448161.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9868: Policy loss: 0.433258. Value loss: 184.992386. Entropy: 0.413854.\n",
      "Iteration 9869: Policy loss: 1.122923. Value loss: 95.132683. Entropy: 0.400032.\n",
      "Iteration 9870: Policy loss: 0.425836. Value loss: 79.899971. Entropy: 0.420746.\n",
      "episode: 4264   score: 515.0  epsilon: 1.0    steps: 122  evaluation reward: 365.7\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9871: Policy loss: 3.314053. Value loss: 61.382267. Entropy: 0.539799.\n",
      "Iteration 9872: Policy loss: 3.306225. Value loss: 26.604172. Entropy: 0.566707.\n",
      "Iteration 9873: Policy loss: 3.521269. Value loss: 23.583241. Entropy: 0.552165.\n",
      "episode: 4265   score: 210.0  epsilon: 1.0    steps: 509  evaluation reward: 364.1\n",
      "episode: 4266   score: 490.0  epsilon: 1.0    steps: 1023  evaluation reward: 365.45\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9874: Policy loss: 1.525865. Value loss: 43.343052. Entropy: 0.518460.\n",
      "Iteration 9875: Policy loss: 1.793296. Value loss: 20.118216. Entropy: 0.511563.\n",
      "Iteration 9876: Policy loss: 1.420317. Value loss: 16.823881. Entropy: 0.489400.\n",
      "episode: 4267   score: 280.0  epsilon: 1.0    steps: 275  evaluation reward: 365.35\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9877: Policy loss: 1.614051. Value loss: 28.540255. Entropy: 0.348684.\n",
      "Iteration 9878: Policy loss: 1.948736. Value loss: 18.394314. Entropy: 0.368733.\n",
      "Iteration 9879: Policy loss: 1.783311. Value loss: 12.665904. Entropy: 0.384426.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9880: Policy loss: 2.118124. Value loss: 38.812462. Entropy: 0.619837.\n",
      "Iteration 9881: Policy loss: 1.360326. Value loss: 20.201958. Entropy: 0.606082.\n",
      "Iteration 9882: Policy loss: 1.786710. Value loss: 17.112804. Entropy: 0.609226.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9883: Policy loss: 1.628568. Value loss: 33.273918. Entropy: 0.563220.\n",
      "Iteration 9884: Policy loss: 1.697457. Value loss: 17.037086. Entropy: 0.575401.\n",
      "Iteration 9885: Policy loss: 1.635931. Value loss: 12.814141. Entropy: 0.575579.\n",
      "episode: 4268   score: 305.0  epsilon: 1.0    steps: 606  evaluation reward: 363.5\n",
      "episode: 4269   score: 255.0  epsilon: 1.0    steps: 711  evaluation reward: 361.8\n",
      "episode: 4270   score: 315.0  epsilon: 1.0    steps: 861  evaluation reward: 362.0\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9886: Policy loss: 2.145806. Value loss: 32.625622. Entropy: 0.589728.\n",
      "Iteration 9887: Policy loss: 2.189619. Value loss: 20.501692. Entropy: 0.596199.\n",
      "Iteration 9888: Policy loss: 2.163736. Value loss: 16.522694. Entropy: 0.610044.\n",
      "episode: 4271   score: 505.0  epsilon: 1.0    steps: 139  evaluation reward: 357.5\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9889: Policy loss: -0.899587. Value loss: 29.259317. Entropy: 0.510490.\n",
      "Iteration 9890: Policy loss: -0.916931. Value loss: 20.385185. Entropy: 0.496660.\n",
      "Iteration 9891: Policy loss: -0.671992. Value loss: 16.798075. Entropy: 0.506351.\n",
      "episode: 4272   score: 260.0  epsilon: 1.0    steps: 61  evaluation reward: 356.45\n",
      "episode: 4273   score: 210.0  epsilon: 1.0    steps: 290  evaluation reward: 356.45\n",
      "episode: 4274   score: 210.0  epsilon: 1.0    steps: 394  evaluation reward: 356.45\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9892: Policy loss: -0.115083. Value loss: 16.776058. Entropy: 0.470670.\n",
      "Iteration 9893: Policy loss: -0.176747. Value loss: 11.969959. Entropy: 0.467242.\n",
      "Iteration 9894: Policy loss: -0.042858. Value loss: 10.318499. Entropy: 0.472220.\n",
      "episode: 4275   score: 260.0  epsilon: 1.0    steps: 919  evaluation reward: 357.0\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9895: Policy loss: -2.618078. Value loss: 304.854248. Entropy: 0.496273.\n",
      "Iteration 9896: Policy loss: -1.728298. Value loss: 91.442833. Entropy: 0.487827.\n",
      "Iteration 9897: Policy loss: -1.292817. Value loss: 74.617401. Entropy: 0.505186.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9898: Policy loss: -0.169489. Value loss: 31.579514. Entropy: 0.620519.\n",
      "Iteration 9899: Policy loss: -0.361007. Value loss: 18.348539. Entropy: 0.642700.\n",
      "Iteration 9900: Policy loss: -0.077114. Value loss: 15.150565. Entropy: 0.632583.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9901: Policy loss: 2.568678. Value loss: 74.409309. Entropy: 0.610212.\n",
      "Iteration 9902: Policy loss: 2.947813. Value loss: 31.254581. Entropy: 0.623278.\n",
      "Iteration 9903: Policy loss: 2.817536. Value loss: 20.178501. Entropy: 0.623586.\n",
      "episode: 4276   score: 155.0  epsilon: 1.0    steps: 651  evaluation reward: 356.15\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9904: Policy loss: -2.665807. Value loss: 278.821808. Entropy: 0.477334.\n",
      "Iteration 9905: Policy loss: -0.248540. Value loss: 101.493340. Entropy: 0.450037.\n",
      "Iteration 9906: Policy loss: -1.862433. Value loss: 69.064522. Entropy: 0.504094.\n",
      "episode: 4277   score: 210.0  epsilon: 1.0    steps: 421  evaluation reward: 356.15\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9907: Policy loss: -2.518323. Value loss: 211.025116. Entropy: 0.579675.\n",
      "Iteration 9908: Policy loss: -2.221901. Value loss: 128.202850. Entropy: 0.550558.\n",
      "Iteration 9909: Policy loss: -2.155880. Value loss: 91.577400. Entropy: 0.544789.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4278   score: 210.0  epsilon: 1.0    steps: 333  evaluation reward: 351.55\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9910: Policy loss: -1.219724. Value loss: 55.334145. Entropy: 0.565871.\n",
      "Iteration 9911: Policy loss: -1.216798. Value loss: 27.185097. Entropy: 0.559117.\n",
      "Iteration 9912: Policy loss: -0.969816. Value loss: 18.898607. Entropy: 0.545616.\n",
      "episode: 4279   score: 405.0  epsilon: 1.0    steps: 190  evaluation reward: 353.2\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9913: Policy loss: 2.588674. Value loss: 57.752415. Entropy: 0.666212.\n",
      "Iteration 9914: Policy loss: 2.702779. Value loss: 21.251747. Entropy: 0.642240.\n",
      "Iteration 9915: Policy loss: 2.460442. Value loss: 15.169432. Entropy: 0.636572.\n",
      "episode: 4280   score: 430.0  epsilon: 1.0    steps: 47  evaluation reward: 354.9\n",
      "episode: 4281   score: 570.0  epsilon: 1.0    steps: 830  evaluation reward: 357.25\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9916: Policy loss: 2.548476. Value loss: 35.886086. Entropy: 0.415954.\n",
      "Iteration 9917: Policy loss: 2.514864. Value loss: 18.303217. Entropy: 0.427897.\n",
      "Iteration 9918: Policy loss: 2.537611. Value loss: 15.048401. Entropy: 0.429078.\n",
      "episode: 4282   score: 515.0  epsilon: 1.0    steps: 905  evaluation reward: 357.5\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9919: Policy loss: 0.532233. Value loss: 32.292702. Entropy: 0.576807.\n",
      "Iteration 9920: Policy loss: 0.627837. Value loss: 17.213438. Entropy: 0.592631.\n",
      "Iteration 9921: Policy loss: 0.375218. Value loss: 12.908488. Entropy: 0.598237.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9922: Policy loss: 2.148946. Value loss: 326.962708. Entropy: 0.677002.\n",
      "Iteration 9923: Policy loss: 3.295801. Value loss: 122.820854. Entropy: 0.674865.\n",
      "Iteration 9924: Policy loss: 1.908342. Value loss: 111.831062. Entropy: 0.635734.\n",
      "episode: 4283   score: 205.0  epsilon: 1.0    steps: 488  evaluation reward: 353.7\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9925: Policy loss: 2.140676. Value loss: 33.055805. Entropy: 0.423581.\n",
      "Iteration 9926: Policy loss: 2.681525. Value loss: 20.857189. Entropy: 0.408831.\n",
      "Iteration 9927: Policy loss: 2.157118. Value loss: 15.800106. Entropy: 0.403770.\n",
      "episode: 4284   score: 670.0  epsilon: 1.0    steps: 609  evaluation reward: 354.35\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9928: Policy loss: -2.201960. Value loss: 318.787781. Entropy: 0.513078.\n",
      "Iteration 9929: Policy loss: -1.485303. Value loss: 151.258957. Entropy: 0.538843.\n",
      "Iteration 9930: Policy loss: -1.851704. Value loss: 140.635468. Entropy: 0.500448.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9931: Policy loss: -0.340097. Value loss: 40.462379. Entropy: 0.529386.\n",
      "Iteration 9932: Policy loss: -0.376795. Value loss: 24.343637. Entropy: 0.547671.\n",
      "Iteration 9933: Policy loss: -0.365792. Value loss: 16.739363. Entropy: 0.536404.\n",
      "episode: 4285   score: 285.0  epsilon: 1.0    steps: 136  evaluation reward: 351.35\n",
      "episode: 4286   score: 290.0  epsilon: 1.0    steps: 287  evaluation reward: 348.35\n",
      "episode: 4287   score: 295.0  epsilon: 1.0    steps: 722  evaluation reward: 349.2\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9934: Policy loss: 2.147237. Value loss: 31.250923. Entropy: 0.447068.\n",
      "Iteration 9935: Policy loss: 2.372590. Value loss: 21.472479. Entropy: 0.472292.\n",
      "Iteration 9936: Policy loss: 2.095167. Value loss: 15.763581. Entropy: 0.463178.\n",
      "episode: 4288   score: 270.0  epsilon: 1.0    steps: 10  evaluation reward: 348.6\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9937: Policy loss: -0.814921. Value loss: 45.912949. Entropy: 0.466062.\n",
      "Iteration 9938: Policy loss: -0.843235. Value loss: 26.524338. Entropy: 0.475480.\n",
      "Iteration 9939: Policy loss: -0.931678. Value loss: 20.886471. Entropy: 0.482162.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9940: Policy loss: 1.825559. Value loss: 40.277920. Entropy: 0.517378.\n",
      "Iteration 9941: Policy loss: 2.289820. Value loss: 17.908180. Entropy: 0.512775.\n",
      "Iteration 9942: Policy loss: 1.699164. Value loss: 13.230776. Entropy: 0.538489.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9943: Policy loss: -0.161471. Value loss: 219.378510. Entropy: 0.476664.\n",
      "Iteration 9944: Policy loss: 0.083141. Value loss: 117.815704. Entropy: 0.486955.\n",
      "Iteration 9945: Policy loss: 0.275360. Value loss: 70.131363. Entropy: 0.499665.\n",
      "episode: 4289   score: 360.0  epsilon: 1.0    steps: 849  evaluation reward: 348.3\n",
      "episode: 4290   score: 550.0  epsilon: 1.0    steps: 1011  evaluation reward: 348.9\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9946: Policy loss: 4.699734. Value loss: 36.163437. Entropy: 0.547062.\n",
      "Iteration 9947: Policy loss: 4.671552. Value loss: 19.162947. Entropy: 0.546657.\n",
      "Iteration 9948: Policy loss: 4.731582. Value loss: 16.489328. Entropy: 0.557610.\n",
      "episode: 4291   score: 255.0  epsilon: 1.0    steps: 456  evaluation reward: 349.35\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9949: Policy loss: -0.111636. Value loss: 31.272825. Entropy: 0.606651.\n",
      "Iteration 9950: Policy loss: 0.058193. Value loss: 18.839195. Entropy: 0.609406.\n",
      "Iteration 9951: Policy loss: 0.084705. Value loss: 12.998059. Entropy: 0.622677.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9952: Policy loss: 1.473001. Value loss: 16.684717. Entropy: 0.439014.\n",
      "Iteration 9953: Policy loss: 1.528608. Value loss: 8.902691. Entropy: 0.452178.\n",
      "Iteration 9954: Policy loss: 1.495158. Value loss: 7.854900. Entropy: 0.432539.\n",
      "episode: 4292   score: 310.0  epsilon: 1.0    steps: 217  evaluation reward: 348.0\n",
      "episode: 4293   score: 440.0  epsilon: 1.0    steps: 265  evaluation reward: 350.0\n",
      "episode: 4294   score: 245.0  epsilon: 1.0    steps: 659  evaluation reward: 347.65\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9955: Policy loss: 2.123298. Value loss: 39.715725. Entropy: 0.728640.\n",
      "Iteration 9956: Policy loss: 2.064024. Value loss: 20.014450. Entropy: 0.715232.\n",
      "Iteration 9957: Policy loss: 1.719521. Value loss: 15.907534. Entropy: 0.739408.\n",
      "episode: 4295   score: 355.0  epsilon: 1.0    steps: 568  evaluation reward: 345.5\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9958: Policy loss: 5.653829. Value loss: 36.931866. Entropy: 0.518232.\n",
      "Iteration 9959: Policy loss: 5.558342. Value loss: 18.898415. Entropy: 0.522926.\n",
      "Iteration 9960: Policy loss: 5.405788. Value loss: 13.590342. Entropy: 0.526522.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9961: Policy loss: 2.995777. Value loss: 26.436588. Entropy: 0.519784.\n",
      "Iteration 9962: Policy loss: 3.024477. Value loss: 15.247910. Entropy: 0.513915.\n",
      "Iteration 9963: Policy loss: 2.960619. Value loss: 12.418627. Entropy: 0.542958.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9964: Policy loss: -1.119708. Value loss: 136.038956. Entropy: 0.533712.\n",
      "Iteration 9965: Policy loss: -1.441260. Value loss: 48.454353. Entropy: 0.534560.\n",
      "Iteration 9966: Policy loss: -1.042255. Value loss: 24.827221. Entropy: 0.519558.\n",
      "episode: 4296   score: 440.0  epsilon: 1.0    steps: 4  evaluation reward: 346.3\n",
      "episode: 4297   score: 210.0  epsilon: 1.0    steps: 922  evaluation reward: 344.35\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9967: Policy loss: -0.750802. Value loss: 210.190933. Entropy: 0.460988.\n",
      "Iteration 9968: Policy loss: -1.961780. Value loss: 134.947449. Entropy: 0.469578.\n",
      "Iteration 9969: Policy loss: -0.900589. Value loss: 74.100410. Entropy: 0.478639.\n",
      "episode: 4298   score: 380.0  epsilon: 1.0    steps: 758  evaluation reward: 343.05\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9970: Policy loss: 1.136098. Value loss: 35.476990. Entropy: 0.453873.\n",
      "Iteration 9971: Policy loss: 1.185351. Value loss: 15.761138. Entropy: 0.467571.\n",
      "Iteration 9972: Policy loss: 1.159028. Value loss: 13.360955. Entropy: 0.456400.\n",
      "episode: 4299   score: 210.0  epsilon: 1.0    steps: 345  evaluation reward: 343.05\n",
      "episode: 4300   score: 440.0  epsilon: 1.0    steps: 393  evaluation reward: 341.55\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9973: Policy loss: -0.457366. Value loss: 33.799530. Entropy: 0.594306.\n",
      "Iteration 9974: Policy loss: -0.613267. Value loss: 22.493610. Entropy: 0.588890.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9975: Policy loss: -0.470191. Value loss: 16.877459. Entropy: 0.597133.\n",
      "now time :  2019-02-25 21:46:31.999478\n",
      "episode: 4301   score: 270.0  epsilon: 1.0    steps: 549  evaluation reward: 341.95\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9976: Policy loss: 0.857109. Value loss: 23.034527. Entropy: 0.441790.\n",
      "Iteration 9977: Policy loss: 0.822458. Value loss: 11.883074. Entropy: 0.439537.\n",
      "Iteration 9978: Policy loss: 0.986623. Value loss: 10.541125. Entropy: 0.445387.\n",
      "episode: 4302   score: 265.0  epsilon: 1.0    steps: 160  evaluation reward: 341.3\n",
      "episode: 4303   score: 315.0  epsilon: 1.0    steps: 892  evaluation reward: 338.15\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9979: Policy loss: 1.353382. Value loss: 23.040882. Entropy: 0.467669.\n",
      "Iteration 9980: Policy loss: 1.336816. Value loss: 15.128527. Entropy: 0.473438.\n",
      "Iteration 9981: Policy loss: 1.429868. Value loss: 11.375375. Entropy: 0.443252.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9982: Policy loss: -0.362741. Value loss: 19.552534. Entropy: 0.461808.\n",
      "Iteration 9983: Policy loss: -0.330926. Value loss: 11.299566. Entropy: 0.436844.\n",
      "Iteration 9984: Policy loss: -0.285088. Value loss: 8.825935. Entropy: 0.445281.\n",
      "episode: 4304   score: 260.0  epsilon: 1.0    steps: 3  evaluation reward: 338.65\n",
      "episode: 4305   score: 260.0  epsilon: 1.0    steps: 972  evaluation reward: 339.15\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9985: Policy loss: 0.539202. Value loss: 29.838594. Entropy: 0.393118.\n",
      "Iteration 9986: Policy loss: 1.053766. Value loss: 18.172001. Entropy: 0.441518.\n",
      "Iteration 9987: Policy loss: 0.606561. Value loss: 14.977484. Entropy: 0.413527.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9988: Policy loss: 0.126842. Value loss: 25.669411. Entropy: 0.410101.\n",
      "Iteration 9989: Policy loss: 0.252706. Value loss: 15.202452. Entropy: 0.405281.\n",
      "Iteration 9990: Policy loss: 0.127498. Value loss: 11.505258. Entropy: 0.401161.\n",
      "episode: 4306   score: 260.0  epsilon: 1.0    steps: 409  evaluation reward: 339.15\n",
      "episode: 4307   score: 225.0  epsilon: 1.0    steps: 609  evaluation reward: 338.8\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9991: Policy loss: -1.853394. Value loss: 23.111267. Entropy: 0.521601.\n",
      "Iteration 9992: Policy loss: -1.641286. Value loss: 12.741490. Entropy: 0.546171.\n",
      "Iteration 9993: Policy loss: -1.874631. Value loss: 10.286379. Entropy: 0.536680.\n",
      "episode: 4308   score: 365.0  epsilon: 1.0    steps: 767  evaluation reward: 339.65\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9994: Policy loss: -1.748950. Value loss: 122.174385. Entropy: 0.419479.\n",
      "Iteration 9995: Policy loss: -1.971369. Value loss: 75.988701. Entropy: 0.425379.\n",
      "Iteration 9996: Policy loss: -1.715333. Value loss: 23.510239. Entropy: 0.418914.\n",
      "episode: 4309   score: 260.0  epsilon: 1.0    steps: 151  evaluation reward: 339.25\n",
      "episode: 4310   score: 325.0  epsilon: 1.0    steps: 321  evaluation reward: 338.95\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9997: Policy loss: 0.052108. Value loss: 28.658329. Entropy: 0.565986.\n",
      "Iteration 9998: Policy loss: 0.114068. Value loss: 20.700693. Entropy: 0.552639.\n",
      "Iteration 9999: Policy loss: -0.103967. Value loss: 15.633766. Entropy: 0.533046.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 10000: Policy loss: -1.531371. Value loss: 27.789396. Entropy: 0.533237.\n",
      "Iteration 10001: Policy loss: -1.368495. Value loss: 15.628702. Entropy: 0.522739.\n",
      "Iteration 10002: Policy loss: -1.368919. Value loss: 14.029435. Entropy: 0.529146.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10003: Policy loss: 1.036757. Value loss: 38.332821. Entropy: 0.468503.\n",
      "Iteration 10004: Policy loss: 1.086940. Value loss: 18.641254. Entropy: 0.474401.\n",
      "Iteration 10005: Policy loss: 0.853841. Value loss: 14.332505. Entropy: 0.471074.\n",
      "episode: 4311   score: 260.0  epsilon: 1.0    steps: 899  evaluation reward: 339.2\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10006: Policy loss: -0.846652. Value loss: 205.024323. Entropy: 0.627745.\n",
      "Iteration 10007: Policy loss: 0.029452. Value loss: 99.422859. Entropy: 0.613466.\n",
      "Iteration 10008: Policy loss: -0.801269. Value loss: 56.086021. Entropy: 0.599511.\n",
      "episode: 4312   score: 240.0  epsilon: 1.0    steps: 627  evaluation reward: 339.5\n",
      "episode: 4313   score: 565.0  epsilon: 1.0    steps: 892  evaluation reward: 342.05\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10009: Policy loss: 1.996293. Value loss: 33.077496. Entropy: 0.656896.\n",
      "Iteration 10010: Policy loss: 2.094648. Value loss: 19.576740. Entropy: 0.663287.\n",
      "Iteration 10011: Policy loss: 2.183140. Value loss: 15.354488. Entropy: 0.673711.\n",
      "episode: 4314   score: 365.0  epsilon: 1.0    steps: 477  evaluation reward: 343.1\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10012: Policy loss: -1.840945. Value loss: 202.094727. Entropy: 0.692561.\n",
      "Iteration 10013: Policy loss: -3.299967. Value loss: 158.296219. Entropy: 0.671176.\n",
      "Iteration 10014: Policy loss: -1.712945. Value loss: 94.892937. Entropy: 0.664348.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10015: Policy loss: 0.050242. Value loss: 18.554218. Entropy: 0.528146.\n",
      "Iteration 10016: Policy loss: -0.157943. Value loss: 13.512209. Entropy: 0.498399.\n",
      "Iteration 10017: Policy loss: -0.031861. Value loss: 11.759785. Entropy: 0.514292.\n",
      "episode: 4315   score: 410.0  epsilon: 1.0    steps: 278  evaluation reward: 343.75\n",
      "episode: 4316   score: 335.0  epsilon: 1.0    steps: 752  evaluation reward: 342.35\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10018: Policy loss: 1.828691. Value loss: 41.931118. Entropy: 0.582408.\n",
      "Iteration 10019: Policy loss: 1.878723. Value loss: 21.230597. Entropy: 0.578591.\n",
      "Iteration 10020: Policy loss: 1.804168. Value loss: 15.838407. Entropy: 0.569613.\n",
      "episode: 4317   score: 665.0  epsilon: 1.0    steps: 39  evaluation reward: 344.6\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10021: Policy loss: 3.728129. Value loss: 72.905380. Entropy: 0.341200.\n",
      "Iteration 10022: Policy loss: 3.699436. Value loss: 29.301964. Entropy: 0.347299.\n",
      "Iteration 10023: Policy loss: 3.956798. Value loss: 26.147846. Entropy: 0.341931.\n",
      "episode: 4318   score: 465.0  epsilon: 1.0    steps: 231  evaluation reward: 346.25\n",
      "episode: 4319   score: 275.0  epsilon: 1.0    steps: 926  evaluation reward: 344.5\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10024: Policy loss: 1.767686. Value loss: 23.252207. Entropy: 0.427491.\n",
      "Iteration 10025: Policy loss: 1.682095. Value loss: 16.863356. Entropy: 0.427462.\n",
      "Iteration 10026: Policy loss: 1.408677. Value loss: 11.757831. Entropy: 0.447341.\n",
      "episode: 4320   score: 240.0  epsilon: 1.0    steps: 624  evaluation reward: 341.85\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10027: Policy loss: 1.501526. Value loss: 29.331755. Entropy: 0.374775.\n",
      "Iteration 10028: Policy loss: 1.152586. Value loss: 16.305044. Entropy: 0.387306.\n",
      "Iteration 10029: Policy loss: 1.819680. Value loss: 12.881566. Entropy: 0.403430.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10030: Policy loss: 0.656162. Value loss: 37.890972. Entropy: 0.624199.\n",
      "Iteration 10031: Policy loss: 0.801404. Value loss: 18.849234. Entropy: 0.664058.\n",
      "Iteration 10032: Policy loss: 0.629175. Value loss: 16.028637. Entropy: 0.654555.\n",
      "episode: 4321   score: 290.0  epsilon: 1.0    steps: 773  evaluation reward: 341.45\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10033: Policy loss: 0.770031. Value loss: 30.893332. Entropy: 0.462057.\n",
      "Iteration 10034: Policy loss: 0.571893. Value loss: 16.277512. Entropy: 0.467908.\n",
      "Iteration 10035: Policy loss: 0.734423. Value loss: 11.293139. Entropy: 0.462225.\n",
      "episode: 4322   score: 210.0  epsilon: 1.0    steps: 26  evaluation reward: 340.85\n",
      "episode: 4323   score: 360.0  epsilon: 1.0    steps: 412  evaluation reward: 340.6\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10036: Policy loss: 0.404912. Value loss: 17.151335. Entropy: 0.437955.\n",
      "Iteration 10037: Policy loss: 0.364376. Value loss: 11.945418. Entropy: 0.433232.\n",
      "Iteration 10038: Policy loss: 0.586112. Value loss: 11.052955. Entropy: 0.448391.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10039: Policy loss: -0.160039. Value loss: 30.609619. Entropy: 0.379107.\n",
      "Iteration 10040: Policy loss: -0.045416. Value loss: 15.941801. Entropy: 0.367567.\n",
      "Iteration 10041: Policy loss: -0.146791. Value loss: 11.832780. Entropy: 0.352539.\n",
      "episode: 4324   score: 260.0  epsilon: 1.0    steps: 232  evaluation reward: 340.1\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10042: Policy loss: 4.659501. Value loss: 42.686008. Entropy: 0.334681.\n",
      "Iteration 10043: Policy loss: 4.856713. Value loss: 17.966764. Entropy: 0.336907.\n",
      "Iteration 10044: Policy loss: 4.334857. Value loss: 15.498576. Entropy: 0.340716.\n",
      "episode: 4325   score: 435.0  epsilon: 1.0    steps: 352  evaluation reward: 342.05\n",
      "episode: 4326   score: 320.0  epsilon: 1.0    steps: 689  evaluation reward: 343.0\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10045: Policy loss: 1.610913. Value loss: 20.358809. Entropy: 0.347674.\n",
      "Iteration 10046: Policy loss: 1.481307. Value loss: 11.715429. Entropy: 0.332626.\n",
      "Iteration 10047: Policy loss: 1.424821. Value loss: 10.505245. Entropy: 0.344657.\n",
      "episode: 4327   score: 130.0  epsilon: 1.0    steps: 817  evaluation reward: 340.85\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10048: Policy loss: 0.617195. Value loss: 17.194780. Entropy: 0.338970.\n",
      "Iteration 10049: Policy loss: 0.754314. Value loss: 11.638368. Entropy: 0.352723.\n",
      "Iteration 10050: Policy loss: 0.816229. Value loss: 10.456048. Entropy: 0.342161.\n",
      "episode: 4328   score: 335.0  epsilon: 1.0    steps: 897  evaluation reward: 340.75\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10051: Policy loss: -0.073092. Value loss: 21.003542. Entropy: 0.440360.\n",
      "Iteration 10052: Policy loss: -0.009162. Value loss: 11.209976. Entropy: 0.417922.\n",
      "Iteration 10053: Policy loss: 0.064559. Value loss: 8.884215. Entropy: 0.429583.\n",
      "episode: 4329   score: 240.0  epsilon: 1.0    steps: 31  evaluation reward: 338.25\n",
      "episode: 4330   score: 275.0  epsilon: 1.0    steps: 430  evaluation reward: 338.25\n",
      "episode: 4331   score: 290.0  epsilon: 1.0    steps: 577  evaluation reward: 338.55\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10054: Policy loss: 1.157667. Value loss: 25.986689. Entropy: 0.396867.\n",
      "Iteration 10055: Policy loss: 1.509732. Value loss: 13.688912. Entropy: 0.364837.\n",
      "Iteration 10056: Policy loss: 1.448039. Value loss: 11.970587. Entropy: 0.390086.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10057: Policy loss: 1.005163. Value loss: 21.364737. Entropy: 0.488538.\n",
      "Iteration 10058: Policy loss: 1.059378. Value loss: 9.249120. Entropy: 0.489340.\n",
      "Iteration 10059: Policy loss: 0.896744. Value loss: 8.995093. Entropy: 0.504458.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10060: Policy loss: 0.228820. Value loss: 23.179781. Entropy: 0.470892.\n",
      "Iteration 10061: Policy loss: 0.513882. Value loss: 15.595361. Entropy: 0.464676.\n",
      "Iteration 10062: Policy loss: 0.275018. Value loss: 11.494358. Entropy: 0.464926.\n",
      "episode: 4332   score: 225.0  epsilon: 1.0    steps: 737  evaluation reward: 338.1\n",
      "episode: 4333   score: 125.0  epsilon: 1.0    steps: 873  evaluation reward: 336.95\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10063: Policy loss: 1.924645. Value loss: 15.647618. Entropy: 0.320971.\n",
      "Iteration 10064: Policy loss: 1.964330. Value loss: 10.036380. Entropy: 0.353444.\n",
      "Iteration 10065: Policy loss: 1.922945. Value loss: 8.050733. Entropy: 0.349817.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10066: Policy loss: 0.304214. Value loss: 30.056276. Entropy: 0.397401.\n",
      "Iteration 10067: Policy loss: 0.334207. Value loss: 15.825678. Entropy: 0.413659.\n",
      "Iteration 10068: Policy loss: 0.299883. Value loss: 13.234502. Entropy: 0.415726.\n",
      "episode: 4334   score: 210.0  epsilon: 1.0    steps: 607  evaluation reward: 333.05\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10069: Policy loss: 2.230693. Value loss: 18.575003. Entropy: 0.506433.\n",
      "Iteration 10070: Policy loss: 2.296780. Value loss: 12.460036. Entropy: 0.515429.\n",
      "Iteration 10071: Policy loss: 2.283648. Value loss: 8.121363. Entropy: 0.536521.\n",
      "episode: 4335   score: 405.0  epsilon: 1.0    steps: 144  evaluation reward: 330.9\n",
      "episode: 4336   score: 260.0  epsilon: 1.0    steps: 400  evaluation reward: 330.5\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10072: Policy loss: -1.122727. Value loss: 23.219477. Entropy: 0.443887.\n",
      "Iteration 10073: Policy loss: -1.092649. Value loss: 16.714357. Entropy: 0.432080.\n",
      "Iteration 10074: Policy loss: -1.125559. Value loss: 12.291288. Entropy: 0.439410.\n",
      "episode: 4337   score: 350.0  epsilon: 1.0    steps: 1024  evaluation reward: 331.4\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10075: Policy loss: -0.273537. Value loss: 25.717140. Entropy: 0.514768.\n",
      "Iteration 10076: Policy loss: 0.056639. Value loss: 11.783617. Entropy: 0.525357.\n",
      "Iteration 10077: Policy loss: -0.147114. Value loss: 10.864003. Entropy: 0.515795.\n",
      "episode: 4338   score: 345.0  epsilon: 1.0    steps: 15  evaluation reward: 330.85\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10078: Policy loss: 1.333971. Value loss: 29.477150. Entropy: 0.461636.\n",
      "Iteration 10079: Policy loss: 1.447283. Value loss: 15.818079. Entropy: 0.473649.\n",
      "Iteration 10080: Policy loss: 1.176738. Value loss: 14.014060. Entropy: 0.460849.\n",
      "episode: 4339   score: 285.0  epsilon: 1.0    steps: 852  evaluation reward: 330.85\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10081: Policy loss: -0.457286. Value loss: 34.632687. Entropy: 0.344450.\n",
      "Iteration 10082: Policy loss: -0.438749. Value loss: 17.247398. Entropy: 0.340153.\n",
      "Iteration 10083: Policy loss: -0.416885. Value loss: 13.853974. Entropy: 0.356750.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10084: Policy loss: 3.147573. Value loss: 20.502661. Entropy: 0.443923.\n",
      "Iteration 10085: Policy loss: 3.212369. Value loss: 11.582218. Entropy: 0.446968.\n",
      "Iteration 10086: Policy loss: 3.166454. Value loss: 7.130306. Entropy: 0.433324.\n",
      "episode: 4340   score: 260.0  epsilon: 1.0    steps: 629  evaluation reward: 331.35\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10087: Policy loss: -0.035013. Value loss: 279.907257. Entropy: 0.550976.\n",
      "Iteration 10088: Policy loss: 0.701026. Value loss: 119.845131. Entropy: 0.492806.\n",
      "Iteration 10089: Policy loss: 1.055994. Value loss: 73.750702. Entropy: 0.472677.\n",
      "episode: 4341   score: 300.0  epsilon: 1.0    steps: 241  evaluation reward: 331.75\n",
      "episode: 4342   score: 220.0  epsilon: 1.0    steps: 407  evaluation reward: 331.35\n",
      "episode: 4343   score: 605.0  epsilon: 1.0    steps: 674  evaluation reward: 334.9\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10090: Policy loss: 0.898845. Value loss: 24.001326. Entropy: 0.456875.\n",
      "Iteration 10091: Policy loss: 0.817061. Value loss: 14.465220. Entropy: 0.430284.\n",
      "Iteration 10092: Policy loss: 0.737619. Value loss: 10.441641. Entropy: 0.442366.\n",
      "episode: 4344   score: 125.0  epsilon: 1.0    steps: 13  evaluation reward: 333.55\n",
      "episode: 4345   score: 140.0  epsilon: 1.0    steps: 937  evaluation reward: 330.3\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10093: Policy loss: -1.925329. Value loss: 45.143688. Entropy: 0.343266.\n",
      "Iteration 10094: Policy loss: -1.612055. Value loss: 26.276455. Entropy: 0.324010.\n",
      "Iteration 10095: Policy loss: -1.695529. Value loss: 22.190128. Entropy: 0.322640.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10096: Policy loss: -1.494030. Value loss: 40.581852. Entropy: 0.505513.\n",
      "Iteration 10097: Policy loss: -1.620042. Value loss: 22.408049. Entropy: 0.503218.\n",
      "Iteration 10098: Policy loss: -1.761710. Value loss: 17.379789. Entropy: 0.488774.\n",
      "episode: 4346   score: 480.0  epsilon: 1.0    steps: 296  evaluation reward: 329.85\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10099: Policy loss: 1.784859. Value loss: 42.487549. Entropy: 0.386769.\n",
      "Iteration 10100: Policy loss: 1.605601. Value loss: 19.429155. Entropy: 0.364427.\n",
      "Iteration 10101: Policy loss: 1.649450. Value loss: 15.330555. Entropy: 0.373888.\n",
      "episode: 4347   score: 180.0  epsilon: 1.0    steps: 540  evaluation reward: 327.65\n",
      "episode: 4348   score: 205.0  epsilon: 1.0    steps: 893  evaluation reward: 326.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10102: Policy loss: 1.577945. Value loss: 23.563925. Entropy: 0.216257.\n",
      "Iteration 10103: Policy loss: 1.668675. Value loss: 13.307127. Entropy: 0.191285.\n",
      "Iteration 10104: Policy loss: 1.302428. Value loss: 8.380317. Entropy: 0.202531.\n",
      "episode: 4349   score: 210.0  epsilon: 1.0    steps: 447  evaluation reward: 321.1\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10105: Policy loss: 0.925027. Value loss: 30.254612. Entropy: 0.306203.\n",
      "Iteration 10106: Policy loss: 0.942607. Value loss: 15.394009. Entropy: 0.293808.\n",
      "Iteration 10107: Policy loss: 0.473757. Value loss: 13.489351. Entropy: 0.303215.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10108: Policy loss: 3.104066. Value loss: 22.965027. Entropy: 0.429245.\n",
      "Iteration 10109: Policy loss: 3.092083. Value loss: 12.168923. Entropy: 0.463691.\n",
      "Iteration 10110: Policy loss: 3.009072. Value loss: 8.799736. Entropy: 0.494912.\n",
      "episode: 4350   score: 265.0  epsilon: 1.0    steps: 114  evaluation reward: 321.15\n",
      "now time :  2019-02-25 21:49:02.271404\n",
      "episode: 4351   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 318.9\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10111: Policy loss: 1.228792. Value loss: 39.046593. Entropy: 0.461273.\n",
      "Iteration 10112: Policy loss: 1.392453. Value loss: 19.658991. Entropy: 0.477560.\n",
      "Iteration 10113: Policy loss: 1.258025. Value loss: 14.078628. Entropy: 0.480187.\n",
      "episode: 4352   score: 210.0  epsilon: 1.0    steps: 306  evaluation reward: 317.9\n",
      "episode: 4353   score: 315.0  epsilon: 1.0    steps: 680  evaluation reward: 318.4\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10114: Policy loss: 0.412681. Value loss: 16.803358. Entropy: 0.378736.\n",
      "Iteration 10115: Policy loss: 0.033600. Value loss: 10.046356. Entropy: 0.368923.\n",
      "Iteration 10116: Policy loss: 0.269003. Value loss: 8.442053. Entropy: 0.369486.\n",
      "episode: 4354   score: 185.0  epsilon: 1.0    steps: 445  evaluation reward: 316.5\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10117: Policy loss: -0.497932. Value loss: 31.258436. Entropy: 0.499662.\n",
      "Iteration 10118: Policy loss: -0.599250. Value loss: 20.049570. Entropy: 0.507023.\n",
      "Iteration 10119: Policy loss: -0.716412. Value loss: 15.188435. Entropy: 0.503270.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10120: Policy loss: 2.855438. Value loss: 38.074081. Entropy: 0.392305.\n",
      "Iteration 10121: Policy loss: 3.202507. Value loss: 16.197697. Entropy: 0.400112.\n",
      "Iteration 10122: Policy loss: 2.728806. Value loss: 11.529653. Entropy: 0.438839.\n",
      "episode: 4355   score: 270.0  epsilon: 1.0    steps: 804  evaluation reward: 315.25\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10123: Policy loss: -1.540126. Value loss: 26.775795. Entropy: 0.395785.\n",
      "Iteration 10124: Policy loss: -1.398719. Value loss: 12.731363. Entropy: 0.386383.\n",
      "Iteration 10125: Policy loss: -1.565404. Value loss: 9.613840. Entropy: 0.397051.\n",
      "episode: 4356   score: 355.0  epsilon: 1.0    steps: 579  evaluation reward: 315.15\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10126: Policy loss: -0.589571. Value loss: 255.741577. Entropy: 0.534009.\n",
      "Iteration 10127: Policy loss: -1.001126. Value loss: 142.971222. Entropy: 0.541716.\n",
      "Iteration 10128: Policy loss: -1.192232. Value loss: 106.353745. Entropy: 0.560246.\n",
      "episode: 4357   score: 210.0  epsilon: 1.0    steps: 697  evaluation reward: 313.25\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10129: Policy loss: 1.108914. Value loss: 22.103777. Entropy: 0.575748.\n",
      "Iteration 10130: Policy loss: 1.020312. Value loss: 12.801682. Entropy: 0.604410.\n",
      "Iteration 10131: Policy loss: 0.932270. Value loss: 11.127012. Entropy: 0.594281.\n",
      "episode: 4358   score: 410.0  epsilon: 1.0    steps: 941  evaluation reward: 311.9\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10132: Policy loss: 0.430799. Value loss: 29.028074. Entropy: 0.532992.\n",
      "Iteration 10133: Policy loss: 0.340974. Value loss: 20.457033. Entropy: 0.549550.\n",
      "Iteration 10134: Policy loss: 0.203540. Value loss: 16.027328. Entropy: 0.564882.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10135: Policy loss: 1.569410. Value loss: 27.570171. Entropy: 0.794242.\n",
      "Iteration 10136: Policy loss: 1.433006. Value loss: 16.784914. Entropy: 0.769822.\n",
      "Iteration 10137: Policy loss: 1.628476. Value loss: 11.878540. Entropy: 0.791826.\n",
      "episode: 4359   score: 515.0  epsilon: 1.0    steps: 205  evaluation reward: 314.0\n",
      "episode: 4360   score: 210.0  epsilon: 1.0    steps: 801  evaluation reward: 312.7\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10138: Policy loss: 1.779981. Value loss: 26.332174. Entropy: 0.599849.\n",
      "Iteration 10139: Policy loss: 1.828097. Value loss: 15.629802. Entropy: 0.590939.\n",
      "Iteration 10140: Policy loss: 1.831672. Value loss: 13.001168. Entropy: 0.593339.\n",
      "episode: 4361   score: 290.0  epsilon: 1.0    steps: 285  evaluation reward: 312.9\n",
      "episode: 4362   score: 340.0  epsilon: 1.0    steps: 420  evaluation reward: 313.55\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10141: Policy loss: -0.445109. Value loss: 20.210619. Entropy: 0.392021.\n",
      "Iteration 10142: Policy loss: -0.508365. Value loss: 12.736481. Entropy: 0.369438.\n",
      "Iteration 10143: Policy loss: -0.374634. Value loss: 10.438166. Entropy: 0.394009.\n",
      "episode: 4363   score: 240.0  epsilon: 1.0    steps: 93  evaluation reward: 312.45\n",
      "episode: 4364   score: 210.0  epsilon: 1.0    steps: 685  evaluation reward: 309.4\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10144: Policy loss: 1.151739. Value loss: 34.967903. Entropy: 0.522387.\n",
      "Iteration 10145: Policy loss: 0.913223. Value loss: 22.330379. Entropy: 0.493567.\n",
      "Iteration 10146: Policy loss: 1.170090. Value loss: 19.650520. Entropy: 0.510752.\n",
      "episode: 4365   score: 245.0  epsilon: 1.0    steps: 552  evaluation reward: 309.75\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10147: Policy loss: 0.066355. Value loss: 21.593500. Entropy: 0.517913.\n",
      "Iteration 10148: Policy loss: 0.235785. Value loss: 13.016988. Entropy: 0.557372.\n",
      "Iteration 10149: Policy loss: 0.103302. Value loss: 11.941970. Entropy: 0.540628.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10150: Policy loss: -1.541285. Value loss: 29.626617. Entropy: 0.583958.\n",
      "Iteration 10151: Policy loss: -1.452507. Value loss: 17.455030. Entropy: 0.586535.\n",
      "Iteration 10152: Policy loss: -1.734848. Value loss: 18.305187. Entropy: 0.604173.\n",
      "episode: 4366   score: 260.0  epsilon: 1.0    steps: 952  evaluation reward: 307.45\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10153: Policy loss: 0.748663. Value loss: 23.061432. Entropy: 0.514917.\n",
      "Iteration 10154: Policy loss: 0.461229. Value loss: 13.859900. Entropy: 0.533776.\n",
      "Iteration 10155: Policy loss: 0.701389. Value loss: 8.680410. Entropy: 0.531322.\n",
      "episode: 4367   score: 180.0  epsilon: 1.0    steps: 118  evaluation reward: 306.45\n",
      "episode: 4368   score: 210.0  epsilon: 1.0    steps: 461  evaluation reward: 305.5\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10156: Policy loss: 0.340220. Value loss: 29.322393. Entropy: 0.435851.\n",
      "Iteration 10157: Policy loss: 0.389951. Value loss: 15.644023. Entropy: 0.429922.\n",
      "Iteration 10158: Policy loss: 0.157954. Value loss: 11.398402. Entropy: 0.420528.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10159: Policy loss: -6.702285. Value loss: 183.942352. Entropy: 0.573715.\n",
      "Iteration 10160: Policy loss: -5.702260. Value loss: 53.998245. Entropy: 0.602309.\n",
      "Iteration 10161: Policy loss: -6.604609. Value loss: 47.044903. Entropy: 0.562556.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10162: Policy loss: 0.989205. Value loss: 28.097662. Entropy: 0.711478.\n",
      "Iteration 10163: Policy loss: 0.848072. Value loss: 17.242128. Entropy: 0.726189.\n",
      "Iteration 10164: Policy loss: 0.670348. Value loss: 12.252535. Entropy: 0.750380.\n",
      "episode: 4369   score: 390.0  epsilon: 1.0    steps: 280  evaluation reward: 306.85\n",
      "episode: 4370   score: 285.0  epsilon: 1.0    steps: 716  evaluation reward: 306.55\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10165: Policy loss: 0.602621. Value loss: 29.840715. Entropy: 0.570596.\n",
      "Iteration 10166: Policy loss: 0.789799. Value loss: 19.338381. Entropy: 0.571895.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10167: Policy loss: 0.536857. Value loss: 15.274498. Entropy: 0.550150.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10168: Policy loss: -0.626236. Value loss: 231.444550. Entropy: 0.413563.\n",
      "Iteration 10169: Policy loss: -1.468752. Value loss: 169.397308. Entropy: 0.370909.\n",
      "Iteration 10170: Policy loss: -1.032257. Value loss: 123.792351. Entropy: 0.377371.\n",
      "episode: 4371   score: 210.0  epsilon: 1.0    steps: 498  evaluation reward: 303.6\n",
      "episode: 4372   score: 265.0  epsilon: 1.0    steps: 545  evaluation reward: 303.65\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10171: Policy loss: -0.783303. Value loss: 248.660461. Entropy: 0.452950.\n",
      "Iteration 10172: Policy loss: -0.204883. Value loss: 103.711723. Entropy: 0.444172.\n",
      "Iteration 10173: Policy loss: -0.534150. Value loss: 55.169674. Entropy: 0.479928.\n",
      "episode: 4373   score: 490.0  epsilon: 1.0    steps: 948  evaluation reward: 306.45\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10174: Policy loss: -0.670034. Value loss: 241.051010. Entropy: 0.464290.\n",
      "Iteration 10175: Policy loss: -1.750283. Value loss: 176.308929. Entropy: 0.442073.\n",
      "Iteration 10176: Policy loss: -0.947954. Value loss: 144.594788. Entropy: 0.427280.\n",
      "episode: 4374   score: 575.0  epsilon: 1.0    steps: 156  evaluation reward: 310.1\n",
      "episode: 4375   score: 210.0  epsilon: 1.0    steps: 278  evaluation reward: 309.6\n",
      "episode: 4376   score: 395.0  epsilon: 1.0    steps: 852  evaluation reward: 312.0\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10177: Policy loss: -0.172080. Value loss: 36.550892. Entropy: 0.291370.\n",
      "Iteration 10178: Policy loss: -0.152927. Value loss: 20.190813. Entropy: 0.319096.\n",
      "Iteration 10179: Policy loss: -0.401052. Value loss: 19.089199. Entropy: 0.317102.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10180: Policy loss: 1.765597. Value loss: 32.131496. Entropy: 0.420634.\n",
      "Iteration 10181: Policy loss: 1.636752. Value loss: 19.932280. Entropy: 0.461693.\n",
      "Iteration 10182: Policy loss: 1.540556. Value loss: 17.031706. Entropy: 0.436821.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10183: Policy loss: 1.447209. Value loss: 38.475094. Entropy: 0.362348.\n",
      "Iteration 10184: Policy loss: 1.374222. Value loss: 24.052008. Entropy: 0.374781.\n",
      "Iteration 10185: Policy loss: 1.369869. Value loss: 21.663191. Entropy: 0.369193.\n",
      "episode: 4377   score: 605.0  epsilon: 1.0    steps: 119  evaluation reward: 315.95\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10186: Policy loss: 2.370788. Value loss: 41.377071. Entropy: 0.458252.\n",
      "Iteration 10187: Policy loss: 2.248160. Value loss: 16.678337. Entropy: 0.475651.\n",
      "Iteration 10188: Policy loss: 2.372796. Value loss: 12.550192. Entropy: 0.455606.\n",
      "episode: 4378   score: 210.0  epsilon: 1.0    steps: 143  evaluation reward: 315.95\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10189: Policy loss: 3.455789. Value loss: 35.785988. Entropy: 0.395880.\n",
      "Iteration 10190: Policy loss: 3.557261. Value loss: 16.964249. Entropy: 0.393699.\n",
      "Iteration 10191: Policy loss: 3.487164. Value loss: 12.055802. Entropy: 0.383169.\n",
      "episode: 4379   score: 210.0  epsilon: 1.0    steps: 323  evaluation reward: 314.0\n",
      "episode: 4380   score: 260.0  epsilon: 1.0    steps: 630  evaluation reward: 312.3\n",
      "episode: 4381   score: 420.0  epsilon: 1.0    steps: 1004  evaluation reward: 310.8\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10192: Policy loss: 3.057989. Value loss: 49.351490. Entropy: 0.288951.\n",
      "Iteration 10193: Policy loss: 3.019012. Value loss: 26.549988. Entropy: 0.274316.\n",
      "Iteration 10194: Policy loss: 2.904956. Value loss: 24.136663. Entropy: 0.285208.\n",
      "episode: 4382   score: 285.0  epsilon: 1.0    steps: 479  evaluation reward: 308.5\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10195: Policy loss: 0.385526. Value loss: 28.492476. Entropy: 0.432470.\n",
      "Iteration 10196: Policy loss: 0.524205. Value loss: 15.452890. Entropy: 0.449815.\n",
      "Iteration 10197: Policy loss: -0.013746. Value loss: 12.719620. Entropy: 0.435740.\n",
      "episode: 4383   score: 515.0  epsilon: 1.0    steps: 679  evaluation reward: 311.6\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10198: Policy loss: 0.561508. Value loss: 33.557308. Entropy: 0.423990.\n",
      "Iteration 10199: Policy loss: 0.794650. Value loss: 15.585557. Entropy: 0.436450.\n",
      "Iteration 10200: Policy loss: 0.741679. Value loss: 13.799253. Entropy: 0.429639.\n",
      "episode: 4384   score: 260.0  epsilon: 1.0    steps: 790  evaluation reward: 307.5\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10201: Policy loss: 1.339943. Value loss: 23.938993. Entropy: 0.439564.\n",
      "Iteration 10202: Policy loss: 1.226411. Value loss: 16.583982. Entropy: 0.520271.\n",
      "Iteration 10203: Policy loss: 1.248717. Value loss: 13.844506. Entropy: 0.520767.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10204: Policy loss: 1.494666. Value loss: 23.601433. Entropy: 0.252063.\n",
      "Iteration 10205: Policy loss: 1.311221. Value loss: 14.863837. Entropy: 0.234881.\n",
      "Iteration 10206: Policy loss: 1.449039. Value loss: 14.317019. Entropy: 0.246400.\n",
      "episode: 4385   score: 285.0  epsilon: 1.0    steps: 103  evaluation reward: 307.5\n",
      "episode: 4386   score: 210.0  epsilon: 1.0    steps: 146  evaluation reward: 306.7\n",
      "episode: 4387   score: 180.0  epsilon: 1.0    steps: 513  evaluation reward: 305.55\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10207: Policy loss: 1.046765. Value loss: 14.918562. Entropy: 0.362317.\n",
      "Iteration 10208: Policy loss: 1.006915. Value loss: 8.801909. Entropy: 0.381250.\n",
      "Iteration 10209: Policy loss: 1.013889. Value loss: 8.175586. Entropy: 0.378213.\n",
      "episode: 4388   score: 150.0  epsilon: 1.0    steps: 641  evaluation reward: 304.35\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10210: Policy loss: -0.620233. Value loss: 21.388451. Entropy: 0.368044.\n",
      "Iteration 10211: Policy loss: -0.636160. Value loss: 14.528685. Entropy: 0.376990.\n",
      "Iteration 10212: Policy loss: -0.540951. Value loss: 9.674907. Entropy: 0.403845.\n",
      "episode: 4389   score: 210.0  epsilon: 1.0    steps: 466  evaluation reward: 302.85\n",
      "episode: 4390   score: 295.0  epsilon: 1.0    steps: 899  evaluation reward: 300.3\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10213: Policy loss: -0.041160. Value loss: 19.985947. Entropy: 0.396500.\n",
      "Iteration 10214: Policy loss: -0.116778. Value loss: 15.242774. Entropy: 0.402846.\n",
      "Iteration 10215: Policy loss: 0.066017. Value loss: 10.576219. Entropy: 0.403173.\n",
      "episode: 4391   score: 125.0  epsilon: 1.0    steps: 161  evaluation reward: 299.0\n",
      "episode: 4392   score: 255.0  epsilon: 1.0    steps: 296  evaluation reward: 298.45\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10216: Policy loss: 0.603793. Value loss: 24.913975. Entropy: 0.295918.\n",
      "Iteration 10217: Policy loss: 0.806713. Value loss: 16.219851. Entropy: 0.303675.\n",
      "Iteration 10218: Policy loss: 0.629012. Value loss: 16.266804. Entropy: 0.309386.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10219: Policy loss: -0.442392. Value loss: 19.850590. Entropy: 0.403117.\n",
      "Iteration 10220: Policy loss: -0.151978. Value loss: 15.748622. Entropy: 0.423269.\n",
      "Iteration 10221: Policy loss: -0.090377. Value loss: 13.192968. Entropy: 0.439645.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10222: Policy loss: 0.114896. Value loss: 24.997427. Entropy: 0.436852.\n",
      "Iteration 10223: Policy loss: 0.126986. Value loss: 13.637362. Entropy: 0.429054.\n",
      "Iteration 10224: Policy loss: 0.195593. Value loss: 9.723401. Entropy: 0.454320.\n",
      "episode: 4393   score: 260.0  epsilon: 1.0    steps: 525  evaluation reward: 296.65\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10225: Policy loss: 0.185993. Value loss: 18.942963. Entropy: 0.250215.\n",
      "Iteration 10226: Policy loss: 0.218060. Value loss: 10.421309. Entropy: 0.261921.\n",
      "Iteration 10227: Policy loss: 0.137011. Value loss: 10.444343. Entropy: 0.242375.\n",
      "episode: 4394   score: 260.0  epsilon: 1.0    steps: 20  evaluation reward: 296.8\n",
      "episode: 4395   score: 180.0  epsilon: 1.0    steps: 198  evaluation reward: 295.05\n",
      "episode: 4396   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 292.75\n",
      "episode: 4397   score: 285.0  epsilon: 1.0    steps: 652  evaluation reward: 293.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4398   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 291.8\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10228: Policy loss: 0.239651. Value loss: 14.860242. Entropy: 0.224446.\n",
      "Iteration 10229: Policy loss: 0.383039. Value loss: 10.361681. Entropy: 0.252349.\n",
      "Iteration 10230: Policy loss: 0.307338. Value loss: 11.115982. Entropy: 0.240659.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10231: Policy loss: 0.915608. Value loss: 26.766533. Entropy: 0.312081.\n",
      "Iteration 10232: Policy loss: 0.670927. Value loss: 16.055954. Entropy: 0.311290.\n",
      "Iteration 10233: Policy loss: 0.961060. Value loss: 13.277185. Entropy: 0.288729.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10234: Policy loss: 0.784791. Value loss: 27.401163. Entropy: 0.416791.\n",
      "Iteration 10235: Policy loss: 0.981069. Value loss: 17.497747. Entropy: 0.427945.\n",
      "Iteration 10236: Policy loss: 0.987362. Value loss: 10.639919. Entropy: 0.425782.\n",
      "episode: 4399   score: 360.0  epsilon: 1.0    steps: 810  evaluation reward: 293.3\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10237: Policy loss: 0.796151. Value loss: 22.723194. Entropy: 0.324691.\n",
      "Iteration 10238: Policy loss: 0.766625. Value loss: 12.723471. Entropy: 0.302828.\n",
      "Iteration 10239: Policy loss: 0.866671. Value loss: 10.232043. Entropy: 0.315395.\n",
      "episode: 4400   score: 150.0  epsilon: 1.0    steps: 43  evaluation reward: 290.4\n",
      "now time :  2019-02-25 21:51:26.833095\n",
      "episode: 4401   score: 180.0  epsilon: 1.0    steps: 197  evaluation reward: 289.5\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10240: Policy loss: -0.099298. Value loss: 179.258347. Entropy: 0.396173.\n",
      "Iteration 10241: Policy loss: 0.462697. Value loss: 151.203217. Entropy: 0.362488.\n",
      "Iteration 10242: Policy loss: 0.022529. Value loss: 121.288101. Entropy: 0.334652.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10243: Policy loss: 2.584698. Value loss: 36.067173. Entropy: 0.322107.\n",
      "Iteration 10244: Policy loss: 2.722191. Value loss: 18.330126. Entropy: 0.338358.\n",
      "Iteration 10245: Policy loss: 2.781440. Value loss: 11.539680. Entropy: 0.333263.\n",
      "episode: 4402   score: 490.0  epsilon: 1.0    steps: 599  evaluation reward: 291.75\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10246: Policy loss: -0.031468. Value loss: 21.310799. Entropy: 0.356210.\n",
      "Iteration 10247: Policy loss: -0.096551. Value loss: 11.091241. Entropy: 0.355055.\n",
      "Iteration 10248: Policy loss: -0.018494. Value loss: 9.619419. Entropy: 0.355241.\n",
      "episode: 4403   score: 260.0  epsilon: 1.0    steps: 1013  evaluation reward: 291.2\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10249: Policy loss: 0.944717. Value loss: 29.409622. Entropy: 0.229807.\n",
      "Iteration 10250: Policy loss: 0.885032. Value loss: 17.173098. Entropy: 0.232250.\n",
      "Iteration 10251: Policy loss: 0.811593. Value loss: 11.831466. Entropy: 0.225816.\n",
      "episode: 4404   score: 295.0  epsilon: 1.0    steps: 433  evaluation reward: 291.55\n",
      "episode: 4405   score: 210.0  epsilon: 1.0    steps: 834  evaluation reward: 291.05\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10252: Policy loss: -0.381740. Value loss: 19.915140. Entropy: 0.332978.\n",
      "Iteration 10253: Policy loss: -0.108212. Value loss: 11.922296. Entropy: 0.318799.\n",
      "Iteration 10254: Policy loss: -0.339594. Value loss: 10.123328. Entropy: 0.331312.\n",
      "episode: 4406   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 290.55\n",
      "episode: 4407   score: 280.0  epsilon: 1.0    steps: 347  evaluation reward: 291.1\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10255: Policy loss: -1.137476. Value loss: 21.583038. Entropy: 0.299193.\n",
      "Iteration 10256: Policy loss: -1.162501. Value loss: 12.649637. Entropy: 0.289031.\n",
      "Iteration 10257: Policy loss: -0.919489. Value loss: 11.428622. Entropy: 0.314697.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10258: Policy loss: -0.348414. Value loss: 20.259594. Entropy: 0.244441.\n",
      "Iteration 10259: Policy loss: -0.470537. Value loss: 11.608709. Entropy: 0.284424.\n",
      "Iteration 10260: Policy loss: -0.404489. Value loss: 10.600418. Entropy: 0.272257.\n",
      "episode: 4408   score: 285.0  epsilon: 1.0    steps: 683  evaluation reward: 290.3\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10261: Policy loss: 0.254121. Value loss: 224.647247. Entropy: 0.293734.\n",
      "Iteration 10262: Policy loss: 0.754888. Value loss: 85.272781. Entropy: 0.258737.\n",
      "Iteration 10263: Policy loss: 0.603718. Value loss: 66.778481. Entropy: 0.249573.\n",
      "episode: 4409   score: 525.0  epsilon: 1.0    steps: 235  evaluation reward: 292.95\n",
      "episode: 4410   score: 260.0  epsilon: 1.0    steps: 640  evaluation reward: 292.3\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10264: Policy loss: -0.775430. Value loss: 34.270317. Entropy: 0.321030.\n",
      "Iteration 10265: Policy loss: -0.961026. Value loss: 15.801441. Entropy: 0.309498.\n",
      "Iteration 10266: Policy loss: -0.975654. Value loss: 11.841597. Entropy: 0.334596.\n",
      "episode: 4411   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 291.8\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10267: Policy loss: -0.565155. Value loss: 19.967581. Entropy: 0.267276.\n",
      "Iteration 10268: Policy loss: -1.043890. Value loss: 12.409850. Entropy: 0.295945.\n",
      "Iteration 10269: Policy loss: -0.579818. Value loss: 9.709275. Entropy: 0.283222.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10270: Policy loss: 1.882239. Value loss: 22.756937. Entropy: 0.248319.\n",
      "Iteration 10271: Policy loss: 1.635419. Value loss: 13.371650. Entropy: 0.263579.\n",
      "Iteration 10272: Policy loss: 1.866807. Value loss: 10.437145. Entropy: 0.279764.\n",
      "episode: 4412   score: 275.0  epsilon: 1.0    steps: 510  evaluation reward: 292.15\n",
      "episode: 4413   score: 260.0  epsilon: 1.0    steps: 844  evaluation reward: 289.1\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10273: Policy loss: 0.592218. Value loss: 29.400064. Entropy: 0.221528.\n",
      "Iteration 10274: Policy loss: 0.278160. Value loss: 16.103168. Entropy: 0.208927.\n",
      "Iteration 10275: Policy loss: 0.488751. Value loss: 15.086728. Entropy: 0.203159.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10276: Policy loss: 0.526989. Value loss: 31.820139. Entropy: 0.226911.\n",
      "Iteration 10277: Policy loss: 0.428015. Value loss: 15.327707. Entropy: 0.213807.\n",
      "Iteration 10278: Policy loss: 0.476176. Value loss: 9.996368. Entropy: 0.222242.\n",
      "episode: 4414   score: 355.0  epsilon: 1.0    steps: 994  evaluation reward: 289.0\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10279: Policy loss: 0.394517. Value loss: 31.940363. Entropy: 0.206731.\n",
      "Iteration 10280: Policy loss: 0.391604. Value loss: 18.617687. Entropy: 0.225139.\n",
      "Iteration 10281: Policy loss: 0.429581. Value loss: 12.580043. Entropy: 0.214349.\n",
      "episode: 4415   score: 335.0  epsilon: 1.0    steps: 9  evaluation reward: 288.25\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10282: Policy loss: 1.094998. Value loss: 22.336374. Entropy: 0.275865.\n",
      "Iteration 10283: Policy loss: 1.377720. Value loss: 12.211120. Entropy: 0.255356.\n",
      "Iteration 10284: Policy loss: 1.266258. Value loss: 10.075913. Entropy: 0.254752.\n",
      "episode: 4416   score: 215.0  epsilon: 1.0    steps: 625  evaluation reward: 287.05\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10285: Policy loss: 3.642824. Value loss: 25.120195. Entropy: 0.302536.\n",
      "Iteration 10286: Policy loss: 3.349707. Value loss: 13.476912. Entropy: 0.312501.\n",
      "Iteration 10287: Policy loss: 3.394075. Value loss: 12.275422. Entropy: 0.332498.\n",
      "episode: 4417   score: 225.0  epsilon: 1.0    steps: 325  evaluation reward: 282.65\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10288: Policy loss: 1.076023. Value loss: 31.647936. Entropy: 0.353286.\n",
      "Iteration 10289: Policy loss: 1.062575. Value loss: 19.167236. Entropy: 0.345304.\n",
      "Iteration 10290: Policy loss: 1.225847. Value loss: 14.634761. Entropy: 0.360056.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10291: Policy loss: -1.042831. Value loss: 25.199917. Entropy: 0.235364.\n",
      "Iteration 10292: Policy loss: -1.155904. Value loss: 15.208214. Entropy: 0.249612.\n",
      "Iteration 10293: Policy loss: -1.107726. Value loss: 10.284474. Entropy: 0.230880.\n",
      "episode: 4418   score: 280.0  epsilon: 1.0    steps: 502  evaluation reward: 280.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4419   score: 210.0  epsilon: 1.0    steps: 1020  evaluation reward: 280.15\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10294: Policy loss: 1.641513. Value loss: 27.574221. Entropy: 0.331004.\n",
      "Iteration 10295: Policy loss: 1.102782. Value loss: 20.674601. Entropy: 0.353672.\n",
      "Iteration 10296: Policy loss: 1.494433. Value loss: 17.641613. Entropy: 0.357388.\n",
      "episode: 4420   score: 210.0  epsilon: 1.0    steps: 5  evaluation reward: 279.85\n",
      "episode: 4421   score: 365.0  epsilon: 1.0    steps: 248  evaluation reward: 280.6\n",
      "episode: 4422   score: 420.0  epsilon: 1.0    steps: 682  evaluation reward: 282.7\n",
      "episode: 4423   score: 260.0  epsilon: 1.0    steps: 807  evaluation reward: 281.7\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10297: Policy loss: 1.144674. Value loss: 22.251036. Entropy: 0.360990.\n",
      "Iteration 10298: Policy loss: 0.830730. Value loss: 14.726766. Entropy: 0.361537.\n",
      "Iteration 10299: Policy loss: 1.038965. Value loss: 12.624161. Entropy: 0.360302.\n",
      "episode: 4424   score: 210.0  epsilon: 1.0    steps: 295  evaluation reward: 281.2\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10300: Policy loss: -2.255244. Value loss: 23.141525. Entropy: 0.319695.\n",
      "Iteration 10301: Policy loss: -2.320665. Value loss: 14.320889. Entropy: 0.324920.\n",
      "Iteration 10302: Policy loss: -2.122382. Value loss: 14.734557. Entropy: 0.308261.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10303: Policy loss: 0.524123. Value loss: 19.626431. Entropy: 0.429193.\n",
      "Iteration 10304: Policy loss: 0.645340. Value loss: 12.267371. Entropy: 0.433491.\n",
      "Iteration 10305: Policy loss: 0.596890. Value loss: 11.886138. Entropy: 0.462834.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10306: Policy loss: 0.464961. Value loss: 12.477627. Entropy: 0.378458.\n",
      "Iteration 10307: Policy loss: 0.509088. Value loss: 9.293971. Entropy: 0.363843.\n",
      "Iteration 10308: Policy loss: 0.527179. Value loss: 5.629065. Entropy: 0.374229.\n",
      "episode: 4425   score: 210.0  epsilon: 1.0    steps: 3  evaluation reward: 278.95\n",
      "episode: 4426   score: 210.0  epsilon: 1.0    steps: 655  evaluation reward: 277.85\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10309: Policy loss: -1.483245. Value loss: 19.456079. Entropy: 0.505898.\n",
      "Iteration 10310: Policy loss: -1.420880. Value loss: 8.784888. Entropy: 0.510502.\n",
      "Iteration 10311: Policy loss: -1.469149. Value loss: 7.347609. Entropy: 0.484550.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10312: Policy loss: -1.661830. Value loss: 28.260296. Entropy: 0.368426.\n",
      "Iteration 10313: Policy loss: -1.198908. Value loss: 15.777246. Entropy: 0.364318.\n",
      "Iteration 10314: Policy loss: -1.347396. Value loss: 11.287318. Entropy: 0.343806.\n",
      "episode: 4427   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 278.65\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10315: Policy loss: 1.557327. Value loss: 25.009989. Entropy: 0.318452.\n",
      "Iteration 10316: Policy loss: 1.301778. Value loss: 12.553907. Entropy: 0.337947.\n",
      "Iteration 10317: Policy loss: 1.675709. Value loss: 10.821727. Entropy: 0.331507.\n",
      "episode: 4428   score: 360.0  epsilon: 1.0    steps: 482  evaluation reward: 278.9\n",
      "episode: 4429   score: 370.0  epsilon: 1.0    steps: 619  evaluation reward: 280.2\n",
      "episode: 4430   score: 270.0  epsilon: 1.0    steps: 961  evaluation reward: 280.15\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10318: Policy loss: 2.122992. Value loss: 18.981579. Entropy: 0.196678.\n",
      "Iteration 10319: Policy loss: 2.252821. Value loss: 11.280581. Entropy: 0.229824.\n",
      "Iteration 10320: Policy loss: 2.178577. Value loss: 9.808382. Entropy: 0.228964.\n",
      "episode: 4431   score: 240.0  epsilon: 1.0    steps: 374  evaluation reward: 279.65\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10321: Policy loss: -0.399216. Value loss: 29.661766. Entropy: 0.537172.\n",
      "Iteration 10322: Policy loss: -0.388274. Value loss: 14.120876. Entropy: 0.521207.\n",
      "Iteration 10323: Policy loss: -0.461406. Value loss: 9.902025. Entropy: 0.531809.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10324: Policy loss: 0.858356. Value loss: 27.252125. Entropy: 0.645631.\n",
      "Iteration 10325: Policy loss: 0.909900. Value loss: 20.115618. Entropy: 0.646879.\n",
      "Iteration 10326: Policy loss: 0.979087. Value loss: 14.493824. Entropy: 0.664579.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10327: Policy loss: -0.010749. Value loss: 18.667843. Entropy: 0.380152.\n",
      "Iteration 10328: Policy loss: 0.083092. Value loss: 7.563095. Entropy: 0.386378.\n",
      "Iteration 10329: Policy loss: -0.244505. Value loss: 4.480097. Entropy: 0.377600.\n",
      "episode: 4432   score: 285.0  epsilon: 1.0    steps: 96  evaluation reward: 280.25\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10330: Policy loss: 1.432843. Value loss: 10.931214. Entropy: 0.380378.\n",
      "Iteration 10331: Policy loss: 1.332952. Value loss: 5.975658. Entropy: 0.399878.\n",
      "Iteration 10332: Policy loss: 1.441921. Value loss: 3.719261. Entropy: 0.416797.\n",
      "episode: 4433   score: 210.0  epsilon: 1.0    steps: 204  evaluation reward: 281.1\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10333: Policy loss: -2.057669. Value loss: 285.541809. Entropy: 0.641236.\n",
      "Iteration 10334: Policy loss: -1.419936. Value loss: 138.822540. Entropy: 0.627071.\n",
      "Iteration 10335: Policy loss: -1.524185. Value loss: 80.405540. Entropy: 0.599321.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10336: Policy loss: 1.479490. Value loss: 14.108053. Entropy: 0.444676.\n",
      "Iteration 10337: Policy loss: 1.555299. Value loss: 10.595143. Entropy: 0.454164.\n",
      "Iteration 10338: Policy loss: 1.483710. Value loss: 7.247025. Entropy: 0.464729.\n",
      "episode: 4434   score: 475.0  epsilon: 1.0    steps: 612  evaluation reward: 283.75\n",
      "episode: 4435   score: 355.0  epsilon: 1.0    steps: 724  evaluation reward: 283.25\n",
      "episode: 4436   score: 555.0  epsilon: 1.0    steps: 816  evaluation reward: 286.2\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10339: Policy loss: 3.408518. Value loss: 22.915972. Entropy: 0.345663.\n",
      "Iteration 10340: Policy loss: 3.454404. Value loss: 18.304918. Entropy: 0.349682.\n",
      "Iteration 10341: Policy loss: 3.401360. Value loss: 12.330202. Entropy: 0.374104.\n",
      "episode: 4437   score: 295.0  epsilon: 1.0    steps: 1011  evaluation reward: 285.65\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10342: Policy loss: 0.125853. Value loss: 22.669680. Entropy: 0.364412.\n",
      "Iteration 10343: Policy loss: 0.141389. Value loss: 14.718004. Entropy: 0.357241.\n",
      "Iteration 10344: Policy loss: 0.181758. Value loss: 11.368137. Entropy: 0.356130.\n",
      "episode: 4438   score: 210.0  epsilon: 1.0    steps: 205  evaluation reward: 284.3\n",
      "episode: 4439   score: 265.0  epsilon: 1.0    steps: 335  evaluation reward: 284.1\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10345: Policy loss: 0.426407. Value loss: 26.429632. Entropy: 0.534095.\n",
      "Iteration 10346: Policy loss: 0.382251. Value loss: 16.311632. Entropy: 0.512786.\n",
      "Iteration 10347: Policy loss: 0.144420. Value loss: 12.552363. Entropy: 0.504731.\n",
      "episode: 4440   score: 300.0  epsilon: 1.0    steps: 483  evaluation reward: 284.5\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10348: Policy loss: 0.541054. Value loss: 27.513643. Entropy: 0.436311.\n",
      "Iteration 10349: Policy loss: 0.179469. Value loss: 17.226070. Entropy: 0.454117.\n",
      "Iteration 10350: Policy loss: 0.246120. Value loss: 12.549583. Entropy: 0.454870.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10351: Policy loss: 1.827736. Value loss: 17.084154. Entropy: 0.471375.\n",
      "Iteration 10352: Policy loss: 1.624402. Value loss: 9.791298. Entropy: 0.472220.\n",
      "Iteration 10353: Policy loss: 1.731040. Value loss: 7.841630. Entropy: 0.469792.\n",
      "episode: 4441   score: 270.0  epsilon: 1.0    steps: 45  evaluation reward: 284.2\n",
      "episode: 4442   score: 210.0  epsilon: 1.0    steps: 861  evaluation reward: 284.1\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10354: Policy loss: 0.468604. Value loss: 16.783007. Entropy: 0.445089.\n",
      "Iteration 10355: Policy loss: 0.176768. Value loss: 14.040948. Entropy: 0.438332.\n",
      "Iteration 10356: Policy loss: 0.585993. Value loss: 9.824180. Entropy: 0.434625.\n",
      "episode: 4443   score: 180.0  epsilon: 1.0    steps: 218  evaluation reward: 279.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10357: Policy loss: 1.046426. Value loss: 11.661334. Entropy: 0.370559.\n",
      "Iteration 10358: Policy loss: 1.115487. Value loss: 8.881270. Entropy: 0.372548.\n",
      "Iteration 10359: Policy loss: 0.962223. Value loss: 6.794087. Entropy: 0.371183.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10360: Policy loss: 0.127395. Value loss: 15.297722. Entropy: 0.276485.\n",
      "Iteration 10361: Policy loss: 0.301486. Value loss: 10.064019. Entropy: 0.266583.\n",
      "Iteration 10362: Policy loss: 0.293288. Value loss: 6.851783. Entropy: 0.274222.\n",
      "episode: 4444   score: 210.0  epsilon: 1.0    steps: 366  evaluation reward: 280.7\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10363: Policy loss: 1.153941. Value loss: 15.819763. Entropy: 0.282669.\n",
      "Iteration 10364: Policy loss: 1.327336. Value loss: 9.583534. Entropy: 0.269776.\n",
      "Iteration 10365: Policy loss: 1.313145. Value loss: 7.875005. Entropy: 0.287415.\n",
      "episode: 4445   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 281.4\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10366: Policy loss: 0.406643. Value loss: 26.118002. Entropy: 0.588947.\n",
      "Iteration 10367: Policy loss: 0.751510. Value loss: 10.090789. Entropy: 0.581316.\n",
      "Iteration 10368: Policy loss: 0.513530. Value loss: 9.224820. Entropy: 0.584091.\n",
      "episode: 4446   score: 180.0  epsilon: 1.0    steps: 194  evaluation reward: 278.4\n",
      "episode: 4447   score: 305.0  epsilon: 1.0    steps: 747  evaluation reward: 279.65\n",
      "episode: 4448   score: 340.0  epsilon: 1.0    steps: 1006  evaluation reward: 281.0\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10369: Policy loss: 0.955585. Value loss: 25.793293. Entropy: 0.627784.\n",
      "Iteration 10370: Policy loss: 1.045137. Value loss: 14.698639. Entropy: 0.611216.\n",
      "Iteration 10371: Policy loss: 1.054774. Value loss: 10.161169. Entropy: 0.617943.\n",
      "episode: 4449   score: 315.0  epsilon: 1.0    steps: 535  evaluation reward: 282.05\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10372: Policy loss: 0.580419. Value loss: 12.589048. Entropy: 0.464063.\n",
      "Iteration 10373: Policy loss: 0.382994. Value loss: 9.372813. Entropy: 0.455249.\n",
      "Iteration 10374: Policy loss: 0.295801. Value loss: 7.806194. Entropy: 0.438374.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10375: Policy loss: 0.251679. Value loss: 18.614136. Entropy: 0.286706.\n",
      "Iteration 10376: Policy loss: 0.174715. Value loss: 12.689703. Entropy: 0.300808.\n",
      "Iteration 10377: Policy loss: 0.063611. Value loss: 9.545293. Entropy: 0.301506.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10378: Policy loss: -0.607869. Value loss: 19.651440. Entropy: 0.285994.\n",
      "Iteration 10379: Policy loss: -0.665687. Value loss: 11.007177. Entropy: 0.288469.\n",
      "Iteration 10380: Policy loss: -0.635853. Value loss: 9.468150. Entropy: 0.298069.\n",
      "episode: 4450   score: 270.0  epsilon: 1.0    steps: 71  evaluation reward: 282.1\n",
      "now time :  2019-02-25 21:54:04.734953\n",
      "episode: 4451   score: 155.0  epsilon: 1.0    steps: 309  evaluation reward: 281.55\n",
      "episode: 4452   score: 210.0  epsilon: 1.0    steps: 473  evaluation reward: 281.55\n",
      "episode: 4453   score: 330.0  epsilon: 1.0    steps: 799  evaluation reward: 281.7\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10381: Policy loss: -0.264428. Value loss: 19.392859. Entropy: 0.383535.\n",
      "Iteration 10382: Policy loss: -0.179681. Value loss: 12.589241. Entropy: 0.380099.\n",
      "Iteration 10383: Policy loss: -0.170704. Value loss: 10.111450. Entropy: 0.395725.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10384: Policy loss: -0.133086. Value loss: 18.605242. Entropy: 0.467584.\n",
      "Iteration 10385: Policy loss: -0.485538. Value loss: 13.670328. Entropy: 0.448438.\n",
      "Iteration 10386: Policy loss: 0.091180. Value loss: 9.967988. Entropy: 0.469466.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10387: Policy loss: 0.172757. Value loss: 9.595269. Entropy: 0.341477.\n",
      "Iteration 10388: Policy loss: 0.213459. Value loss: 6.940114. Entropy: 0.335168.\n",
      "Iteration 10389: Policy loss: 0.062416. Value loss: 4.706074. Entropy: 0.339314.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10390: Policy loss: 0.021419. Value loss: 9.917759. Entropy: 0.195765.\n",
      "Iteration 10391: Policy loss: 0.120796. Value loss: 5.686381. Entropy: 0.185762.\n",
      "Iteration 10392: Policy loss: 0.052541. Value loss: 4.676386. Entropy: 0.200217.\n",
      "episode: 4454   score: 210.0  epsilon: 1.0    steps: 662  evaluation reward: 281.95\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10393: Policy loss: 1.541669. Value loss: 17.341223. Entropy: 0.337887.\n",
      "Iteration 10394: Policy loss: 1.401437. Value loss: 8.709335. Entropy: 0.309700.\n",
      "Iteration 10395: Policy loss: 1.410652. Value loss: 6.925568. Entropy: 0.323924.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10396: Policy loss: -0.571077. Value loss: 11.673250. Entropy: 0.351236.\n",
      "Iteration 10397: Policy loss: -0.529743. Value loss: 5.438632. Entropy: 0.353480.\n",
      "Iteration 10398: Policy loss: -0.691278. Value loss: 4.198726. Entropy: 0.353521.\n",
      "episode: 4455   score: 210.0  epsilon: 1.0    steps: 116  evaluation reward: 281.35\n",
      "episode: 4456   score: 335.0  epsilon: 1.0    steps: 209  evaluation reward: 281.15\n",
      "episode: 4457   score: 215.0  epsilon: 1.0    steps: 341  evaluation reward: 281.2\n",
      "episode: 4458   score: 210.0  epsilon: 1.0    steps: 388  evaluation reward: 279.2\n",
      "episode: 4459   score: 295.0  epsilon: 1.0    steps: 612  evaluation reward: 277.0\n",
      "episode: 4460   score: 275.0  epsilon: 1.0    steps: 981  evaluation reward: 277.65\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10399: Policy loss: 2.188811. Value loss: 22.351713. Entropy: 0.549179.\n",
      "Iteration 10400: Policy loss: 2.463001. Value loss: 14.357639. Entropy: 0.560270.\n",
      "Iteration 10401: Policy loss: 2.322382. Value loss: 12.854525. Entropy: 0.548804.\n",
      "episode: 4461   score: 260.0  epsilon: 1.0    steps: 884  evaluation reward: 277.35\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10402: Policy loss: -0.653614. Value loss: 27.105112. Entropy: 0.481451.\n",
      "Iteration 10403: Policy loss: -1.155397. Value loss: 19.695866. Entropy: 0.501080.\n",
      "Iteration 10404: Policy loss: -0.624198. Value loss: 18.284746. Entropy: 0.462787.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10405: Policy loss: -0.250401. Value loss: 23.836449. Entropy: 0.691678.\n",
      "Iteration 10406: Policy loss: 0.173406. Value loss: 14.603249. Entropy: 0.667189.\n",
      "Iteration 10407: Policy loss: 0.150694. Value loss: 13.874498. Entropy: 0.690929.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10408: Policy loss: -0.306040. Value loss: 14.998356. Entropy: 0.414737.\n",
      "Iteration 10409: Policy loss: -0.229190. Value loss: 10.054380. Entropy: 0.428754.\n",
      "Iteration 10410: Policy loss: -0.317003. Value loss: 7.685735. Entropy: 0.400998.\n",
      "episode: 4462   score: 240.0  epsilon: 1.0    steps: 696  evaluation reward: 276.35\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10411: Policy loss: -0.697764. Value loss: 9.209027. Entropy: 0.379400.\n",
      "Iteration 10412: Policy loss: -0.727933. Value loss: 6.381154. Entropy: 0.361411.\n",
      "Iteration 10413: Policy loss: -0.710353. Value loss: 5.686224. Entropy: 0.361506.\n",
      "episode: 4463   score: 210.0  epsilon: 1.0    steps: 962  evaluation reward: 276.05\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10414: Policy loss: -2.017933. Value loss: 17.754128. Entropy: 0.401931.\n",
      "Iteration 10415: Policy loss: -2.157830. Value loss: 9.249587. Entropy: 0.387745.\n",
      "Iteration 10416: Policy loss: -1.878401. Value loss: 5.395805. Entropy: 0.383836.\n",
      "episode: 4464   score: 215.0  epsilon: 1.0    steps: 407  evaluation reward: 276.1\n",
      "episode: 4465   score: 285.0  epsilon: 1.0    steps: 640  evaluation reward: 276.5\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10417: Policy loss: -2.625927. Value loss: 25.896889. Entropy: 0.471797.\n",
      "Iteration 10418: Policy loss: -2.776254. Value loss: 15.806316. Entropy: 0.465457.\n",
      "Iteration 10419: Policy loss: -2.816560. Value loss: 14.270146. Entropy: 0.440299.\n",
      "episode: 4466   score: 330.0  epsilon: 1.0    steps: 218  evaluation reward: 277.2\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10420: Policy loss: 1.274211. Value loss: 33.154724. Entropy: 0.409550.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10421: Policy loss: 1.172426. Value loss: 19.228342. Entropy: 0.432983.\n",
      "Iteration 10422: Policy loss: 0.973619. Value loss: 14.275011. Entropy: 0.421452.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10423: Policy loss: 0.043808. Value loss: 11.790151. Entropy: 0.332136.\n",
      "Iteration 10424: Policy loss: 0.019533. Value loss: 7.208956. Entropy: 0.333170.\n",
      "Iteration 10425: Policy loss: 0.028418. Value loss: 4.424375. Entropy: 0.339922.\n",
      "episode: 4467   score: 390.0  epsilon: 1.0    steps: 262  evaluation reward: 279.3\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10426: Policy loss: 0.546629. Value loss: 24.965372. Entropy: 0.449812.\n",
      "Iteration 10427: Policy loss: 0.374990. Value loss: 12.207142. Entropy: 0.468752.\n",
      "Iteration 10428: Policy loss: 0.774553. Value loss: 9.839233. Entropy: 0.452136.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10429: Policy loss: 0.310986. Value loss: 21.252855. Entropy: 0.612163.\n",
      "Iteration 10430: Policy loss: 0.524264. Value loss: 11.082303. Entropy: 0.603348.\n",
      "Iteration 10431: Policy loss: 0.023578. Value loss: 7.535920. Entropy: 0.617432.\n",
      "episode: 4468   score: 275.0  epsilon: 1.0    steps: 779  evaluation reward: 279.95\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10432: Policy loss: -1.486729. Value loss: 31.525221. Entropy: 0.547424.\n",
      "Iteration 10433: Policy loss: -1.525499. Value loss: 15.331178. Entropy: 0.570182.\n",
      "Iteration 10434: Policy loss: -1.445568. Value loss: 11.114408. Entropy: 0.549678.\n",
      "episode: 4469   score: 525.0  epsilon: 1.0    steps: 45  evaluation reward: 281.3\n",
      "episode: 4470   score: 255.0  epsilon: 1.0    steps: 412  evaluation reward: 281.0\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10435: Policy loss: 1.384678. Value loss: 19.372936. Entropy: 0.498396.\n",
      "Iteration 10436: Policy loss: 1.335879. Value loss: 10.139414. Entropy: 0.525716.\n",
      "Iteration 10437: Policy loss: 1.289955. Value loss: 10.125387. Entropy: 0.509727.\n",
      "episode: 4471   score: 420.0  epsilon: 1.0    steps: 704  evaluation reward: 283.1\n",
      "episode: 4472   score: 285.0  epsilon: 1.0    steps: 919  evaluation reward: 283.3\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10438: Policy loss: 0.938704. Value loss: 16.534391. Entropy: 0.524125.\n",
      "Iteration 10439: Policy loss: 0.913088. Value loss: 9.914591. Entropy: 0.524412.\n",
      "Iteration 10440: Policy loss: 0.922938. Value loss: 8.610352. Entropy: 0.533674.\n",
      "episode: 4473   score: 240.0  epsilon: 1.0    steps: 157  evaluation reward: 280.8\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10441: Policy loss: 1.636471. Value loss: 15.871278. Entropy: 0.456656.\n",
      "Iteration 10442: Policy loss: 1.594194. Value loss: 9.636369. Entropy: 0.467484.\n",
      "Iteration 10443: Policy loss: 1.718307. Value loss: 7.577136. Entropy: 0.475098.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10444: Policy loss: 0.715275. Value loss: 11.351909. Entropy: 0.394016.\n",
      "Iteration 10445: Policy loss: 0.753278. Value loss: 8.140299. Entropy: 0.372441.\n",
      "Iteration 10446: Policy loss: 0.861109. Value loss: 7.926733. Entropy: 0.398363.\n",
      "episode: 4474   score: 265.0  epsilon: 1.0    steps: 350  evaluation reward: 277.7\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10447: Policy loss: 0.165389. Value loss: 17.212395. Entropy: 0.492953.\n",
      "Iteration 10448: Policy loss: -0.014037. Value loss: 10.284472. Entropy: 0.458183.\n",
      "Iteration 10449: Policy loss: 0.072370. Value loss: 8.004185. Entropy: 0.503447.\n",
      "episode: 4475   score: 210.0  epsilon: 1.0    steps: 465  evaluation reward: 277.7\n",
      "episode: 4476   score: 240.0  epsilon: 1.0    steps: 771  evaluation reward: 276.15\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10450: Policy loss: 0.610331. Value loss: 18.156317. Entropy: 0.521237.\n",
      "Iteration 10451: Policy loss: 0.630799. Value loss: 11.899888. Entropy: 0.547794.\n",
      "Iteration 10452: Policy loss: 0.505626. Value loss: 8.327952. Entropy: 0.538532.\n",
      "episode: 4477   score: 265.0  epsilon: 1.0    steps: 67  evaluation reward: 272.75\n",
      "episode: 4478   score: 160.0  epsilon: 1.0    steps: 982  evaluation reward: 272.25\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10453: Policy loss: 0.327783. Value loss: 20.499949. Entropy: 0.499079.\n",
      "Iteration 10454: Policy loss: 0.420120. Value loss: 11.676484. Entropy: 0.501309.\n",
      "Iteration 10455: Policy loss: 0.257349. Value loss: 10.385361. Entropy: 0.503907.\n",
      "episode: 4479   score: 335.0  epsilon: 1.0    steps: 617  evaluation reward: 273.5\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10456: Policy loss: 1.065422. Value loss: 19.043892. Entropy: 0.421020.\n",
      "Iteration 10457: Policy loss: 1.054896. Value loss: 11.981683. Entropy: 0.420132.\n",
      "Iteration 10458: Policy loss: 0.861967. Value loss: 8.576492. Entropy: 0.410740.\n",
      "episode: 4480   score: 285.0  epsilon: 1.0    steps: 685  evaluation reward: 273.75\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10459: Policy loss: 0.379434. Value loss: 23.872675. Entropy: 0.633789.\n",
      "Iteration 10460: Policy loss: 0.432026. Value loss: 12.498275. Entropy: 0.624672.\n",
      "Iteration 10461: Policy loss: 0.561610. Value loss: 9.971718. Entropy: 0.639161.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10462: Policy loss: 0.184179. Value loss: 12.668987. Entropy: 0.550526.\n",
      "Iteration 10463: Policy loss: 0.185327. Value loss: 6.421880. Entropy: 0.536287.\n",
      "Iteration 10464: Policy loss: 0.082912. Value loss: 5.825825. Entropy: 0.587463.\n",
      "episode: 4481   score: 285.0  epsilon: 1.0    steps: 138  evaluation reward: 272.4\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10465: Policy loss: -1.345951. Value loss: 15.767043. Entropy: 0.365888.\n",
      "Iteration 10466: Policy loss: -1.344752. Value loss: 10.592217. Entropy: 0.351667.\n",
      "Iteration 10467: Policy loss: -1.262741. Value loss: 8.479595. Entropy: 0.350639.\n",
      "episode: 4482   score: 270.0  epsilon: 1.0    steps: 818  evaluation reward: 272.25\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10468: Policy loss: 0.229981. Value loss: 13.066324. Entropy: 0.267933.\n",
      "Iteration 10469: Policy loss: 0.236689. Value loss: 7.003115. Entropy: 0.265595.\n",
      "Iteration 10470: Policy loss: 0.258040. Value loss: 6.705098. Entropy: 0.269249.\n",
      "episode: 4483   score: 245.0  epsilon: 1.0    steps: 70  evaluation reward: 269.55\n",
      "episode: 4484   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 269.05\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10471: Policy loss: 1.729952. Value loss: 15.048794. Entropy: 0.418223.\n",
      "Iteration 10472: Policy loss: 1.810216. Value loss: 9.063495. Entropy: 0.420119.\n",
      "Iteration 10473: Policy loss: 1.636952. Value loss: 7.303415. Entropy: 0.402135.\n",
      "episode: 4485   score: 210.0  epsilon: 1.0    steps: 997  evaluation reward: 268.3\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10474: Policy loss: 0.471290. Value loss: 18.911222. Entropy: 0.566329.\n",
      "Iteration 10475: Policy loss: 0.243216. Value loss: 8.716852. Entropy: 0.548285.\n",
      "Iteration 10476: Policy loss: 0.467187. Value loss: 8.771700. Entropy: 0.552489.\n",
      "episode: 4486   score: 350.0  epsilon: 1.0    steps: 261  evaluation reward: 269.7\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10477: Policy loss: 1.398042. Value loss: 21.051184. Entropy: 0.602173.\n",
      "Iteration 10478: Policy loss: 1.125025. Value loss: 11.558205. Entropy: 0.581011.\n",
      "Iteration 10479: Policy loss: 1.094858. Value loss: 8.401258. Entropy: 0.601093.\n",
      "episode: 4487   score: 260.0  epsilon: 1.0    steps: 514  evaluation reward: 270.5\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10480: Policy loss: 0.269065. Value loss: 17.877251. Entropy: 0.549867.\n",
      "Iteration 10481: Policy loss: 0.240843. Value loss: 8.891450. Entropy: 0.544555.\n",
      "Iteration 10482: Policy loss: 0.131661. Value loss: 8.328962. Entropy: 0.542714.\n",
      "episode: 4488   score: 235.0  epsilon: 1.0    steps: 714  evaluation reward: 271.35\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10483: Policy loss: 0.694723. Value loss: 14.207232. Entropy: 0.470521.\n",
      "Iteration 10484: Policy loss: 0.701890. Value loss: 6.652986. Entropy: 0.480763.\n",
      "Iteration 10485: Policy loss: 0.598996. Value loss: 5.708829. Entropy: 0.470302.\n",
      "episode: 4489   score: 215.0  epsilon: 1.0    steps: 775  evaluation reward: 271.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10486: Policy loss: -2.020324. Value loss: 23.867691. Entropy: 0.451328.\n",
      "Iteration 10487: Policy loss: -2.073666. Value loss: 12.853422. Entropy: 0.439353.\n",
      "Iteration 10488: Policy loss: -2.008363. Value loss: 10.459555. Entropy: 0.441241.\n",
      "episode: 4490   score: 295.0  epsilon: 1.0    steps: 150  evaluation reward: 271.4\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10489: Policy loss: -0.479025. Value loss: 18.585262. Entropy: 0.640286.\n",
      "Iteration 10490: Policy loss: -0.565851. Value loss: 10.876766. Entropy: 0.632192.\n",
      "Iteration 10491: Policy loss: -0.447335. Value loss: 9.586001. Entropy: 0.663236.\n",
      "episode: 4491   score: 315.0  epsilon: 1.0    steps: 31  evaluation reward: 273.3\n",
      "episode: 4492   score: 260.0  epsilon: 1.0    steps: 1024  evaluation reward: 273.35\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10492: Policy loss: -0.391492. Value loss: 19.764091. Entropy: 0.648525.\n",
      "Iteration 10493: Policy loss: -0.459560. Value loss: 8.655830. Entropy: 0.640097.\n",
      "Iteration 10494: Policy loss: -0.441549. Value loss: 7.962914. Entropy: 0.648944.\n",
      "episode: 4493   score: 330.0  epsilon: 1.0    steps: 446  evaluation reward: 274.05\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10495: Policy loss: -0.108018. Value loss: 13.681818. Entropy: 0.559695.\n",
      "Iteration 10496: Policy loss: 0.053557. Value loss: 11.141917. Entropy: 0.547374.\n",
      "Iteration 10497: Policy loss: -0.121863. Value loss: 7.805352. Entropy: 0.556190.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10498: Policy loss: -1.299459. Value loss: 19.794825. Entropy: 0.518202.\n",
      "Iteration 10499: Policy loss: -1.453540. Value loss: 9.245897. Entropy: 0.515459.\n",
      "Iteration 10500: Policy loss: -1.515240. Value loss: 7.561092. Entropy: 0.536934.\n",
      "episode: 4494   score: 285.0  epsilon: 1.0    steps: 759  evaluation reward: 274.3\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10501: Policy loss: 1.082478. Value loss: 26.207588. Entropy: 0.589509.\n",
      "Iteration 10502: Policy loss: 1.090266. Value loss: 14.795104. Entropy: 0.583875.\n",
      "Iteration 10503: Policy loss: 1.112308. Value loss: 11.465017. Entropy: 0.592268.\n",
      "episode: 4495   score: 260.0  epsilon: 1.0    steps: 779  evaluation reward: 275.1\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10504: Policy loss: -0.104833. Value loss: 20.553171. Entropy: 0.504719.\n",
      "Iteration 10505: Policy loss: -0.403687. Value loss: 13.497605. Entropy: 0.532637.\n",
      "Iteration 10506: Policy loss: -0.151576. Value loss: 9.012298. Entropy: 0.519653.\n",
      "episode: 4496   score: 250.0  epsilon: 1.0    steps: 169  evaluation reward: 275.5\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10507: Policy loss: 0.431628. Value loss: 16.720032. Entropy: 0.540606.\n",
      "Iteration 10508: Policy loss: 0.538052. Value loss: 9.611061. Entropy: 0.550985.\n",
      "Iteration 10509: Policy loss: 0.747540. Value loss: 8.601108. Entropy: 0.557735.\n",
      "episode: 4497   score: 210.0  epsilon: 1.0    steps: 78  evaluation reward: 274.75\n",
      "episode: 4498   score: 495.0  epsilon: 1.0    steps: 370  evaluation reward: 277.6\n",
      "episode: 4499   score: 380.0  epsilon: 1.0    steps: 579  evaluation reward: 277.8\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10510: Policy loss: 1.465500. Value loss: 22.317095. Entropy: 0.478393.\n",
      "Iteration 10511: Policy loss: 1.432362. Value loss: 17.188295. Entropy: 0.478579.\n",
      "Iteration 10512: Policy loss: 1.403862. Value loss: 11.987808. Entropy: 0.503205.\n",
      "episode: 4500   score: 240.0  epsilon: 1.0    steps: 904  evaluation reward: 278.7\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10513: Policy loss: 1.497549. Value loss: 19.216957. Entropy: 0.558322.\n",
      "Iteration 10514: Policy loss: 1.767389. Value loss: 15.392617. Entropy: 0.573861.\n",
      "Iteration 10515: Policy loss: 1.513328. Value loss: 12.274621. Entropy: 0.580175.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10516: Policy loss: 2.309260. Value loss: 27.711279. Entropy: 0.614165.\n",
      "Iteration 10517: Policy loss: 2.333513. Value loss: 16.306919. Entropy: 0.640772.\n",
      "Iteration 10518: Policy loss: 2.309063. Value loss: 11.856225. Entropy: 0.645484.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10519: Policy loss: -0.038342. Value loss: 19.207409. Entropy: 0.538893.\n",
      "Iteration 10520: Policy loss: -0.297784. Value loss: 10.966502. Entropy: 0.529459.\n",
      "Iteration 10521: Policy loss: -0.235639. Value loss: 9.338130. Entropy: 0.526969.\n",
      "now time :  2019-02-25 21:56:44.218046\n",
      "episode: 4501   score: 210.0  epsilon: 1.0    steps: 691  evaluation reward: 279.0\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10522: Policy loss: 0.076792. Value loss: 13.512155. Entropy: 0.302120.\n",
      "Iteration 10523: Policy loss: 0.224071. Value loss: 8.748978. Entropy: 0.296304.\n",
      "Iteration 10524: Policy loss: 0.053621. Value loss: 6.057662. Entropy: 0.318132.\n",
      "episode: 4502   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 276.2\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10525: Policy loss: 0.543168. Value loss: 26.008308. Entropy: 0.378044.\n",
      "Iteration 10526: Policy loss: 0.308987. Value loss: 14.648286. Entropy: 0.418331.\n",
      "Iteration 10527: Policy loss: 0.480974. Value loss: 10.334961. Entropy: 0.412012.\n",
      "episode: 4503   score: 210.0  epsilon: 1.0    steps: 150  evaluation reward: 275.7\n",
      "episode: 4504   score: 365.0  epsilon: 1.0    steps: 509  evaluation reward: 276.4\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10528: Policy loss: 0.347693. Value loss: 28.231697. Entropy: 0.659071.\n",
      "Iteration 10529: Policy loss: 0.297989. Value loss: 15.224082. Entropy: 0.670124.\n",
      "Iteration 10530: Policy loss: 0.260507. Value loss: 12.769617. Entropy: 0.672188.\n",
      "episode: 4505   score: 265.0  epsilon: 1.0    steps: 283  evaluation reward: 276.95\n",
      "episode: 4506   score: 375.0  epsilon: 1.0    steps: 868  evaluation reward: 278.6\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10531: Policy loss: -0.108426. Value loss: 27.678890. Entropy: 0.704611.\n",
      "Iteration 10532: Policy loss: 0.010823. Value loss: 17.192820. Entropy: 0.724712.\n",
      "Iteration 10533: Policy loss: -0.160775. Value loss: 14.375388. Entropy: 0.721295.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10534: Policy loss: -0.622352. Value loss: 21.123142. Entropy: 0.619904.\n",
      "Iteration 10535: Policy loss: -0.944310. Value loss: 11.128529. Entropy: 0.585736.\n",
      "Iteration 10536: Policy loss: -0.844910. Value loss: 9.735582. Entropy: 0.582747.\n",
      "episode: 4507   score: 365.0  epsilon: 1.0    steps: 11  evaluation reward: 279.45\n",
      "episode: 4508   score: 355.0  epsilon: 1.0    steps: 1009  evaluation reward: 280.15\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10537: Policy loss: 0.993139. Value loss: 14.271457. Entropy: 0.526236.\n",
      "Iteration 10538: Policy loss: 0.778335. Value loss: 10.181261. Entropy: 0.557711.\n",
      "Iteration 10539: Policy loss: 0.936424. Value loss: 9.170078. Entropy: 0.497478.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10540: Policy loss: 0.951052. Value loss: 14.892618. Entropy: 0.298317.\n",
      "Iteration 10541: Policy loss: 1.186218. Value loss: 5.981751. Entropy: 0.300711.\n",
      "Iteration 10542: Policy loss: 1.075870. Value loss: 3.955862. Entropy: 0.278739.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10543: Policy loss: 1.908123. Value loss: 15.692904. Entropy: 0.444597.\n",
      "Iteration 10544: Policy loss: 1.980339. Value loss: 7.074496. Entropy: 0.472651.\n",
      "Iteration 10545: Policy loss: 1.861643. Value loss: 6.244137. Entropy: 0.449599.\n",
      "episode: 4509   score: 260.0  epsilon: 1.0    steps: 158  evaluation reward: 277.5\n",
      "episode: 4510   score: 260.0  epsilon: 1.0    steps: 650  evaluation reward: 277.5\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10546: Policy loss: 1.384045. Value loss: 25.682518. Entropy: 0.545023.\n",
      "Iteration 10547: Policy loss: 1.423159. Value loss: 15.609927. Entropy: 0.564292.\n",
      "Iteration 10548: Policy loss: 1.435826. Value loss: 10.767868. Entropy: 0.572108.\n",
      "episode: 4511   score: 180.0  epsilon: 1.0    steps: 834  evaluation reward: 277.2\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10549: Policy loss: 0.558375. Value loss: 25.720192. Entropy: 0.743781.\n",
      "Iteration 10550: Policy loss: 0.676288. Value loss: 12.568878. Entropy: 0.723049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10551: Policy loss: 0.661860. Value loss: 9.572926. Entropy: 0.756042.\n",
      "episode: 4512   score: 245.0  epsilon: 1.0    steps: 408  evaluation reward: 276.9\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10552: Policy loss: 0.148360. Value loss: 22.159664. Entropy: 0.651997.\n",
      "Iteration 10553: Policy loss: 0.344181. Value loss: 12.122003. Entropy: 0.671541.\n",
      "Iteration 10554: Policy loss: 0.406385. Value loss: 10.382878. Entropy: 0.635647.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10555: Policy loss: -0.526001. Value loss: 22.486141. Entropy: 0.419609.\n",
      "Iteration 10556: Policy loss: -0.653108. Value loss: 11.127410. Entropy: 0.410378.\n",
      "Iteration 10557: Policy loss: -0.405769. Value loss: 9.405242. Entropy: 0.400306.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10558: Policy loss: 1.485553. Value loss: 15.380131. Entropy: 0.593295.\n",
      "Iteration 10559: Policy loss: 1.470163. Value loss: 9.057488. Entropy: 0.601338.\n",
      "Iteration 10560: Policy loss: 1.589406. Value loss: 7.452712. Entropy: 0.595313.\n",
      "episode: 4513   score: 480.0  epsilon: 1.0    steps: 325  evaluation reward: 279.1\n",
      "episode: 4514   score: 210.0  epsilon: 1.0    steps: 517  evaluation reward: 277.65\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10561: Policy loss: -0.490155. Value loss: 16.970285. Entropy: 0.458620.\n",
      "Iteration 10562: Policy loss: -0.493097. Value loss: 11.318301. Entropy: 0.490662.\n",
      "Iteration 10563: Policy loss: -0.592759. Value loss: 10.431769. Entropy: 0.480776.\n",
      "episode: 4515   score: 275.0  epsilon: 1.0    steps: 9  evaluation reward: 277.05\n",
      "episode: 4516   score: 240.0  epsilon: 1.0    steps: 661  evaluation reward: 277.3\n",
      "episode: 4517   score: 365.0  epsilon: 1.0    steps: 971  evaluation reward: 278.7\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10564: Policy loss: 1.066331. Value loss: 23.535294. Entropy: 0.623386.\n",
      "Iteration 10565: Policy loss: 1.154087. Value loss: 16.435890. Entropy: 0.625720.\n",
      "Iteration 10566: Policy loss: 1.046862. Value loss: 15.327012. Entropy: 0.642950.\n",
      "episode: 4518   score: 260.0  epsilon: 1.0    steps: 859  evaluation reward: 278.5\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10567: Policy loss: 0.370827. Value loss: 28.061325. Entropy: 0.560070.\n",
      "Iteration 10568: Policy loss: 0.411743. Value loss: 17.810038. Entropy: 0.588261.\n",
      "Iteration 10569: Policy loss: 0.515266. Value loss: 15.667815. Entropy: 0.562441.\n",
      "episode: 4519   score: 260.0  epsilon: 1.0    steps: 431  evaluation reward: 279.0\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10570: Policy loss: 2.070701. Value loss: 21.491222. Entropy: 0.668748.\n",
      "Iteration 10571: Policy loss: 2.044172. Value loss: 13.313078. Entropy: 0.687443.\n",
      "Iteration 10572: Policy loss: 2.157223. Value loss: 12.066705. Entropy: 0.705672.\n",
      "episode: 4520   score: 105.0  epsilon: 1.0    steps: 986  evaluation reward: 277.95\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10573: Policy loss: 0.340966. Value loss: 15.382825. Entropy: 0.422333.\n",
      "Iteration 10574: Policy loss: 0.460283. Value loss: 7.987306. Entropy: 0.420493.\n",
      "Iteration 10575: Policy loss: 0.436986. Value loss: 5.707081. Entropy: 0.420474.\n",
      "episode: 4521   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 276.4\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10576: Policy loss: -1.398702. Value loss: 19.756088. Entropy: 0.314272.\n",
      "Iteration 10577: Policy loss: -1.396029. Value loss: 11.910446. Entropy: 0.303591.\n",
      "Iteration 10578: Policy loss: -1.380805. Value loss: 9.244394. Entropy: 0.296870.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10579: Policy loss: 1.764388. Value loss: 18.883348. Entropy: 0.480202.\n",
      "Iteration 10580: Policy loss: 1.763385. Value loss: 11.309585. Entropy: 0.528352.\n",
      "Iteration 10581: Policy loss: 1.740927. Value loss: 8.760326. Entropy: 0.498379.\n",
      "episode: 4522   score: 255.0  epsilon: 1.0    steps: 631  evaluation reward: 274.75\n",
      "episode: 4523   score: 285.0  epsilon: 1.0    steps: 685  evaluation reward: 275.0\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10582: Policy loss: -0.830299. Value loss: 21.054832. Entropy: 0.675052.\n",
      "Iteration 10583: Policy loss: -0.924340. Value loss: 13.112379. Entropy: 0.701497.\n",
      "Iteration 10584: Policy loss: -0.743352. Value loss: 10.760839. Entropy: 0.676461.\n",
      "episode: 4524   score: 425.0  epsilon: 1.0    steps: 213  evaluation reward: 277.15\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10585: Policy loss: 0.307902. Value loss: 19.962532. Entropy: 0.700047.\n",
      "Iteration 10586: Policy loss: 0.452764. Value loss: 12.086531. Entropy: 0.692095.\n",
      "Iteration 10587: Policy loss: 0.227210. Value loss: 10.201930. Entropy: 0.698180.\n",
      "episode: 4525   score: 400.0  epsilon: 1.0    steps: 13  evaluation reward: 279.05\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10588: Policy loss: -2.538765. Value loss: 25.645922. Entropy: 0.607492.\n",
      "Iteration 10589: Policy loss: -2.652499. Value loss: 13.830516. Entropy: 0.580620.\n",
      "Iteration 10590: Policy loss: -2.710351. Value loss: 11.589761. Entropy: 0.584968.\n",
      "episode: 4526   score: 395.0  epsilon: 1.0    steps: 802  evaluation reward: 280.9\n",
      "episode: 4527   score: 260.0  epsilon: 1.0    steps: 935  evaluation reward: 281.4\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10591: Policy loss: 0.186658. Value loss: 12.867093. Entropy: 0.458659.\n",
      "Iteration 10592: Policy loss: 0.023010. Value loss: 11.426545. Entropy: 0.460159.\n",
      "Iteration 10593: Policy loss: 0.216632. Value loss: 6.926263. Entropy: 0.463875.\n",
      "episode: 4528   score: 265.0  epsilon: 1.0    steps: 465  evaluation reward: 280.45\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10594: Policy loss: 1.177733. Value loss: 21.828781. Entropy: 0.352378.\n",
      "Iteration 10595: Policy loss: 1.259023. Value loss: 10.573674. Entropy: 0.367992.\n",
      "Iteration 10596: Policy loss: 1.137083. Value loss: 9.954882. Entropy: 0.349119.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10597: Policy loss: -0.566119. Value loss: 14.655622. Entropy: 0.270938.\n",
      "Iteration 10598: Policy loss: -0.869604. Value loss: 7.321387. Entropy: 0.269309.\n",
      "Iteration 10599: Policy loss: -0.780155. Value loss: 6.347933. Entropy: 0.286863.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10600: Policy loss: 1.335842. Value loss: 24.252625. Entropy: 0.599705.\n",
      "Iteration 10601: Policy loss: 1.361685. Value loss: 16.890203. Entropy: 0.604191.\n",
      "Iteration 10602: Policy loss: 1.657263. Value loss: 11.638441. Entropy: 0.653306.\n",
      "episode: 4529   score: 340.0  epsilon: 1.0    steps: 728  evaluation reward: 280.15\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10603: Policy loss: 2.044942. Value loss: 26.906656. Entropy: 0.693808.\n",
      "Iteration 10604: Policy loss: 1.731437. Value loss: 15.031425. Entropy: 0.683465.\n",
      "Iteration 10605: Policy loss: 1.994748. Value loss: 13.691601. Entropy: 0.687716.\n",
      "episode: 4530   score: 250.0  epsilon: 1.0    steps: 108  evaluation reward: 279.95\n",
      "episode: 4531   score: 285.0  epsilon: 1.0    steps: 197  evaluation reward: 280.4\n",
      "episode: 4532   score: 380.0  epsilon: 1.0    steps: 317  evaluation reward: 281.35\n",
      "episode: 4533   score: 390.0  epsilon: 1.0    steps: 571  evaluation reward: 283.15\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10606: Policy loss: 2.812278. Value loss: 35.754295. Entropy: 0.687512.\n",
      "Iteration 10607: Policy loss: 2.572523. Value loss: 16.746634. Entropy: 0.684694.\n",
      "Iteration 10608: Policy loss: 2.663565. Value loss: 15.811895. Entropy: 0.694063.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10609: Policy loss: 2.384247. Value loss: 39.719105. Entropy: 0.588263.\n",
      "Iteration 10610: Policy loss: 2.579662. Value loss: 19.944717. Entropy: 0.587722.\n",
      "Iteration 10611: Policy loss: 2.682629. Value loss: 16.155916. Entropy: 0.577151.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10612: Policy loss: -1.283584. Value loss: 29.714956. Entropy: 0.467194.\n",
      "Iteration 10613: Policy loss: -1.465393. Value loss: 21.447865. Entropy: 0.490478.\n",
      "Iteration 10614: Policy loss: -1.969122. Value loss: 15.821695. Entropy: 0.491766.\n",
      "episode: 4534   score: 315.0  epsilon: 1.0    steps: 447  evaluation reward: 281.55\n",
      "episode: 4535   score: 260.0  epsilon: 1.0    steps: 777  evaluation reward: 280.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10615: Policy loss: 0.567329. Value loss: 24.415401. Entropy: 0.326415.\n",
      "Iteration 10616: Policy loss: 0.563087. Value loss: 11.740798. Entropy: 0.349077.\n",
      "Iteration 10617: Policy loss: 0.620112. Value loss: 9.295943. Entropy: 0.346703.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10618: Policy loss: -0.889736. Value loss: 20.167263. Entropy: 0.533675.\n",
      "Iteration 10619: Policy loss: -1.020937. Value loss: 15.574713. Entropy: 0.523537.\n",
      "Iteration 10620: Policy loss: -1.038688. Value loss: 11.021159. Entropy: 0.515951.\n",
      "episode: 4536   score: 240.0  epsilon: 1.0    steps: 695  evaluation reward: 277.45\n",
      "episode: 4537   score: 470.0  epsilon: 1.0    steps: 1011  evaluation reward: 279.2\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10621: Policy loss: 0.846027. Value loss: 18.234623. Entropy: 0.603577.\n",
      "Iteration 10622: Policy loss: 0.703947. Value loss: 11.744413. Entropy: 0.621772.\n",
      "Iteration 10623: Policy loss: 0.680743. Value loss: 8.947601. Entropy: 0.638922.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10624: Policy loss: 1.733492. Value loss: 15.070708. Entropy: 0.691517.\n",
      "Iteration 10625: Policy loss: 1.585574. Value loss: 9.389129. Entropy: 0.710371.\n",
      "Iteration 10626: Policy loss: 1.482127. Value loss: 6.677701. Entropy: 0.714756.\n",
      "episode: 4538   score: 270.0  epsilon: 1.0    steps: 17  evaluation reward: 279.8\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10627: Policy loss: -1.126985. Value loss: 18.752768. Entropy: 0.608280.\n",
      "Iteration 10628: Policy loss: -1.393981. Value loss: 9.308952. Entropy: 0.579728.\n",
      "Iteration 10629: Policy loss: -1.283540. Value loss: 8.208138. Entropy: 0.594801.\n",
      "episode: 4539   score: 275.0  epsilon: 1.0    steps: 174  evaluation reward: 279.9\n",
      "episode: 4540   score: 335.0  epsilon: 1.0    steps: 376  evaluation reward: 280.25\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10630: Policy loss: -0.250026. Value loss: 17.275955. Entropy: 0.575764.\n",
      "Iteration 10631: Policy loss: -0.413447. Value loss: 12.893317. Entropy: 0.558073.\n",
      "Iteration 10632: Policy loss: -0.283528. Value loss: 9.926558. Entropy: 0.571255.\n",
      "episode: 4541   score: 280.0  epsilon: 1.0    steps: 502  evaluation reward: 280.35\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10633: Policy loss: 0.512660. Value loss: 13.162626. Entropy: 0.604442.\n",
      "Iteration 10634: Policy loss: 0.376241. Value loss: 9.471440. Entropy: 0.595046.\n",
      "Iteration 10635: Policy loss: 0.403281. Value loss: 7.885093. Entropy: 0.596712.\n",
      "episode: 4542   score: 320.0  epsilon: 1.0    steps: 612  evaluation reward: 281.45\n",
      "episode: 4543   score: 265.0  epsilon: 1.0    steps: 862  evaluation reward: 282.3\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10636: Policy loss: -0.475803. Value loss: 17.200975. Entropy: 0.387749.\n",
      "Iteration 10637: Policy loss: -0.343831. Value loss: 12.647967. Entropy: 0.406962.\n",
      "Iteration 10638: Policy loss: -0.615695. Value loss: 9.406791. Entropy: 0.403783.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10639: Policy loss: -1.130641. Value loss: 29.284164. Entropy: 0.490104.\n",
      "Iteration 10640: Policy loss: -0.772674. Value loss: 16.677406. Entropy: 0.496138.\n",
      "Iteration 10641: Policy loss: -0.952420. Value loss: 13.647197. Entropy: 0.511766.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10642: Policy loss: 0.050726. Value loss: 26.577290. Entropy: 0.574465.\n",
      "Iteration 10643: Policy loss: -0.017448. Value loss: 14.746877. Entropy: 0.562278.\n",
      "Iteration 10644: Policy loss: 0.075434. Value loss: 12.654263. Entropy: 0.560874.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10645: Policy loss: -1.343287. Value loss: 19.588837. Entropy: 0.558923.\n",
      "Iteration 10646: Policy loss: -1.293776. Value loss: 10.393747. Entropy: 0.558839.\n",
      "Iteration 10647: Policy loss: -1.153698. Value loss: 10.578800. Entropy: 0.564454.\n",
      "episode: 4544   score: 365.0  epsilon: 1.0    steps: 120  evaluation reward: 283.85\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10648: Policy loss: 0.057160. Value loss: 37.228764. Entropy: 0.720066.\n",
      "Iteration 10649: Policy loss: 0.211793. Value loss: 19.227921. Entropy: 0.692060.\n",
      "Iteration 10650: Policy loss: 0.189378. Value loss: 15.488790. Entropy: 0.705769.\n",
      "episode: 4545   score: 275.0  epsilon: 1.0    steps: 157  evaluation reward: 284.5\n",
      "episode: 4546   score: 480.0  epsilon: 1.0    steps: 695  evaluation reward: 287.5\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10651: Policy loss: 0.494517. Value loss: 21.915426. Entropy: 0.460428.\n",
      "Iteration 10652: Policy loss: 0.602558. Value loss: 11.054529. Entropy: 0.464175.\n",
      "Iteration 10653: Policy loss: 0.550701. Value loss: 9.358248. Entropy: 0.427317.\n",
      "episode: 4547   score: 345.0  epsilon: 1.0    steps: 306  evaluation reward: 287.9\n",
      "episode: 4548   score: 285.0  epsilon: 1.0    steps: 891  evaluation reward: 287.35\n",
      "episode: 4549   score: 465.0  epsilon: 1.0    steps: 1012  evaluation reward: 288.85\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10654: Policy loss: 0.509484. Value loss: 31.751444. Entropy: 0.486120.\n",
      "Iteration 10655: Policy loss: 0.343828. Value loss: 21.359518. Entropy: 0.469482.\n",
      "Iteration 10656: Policy loss: 0.073538. Value loss: 21.703249. Entropy: 0.452746.\n",
      "episode: 4550   score: 250.0  epsilon: 1.0    steps: 483  evaluation reward: 288.65\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10657: Policy loss: 1.269691. Value loss: 18.493395. Entropy: 0.522421.\n",
      "Iteration 10658: Policy loss: 1.319099. Value loss: 10.969233. Entropy: 0.538800.\n",
      "Iteration 10659: Policy loss: 1.205918. Value loss: 10.437357. Entropy: 0.538014.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10660: Policy loss: 0.534831. Value loss: 24.459414. Entropy: 0.595843.\n",
      "Iteration 10661: Policy loss: 0.884932. Value loss: 12.699485. Entropy: 0.608178.\n",
      "Iteration 10662: Policy loss: 1.008089. Value loss: 7.941855. Entropy: 0.581705.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10663: Policy loss: -0.585265. Value loss: 11.238424. Entropy: 0.505611.\n",
      "Iteration 10664: Policy loss: -0.615646. Value loss: 6.086574. Entropy: 0.470596.\n",
      "Iteration 10665: Policy loss: -0.537838. Value loss: 5.650139. Entropy: 0.476790.\n",
      "now time :  2019-02-25 21:59:25.165814\n",
      "episode: 4551   score: 410.0  epsilon: 1.0    steps: 620  evaluation reward: 291.2\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10666: Policy loss: 0.339464. Value loss: 17.125393. Entropy: 0.220181.\n",
      "Iteration 10667: Policy loss: 0.473687. Value loss: 10.094906. Entropy: 0.218071.\n",
      "Iteration 10668: Policy loss: 0.353041. Value loss: 9.060206. Entropy: 0.217780.\n",
      "episode: 4552   score: 290.0  epsilon: 1.0    steps: 43  evaluation reward: 292.0\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10669: Policy loss: 1.895755. Value loss: 28.349089. Entropy: 0.593212.\n",
      "Iteration 10670: Policy loss: 1.848347. Value loss: 14.628284. Entropy: 0.576382.\n",
      "Iteration 10671: Policy loss: 1.826176. Value loss: 12.575806. Entropy: 0.564683.\n",
      "episode: 4553   score: 285.0  epsilon: 1.0    steps: 235  evaluation reward: 291.55\n",
      "episode: 4554   score: 315.0  epsilon: 1.0    steps: 648  evaluation reward: 292.6\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10672: Policy loss: 1.912733. Value loss: 29.664713. Entropy: 0.611620.\n",
      "Iteration 10673: Policy loss: 2.161494. Value loss: 21.952816. Entropy: 0.611326.\n",
      "Iteration 10674: Policy loss: 2.029951. Value loss: 15.211158. Entropy: 0.595363.\n",
      "episode: 4555   score: 230.0  epsilon: 1.0    steps: 265  evaluation reward: 292.8\n",
      "episode: 4556   score: 245.0  epsilon: 1.0    steps: 770  evaluation reward: 291.9\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10675: Policy loss: 2.705129. Value loss: 27.657595. Entropy: 0.448759.\n",
      "Iteration 10676: Policy loss: 2.836417. Value loss: 19.307249. Entropy: 0.461257.\n",
      "Iteration 10677: Policy loss: 2.677370. Value loss: 15.611672. Entropy: 0.486707.\n",
      "episode: 4557   score: 240.0  epsilon: 1.0    steps: 390  evaluation reward: 292.15\n",
      "episode: 4558   score: 285.0  epsilon: 1.0    steps: 953  evaluation reward: 292.9\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10678: Policy loss: 0.193785. Value loss: 23.786951. Entropy: 0.495360.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10679: Policy loss: 0.131804. Value loss: 13.928133. Entropy: 0.475022.\n",
      "Iteration 10680: Policy loss: 0.126269. Value loss: 13.475085. Entropy: 0.494089.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10681: Policy loss: -0.943108. Value loss: 13.093358. Entropy: 0.423797.\n",
      "Iteration 10682: Policy loss: -0.864875. Value loss: 8.472084. Entropy: 0.419332.\n",
      "Iteration 10683: Policy loss: -0.894357. Value loss: 6.991579. Entropy: 0.428587.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10684: Policy loss: -0.216108. Value loss: 8.629513. Entropy: 0.401901.\n",
      "Iteration 10685: Policy loss: -0.231309. Value loss: 5.934552. Entropy: 0.407365.\n",
      "Iteration 10686: Policy loss: -0.012966. Value loss: 4.980376. Entropy: 0.410351.\n",
      "episode: 4559   score: 290.0  epsilon: 1.0    steps: 594  evaluation reward: 292.85\n",
      "episode: 4560   score: 210.0  epsilon: 1.0    steps: 653  evaluation reward: 292.2\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10687: Policy loss: 1.126436. Value loss: 20.114527. Entropy: 0.625188.\n",
      "Iteration 10688: Policy loss: 0.982120. Value loss: 12.553058. Entropy: 0.604333.\n",
      "Iteration 10689: Policy loss: 1.100680. Value loss: 11.254264. Entropy: 0.602552.\n",
      "episode: 4561   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 291.7\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10690: Policy loss: -0.344210. Value loss: 29.905878. Entropy: 0.625904.\n",
      "Iteration 10691: Policy loss: -0.377045. Value loss: 12.848534. Entropy: 0.658159.\n",
      "Iteration 10692: Policy loss: -0.520367. Value loss: 13.189746. Entropy: 0.656992.\n",
      "episode: 4562   score: 260.0  epsilon: 1.0    steps: 344  evaluation reward: 291.9\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10693: Policy loss: 3.726402. Value loss: 39.568905. Entropy: 0.546417.\n",
      "Iteration 10694: Policy loss: 3.437165. Value loss: 21.913424. Entropy: 0.545166.\n",
      "Iteration 10695: Policy loss: 3.405912. Value loss: 19.122618. Entropy: 0.530857.\n",
      "episode: 4563   score: 315.0  epsilon: 1.0    steps: 465  evaluation reward: 292.95\n",
      "episode: 4564   score: 155.0  epsilon: 1.0    steps: 722  evaluation reward: 292.35\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10696: Policy loss: 0.738547. Value loss: 26.623899. Entropy: 0.808826.\n",
      "Iteration 10697: Policy loss: 1.163700. Value loss: 12.566124. Entropy: 0.806172.\n",
      "Iteration 10698: Policy loss: 1.049548. Value loss: 11.399989. Entropy: 0.811042.\n",
      "episode: 4565   score: 345.0  epsilon: 1.0    steps: 956  evaluation reward: 292.95\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10699: Policy loss: 1.649785. Value loss: 22.148920. Entropy: 0.663544.\n",
      "Iteration 10700: Policy loss: 1.732120. Value loss: 8.080432. Entropy: 0.687942.\n",
      "Iteration 10701: Policy loss: 1.526142. Value loss: 8.813467. Entropy: 0.671616.\n",
      "episode: 4566   score: 210.0  epsilon: 1.0    steps: 619  evaluation reward: 291.75\n",
      "episode: 4567   score: 390.0  epsilon: 1.0    steps: 877  evaluation reward: 291.75\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10702: Policy loss: -0.633099. Value loss: 19.639488. Entropy: 0.471723.\n",
      "Iteration 10703: Policy loss: -0.468139. Value loss: 10.256877. Entropy: 0.486690.\n",
      "Iteration 10704: Policy loss: -0.549613. Value loss: 9.913604. Entropy: 0.476965.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10705: Policy loss: 0.178325. Value loss: 10.137597. Entropy: 0.404709.\n",
      "Iteration 10706: Policy loss: 0.047879. Value loss: 5.499934. Entropy: 0.391917.\n",
      "Iteration 10707: Policy loss: 0.176993. Value loss: 4.881890. Entropy: 0.418739.\n",
      "episode: 4568   score: 420.0  epsilon: 1.0    steps: 192  evaluation reward: 293.2\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10708: Policy loss: -0.070703. Value loss: 15.327707. Entropy: 0.396553.\n",
      "Iteration 10709: Policy loss: -0.058540. Value loss: 9.272562. Entropy: 0.392376.\n",
      "Iteration 10710: Policy loss: -0.078198. Value loss: 9.321738. Entropy: 0.393565.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10711: Policy loss: 0.988354. Value loss: 18.797657. Entropy: 0.470758.\n",
      "Iteration 10712: Policy loss: 1.161678. Value loss: 11.981429. Entropy: 0.451911.\n",
      "Iteration 10713: Policy loss: 1.053474. Value loss: 9.789988. Entropy: 0.482343.\n",
      "episode: 4569   score: 235.0  epsilon: 1.0    steps: 51  evaluation reward: 290.3\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10714: Policy loss: -0.881966. Value loss: 19.494257. Entropy: 0.742053.\n",
      "Iteration 10715: Policy loss: -1.006569. Value loss: 9.631080. Entropy: 0.751851.\n",
      "Iteration 10716: Policy loss: -0.846841. Value loss: 6.860901. Entropy: 0.742500.\n",
      "episode: 4570   score: 260.0  epsilon: 1.0    steps: 285  evaluation reward: 290.35\n",
      "episode: 4571   score: 215.0  epsilon: 1.0    steps: 390  evaluation reward: 288.3\n",
      "episode: 4572   score: 300.0  epsilon: 1.0    steps: 681  evaluation reward: 288.45\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10717: Policy loss: -0.654537. Value loss: 15.481091. Entropy: 0.685007.\n",
      "Iteration 10718: Policy loss: -0.632144. Value loss: 12.340541. Entropy: 0.668600.\n",
      "Iteration 10719: Policy loss: -0.571677. Value loss: 8.508788. Entropy: 0.680693.\n",
      "episode: 4573   score: 270.0  epsilon: 1.0    steps: 940  evaluation reward: 288.75\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10720: Policy loss: -1.424103. Value loss: 27.300213. Entropy: 0.619617.\n",
      "Iteration 10721: Policy loss: -1.241004. Value loss: 15.116549. Entropy: 0.636181.\n",
      "Iteration 10722: Policy loss: -1.589978. Value loss: 12.390163. Entropy: 0.607950.\n",
      "episode: 4574   score: 215.0  epsilon: 1.0    steps: 239  evaluation reward: 288.25\n",
      "episode: 4575   score: 240.0  epsilon: 1.0    steps: 784  evaluation reward: 288.55\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10723: Policy loss: 1.468066. Value loss: 25.340595. Entropy: 0.449745.\n",
      "Iteration 10724: Policy loss: 1.440839. Value loss: 15.805719. Entropy: 0.463633.\n",
      "Iteration 10725: Policy loss: 1.373132. Value loss: 13.528026. Entropy: 0.479654.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10726: Policy loss: -0.092207. Value loss: 14.439049. Entropy: 0.299905.\n",
      "Iteration 10727: Policy loss: -0.035527. Value loss: 7.084449. Entropy: 0.266906.\n",
      "Iteration 10728: Policy loss: 0.124824. Value loss: 5.729018. Entropy: 0.291716.\n",
      "episode: 4576   score: 135.0  epsilon: 1.0    steps: 415  evaluation reward: 287.5\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10729: Policy loss: 1.148969. Value loss: 17.184338. Entropy: 0.530162.\n",
      "Iteration 10730: Policy loss: 1.171323. Value loss: 9.780753. Entropy: 0.536515.\n",
      "Iteration 10731: Policy loss: 1.056139. Value loss: 8.011266. Entropy: 0.530015.\n",
      "episode: 4577   score: 170.0  epsilon: 1.0    steps: 88  evaluation reward: 286.55\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10732: Policy loss: 1.020429. Value loss: 17.738510. Entropy: 0.552203.\n",
      "Iteration 10733: Policy loss: 1.194141. Value loss: 10.331182. Entropy: 0.560726.\n",
      "Iteration 10734: Policy loss: 0.990569. Value loss: 6.613798. Entropy: 0.542940.\n",
      "episode: 4578   score: 320.0  epsilon: 1.0    steps: 604  evaluation reward: 288.15\n",
      "episode: 4579   score: 290.0  epsilon: 1.0    steps: 764  evaluation reward: 287.7\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10735: Policy loss: -1.080318. Value loss: 16.215576. Entropy: 0.595584.\n",
      "Iteration 10736: Policy loss: -1.258025. Value loss: 10.406831. Entropy: 0.563709.\n",
      "Iteration 10737: Policy loss: -0.926045. Value loss: 7.757699. Entropy: 0.590239.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10738: Policy loss: -0.135183. Value loss: 15.260487. Entropy: 0.573977.\n",
      "Iteration 10739: Policy loss: -0.091317. Value loss: 8.680326. Entropy: 0.577507.\n",
      "Iteration 10740: Policy loss: -0.114784. Value loss: 7.285309. Entropy: 0.575834.\n",
      "episode: 4580   score: 365.0  epsilon: 1.0    steps: 263  evaluation reward: 288.5\n",
      "episode: 4581   score: 245.0  epsilon: 1.0    steps: 811  evaluation reward: 288.1\n",
      "episode: 4582   score: 245.0  epsilon: 1.0    steps: 987  evaluation reward: 287.85\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10741: Policy loss: 0.643131. Value loss: 17.814138. Entropy: 0.533352.\n",
      "Iteration 10742: Policy loss: 0.912295. Value loss: 11.772941. Entropy: 0.515383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10743: Policy loss: 0.758735. Value loss: 11.523955. Entropy: 0.521762.\n",
      "episode: 4583   score: 260.0  epsilon: 1.0    steps: 187  evaluation reward: 288.0\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10744: Policy loss: -4.183414. Value loss: 248.052475. Entropy: 0.389426.\n",
      "Iteration 10745: Policy loss: -4.444812. Value loss: 101.465248. Entropy: 0.348982.\n",
      "Iteration 10746: Policy loss: -3.891426. Value loss: 132.165222. Entropy: 0.300474.\n",
      "episode: 4584   score: 265.0  epsilon: 1.0    steps: 484  evaluation reward: 288.55\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10747: Policy loss: -1.973238. Value loss: 22.595589. Entropy: 0.269497.\n",
      "Iteration 10748: Policy loss: -1.783730. Value loss: 17.007816. Entropy: 0.267783.\n",
      "Iteration 10749: Policy loss: -1.922092. Value loss: 10.817715. Entropy: 0.268898.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10750: Policy loss: 2.338764. Value loss: 51.237774. Entropy: 0.414902.\n",
      "Iteration 10751: Policy loss: 2.926001. Value loss: 22.574587. Entropy: 0.472132.\n",
      "Iteration 10752: Policy loss: 2.274152. Value loss: 14.034746. Entropy: 0.440952.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10753: Policy loss: 5.244389. Value loss: 58.810928. Entropy: 0.517231.\n",
      "Iteration 10754: Policy loss: 5.263312. Value loss: 30.804396. Entropy: 0.469455.\n",
      "Iteration 10755: Policy loss: 5.036129. Value loss: 21.070955. Entropy: 0.495935.\n",
      "episode: 4585   score: 85.0  epsilon: 1.0    steps: 452  evaluation reward: 287.3\n",
      "episode: 4586   score: 535.0  epsilon: 1.0    steps: 546  evaluation reward: 289.15\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10756: Policy loss: 3.031962. Value loss: 21.100107. Entropy: 0.545290.\n",
      "Iteration 10757: Policy loss: 2.999356. Value loss: 10.972746. Entropy: 0.581777.\n",
      "Iteration 10758: Policy loss: 2.895068. Value loss: 9.239871. Entropy: 0.553932.\n",
      "episode: 4587   score: 240.0  epsilon: 1.0    steps: 325  evaluation reward: 288.95\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10759: Policy loss: -1.402936. Value loss: 31.695292. Entropy: 0.573866.\n",
      "Iteration 10760: Policy loss: -1.681234. Value loss: 20.401260. Entropy: 0.584518.\n",
      "Iteration 10761: Policy loss: -1.341091. Value loss: 15.782594. Entropy: 0.595011.\n",
      "episode: 4588   score: 345.0  epsilon: 1.0    steps: 106  evaluation reward: 290.05\n",
      "episode: 4589   score: 315.0  epsilon: 1.0    steps: 804  evaluation reward: 291.05\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10762: Policy loss: 0.372671. Value loss: 19.647800. Entropy: 0.560665.\n",
      "Iteration 10763: Policy loss: 0.513170. Value loss: 15.567184. Entropy: 0.562416.\n",
      "Iteration 10764: Policy loss: 0.343038. Value loss: 8.212445. Entropy: 0.548120.\n",
      "episode: 4590   score: 325.0  epsilon: 1.0    steps: 743  evaluation reward: 291.35\n",
      "episode: 4591   score: 350.0  epsilon: 1.0    steps: 925  evaluation reward: 291.7\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10765: Policy loss: 2.793553. Value loss: 24.869003. Entropy: 0.438552.\n",
      "Iteration 10766: Policy loss: 2.371861. Value loss: 16.265396. Entropy: 0.450743.\n",
      "Iteration 10767: Policy loss: 2.619425. Value loss: 11.430680. Entropy: 0.477465.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10768: Policy loss: 0.481248. Value loss: 24.915508. Entropy: 0.499484.\n",
      "Iteration 10769: Policy loss: 0.319039. Value loss: 13.474686. Entropy: 0.498956.\n",
      "Iteration 10770: Policy loss: 0.377779. Value loss: 10.487077. Entropy: 0.497617.\n",
      "episode: 4592   score: 210.0  epsilon: 1.0    steps: 299  evaluation reward: 291.2\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10771: Policy loss: -0.951286. Value loss: 37.754196. Entropy: 0.581384.\n",
      "Iteration 10772: Policy loss: -1.030842. Value loss: 21.599306. Entropy: 0.596850.\n",
      "Iteration 10773: Policy loss: -1.138657. Value loss: 18.951361. Entropy: 0.590847.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10774: Policy loss: 0.839644. Value loss: 27.914257. Entropy: 0.553770.\n",
      "Iteration 10775: Policy loss: 1.200338. Value loss: 11.846648. Entropy: 0.560829.\n",
      "Iteration 10776: Policy loss: 0.990072. Value loss: 11.335959. Entropy: 0.589293.\n",
      "episode: 4593   score: 380.0  epsilon: 1.0    steps: 199  evaluation reward: 291.7\n",
      "episode: 4594   score: 340.0  epsilon: 1.0    steps: 413  evaluation reward: 292.25\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10777: Policy loss: 1.475347. Value loss: 24.097874. Entropy: 0.540819.\n",
      "Iteration 10778: Policy loss: 1.695518. Value loss: 12.591927. Entropy: 0.556934.\n",
      "Iteration 10779: Policy loss: 1.365770. Value loss: 11.704875. Entropy: 0.566497.\n",
      "episode: 4595   score: 225.0  epsilon: 1.0    steps: 877  evaluation reward: 291.9\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10780: Policy loss: 1.238004. Value loss: 25.557571. Entropy: 0.542663.\n",
      "Iteration 10781: Policy loss: 1.399934. Value loss: 18.462479. Entropy: 0.539635.\n",
      "Iteration 10782: Policy loss: 1.264676. Value loss: 15.688792. Entropy: 0.526166.\n",
      "episode: 4596   score: 240.0  epsilon: 1.0    steps: 762  evaluation reward: 291.8\n",
      "episode: 4597   score: 325.0  epsilon: 1.0    steps: 907  evaluation reward: 292.95\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10783: Policy loss: 0.897537. Value loss: 21.045963. Entropy: 0.581768.\n",
      "Iteration 10784: Policy loss: 0.777968. Value loss: 13.620168. Entropy: 0.565607.\n",
      "Iteration 10785: Policy loss: 0.702702. Value loss: 11.633757. Entropy: 0.576513.\n",
      "episode: 4598   score: 335.0  epsilon: 1.0    steps: 541  evaluation reward: 291.35\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10786: Policy loss: 0.335593. Value loss: 22.320715. Entropy: 0.483398.\n",
      "Iteration 10787: Policy loss: 0.402026. Value loss: 13.133619. Entropy: 0.500533.\n",
      "Iteration 10788: Policy loss: 0.292493. Value loss: 10.695095. Entropy: 0.510545.\n",
      "episode: 4599   score: 340.0  epsilon: 1.0    steps: 116  evaluation reward: 290.95\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10789: Policy loss: -1.351353. Value loss: 19.671093. Entropy: 0.423438.\n",
      "Iteration 10790: Policy loss: -1.515982. Value loss: 8.528128. Entropy: 0.436242.\n",
      "Iteration 10791: Policy loss: -1.753247. Value loss: 9.231948. Entropy: 0.440061.\n",
      "episode: 4600   score: 320.0  epsilon: 1.0    steps: 328  evaluation reward: 291.75\n",
      "now time :  2019-02-25 22:01:45.513861\n",
      "episode: 4601   score: 210.0  epsilon: 1.0    steps: 454  evaluation reward: 291.75\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10792: Policy loss: -0.121909. Value loss: 18.357037. Entropy: 0.377287.\n",
      "Iteration 10793: Policy loss: 0.025676. Value loss: 9.517038. Entropy: 0.344067.\n",
      "Iteration 10794: Policy loss: 0.053017. Value loss: 8.414090. Entropy: 0.371861.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10795: Policy loss: 0.602205. Value loss: 15.623151. Entropy: 0.537019.\n",
      "Iteration 10796: Policy loss: 0.391069. Value loss: 10.839200. Entropy: 0.550328.\n",
      "Iteration 10797: Policy loss: 0.383127. Value loss: 8.205911. Entropy: 0.556430.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10798: Policy loss: 0.959368. Value loss: 20.270512. Entropy: 0.435371.\n",
      "Iteration 10799: Policy loss: 0.860979. Value loss: 12.401719. Entropy: 0.446824.\n",
      "Iteration 10800: Policy loss: 0.824318. Value loss: 11.219098. Entropy: 0.451484.\n",
      "episode: 4602   score: 245.0  epsilon: 1.0    steps: 832  evaluation reward: 292.1\n",
      "episode: 4603   score: 280.0  epsilon: 1.0    steps: 960  evaluation reward: 292.8\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10801: Policy loss: 0.148587. Value loss: 26.980608. Entropy: 0.415350.\n",
      "Iteration 10802: Policy loss: 0.199480. Value loss: 16.438297. Entropy: 0.417721.\n",
      "Iteration 10803: Policy loss: -0.002697. Value loss: 13.381186. Entropy: 0.430704.\n",
      "episode: 4604   score: 390.0  epsilon: 1.0    steps: 223  evaluation reward: 293.05\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10804: Policy loss: 0.283172. Value loss: 23.054922. Entropy: 0.627517.\n",
      "Iteration 10805: Policy loss: 0.388141. Value loss: 13.293617. Entropy: 0.605746.\n",
      "Iteration 10806: Policy loss: 0.445022. Value loss: 11.503035. Entropy: 0.605805.\n",
      "episode: 4605   score: 330.0  epsilon: 1.0    steps: 728  evaluation reward: 293.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10807: Policy loss: 0.885005. Value loss: 22.174000. Entropy: 0.491976.\n",
      "Iteration 10808: Policy loss: 1.099859. Value loss: 14.537296. Entropy: 0.501188.\n",
      "Iteration 10809: Policy loss: 0.912597. Value loss: 15.569583. Entropy: 0.514316.\n",
      "episode: 4606   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 292.55\n",
      "episode: 4607   score: 345.0  epsilon: 1.0    steps: 563  evaluation reward: 292.35\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10810: Policy loss: 0.112947. Value loss: 23.400610. Entropy: 0.586304.\n",
      "Iteration 10811: Policy loss: 0.350771. Value loss: 14.474855. Entropy: 0.573603.\n",
      "Iteration 10812: Policy loss: 0.120743. Value loss: 12.295529. Entropy: 0.547379.\n",
      "episode: 4608   score: 330.0  epsilon: 1.0    steps: 7  evaluation reward: 292.1\n",
      "episode: 4609   score: 280.0  epsilon: 1.0    steps: 284  evaluation reward: 292.3\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10813: Policy loss: 0.932494. Value loss: 25.227516. Entropy: 0.563750.\n",
      "Iteration 10814: Policy loss: 0.926114. Value loss: 13.958786. Entropy: 0.570913.\n",
      "Iteration 10815: Policy loss: 0.943178. Value loss: 12.211016. Entropy: 0.556593.\n",
      "episode: 4610   score: 210.0  epsilon: 1.0    steps: 813  evaluation reward: 291.8\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10816: Policy loss: 1.330303. Value loss: 41.020699. Entropy: 0.486945.\n",
      "Iteration 10817: Policy loss: 1.078001. Value loss: 22.354799. Entropy: 0.474348.\n",
      "Iteration 10818: Policy loss: 1.415651. Value loss: 18.145407. Entropy: 0.473506.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10819: Policy loss: 1.288746. Value loss: 25.970066. Entropy: 0.563554.\n",
      "Iteration 10820: Policy loss: 1.331978. Value loss: 13.565358. Entropy: 0.554117.\n",
      "Iteration 10821: Policy loss: 1.311610. Value loss: 11.073322. Entropy: 0.555599.\n",
      "episode: 4611   score: 245.0  epsilon: 1.0    steps: 1011  evaluation reward: 292.45\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10822: Policy loss: -2.640918. Value loss: 290.836273. Entropy: 0.577485.\n",
      "Iteration 10823: Policy loss: -2.036150. Value loss: 129.539215. Entropy: 0.478809.\n",
      "Iteration 10824: Policy loss: -1.417503. Value loss: 99.446861. Entropy: 0.523570.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10825: Policy loss: -3.790484. Value loss: 470.188263. Entropy: 0.520302.\n",
      "Iteration 10826: Policy loss: -2.730585. Value loss: 189.636719. Entropy: 0.541567.\n",
      "Iteration 10827: Policy loss: -2.870663. Value loss: 118.416801. Entropy: 0.520372.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10828: Policy loss: 0.615782. Value loss: 48.964344. Entropy: 0.634556.\n",
      "Iteration 10829: Policy loss: 0.869996. Value loss: 25.452663. Entropy: 0.655109.\n",
      "Iteration 10830: Policy loss: 0.596958. Value loss: 19.150146. Entropy: 0.645270.\n",
      "episode: 4612   score: 375.0  epsilon: 1.0    steps: 134  evaluation reward: 293.75\n",
      "episode: 4613   score: 210.0  epsilon: 1.0    steps: 366  evaluation reward: 291.05\n",
      "episode: 4614   score: 410.0  epsilon: 1.0    steps: 412  evaluation reward: 293.05\n",
      "episode: 4615   score: 485.0  epsilon: 1.0    steps: 678  evaluation reward: 295.15\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10831: Policy loss: -0.755434. Value loss: 43.537842. Entropy: 0.495789.\n",
      "Iteration 10832: Policy loss: -0.915852. Value loss: 28.982901. Entropy: 0.506810.\n",
      "Iteration 10833: Policy loss: -0.882064. Value loss: 23.751371. Entropy: 0.519562.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10834: Policy loss: 0.436808. Value loss: 50.648319. Entropy: 0.409032.\n",
      "Iteration 10835: Policy loss: -0.015228. Value loss: 33.377026. Entropy: 0.398067.\n",
      "Iteration 10836: Policy loss: -0.001993. Value loss: 21.568472. Entropy: 0.381476.\n",
      "episode: 4616   score: 315.0  epsilon: 1.0    steps: 788  evaluation reward: 295.9\n",
      "episode: 4617   score: 210.0  epsilon: 1.0    steps: 1017  evaluation reward: 294.35\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10837: Policy loss: 1.054504. Value loss: 35.481323. Entropy: 0.497919.\n",
      "Iteration 10838: Policy loss: 0.570404. Value loss: 23.682882. Entropy: 0.489117.\n",
      "Iteration 10839: Policy loss: 0.890650. Value loss: 17.999249. Entropy: 0.489230.\n",
      "episode: 4618   score: 635.0  epsilon: 1.0    steps: 35  evaluation reward: 298.1\n",
      "episode: 4619   score: 135.0  epsilon: 1.0    steps: 381  evaluation reward: 296.85\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10840: Policy loss: 0.762838. Value loss: 21.944847. Entropy: 0.371711.\n",
      "Iteration 10841: Policy loss: 1.197491. Value loss: 16.052744. Entropy: 0.379476.\n",
      "Iteration 10842: Policy loss: 0.960539. Value loss: 13.512995. Entropy: 0.391892.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10843: Policy loss: -0.519933. Value loss: 29.755697. Entropy: 0.413768.\n",
      "Iteration 10844: Policy loss: -0.447044. Value loss: 18.794275. Entropy: 0.448236.\n",
      "Iteration 10845: Policy loss: -0.501785. Value loss: 13.779604. Entropy: 0.449577.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10846: Policy loss: -2.079247. Value loss: 30.432508. Entropy: 0.507849.\n",
      "Iteration 10847: Policy loss: -1.811738. Value loss: 15.337154. Entropy: 0.506535.\n",
      "Iteration 10848: Policy loss: -2.157040. Value loss: 13.305675. Entropy: 0.512690.\n",
      "episode: 4620   score: 285.0  epsilon: 1.0    steps: 441  evaluation reward: 298.65\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10849: Policy loss: 1.178908. Value loss: 38.431309. Entropy: 0.763116.\n",
      "Iteration 10850: Policy loss: 1.215637. Value loss: 24.421183. Entropy: 0.780614.\n",
      "Iteration 10851: Policy loss: 1.669835. Value loss: 17.574638. Entropy: 0.770935.\n",
      "episode: 4621   score: 600.0  epsilon: 1.0    steps: 640  evaluation reward: 302.55\n",
      "episode: 4622   score: 275.0  epsilon: 1.0    steps: 676  evaluation reward: 302.75\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10852: Policy loss: 1.690723. Value loss: 24.060312. Entropy: 0.716871.\n",
      "Iteration 10853: Policy loss: 1.799641. Value loss: 12.720217. Entropy: 0.724375.\n",
      "Iteration 10854: Policy loss: 2.004223. Value loss: 9.736354. Entropy: 0.764648.\n",
      "episode: 4623   score: 210.0  epsilon: 1.0    steps: 908  evaluation reward: 302.0\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10855: Policy loss: 1.453811. Value loss: 25.308168. Entropy: 0.451465.\n",
      "Iteration 10856: Policy loss: 1.290333. Value loss: 14.953519. Entropy: 0.479417.\n",
      "Iteration 10857: Policy loss: 1.109708. Value loss: 13.069066. Entropy: 0.475779.\n",
      "episode: 4624   score: 260.0  epsilon: 1.0    steps: 80  evaluation reward: 300.35\n",
      "episode: 4625   score: 335.0  epsilon: 1.0    steps: 159  evaluation reward: 299.7\n",
      "episode: 4626   score: 260.0  epsilon: 1.0    steps: 819  evaluation reward: 298.35\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10858: Policy loss: 2.430880. Value loss: 25.338081. Entropy: 0.463726.\n",
      "Iteration 10859: Policy loss: 2.515823. Value loss: 17.061453. Entropy: 0.472114.\n",
      "Iteration 10860: Policy loss: 2.270429. Value loss: 16.417265. Entropy: 0.493757.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10861: Policy loss: 0.700528. Value loss: 24.842081. Entropy: 0.497680.\n",
      "Iteration 10862: Policy loss: 0.830337. Value loss: 14.577684. Entropy: 0.508326.\n",
      "Iteration 10863: Policy loss: 0.706504. Value loss: 11.823196. Entropy: 0.488827.\n",
      "episode: 4627   score: 240.0  epsilon: 1.0    steps: 265  evaluation reward: 298.15\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10864: Policy loss: 1.217095. Value loss: 21.700947. Entropy: 0.465873.\n",
      "Iteration 10865: Policy loss: 1.225363. Value loss: 11.157737. Entropy: 0.440467.\n",
      "Iteration 10866: Policy loss: 1.095164. Value loss: 9.222071. Entropy: 0.462665.\n",
      "episode: 4628   score: 210.0  epsilon: 1.0    steps: 756  evaluation reward: 297.6\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10867: Policy loss: 0.116229. Value loss: 19.021172. Entropy: 0.467929.\n",
      "Iteration 10868: Policy loss: 0.087818. Value loss: 10.164814. Entropy: 0.499460.\n",
      "Iteration 10869: Policy loss: 0.005547. Value loss: 9.484797. Entropy: 0.517104.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10870: Policy loss: 0.005584. Value loss: 18.895998. Entropy: 0.494920.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10871: Policy loss: 0.031851. Value loss: 8.768229. Entropy: 0.526073.\n",
      "Iteration 10872: Policy loss: -0.075013. Value loss: 5.399871. Entropy: 0.507850.\n",
      "episode: 4629   score: 210.0  epsilon: 1.0    steps: 882  evaluation reward: 296.3\n",
      "episode: 4630   score: 245.0  epsilon: 1.0    steps: 992  evaluation reward: 296.25\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10873: Policy loss: 1.568273. Value loss: 23.696379. Entropy: 0.442419.\n",
      "Iteration 10874: Policy loss: 1.409196. Value loss: 13.588818. Entropy: 0.445350.\n",
      "Iteration 10875: Policy loss: 1.298276. Value loss: 10.958455. Entropy: 0.445776.\n",
      "episode: 4631   score: 225.0  epsilon: 1.0    steps: 112  evaluation reward: 295.65\n",
      "episode: 4632   score: 290.0  epsilon: 1.0    steps: 521  evaluation reward: 294.75\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10876: Policy loss: 0.174657. Value loss: 25.136478. Entropy: 0.409440.\n",
      "Iteration 10877: Policy loss: 0.111968. Value loss: 13.776089. Entropy: 0.406429.\n",
      "Iteration 10878: Policy loss: 0.442281. Value loss: 9.321989. Entropy: 0.401707.\n",
      "episode: 4633   score: 285.0  epsilon: 1.0    steps: 135  evaluation reward: 293.7\n",
      "episode: 4634   score: 375.0  epsilon: 1.0    steps: 444  evaluation reward: 294.3\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10879: Policy loss: 0.977517. Value loss: 21.543678. Entropy: 0.462381.\n",
      "Iteration 10880: Policy loss: 1.194324. Value loss: 14.482864. Entropy: 0.479474.\n",
      "Iteration 10881: Policy loss: 0.967155. Value loss: 12.529112. Entropy: 0.475479.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10882: Policy loss: 0.102075. Value loss: 26.595009. Entropy: 0.435498.\n",
      "Iteration 10883: Policy loss: 0.001039. Value loss: 13.886087. Entropy: 0.431876.\n",
      "Iteration 10884: Policy loss: 0.050212. Value loss: 14.199627. Entropy: 0.383361.\n",
      "episode: 4635   score: 255.0  epsilon: 1.0    steps: 353  evaluation reward: 294.25\n",
      "episode: 4636   score: 210.0  epsilon: 1.0    steps: 661  evaluation reward: 293.95\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10885: Policy loss: -0.419549. Value loss: 14.274587. Entropy: 0.551240.\n",
      "Iteration 10886: Policy loss: -0.382722. Value loss: 9.311543. Entropy: 0.518532.\n",
      "Iteration 10887: Policy loss: -0.356923. Value loss: 9.207240. Entropy: 0.525860.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10888: Policy loss: 0.722860. Value loss: 15.902119. Entropy: 0.464111.\n",
      "Iteration 10889: Policy loss: 0.681527. Value loss: 7.196680. Entropy: 0.452928.\n",
      "Iteration 10890: Policy loss: 0.744159. Value loss: 5.211191. Entropy: 0.466981.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10891: Policy loss: -0.524435. Value loss: 15.194708. Entropy: 0.350117.\n",
      "Iteration 10892: Policy loss: -0.448553. Value loss: 10.512563. Entropy: 0.349544.\n",
      "Iteration 10893: Policy loss: -0.340412. Value loss: 7.189459. Entropy: 0.337253.\n",
      "episode: 4637   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 291.35\n",
      "episode: 4638   score: 240.0  epsilon: 1.0    steps: 568  evaluation reward: 291.05\n",
      "episode: 4639   score: 285.0  epsilon: 1.0    steps: 777  evaluation reward: 291.15\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10894: Policy loss: 0.465803. Value loss: 17.225674. Entropy: 0.504857.\n",
      "Iteration 10895: Policy loss: 0.580707. Value loss: 11.895717. Entropy: 0.528628.\n",
      "Iteration 10896: Policy loss: 0.547885. Value loss: 9.030384. Entropy: 0.492908.\n",
      "episode: 4640   score: 265.0  epsilon: 1.0    steps: 926  evaluation reward: 290.45\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10897: Policy loss: -5.009839. Value loss: 248.645844. Entropy: 0.526195.\n",
      "Iteration 10898: Policy loss: -5.175203. Value loss: 127.270416. Entropy: 0.514546.\n",
      "Iteration 10899: Policy loss: -5.104421. Value loss: 76.783096. Entropy: 0.443777.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10900: Policy loss: -0.657853. Value loss: 26.624840. Entropy: 0.300746.\n",
      "Iteration 10901: Policy loss: -0.592425. Value loss: 13.830660. Entropy: 0.299834.\n",
      "Iteration 10902: Policy loss: -0.404358. Value loss: 12.579597. Entropy: 0.295137.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10903: Policy loss: -1.473332. Value loss: 253.040436. Entropy: 0.561630.\n",
      "Iteration 10904: Policy loss: -1.194952. Value loss: 120.691315. Entropy: 0.575355.\n",
      "Iteration 10905: Policy loss: -1.146592. Value loss: 92.211525. Entropy: 0.581619.\n",
      "episode: 4641   score: 675.0  epsilon: 1.0    steps: 223  evaluation reward: 294.4\n",
      "episode: 4642   score: 240.0  epsilon: 1.0    steps: 262  evaluation reward: 293.6\n",
      "episode: 4643   score: 375.0  epsilon: 1.0    steps: 414  evaluation reward: 294.7\n",
      "episode: 4644   score: 485.0  epsilon: 1.0    steps: 652  evaluation reward: 295.9\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10906: Policy loss: 0.795639. Value loss: 33.878132. Entropy: 0.427013.\n",
      "Iteration 10907: Policy loss: 0.835809. Value loss: 18.177505. Entropy: 0.431633.\n",
      "Iteration 10908: Policy loss: 0.777782. Value loss: 15.752093. Entropy: 0.434510.\n",
      "episode: 4645   score: 210.0  epsilon: 1.0    steps: 43  evaluation reward: 295.25\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10909: Policy loss: 2.471962. Value loss: 35.891449. Entropy: 0.346190.\n",
      "Iteration 10910: Policy loss: 2.634242. Value loss: 20.047871. Entropy: 0.373669.\n",
      "Iteration 10911: Policy loss: 2.674355. Value loss: 16.105413. Entropy: 0.375371.\n",
      "episode: 4646   score: 260.0  epsilon: 1.0    steps: 627  evaluation reward: 293.05\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10912: Policy loss: 1.679660. Value loss: 19.117027. Entropy: 0.378864.\n",
      "Iteration 10913: Policy loss: 1.996860. Value loss: 12.525021. Entropy: 0.369528.\n",
      "Iteration 10914: Policy loss: 2.107677. Value loss: 11.877631. Entropy: 0.383513.\n",
      "episode: 4647   score: 155.0  epsilon: 1.0    steps: 242  evaluation reward: 291.15\n",
      "episode: 4648   score: 265.0  epsilon: 1.0    steps: 831  evaluation reward: 290.95\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10915: Policy loss: 2.001447. Value loss: 24.767445. Entropy: 0.410849.\n",
      "Iteration 10916: Policy loss: 1.657241. Value loss: 12.276328. Entropy: 0.420985.\n",
      "Iteration 10917: Policy loss: 1.848607. Value loss: 13.561069. Entropy: 0.428304.\n",
      "episode: 4649   score: 270.0  epsilon: 1.0    steps: 966  evaluation reward: 289.0\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10918: Policy loss: -0.422966. Value loss: 18.736519. Entropy: 0.466762.\n",
      "Iteration 10919: Policy loss: -0.452009. Value loss: 13.459843. Entropy: 0.488253.\n",
      "Iteration 10920: Policy loss: -0.436114. Value loss: 10.552874. Entropy: 0.471860.\n",
      "episode: 4650   score: 210.0  epsilon: 1.0    steps: 339  evaluation reward: 288.6\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10921: Policy loss: -2.702930. Value loss: 192.939560. Entropy: 0.431228.\n",
      "Iteration 10922: Policy loss: -3.221348. Value loss: 125.748154. Entropy: 0.403895.\n",
      "Iteration 10923: Policy loss: -2.960361. Value loss: 101.471443. Entropy: 0.392558.\n",
      "now time :  2019-02-25 22:04:13.450834\n",
      "episode: 4651   score: 450.0  epsilon: 1.0    steps: 764  evaluation reward: 289.0\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10924: Policy loss: 1.637432. Value loss: 30.696598. Entropy: 0.577795.\n",
      "Iteration 10925: Policy loss: 1.838031. Value loss: 14.042568. Entropy: 0.605391.\n",
      "Iteration 10926: Policy loss: 1.513132. Value loss: 9.424729. Entropy: 0.605002.\n",
      "episode: 4652   score: 260.0  epsilon: 1.0    steps: 435  evaluation reward: 288.7\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10927: Policy loss: 1.676503. Value loss: 52.291950. Entropy: 0.465955.\n",
      "Iteration 10928: Policy loss: 1.562011. Value loss: 19.988499. Entropy: 0.463610.\n",
      "Iteration 10929: Policy loss: 1.718789. Value loss: 14.831480. Entropy: 0.463295.\n",
      "episode: 4653   score: 300.0  epsilon: 1.0    steps: 98  evaluation reward: 288.85\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10930: Policy loss: 4.275697. Value loss: 52.810707. Entropy: 0.442550.\n",
      "Iteration 10931: Policy loss: 4.669294. Value loss: 16.799240. Entropy: 0.438307.\n",
      "Iteration 10932: Policy loss: 4.481199. Value loss: 19.417950. Entropy: 0.429307.\n",
      "episode: 4654   score: 260.0  epsilon: 1.0    steps: 584  evaluation reward: 288.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10933: Policy loss: 0.483083. Value loss: 33.911926. Entropy: 0.380694.\n",
      "Iteration 10934: Policy loss: 0.177191. Value loss: 17.875486. Entropy: 0.349941.\n",
      "Iteration 10935: Policy loss: 0.225036. Value loss: 14.713708. Entropy: 0.349879.\n",
      "episode: 4655   score: 275.0  epsilon: 1.0    steps: 199  evaluation reward: 288.75\n",
      "episode: 4656   score: 275.0  epsilon: 1.0    steps: 889  evaluation reward: 289.05\n",
      "episode: 4657   score: 260.0  epsilon: 1.0    steps: 1011  evaluation reward: 289.25\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10936: Policy loss: 1.883168. Value loss: 29.291775. Entropy: 0.330494.\n",
      "Iteration 10937: Policy loss: 1.820618. Value loss: 18.777880. Entropy: 0.323488.\n",
      "Iteration 10938: Policy loss: 1.997171. Value loss: 17.550322. Entropy: 0.322491.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10939: Policy loss: -0.346610. Value loss: 19.642344. Entropy: 0.306838.\n",
      "Iteration 10940: Policy loss: -0.337508. Value loss: 13.887882. Entropy: 0.310371.\n",
      "Iteration 10941: Policy loss: -0.439407. Value loss: 12.098845. Entropy: 0.312445.\n",
      "episode: 4658   score: 240.0  epsilon: 1.0    steps: 320  evaluation reward: 288.8\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10942: Policy loss: -0.071817. Value loss: 22.468069. Entropy: 0.301767.\n",
      "Iteration 10943: Policy loss: -0.113580. Value loss: 14.620255. Entropy: 0.297674.\n",
      "Iteration 10944: Policy loss: -0.122341. Value loss: 13.991503. Entropy: 0.298016.\n",
      "episode: 4659   score: 260.0  epsilon: 1.0    steps: 472  evaluation reward: 288.5\n",
      "episode: 4660   score: 260.0  epsilon: 1.0    steps: 729  evaluation reward: 289.0\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10945: Policy loss: 1.217141. Value loss: 20.809839. Entropy: 0.367925.\n",
      "Iteration 10946: Policy loss: 1.248692. Value loss: 14.190961. Entropy: 0.385377.\n",
      "Iteration 10947: Policy loss: 1.049600. Value loss: 12.941419. Entropy: 0.374994.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10948: Policy loss: -0.125109. Value loss: 10.623405. Entropy: 0.281963.\n",
      "Iteration 10949: Policy loss: 0.078810. Value loss: 8.308575. Entropy: 0.295782.\n",
      "Iteration 10950: Policy loss: 0.016144. Value loss: 7.180302. Entropy: 0.287706.\n",
      "episode: 4661   score: 260.0  epsilon: 1.0    steps: 20  evaluation reward: 289.5\n",
      "episode: 4662   score: 260.0  epsilon: 1.0    steps: 624  evaluation reward: 289.5\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10951: Policy loss: -0.855635. Value loss: 25.684582. Entropy: 0.180775.\n",
      "Iteration 10952: Policy loss: -0.894774. Value loss: 15.027231. Entropy: 0.158858.\n",
      "Iteration 10953: Policy loss: -0.857687. Value loss: 12.304449. Entropy: 0.155510.\n",
      "episode: 4663   score: 240.0  epsilon: 1.0    steps: 232  evaluation reward: 288.75\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10954: Policy loss: 2.375802. Value loss: 19.808475. Entropy: 0.313644.\n",
      "Iteration 10955: Policy loss: 2.497771. Value loss: 12.384670. Entropy: 0.313645.\n",
      "Iteration 10956: Policy loss: 2.449049. Value loss: 11.267681. Entropy: 0.297992.\n",
      "episode: 4664   score: 215.0  epsilon: 1.0    steps: 782  evaluation reward: 289.35\n",
      "episode: 4665   score: 265.0  epsilon: 1.0    steps: 923  evaluation reward: 288.55\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10957: Policy loss: 0.069668. Value loss: 20.963131. Entropy: 0.229853.\n",
      "Iteration 10958: Policy loss: 0.095588. Value loss: 14.067248. Entropy: 0.235477.\n",
      "Iteration 10959: Policy loss: -0.145435. Value loss: 14.104493. Entropy: 0.231679.\n",
      "episode: 4666   score: 285.0  epsilon: 1.0    steps: 381  evaluation reward: 289.3\n",
      "episode: 4667   score: 210.0  epsilon: 1.0    steps: 471  evaluation reward: 287.5\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10960: Policy loss: 1.369033. Value loss: 23.983982. Entropy: 0.251085.\n",
      "Iteration 10961: Policy loss: 1.376288. Value loss: 17.227747. Entropy: 0.292209.\n",
      "Iteration 10962: Policy loss: 1.123559. Value loss: 14.716021. Entropy: 0.270626.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10963: Policy loss: -0.031673. Value loss: 12.839457. Entropy: 0.458982.\n",
      "Iteration 10964: Policy loss: -0.140349. Value loss: 10.475332. Entropy: 0.454457.\n",
      "Iteration 10965: Policy loss: -0.151457. Value loss: 8.445542. Entropy: 0.464908.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10966: Policy loss: -0.484759. Value loss: 15.443069. Entropy: 0.393547.\n",
      "Iteration 10967: Policy loss: -0.597068. Value loss: 7.900095. Entropy: 0.401940.\n",
      "Iteration 10968: Policy loss: -0.457149. Value loss: 6.585661. Entropy: 0.415227.\n",
      "episode: 4668   score: 290.0  epsilon: 1.0    steps: 65  evaluation reward: 286.2\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10969: Policy loss: 0.054215. Value loss: 26.451416. Entropy: 0.437449.\n",
      "Iteration 10970: Policy loss: -0.166343. Value loss: 14.574217. Entropy: 0.447985.\n",
      "Iteration 10971: Policy loss: 0.296073. Value loss: 10.250848. Entropy: 0.438066.\n",
      "episode: 4669   score: 225.0  epsilon: 1.0    steps: 561  evaluation reward: 286.1\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10972: Policy loss: 1.671091. Value loss: 14.769577. Entropy: 0.449931.\n",
      "Iteration 10973: Policy loss: 1.432539. Value loss: 9.116240. Entropy: 0.466938.\n",
      "Iteration 10974: Policy loss: 1.695167. Value loss: 7.782388. Entropy: 0.465425.\n",
      "episode: 4670   score: 225.0  epsilon: 1.0    steps: 196  evaluation reward: 285.75\n",
      "episode: 4671   score: 345.0  epsilon: 1.0    steps: 644  evaluation reward: 287.05\n",
      "episode: 4672   score: 260.0  epsilon: 1.0    steps: 1000  evaluation reward: 286.65\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10975: Policy loss: -0.341984. Value loss: 19.322432. Entropy: 0.364659.\n",
      "Iteration 10976: Policy loss: -0.257827. Value loss: 14.059352. Entropy: 0.404107.\n",
      "Iteration 10977: Policy loss: -0.476507. Value loss: 11.393270. Entropy: 0.364545.\n",
      "episode: 4673   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 286.05\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10978: Policy loss: 0.644056. Value loss: 19.564829. Entropy: 0.338951.\n",
      "Iteration 10979: Policy loss: 0.606819. Value loss: 11.187347. Entropy: 0.361364.\n",
      "Iteration 10980: Policy loss: 0.541035. Value loss: 9.335790. Entropy: 0.348841.\n",
      "episode: 4674   score: 285.0  epsilon: 1.0    steps: 414  evaluation reward: 286.75\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10981: Policy loss: 0.698102. Value loss: 29.124723. Entropy: 0.433235.\n",
      "Iteration 10982: Policy loss: 0.752473. Value loss: 18.321543. Entropy: 0.449187.\n",
      "Iteration 10983: Policy loss: 0.771086. Value loss: 12.809076. Entropy: 0.443989.\n",
      "episode: 4675   score: 180.0  epsilon: 1.0    steps: 645  evaluation reward: 286.15\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10984: Policy loss: 2.244408. Value loss: 23.311308. Entropy: 0.351049.\n",
      "Iteration 10985: Policy loss: 2.028435. Value loss: 12.757339. Entropy: 0.313362.\n",
      "Iteration 10986: Policy loss: 1.985167. Value loss: 11.203739. Entropy: 0.337269.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10987: Policy loss: 2.122710. Value loss: 25.043680. Entropy: 0.322718.\n",
      "Iteration 10988: Policy loss: 2.266623. Value loss: 15.462330. Entropy: 0.329544.\n",
      "Iteration 10989: Policy loss: 1.815738. Value loss: 12.424475. Entropy: 0.309785.\n",
      "episode: 4676   score: 220.0  epsilon: 1.0    steps: 19  evaluation reward: 287.0\n",
      "episode: 4677   score: 210.0  epsilon: 1.0    steps: 194  evaluation reward: 287.4\n",
      "episode: 4678   score: 210.0  epsilon: 1.0    steps: 335  evaluation reward: 286.3\n",
      "episode: 4679   score: 420.0  epsilon: 1.0    steps: 831  evaluation reward: 287.6\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10990: Policy loss: 0.940555. Value loss: 19.140030. Entropy: 0.422264.\n",
      "Iteration 10991: Policy loss: 1.144111. Value loss: 17.655325. Entropy: 0.426529.\n",
      "Iteration 10992: Policy loss: 0.938597. Value loss: 16.519812. Entropy: 0.421627.\n",
      "episode: 4680   score: 250.0  epsilon: 1.0    steps: 520  evaluation reward: 286.45\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10993: Policy loss: -0.091282. Value loss: 23.224970. Entropy: 0.235601.\n",
      "Iteration 10994: Policy loss: -0.094957. Value loss: 16.694277. Entropy: 0.244595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10995: Policy loss: -0.053301. Value loss: 14.343207. Entropy: 0.260404.\n",
      "episode: 4681   score: 270.0  epsilon: 1.0    steps: 936  evaluation reward: 286.7\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10996: Policy loss: -0.098207. Value loss: 24.146261. Entropy: 0.291388.\n",
      "Iteration 10997: Policy loss: 0.050246. Value loss: 16.604813. Entropy: 0.283788.\n",
      "Iteration 10998: Policy loss: 0.002553. Value loss: 15.595454. Entropy: 0.277734.\n",
      "episode: 4682   score: 155.0  epsilon: 1.0    steps: 24  evaluation reward: 285.8\n",
      "episode: 4683   score: 240.0  epsilon: 1.0    steps: 448  evaluation reward: 285.6\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10999: Policy loss: -0.809148. Value loss: 216.849289. Entropy: 0.386261.\n",
      "Iteration 11000: Policy loss: 0.310427. Value loss: 48.006962. Entropy: 0.400432.\n",
      "Iteration 11001: Policy loss: -0.571550. Value loss: 51.802692. Entropy: 0.378771.\n",
      "episode: 4684   score: 135.0  epsilon: 1.0    steps: 611  evaluation reward: 284.3\n",
      "episode: 4685   score: 440.0  epsilon: 1.0    steps: 721  evaluation reward: 287.85\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11002: Policy loss: 1.133112. Value loss: 19.119652. Entropy: 0.313946.\n",
      "Iteration 11003: Policy loss: 1.110440. Value loss: 12.201976. Entropy: 0.317788.\n",
      "Iteration 11004: Policy loss: 1.228202. Value loss: 10.046262. Entropy: 0.333247.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11005: Policy loss: 0.264782. Value loss: 20.002686. Entropy: 0.321790.\n",
      "Iteration 11006: Policy loss: 0.472687. Value loss: 11.813972. Entropy: 0.311758.\n",
      "Iteration 11007: Policy loss: 0.458300. Value loss: 9.570844. Entropy: 0.306135.\n",
      "episode: 4686   score: 260.0  epsilon: 1.0    steps: 254  evaluation reward: 285.1\n",
      "episode: 4687   score: 180.0  epsilon: 1.0    steps: 917  evaluation reward: 284.5\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11008: Policy loss: 2.664232. Value loss: 30.355286. Entropy: 0.377369.\n",
      "Iteration 11009: Policy loss: 2.545873. Value loss: 18.895256. Entropy: 0.377422.\n",
      "Iteration 11010: Policy loss: 2.491295. Value loss: 15.472672. Entropy: 0.385948.\n",
      "episode: 4688   score: 240.0  epsilon: 1.0    steps: 304  evaluation reward: 283.45\n",
      "episode: 4689   score: 245.0  epsilon: 1.0    steps: 771  evaluation reward: 282.75\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11011: Policy loss: -0.743145. Value loss: 19.082777. Entropy: 0.245030.\n",
      "Iteration 11012: Policy loss: -0.725223. Value loss: 13.464147. Entropy: 0.273942.\n",
      "Iteration 11013: Policy loss: -0.608422. Value loss: 12.862690. Entropy: 0.265106.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11014: Policy loss: -0.965463. Value loss: 15.991383. Entropy: 0.198348.\n",
      "Iteration 11015: Policy loss: -0.872135. Value loss: 10.836835. Entropy: 0.208301.\n",
      "Iteration 11016: Policy loss: -0.908474. Value loss: 8.701843. Entropy: 0.212066.\n",
      "episode: 4690   score: 285.0  epsilon: 1.0    steps: 61  evaluation reward: 282.35\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11017: Policy loss: -1.013945. Value loss: 29.380827. Entropy: 0.424571.\n",
      "Iteration 11018: Policy loss: -1.056769. Value loss: 14.007460. Entropy: 0.446754.\n",
      "Iteration 11019: Policy loss: -1.391940. Value loss: 14.148858. Entropy: 0.426283.\n",
      "episode: 4691   score: 285.0  epsilon: 1.0    steps: 407  evaluation reward: 281.7\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11020: Policy loss: 0.083439. Value loss: 20.067398. Entropy: 0.429397.\n",
      "Iteration 11021: Policy loss: 0.179914. Value loss: 8.917805. Entropy: 0.447595.\n",
      "Iteration 11022: Policy loss: 0.015726. Value loss: 7.410042. Entropy: 0.440285.\n",
      "episode: 4692   score: 260.0  epsilon: 1.0    steps: 730  evaluation reward: 282.2\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11023: Policy loss: -0.364429. Value loss: 26.849258. Entropy: 0.278663.\n",
      "Iteration 11024: Policy loss: -0.009685. Value loss: 14.757214. Entropy: 0.285823.\n",
      "Iteration 11025: Policy loss: -0.475664. Value loss: 11.292542. Entropy: 0.290136.\n",
      "episode: 4693   score: 390.0  epsilon: 1.0    steps: 564  evaluation reward: 282.3\n",
      "episode: 4694   score: 260.0  epsilon: 1.0    steps: 965  evaluation reward: 281.5\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11026: Policy loss: -0.239829. Value loss: 29.227358. Entropy: 0.417122.\n",
      "Iteration 11027: Policy loss: -0.150967. Value loss: 19.496967. Entropy: 0.409742.\n",
      "Iteration 11028: Policy loss: -0.150166. Value loss: 16.873682. Entropy: 0.421538.\n",
      "episode: 4695   score: 295.0  epsilon: 1.0    steps: 254  evaluation reward: 282.2\n",
      "episode: 4696   score: 305.0  epsilon: 1.0    steps: 874  evaluation reward: 282.85\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11029: Policy loss: 3.047659. Value loss: 20.681255. Entropy: 0.381678.\n",
      "Iteration 11030: Policy loss: 3.066879. Value loss: 12.523470. Entropy: 0.365198.\n",
      "Iteration 11031: Policy loss: 3.034907. Value loss: 8.293922. Entropy: 0.363107.\n",
      "episode: 4697   score: 240.0  epsilon: 1.0    steps: 257  evaluation reward: 282.0\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11032: Policy loss: -0.432588. Value loss: 21.899683. Entropy: 0.306303.\n",
      "Iteration 11033: Policy loss: -0.537193. Value loss: 16.870617. Entropy: 0.290708.\n",
      "Iteration 11034: Policy loss: -0.277303. Value loss: 14.743290. Entropy: 0.302479.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11035: Policy loss: -1.724120. Value loss: 38.331810. Entropy: 0.242776.\n",
      "Iteration 11036: Policy loss: -1.144024. Value loss: 18.864960. Entropy: 0.277758.\n",
      "Iteration 11037: Policy loss: -1.308857. Value loss: 15.155776. Entropy: 0.277046.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11038: Policy loss: -0.720054. Value loss: 15.908863. Entropy: 0.273542.\n",
      "Iteration 11039: Policy loss: -0.632909. Value loss: 11.749565. Entropy: 0.253843.\n",
      "Iteration 11040: Policy loss: -0.757377. Value loss: 8.757745. Entropy: 0.275355.\n",
      "episode: 4698   score: 410.0  epsilon: 1.0    steps: 1019  evaluation reward: 282.75\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11041: Policy loss: -2.343941. Value loss: 234.695435. Entropy: 0.422639.\n",
      "Iteration 11042: Policy loss: -1.721438. Value loss: 116.048401. Entropy: 0.407924.\n",
      "Iteration 11043: Policy loss: -2.250137. Value loss: 125.345383. Entropy: 0.395735.\n",
      "episode: 4699   score: 245.0  epsilon: 1.0    steps: 707  evaluation reward: 281.8\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11044: Policy loss: 1.861821. Value loss: 65.251053. Entropy: 0.396037.\n",
      "Iteration 11045: Policy loss: 1.990804. Value loss: 24.579664. Entropy: 0.389757.\n",
      "Iteration 11046: Policy loss: 2.511127. Value loss: 19.787384. Entropy: 0.394753.\n",
      "episode: 4700   score: 445.0  epsilon: 1.0    steps: 70  evaluation reward: 283.05\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11047: Policy loss: 3.794741. Value loss: 42.361691. Entropy: 0.274760.\n",
      "Iteration 11048: Policy loss: 3.899486. Value loss: 21.917130. Entropy: 0.300888.\n",
      "Iteration 11049: Policy loss: 4.304348. Value loss: 17.462324. Entropy: 0.292439.\n",
      "now time :  2019-02-25 22:06:32.437992\n",
      "episode: 4701   score: 265.0  epsilon: 1.0    steps: 199  evaluation reward: 283.6\n",
      "episode: 4702   score: 225.0  epsilon: 1.0    steps: 381  evaluation reward: 283.4\n",
      "episode: 4703   score: 340.0  epsilon: 1.0    steps: 623  evaluation reward: 284.0\n",
      "episode: 4704   score: 260.0  epsilon: 1.0    steps: 833  evaluation reward: 282.7\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11050: Policy loss: 0.698428. Value loss: 27.197100. Entropy: 0.350341.\n",
      "Iteration 11051: Policy loss: 0.807089. Value loss: 15.037923. Entropy: 0.334625.\n",
      "Iteration 11052: Policy loss: 0.417945. Value loss: 19.970985. Entropy: 0.375424.\n",
      "episode: 4705   score: 500.0  epsilon: 1.0    steps: 396  evaluation reward: 284.4\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11053: Policy loss: -0.746713. Value loss: 20.252586. Entropy: 0.356975.\n",
      "Iteration 11054: Policy loss: -0.678541. Value loss: 14.230232. Entropy: 0.339556.\n",
      "Iteration 11055: Policy loss: -0.707635. Value loss: 13.446020. Entropy: 0.357569.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11056: Policy loss: 0.615041. Value loss: 22.322538. Entropy: 0.267653.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11057: Policy loss: 0.519589. Value loss: 15.775380. Entropy: 0.276589.\n",
      "Iteration 11058: Policy loss: 0.472875. Value loss: 13.901408. Entropy: 0.281960.\n",
      "episode: 4706   score: 210.0  epsilon: 1.0    steps: 938  evaluation reward: 283.9\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11059: Policy loss: -0.859324. Value loss: 12.154820. Entropy: 0.325926.\n",
      "Iteration 11060: Policy loss: -0.809614. Value loss: 8.892046. Entropy: 0.326749.\n",
      "Iteration 11061: Policy loss: -0.810988. Value loss: 8.597590. Entropy: 0.318972.\n",
      "episode: 4707   score: 290.0  epsilon: 1.0    steps: 728  evaluation reward: 283.35\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11062: Policy loss: -0.665444. Value loss: 16.142126. Entropy: 0.384485.\n",
      "Iteration 11063: Policy loss: -0.935946. Value loss: 8.502662. Entropy: 0.429268.\n",
      "Iteration 11064: Policy loss: -1.015569. Value loss: 7.723479. Entropy: 0.399852.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11065: Policy loss: 2.100591. Value loss: 21.808561. Entropy: 0.329606.\n",
      "Iteration 11066: Policy loss: 2.209017. Value loss: 11.110150. Entropy: 0.337055.\n",
      "Iteration 11067: Policy loss: 1.917315. Value loss: 7.651745. Entropy: 0.324258.\n",
      "episode: 4708   score: 260.0  epsilon: 1.0    steps: 23  evaluation reward: 282.65\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11068: Policy loss: 0.092817. Value loss: 19.148228. Entropy: 0.346149.\n",
      "Iteration 11069: Policy loss: 0.042390. Value loss: 11.750998. Entropy: 0.361577.\n",
      "Iteration 11070: Policy loss: 0.045457. Value loss: 9.481695. Entropy: 0.357940.\n",
      "episode: 4709   score: 240.0  epsilon: 1.0    steps: 133  evaluation reward: 282.25\n",
      "episode: 4710   score: 285.0  epsilon: 1.0    steps: 300  evaluation reward: 283.0\n",
      "episode: 4711   score: 240.0  epsilon: 1.0    steps: 448  evaluation reward: 282.95\n",
      "episode: 4712   score: 330.0  epsilon: 1.0    steps: 634  evaluation reward: 282.5\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11071: Policy loss: -0.906012. Value loss: 37.857620. Entropy: 0.475761.\n",
      "Iteration 11072: Policy loss: -1.016382. Value loss: 22.364412. Entropy: 0.474036.\n",
      "Iteration 11073: Policy loss: -0.884688. Value loss: 17.922157. Entropy: 0.469637.\n",
      "episode: 4713   score: 355.0  epsilon: 1.0    steps: 791  evaluation reward: 283.95\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11074: Policy loss: -0.550843. Value loss: 22.236038. Entropy: 0.441709.\n",
      "Iteration 11075: Policy loss: -0.302854. Value loss: 15.101493. Entropy: 0.457245.\n",
      "Iteration 11076: Policy loss: -0.504347. Value loss: 11.493650. Entropy: 0.467384.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11077: Policy loss: 0.828539. Value loss: 16.972853. Entropy: 0.408682.\n",
      "Iteration 11078: Policy loss: 0.784570. Value loss: 12.984032. Entropy: 0.402776.\n",
      "Iteration 11079: Policy loss: 0.917091. Value loss: 9.685493. Entropy: 0.424826.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11080: Policy loss: -1.412739. Value loss: 205.362915. Entropy: 0.441927.\n",
      "Iteration 11081: Policy loss: -1.119162. Value loss: 144.578094. Entropy: 0.372432.\n",
      "Iteration 11082: Policy loss: -1.179656. Value loss: 109.335449. Entropy: 0.357268.\n",
      "episode: 4714   score: 245.0  epsilon: 1.0    steps: 677  evaluation reward: 282.3\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11083: Policy loss: -0.843684. Value loss: 24.749071. Entropy: 0.465687.\n",
      "Iteration 11084: Policy loss: -0.401415. Value loss: 14.238051. Entropy: 0.475344.\n",
      "Iteration 11085: Policy loss: -0.584367. Value loss: 9.866793. Entropy: 0.478603.\n",
      "episode: 4715   score: 470.0  epsilon: 1.0    steps: 75  evaluation reward: 282.15\n",
      "episode: 4716   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 281.1\n",
      "episode: 4717   score: 395.0  epsilon: 1.0    steps: 954  evaluation reward: 282.95\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11086: Policy loss: 3.796612. Value loss: 28.294794. Entropy: 0.317876.\n",
      "Iteration 11087: Policy loss: 3.903060. Value loss: 14.890448. Entropy: 0.291771.\n",
      "Iteration 11088: Policy loss: 3.811590. Value loss: 13.657954. Entropy: 0.320682.\n",
      "episode: 4718   score: 240.0  epsilon: 1.0    steps: 485  evaluation reward: 279.0\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11089: Policy loss: 0.728705. Value loss: 26.842510. Entropy: 0.420505.\n",
      "Iteration 11090: Policy loss: 0.657711. Value loss: 14.771375. Entropy: 0.405726.\n",
      "Iteration 11091: Policy loss: 0.691872. Value loss: 10.782714. Entropy: 0.396354.\n",
      "episode: 4719   score: 265.0  epsilon: 1.0    steps: 296  evaluation reward: 280.3\n",
      "episode: 4720   score: 260.0  epsilon: 1.0    steps: 883  evaluation reward: 280.05\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11092: Policy loss: -0.489293. Value loss: 18.993607. Entropy: 0.424485.\n",
      "Iteration 11093: Policy loss: -0.529829. Value loss: 12.241414. Entropy: 0.413646.\n",
      "Iteration 11094: Policy loss: -0.544785. Value loss: 10.950829. Entropy: 0.440937.\n",
      "episode: 4721   score: 255.0  epsilon: 1.0    steps: 607  evaluation reward: 276.6\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11095: Policy loss: 0.471108. Value loss: 14.434934. Entropy: 0.466893.\n",
      "Iteration 11096: Policy loss: 0.472375. Value loss: 12.600478. Entropy: 0.453577.\n",
      "Iteration 11097: Policy loss: 0.434719. Value loss: 10.029865. Entropy: 0.461125.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11098: Policy loss: 0.159399. Value loss: 13.833906. Entropy: 0.376482.\n",
      "Iteration 11099: Policy loss: 0.170061. Value loss: 7.137470. Entropy: 0.361605.\n",
      "Iteration 11100: Policy loss: 0.202109. Value loss: 5.380455. Entropy: 0.380468.\n",
      "episode: 4722   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 275.95\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11101: Policy loss: 0.888565. Value loss: 19.804413. Entropy: 0.261794.\n",
      "Iteration 11102: Policy loss: 0.994161. Value loss: 11.624854. Entropy: 0.279763.\n",
      "Iteration 11103: Policy loss: 0.925706. Value loss: 8.681479. Entropy: 0.290295.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11104: Policy loss: -0.520728. Value loss: 21.649683. Entropy: 0.386421.\n",
      "Iteration 11105: Policy loss: -0.241688. Value loss: 12.976778. Entropy: 0.399612.\n",
      "Iteration 11106: Policy loss: -0.376396. Value loss: 10.012138. Entropy: 0.387119.\n",
      "episode: 4723   score: 275.0  epsilon: 1.0    steps: 78  evaluation reward: 276.6\n",
      "episode: 4724   score: 290.0  epsilon: 1.0    steps: 204  evaluation reward: 276.9\n",
      "episode: 4725   score: 210.0  epsilon: 1.0    steps: 333  evaluation reward: 275.65\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11107: Policy loss: 1.855955. Value loss: 32.431210. Entropy: 0.243104.\n",
      "Iteration 11108: Policy loss: 1.512301. Value loss: 18.302357. Entropy: 0.275892.\n",
      "Iteration 11109: Policy loss: 1.687465. Value loss: 12.938903. Entropy: 0.300341.\n",
      "episode: 4726   score: 210.0  epsilon: 1.0    steps: 407  evaluation reward: 275.15\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11110: Policy loss: 0.825428. Value loss: 26.857008. Entropy: 0.557981.\n",
      "Iteration 11111: Policy loss: 0.842132. Value loss: 16.818748. Entropy: 0.540717.\n",
      "Iteration 11112: Policy loss: 0.985025. Value loss: 16.760223. Entropy: 0.563383.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11113: Policy loss: -2.535571. Value loss: 192.726379. Entropy: 0.487353.\n",
      "Iteration 11114: Policy loss: -2.426552. Value loss: 61.442047. Entropy: 0.488961.\n",
      "Iteration 11115: Policy loss: -2.404770. Value loss: 54.906269. Entropy: 0.466389.\n",
      "episode: 4727   score: 240.0  epsilon: 1.0    steps: 540  evaluation reward: 275.15\n",
      "episode: 4728   score: 420.0  epsilon: 1.0    steps: 891  evaluation reward: 277.25\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11116: Policy loss: 1.088321. Value loss: 29.297901. Entropy: 0.465462.\n",
      "Iteration 11117: Policy loss: 1.039842. Value loss: 17.297596. Entropy: 0.455399.\n",
      "Iteration 11118: Policy loss: 1.056247. Value loss: 15.487232. Entropy: 0.495989.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11119: Policy loss: -2.352904. Value loss: 154.650955. Entropy: 0.485562.\n",
      "Iteration 11120: Policy loss: -2.275924. Value loss: 67.694901. Entropy: 0.465000.\n",
      "Iteration 11121: Policy loss: -2.704272. Value loss: 65.741592. Entropy: 0.485512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4729   score: 350.0  epsilon: 1.0    steps: 742  evaluation reward: 278.65\n",
      "episode: 4730   score: 670.0  epsilon: 1.0    steps: 943  evaluation reward: 282.9\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11122: Policy loss: 2.487108. Value loss: 46.170605. Entropy: 0.302480.\n",
      "Iteration 11123: Policy loss: 5.518005. Value loss: 23.188782. Entropy: 0.279107.\n",
      "Iteration 11124: Policy loss: 2.593189. Value loss: 12.849960. Entropy: 0.271771.\n",
      "episode: 4731   score: 435.0  epsilon: 1.0    steps: 248  evaluation reward: 285.0\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11125: Policy loss: -1.517666. Value loss: 37.926521. Entropy: 0.175579.\n",
      "Iteration 11126: Policy loss: -1.545413. Value loss: 21.217960. Entropy: 0.177829.\n",
      "Iteration 11127: Policy loss: -1.568188. Value loss: 14.736059. Entropy: 0.158368.\n",
      "episode: 4732   score: 300.0  epsilon: 1.0    steps: 493  evaluation reward: 285.1\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11128: Policy loss: 1.543295. Value loss: 43.712925. Entropy: 0.246243.\n",
      "Iteration 11129: Policy loss: 1.823418. Value loss: 22.851698. Entropy: 0.229612.\n",
      "Iteration 11130: Policy loss: 1.727635. Value loss: 20.034979. Entropy: 0.234868.\n",
      "episode: 4733   score: 260.0  epsilon: 1.0    steps: 47  evaluation reward: 284.85\n",
      "episode: 4734   score: 260.0  epsilon: 1.0    steps: 311  evaluation reward: 283.7\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11131: Policy loss: 2.621455. Value loss: 24.031233. Entropy: 0.339301.\n",
      "Iteration 11132: Policy loss: 2.786962. Value loss: 15.711297. Entropy: 0.344478.\n",
      "Iteration 11133: Policy loss: 2.773144. Value loss: 13.805812. Entropy: 0.358191.\n",
      "episode: 4735   score: 265.0  epsilon: 1.0    steps: 591  evaluation reward: 283.8\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11134: Policy loss: 0.311719. Value loss: 26.143711. Entropy: 0.260456.\n",
      "Iteration 11135: Policy loss: 0.268696. Value loss: 19.967417. Entropy: 0.269696.\n",
      "Iteration 11136: Policy loss: 0.456112. Value loss: 16.849743. Entropy: 0.273539.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11137: Policy loss: 1.176841. Value loss: 37.771164. Entropy: 0.226972.\n",
      "Iteration 11138: Policy loss: 1.334006. Value loss: 16.214771. Entropy: 0.221171.\n",
      "Iteration 11139: Policy loss: 1.097107. Value loss: 12.222706. Entropy: 0.229072.\n",
      "episode: 4736   score: 340.0  epsilon: 1.0    steps: 775  evaluation reward: 285.1\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11140: Policy loss: 0.955266. Value loss: 14.403529. Entropy: 0.271936.\n",
      "Iteration 11141: Policy loss: 0.758450. Value loss: 9.367692. Entropy: 0.277540.\n",
      "Iteration 11142: Policy loss: 0.929642. Value loss: 7.936533. Entropy: 0.284121.\n",
      "episode: 4737   score: 305.0  epsilon: 1.0    steps: 735  evaluation reward: 286.05\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11143: Policy loss: 1.800277. Value loss: 16.452652. Entropy: 0.348733.\n",
      "Iteration 11144: Policy loss: 1.917820. Value loss: 9.629644. Entropy: 0.349044.\n",
      "Iteration 11145: Policy loss: 1.744534. Value loss: 9.556437. Entropy: 0.355709.\n",
      "episode: 4738   score: 260.0  epsilon: 1.0    steps: 149  evaluation reward: 286.25\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11146: Policy loss: 0.558638. Value loss: 14.753524. Entropy: 0.192228.\n",
      "Iteration 11147: Policy loss: 0.577287. Value loss: 10.578512. Entropy: 0.192436.\n",
      "Iteration 11148: Policy loss: 0.556876. Value loss: 8.748873. Entropy: 0.205628.\n",
      "episode: 4739   score: 240.0  epsilon: 1.0    steps: 74  evaluation reward: 285.8\n",
      "episode: 4740   score: 260.0  epsilon: 1.0    steps: 335  evaluation reward: 285.75\n",
      "episode: 4741   score: 260.0  epsilon: 1.0    steps: 400  evaluation reward: 281.6\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11149: Policy loss: 0.369527. Value loss: 32.273693. Entropy: 0.157416.\n",
      "Iteration 11150: Policy loss: 0.375275. Value loss: 18.840267. Entropy: 0.155653.\n",
      "Iteration 11151: Policy loss: 0.562590. Value loss: 16.545238. Entropy: 0.192350.\n",
      "episode: 4742   score: 350.0  epsilon: 1.0    steps: 926  evaluation reward: 282.7\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11152: Policy loss: -0.414984. Value loss: 17.497368. Entropy: 0.355850.\n",
      "Iteration 11153: Policy loss: -0.123001. Value loss: 12.114688. Entropy: 0.377012.\n",
      "Iteration 11154: Policy loss: -0.181731. Value loss: 10.849647. Entropy: 0.410068.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11155: Policy loss: -0.190553. Value loss: 17.090559. Entropy: 0.221359.\n",
      "Iteration 11156: Policy loss: -0.078404. Value loss: 10.542717. Entropy: 0.230254.\n",
      "Iteration 11157: Policy loss: -0.168373. Value loss: 7.631114. Entropy: 0.235331.\n",
      "episode: 4743   score: 210.0  epsilon: 1.0    steps: 150  evaluation reward: 281.05\n",
      "episode: 4744   score: 285.0  epsilon: 1.0    steps: 800  evaluation reward: 279.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11158: Policy loss: 0.444056. Value loss: 18.685011. Entropy: 0.433146.\n",
      "Iteration 11159: Policy loss: 0.528099. Value loss: 12.533487. Entropy: 0.404403.\n",
      "Iteration 11160: Policy loss: 0.517475. Value loss: 9.674419. Entropy: 0.413727.\n",
      "episode: 4745   score: 210.0  epsilon: 1.0    steps: 345  evaluation reward: 279.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11161: Policy loss: 0.417619. Value loss: 15.295650. Entropy: 0.376649.\n",
      "Iteration 11162: Policy loss: 0.492793. Value loss: 9.512440. Entropy: 0.369990.\n",
      "Iteration 11163: Policy loss: 0.310237. Value loss: 7.842227. Entropy: 0.392209.\n",
      "episode: 4746   score: 375.0  epsilon: 1.0    steps: 569  evaluation reward: 280.2\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11164: Policy loss: 0.428909. Value loss: 19.556057. Entropy: 0.360531.\n",
      "Iteration 11165: Policy loss: 0.528495. Value loss: 12.016756. Entropy: 0.366079.\n",
      "Iteration 11166: Policy loss: 0.389712. Value loss: 11.336747. Entropy: 0.397811.\n",
      "episode: 4747   score: 240.0  epsilon: 1.0    steps: 423  evaluation reward: 281.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11167: Policy loss: -2.674630. Value loss: 192.074265. Entropy: 0.273704.\n",
      "Iteration 11168: Policy loss: -2.600298. Value loss: 51.122684. Entropy: 0.275736.\n",
      "Iteration 11169: Policy loss: -2.758472. Value loss: 48.986439. Entropy: 0.327677.\n",
      "episode: 4748   score: 465.0  epsilon: 1.0    steps: 48  evaluation reward: 283.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11170: Policy loss: 1.359455. Value loss: 22.511436. Entropy: 0.314603.\n",
      "Iteration 11171: Policy loss: 1.395772. Value loss: 14.049835. Entropy: 0.299221.\n",
      "Iteration 11172: Policy loss: 1.375993. Value loss: 10.910343. Entropy: 0.321491.\n",
      "episode: 4749   score: 380.0  epsilon: 1.0    steps: 1011  evaluation reward: 284.15\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11173: Policy loss: -0.230790. Value loss: 17.581795. Entropy: 0.273717.\n",
      "Iteration 11174: Policy loss: -0.125182. Value loss: 10.818802. Entropy: 0.273316.\n",
      "Iteration 11175: Policy loss: -0.113200. Value loss: 10.734044. Entropy: 0.278939.\n",
      "episode: 4750   score: 290.0  epsilon: 1.0    steps: 193  evaluation reward: 284.95\n",
      "now time :  2019-02-25 22:08:54.589903\n",
      "episode: 4751   score: 260.0  epsilon: 1.0    steps: 844  evaluation reward: 283.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11176: Policy loss: 1.965815. Value loss: 23.119452. Entropy: 0.336142.\n",
      "Iteration 11177: Policy loss: 2.004289. Value loss: 13.803684. Entropy: 0.367100.\n",
      "Iteration 11178: Policy loss: 1.868496. Value loss: 11.536358. Entropy: 0.352718.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11179: Policy loss: -0.817751. Value loss: 30.996180. Entropy: 0.417961.\n",
      "Iteration 11180: Policy loss: -0.691271. Value loss: 19.750597. Entropy: 0.408967.\n",
      "Iteration 11181: Policy loss: -0.619464. Value loss: 15.951306. Entropy: 0.407335.\n",
      "episode: 4752   score: 285.0  epsilon: 1.0    steps: 578  evaluation reward: 283.3\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11182: Policy loss: 0.643787. Value loss: 25.519257. Entropy: 0.221914.\n",
      "Iteration 11183: Policy loss: 0.819195. Value loss: 12.040521. Entropy: 0.222342.\n",
      "Iteration 11184: Policy loss: 0.858940. Value loss: 11.871160. Entropy: 0.228269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4753   score: 285.0  epsilon: 1.0    steps: 428  evaluation reward: 283.15\n",
      "episode: 4754   score: 325.0  epsilon: 1.0    steps: 698  evaluation reward: 283.8\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11185: Policy loss: -5.118914. Value loss: 417.661194. Entropy: 0.491107.\n",
      "Iteration 11186: Policy loss: -5.248228. Value loss: 249.324509. Entropy: 0.450277.\n",
      "Iteration 11187: Policy loss: -4.738023. Value loss: 184.653641. Entropy: 0.435530.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11188: Policy loss: 1.208844. Value loss: 35.605499. Entropy: 0.386283.\n",
      "Iteration 11189: Policy loss: 1.219473. Value loss: 20.723276. Entropy: 0.392840.\n",
      "Iteration 11190: Policy loss: 1.273936. Value loss: 14.346694. Entropy: 0.407501.\n",
      "episode: 4755   score: 310.0  epsilon: 1.0    steps: 127  evaluation reward: 284.15\n",
      "episode: 4756   score: 415.0  epsilon: 1.0    steps: 327  evaluation reward: 285.55\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11191: Policy loss: 1.424843. Value loss: 29.661037. Entropy: 0.414364.\n",
      "Iteration 11192: Policy loss: 1.228663. Value loss: 18.619356. Entropy: 0.409149.\n",
      "Iteration 11193: Policy loss: 1.511878. Value loss: 14.556962. Entropy: 0.409302.\n",
      "episode: 4757   score: 210.0  epsilon: 1.0    steps: 908  evaluation reward: 285.05\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11194: Policy loss: 0.585790. Value loss: 27.442352. Entropy: 0.501643.\n",
      "Iteration 11195: Policy loss: 0.556374. Value loss: 18.215508. Entropy: 0.518093.\n",
      "Iteration 11196: Policy loss: 0.607547. Value loss: 13.238728. Entropy: 0.488189.\n",
      "episode: 4758   score: 475.0  epsilon: 1.0    steps: 186  evaluation reward: 287.4\n",
      "episode: 4759   score: 560.0  epsilon: 1.0    steps: 877  evaluation reward: 290.4\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11197: Policy loss: 2.013706. Value loss: 38.096931. Entropy: 0.509183.\n",
      "Iteration 11198: Policy loss: 2.026772. Value loss: 27.140135. Entropy: 0.509217.\n",
      "Iteration 11199: Policy loss: 2.182397. Value loss: 19.846655. Entropy: 0.519939.\n",
      "episode: 4760   score: 215.0  epsilon: 1.0    steps: 445  evaluation reward: 289.95\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11200: Policy loss: 1.470155. Value loss: 35.959774. Entropy: 0.410302.\n",
      "Iteration 11201: Policy loss: 2.172469. Value loss: 21.695950. Entropy: 0.437574.\n",
      "Iteration 11202: Policy loss: 1.659807. Value loss: 18.217852. Entropy: 0.439212.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11203: Policy loss: 0.893862. Value loss: 31.245403. Entropy: 0.472193.\n",
      "Iteration 11204: Policy loss: 0.792327. Value loss: 15.144770. Entropy: 0.462151.\n",
      "Iteration 11205: Policy loss: 0.855854. Value loss: 12.137959. Entropy: 0.472753.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11206: Policy loss: 0.956176. Value loss: 16.106123. Entropy: 0.295598.\n",
      "Iteration 11207: Policy loss: 0.907911. Value loss: 9.703535. Entropy: 0.302345.\n",
      "Iteration 11208: Policy loss: 1.039541. Value loss: 6.995602. Entropy: 0.301435.\n",
      "episode: 4761   score: 215.0  epsilon: 1.0    steps: 14  evaluation reward: 289.5\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11209: Policy loss: 1.865135. Value loss: 25.109140. Entropy: 0.435240.\n",
      "Iteration 11210: Policy loss: 1.805291. Value loss: 11.458300. Entropy: 0.443939.\n",
      "Iteration 11211: Policy loss: 1.710808. Value loss: 9.614060. Entropy: 0.444157.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11212: Policy loss: -0.737702. Value loss: 24.608427. Entropy: 0.618154.\n",
      "Iteration 11213: Policy loss: -0.534783. Value loss: 9.680315. Entropy: 0.591717.\n",
      "Iteration 11214: Policy loss: -0.525334. Value loss: 6.671793. Entropy: 0.623664.\n",
      "episode: 4762   score: 380.0  epsilon: 1.0    steps: 629  evaluation reward: 290.7\n",
      "episode: 4763   score: 450.0  epsilon: 1.0    steps: 702  evaluation reward: 292.8\n",
      "episode: 4764   score: 260.0  epsilon: 1.0    steps: 888  evaluation reward: 293.25\n",
      "episode: 4765   score: 335.0  epsilon: 1.0    steps: 997  evaluation reward: 293.95\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11215: Policy loss: -0.257589. Value loss: 25.910507. Entropy: 0.444997.\n",
      "Iteration 11216: Policy loss: -0.158523. Value loss: 18.030373. Entropy: 0.471995.\n",
      "Iteration 11217: Policy loss: -0.169996. Value loss: 16.276108. Entropy: 0.470297.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11218: Policy loss: -0.454140. Value loss: 31.587603. Entropy: 0.629170.\n",
      "Iteration 11219: Policy loss: -0.753373. Value loss: 17.160679. Entropy: 0.614913.\n",
      "Iteration 11220: Policy loss: -0.574772. Value loss: 13.417452. Entropy: 0.601030.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11221: Policy loss: -0.117662. Value loss: 150.521881. Entropy: 0.511458.\n",
      "Iteration 11222: Policy loss: 0.252434. Value loss: 71.175285. Entropy: 0.516596.\n",
      "Iteration 11223: Policy loss: -0.218526. Value loss: 104.685501. Entropy: 0.514077.\n",
      "episode: 4766   score: 435.0  epsilon: 1.0    steps: 373  evaluation reward: 295.45\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11224: Policy loss: -5.248265. Value loss: 297.379913. Entropy: 0.614355.\n",
      "Iteration 11225: Policy loss: -4.984869. Value loss: 153.550735. Entropy: 0.620334.\n",
      "Iteration 11226: Policy loss: -4.776292. Value loss: 73.076492. Entropy: 0.628151.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11227: Policy loss: 0.935647. Value loss: 21.761044. Entropy: 0.539842.\n",
      "Iteration 11228: Policy loss: 0.860963. Value loss: 12.467605. Entropy: 0.547424.\n",
      "Iteration 11229: Policy loss: 0.692260. Value loss: 10.042362. Entropy: 0.553502.\n",
      "episode: 4767   score: 650.0  epsilon: 1.0    steps: 181  evaluation reward: 299.85\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11230: Policy loss: 0.772836. Value loss: 35.589592. Entropy: 0.499948.\n",
      "Iteration 11231: Policy loss: 0.451708. Value loss: 21.057302. Entropy: 0.486436.\n",
      "Iteration 11232: Policy loss: 0.789184. Value loss: 16.659903. Entropy: 0.485667.\n",
      "episode: 4768   score: 495.0  epsilon: 1.0    steps: 410  evaluation reward: 301.9\n",
      "episode: 4769   score: 260.0  epsilon: 1.0    steps: 895  evaluation reward: 302.25\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11233: Policy loss: 1.613302. Value loss: 30.195421. Entropy: 0.237319.\n",
      "Iteration 11234: Policy loss: 1.268696. Value loss: 15.617963. Entropy: 0.233450.\n",
      "Iteration 11235: Policy loss: 1.307052. Value loss: 12.711920. Entropy: 0.245053.\n",
      "episode: 4770   score: 430.0  epsilon: 1.0    steps: 903  evaluation reward: 304.3\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11236: Policy loss: 1.393276. Value loss: 22.761402. Entropy: 0.389484.\n",
      "Iteration 11237: Policy loss: 1.536347. Value loss: 13.612527. Entropy: 0.389817.\n",
      "Iteration 11238: Policy loss: 1.378417. Value loss: 10.735767. Entropy: 0.382129.\n",
      "episode: 4771   score: 300.0  epsilon: 1.0    steps: 546  evaluation reward: 303.85\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11239: Policy loss: 2.212260. Value loss: 26.186054. Entropy: 0.347802.\n",
      "Iteration 11240: Policy loss: 2.064837. Value loss: 14.883877. Entropy: 0.359620.\n",
      "Iteration 11241: Policy loss: 2.342815. Value loss: 12.174490. Entropy: 0.361893.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11242: Policy loss: 2.157695. Value loss: 28.631512. Entropy: 0.501917.\n",
      "Iteration 11243: Policy loss: 2.170815. Value loss: 12.575460. Entropy: 0.498960.\n",
      "Iteration 11244: Policy loss: 2.165078. Value loss: 11.599017. Entropy: 0.502566.\n",
      "episode: 4772   score: 260.0  epsilon: 1.0    steps: 338  evaluation reward: 303.85\n",
      "episode: 4773   score: 625.0  epsilon: 1.0    steps: 650  evaluation reward: 308.0\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11245: Policy loss: 1.067674. Value loss: 35.104191. Entropy: 0.605245.\n",
      "Iteration 11246: Policy loss: 1.272008. Value loss: 21.973137. Entropy: 0.597901.\n",
      "Iteration 11247: Policy loss: 0.922108. Value loss: 15.923874. Entropy: 0.580428.\n",
      "episode: 4774   score: 325.0  epsilon: 1.0    steps: 127  evaluation reward: 308.4\n",
      "episode: 4775   score: 315.0  epsilon: 1.0    steps: 186  evaluation reward: 309.75\n",
      "episode: 4776   score: 290.0  epsilon: 1.0    steps: 505  evaluation reward: 310.45\n",
      "Training network. lr: 0.000164. clip: 0.065597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11248: Policy loss: 1.459166. Value loss: 40.728252. Entropy: 0.404712.\n",
      "Iteration 11249: Policy loss: 1.645642. Value loss: 22.563931. Entropy: 0.421309.\n",
      "Iteration 11250: Policy loss: 1.601671. Value loss: 18.632971. Entropy: 0.416867.\n",
      "episode: 4777   score: 225.0  epsilon: 1.0    steps: 812  evaluation reward: 310.6\n",
      "episode: 4778   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 311.35\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11251: Policy loss: 0.724339. Value loss: 18.209871. Entropy: 0.331650.\n",
      "Iteration 11252: Policy loss: 0.795269. Value loss: 12.981327. Entropy: 0.313839.\n",
      "Iteration 11253: Policy loss: 0.715576. Value loss: 12.544946. Entropy: 0.319383.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11254: Policy loss: -0.477783. Value loss: 13.340063. Entropy: 0.452925.\n",
      "Iteration 11255: Policy loss: -0.459740. Value loss: 9.029248. Entropy: 0.473532.\n",
      "Iteration 11256: Policy loss: -0.470379. Value loss: 7.505478. Entropy: 0.449695.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11257: Policy loss: -0.110535. Value loss: 24.016140. Entropy: 0.536591.\n",
      "Iteration 11258: Policy loss: -0.155671. Value loss: 11.813064. Entropy: 0.533423.\n",
      "Iteration 11259: Policy loss: -0.183164. Value loss: 10.135518. Entropy: 0.521043.\n",
      "episode: 4779   score: 310.0  epsilon: 1.0    steps: 545  evaluation reward: 310.25\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11260: Policy loss: 1.313034. Value loss: 24.706215. Entropy: 0.499893.\n",
      "Iteration 11261: Policy loss: 1.176304. Value loss: 14.674078. Entropy: 0.514443.\n",
      "Iteration 11262: Policy loss: 1.076893. Value loss: 9.823668. Entropy: 0.499173.\n",
      "episode: 4780   score: 285.0  epsilon: 1.0    steps: 337  evaluation reward: 310.6\n",
      "episode: 4781   score: 240.0  epsilon: 1.0    steps: 681  evaluation reward: 310.3\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11263: Policy loss: -0.882969. Value loss: 20.804396. Entropy: 0.265842.\n",
      "Iteration 11264: Policy loss: -0.691281. Value loss: 14.400205. Entropy: 0.299687.\n",
      "Iteration 11265: Policy loss: -0.807719. Value loss: 12.392498. Entropy: 0.296540.\n",
      "episode: 4782   score: 240.0  epsilon: 1.0    steps: 129  evaluation reward: 311.15\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11266: Policy loss: 0.483084. Value loss: 18.070112. Entropy: 0.508917.\n",
      "Iteration 11267: Policy loss: 0.478709. Value loss: 9.584545. Entropy: 0.516166.\n",
      "Iteration 11268: Policy loss: 0.463898. Value loss: 8.692627. Entropy: 0.508453.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11269: Policy loss: 1.060575. Value loss: 34.898010. Entropy: 0.548206.\n",
      "Iteration 11270: Policy loss: 0.884742. Value loss: 19.822721. Entropy: 0.537534.\n",
      "Iteration 11271: Policy loss: 0.692806. Value loss: 17.082272. Entropy: 0.530209.\n",
      "episode: 4783   score: 365.0  epsilon: 1.0    steps: 30  evaluation reward: 312.4\n",
      "episode: 4784   score: 335.0  epsilon: 1.0    steps: 448  evaluation reward: 314.4\n",
      "episode: 4785   score: 300.0  epsilon: 1.0    steps: 878  evaluation reward: 313.0\n",
      "episode: 4786   score: 290.0  epsilon: 1.0    steps: 994  evaluation reward: 313.3\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11272: Policy loss: 1.454888. Value loss: 24.008533. Entropy: 0.424637.\n",
      "Iteration 11273: Policy loss: 1.782895. Value loss: 16.588366. Entropy: 0.424160.\n",
      "Iteration 11274: Policy loss: 1.560924. Value loss: 16.313736. Entropy: 0.437444.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11275: Policy loss: 0.505820. Value loss: 14.284159. Entropy: 0.425884.\n",
      "Iteration 11276: Policy loss: 0.236484. Value loss: 12.141057. Entropy: 0.423935.\n",
      "Iteration 11277: Policy loss: 0.450339. Value loss: 8.758950. Entropy: 0.413935.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11278: Policy loss: 0.039702. Value loss: 19.553011. Entropy: 0.432292.\n",
      "Iteration 11279: Policy loss: -0.004670. Value loss: 11.948351. Entropy: 0.441663.\n",
      "Iteration 11280: Policy loss: 0.070105. Value loss: 10.736954. Entropy: 0.422912.\n",
      "episode: 4787   score: 260.0  epsilon: 1.0    steps: 363  evaluation reward: 314.1\n",
      "episode: 4788   score: 260.0  epsilon: 1.0    steps: 673  evaluation reward: 314.3\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11281: Policy loss: 1.238408. Value loss: 13.912995. Entropy: 0.415583.\n",
      "Iteration 11282: Policy loss: 1.130094. Value loss: 8.499517. Entropy: 0.424649.\n",
      "Iteration 11283: Policy loss: 1.089008. Value loss: 8.592111. Entropy: 0.431396.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11284: Policy loss: 0.273515. Value loss: 22.270514. Entropy: 0.615866.\n",
      "Iteration 11285: Policy loss: 0.126253. Value loss: 12.791374. Entropy: 0.612235.\n",
      "Iteration 11286: Policy loss: 0.314291. Value loss: 8.612192. Entropy: 0.636601.\n",
      "episode: 4789   score: 290.0  epsilon: 1.0    steps: 558  evaluation reward: 314.75\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11287: Policy loss: -0.033585. Value loss: 19.234270. Entropy: 0.338859.\n",
      "Iteration 11288: Policy loss: -0.017005. Value loss: 11.393083. Entropy: 0.392348.\n",
      "Iteration 11289: Policy loss: 0.313265. Value loss: 9.123989. Entropy: 0.372415.\n",
      "episode: 4790   score: 265.0  epsilon: 1.0    steps: 111  evaluation reward: 314.55\n",
      "episode: 4791   score: 370.0  epsilon: 1.0    steps: 179  evaluation reward: 315.4\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11290: Policy loss: 1.574285. Value loss: 34.017479. Entropy: 0.651072.\n",
      "Iteration 11291: Policy loss: 1.734920. Value loss: 18.775009. Entropy: 0.672156.\n",
      "Iteration 11292: Policy loss: 1.713015. Value loss: 16.820204. Entropy: 0.690139.\n",
      "episode: 4792   score: 310.0  epsilon: 1.0    steps: 491  evaluation reward: 315.9\n",
      "episode: 4793   score: 275.0  epsilon: 1.0    steps: 813  evaluation reward: 314.75\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11293: Policy loss: -0.440964. Value loss: 17.069868. Entropy: 0.372158.\n",
      "Iteration 11294: Policy loss: -0.497386. Value loss: 13.731337. Entropy: 0.381164.\n",
      "Iteration 11295: Policy loss: -0.502432. Value loss: 12.091751. Entropy: 0.393687.\n",
      "episode: 4794   score: 210.0  epsilon: 1.0    steps: 700  evaluation reward: 314.25\n",
      "episode: 4795   score: 335.0  epsilon: 1.0    steps: 917  evaluation reward: 314.65\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11296: Policy loss: 0.540126. Value loss: 17.993616. Entropy: 0.426756.\n",
      "Iteration 11297: Policy loss: 0.519617. Value loss: 13.885334. Entropy: 0.418600.\n",
      "Iteration 11298: Policy loss: 0.484154. Value loss: 12.257467. Entropy: 0.433384.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11299: Policy loss: 2.477329. Value loss: 20.058386. Entropy: 0.423874.\n",
      "Iteration 11300: Policy loss: 2.586249. Value loss: 15.730438. Entropy: 0.414907.\n",
      "Iteration 11301: Policy loss: 2.134545. Value loss: 12.414087. Entropy: 0.413815.\n",
      "episode: 4796   score: 275.0  epsilon: 1.0    steps: 372  evaluation reward: 314.35\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11302: Policy loss: 1.961154. Value loss: 15.545913. Entropy: 0.508732.\n",
      "Iteration 11303: Policy loss: 1.781451. Value loss: 15.622146. Entropy: 0.503284.\n",
      "Iteration 11304: Policy loss: 1.751728. Value loss: 7.223598. Entropy: 0.528860.\n",
      "episode: 4797   score: 240.0  epsilon: 1.0    steps: 602  evaluation reward: 314.35\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11305: Policy loss: -0.263216. Value loss: 13.614806. Entropy: 0.423970.\n",
      "Iteration 11306: Policy loss: -0.458208. Value loss: 9.949395. Entropy: 0.400967.\n",
      "Iteration 11307: Policy loss: -0.301649. Value loss: 7.484448. Entropy: 0.416169.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11308: Policy loss: 1.753696. Value loss: 14.658851. Entropy: 0.545290.\n",
      "Iteration 11309: Policy loss: 1.562595. Value loss: 9.219730. Entropy: 0.502087.\n",
      "Iteration 11310: Policy loss: 1.629367. Value loss: 7.917214. Entropy: 0.531884.\n",
      "episode: 4798   score: 265.0  epsilon: 1.0    steps: 480  evaluation reward: 312.9\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11311: Policy loss: -0.894697. Value loss: 24.729305. Entropy: 0.453828.\n",
      "Iteration 11312: Policy loss: -0.769039. Value loss: 14.271070. Entropy: 0.457553.\n",
      "Iteration 11313: Policy loss: -0.990026. Value loss: 11.464900. Entropy: 0.476991.\n",
      "episode: 4799   score: 340.0  epsilon: 1.0    steps: 25  evaluation reward: 313.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4800   score: 320.0  epsilon: 1.0    steps: 181  evaluation reward: 312.6\n",
      "now time :  2019-02-25 22:11:28.390336\n",
      "episode: 4801   score: 240.0  epsilon: 1.0    steps: 919  evaluation reward: 312.35\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11314: Policy loss: -1.967307. Value loss: 24.109856. Entropy: 0.619472.\n",
      "Iteration 11315: Policy loss: -2.155925. Value loss: 15.693100. Entropy: 0.623289.\n",
      "Iteration 11316: Policy loss: -2.024253. Value loss: 15.805676. Entropy: 0.599935.\n",
      "episode: 4802   score: 300.0  epsilon: 1.0    steps: 699  evaluation reward: 313.1\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11317: Policy loss: -0.878541. Value loss: 179.647614. Entropy: 0.453328.\n",
      "Iteration 11318: Policy loss: -0.814026. Value loss: 107.057465. Entropy: 0.434457.\n",
      "Iteration 11319: Policy loss: -0.668838. Value loss: 68.888710. Entropy: 0.416575.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11320: Policy loss: 0.249923. Value loss: 27.953888. Entropy: 0.536874.\n",
      "Iteration 11321: Policy loss: 0.422009. Value loss: 16.352556. Entropy: 0.537834.\n",
      "Iteration 11322: Policy loss: 0.573472. Value loss: 12.112190. Entropy: 0.569850.\n",
      "episode: 4803   score: 505.0  epsilon: 1.0    steps: 345  evaluation reward: 314.75\n",
      "episode: 4804   score: 450.0  epsilon: 1.0    steps: 843  evaluation reward: 316.65\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11323: Policy loss: -4.051924. Value loss: 531.538147. Entropy: 0.561315.\n",
      "Iteration 11324: Policy loss: -3.465800. Value loss: 431.812256. Entropy: 0.518865.\n",
      "Iteration 11325: Policy loss: -4.312433. Value loss: 408.717804. Entropy: 0.564229.\n",
      "episode: 4805   score: 260.0  epsilon: 1.0    steps: 517  evaluation reward: 314.25\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11326: Policy loss: 1.556701. Value loss: 56.754719. Entropy: 0.377346.\n",
      "Iteration 11327: Policy loss: 1.399905. Value loss: 27.833069. Entropy: 0.386368.\n",
      "Iteration 11328: Policy loss: 1.504121. Value loss: 22.067139. Entropy: 0.401507.\n",
      "episode: 4806   score: 465.0  epsilon: 1.0    steps: 492  evaluation reward: 316.8\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11329: Policy loss: 0.995573. Value loss: 31.905010. Entropy: 0.331678.\n",
      "Iteration 11330: Policy loss: 1.222975. Value loss: 17.776361. Entropy: 0.329410.\n",
      "Iteration 11331: Policy loss: 1.092660. Value loss: 16.003244. Entropy: 0.345062.\n",
      "episode: 4807   score: 210.0  epsilon: 1.0    steps: 710  evaluation reward: 316.0\n",
      "episode: 4808   score: 520.0  epsilon: 1.0    steps: 1003  evaluation reward: 318.6\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11332: Policy loss: -0.235644. Value loss: 44.437233. Entropy: 0.589552.\n",
      "Iteration 11333: Policy loss: 0.164984. Value loss: 26.105772. Entropy: 0.614639.\n",
      "Iteration 11334: Policy loss: -0.084846. Value loss: 21.032906. Entropy: 0.635623.\n",
      "episode: 4809   score: 485.0  epsilon: 1.0    steps: 27  evaluation reward: 321.05\n",
      "episode: 4810   score: 300.0  epsilon: 1.0    steps: 218  evaluation reward: 321.2\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11335: Policy loss: 3.860373. Value loss: 136.769989. Entropy: 0.500666.\n",
      "Iteration 11336: Policy loss: 3.496356. Value loss: 92.222054. Entropy: 0.457607.\n",
      "Iteration 11337: Policy loss: 4.123960. Value loss: 45.088135. Entropy: 0.498856.\n",
      "episode: 4811   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 320.9\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11338: Policy loss: 2.783268. Value loss: 46.230961. Entropy: 0.449727.\n",
      "Iteration 11339: Policy loss: 2.149404. Value loss: 25.386267. Entropy: 0.453097.\n",
      "Iteration 11340: Policy loss: 2.342301. Value loss: 24.966831. Entropy: 0.452204.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11341: Policy loss: 0.417196. Value loss: 31.571251. Entropy: 0.623745.\n",
      "Iteration 11342: Policy loss: 0.401420. Value loss: 17.815178. Entropy: 0.618570.\n",
      "Iteration 11343: Policy loss: 0.481855. Value loss: 13.995312. Entropy: 0.615471.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11344: Policy loss: 1.427513. Value loss: 26.156422. Entropy: 0.472627.\n",
      "Iteration 11345: Policy loss: 1.143275. Value loss: 12.712443. Entropy: 0.460508.\n",
      "Iteration 11346: Policy loss: 1.410070. Value loss: 11.856278. Entropy: 0.474153.\n",
      "episode: 4812   score: 210.0  epsilon: 1.0    steps: 743  evaluation reward: 319.7\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11347: Policy loss: 1.774277. Value loss: 24.482559. Entropy: 0.664906.\n",
      "Iteration 11348: Policy loss: 1.953496. Value loss: 10.759827. Entropy: 0.651369.\n",
      "Iteration 11349: Policy loss: 1.688172. Value loss: 10.908355. Entropy: 0.657774.\n",
      "episode: 4813   score: 210.0  epsilon: 1.0    steps: 74  evaluation reward: 318.25\n",
      "episode: 4814   score: 275.0  epsilon: 1.0    steps: 429  evaluation reward: 318.55\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11350: Policy loss: -0.177375. Value loss: 29.753633. Entropy: 0.569945.\n",
      "Iteration 11351: Policy loss: 0.116973. Value loss: 20.130081. Entropy: 0.547009.\n",
      "Iteration 11352: Policy loss: 0.153614. Value loss: 17.093079. Entropy: 0.583784.\n",
      "episode: 4815   score: 310.0  epsilon: 1.0    steps: 982  evaluation reward: 316.95\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11353: Policy loss: 1.590475. Value loss: 32.079975. Entropy: 0.523431.\n",
      "Iteration 11354: Policy loss: 1.332905. Value loss: 20.733427. Entropy: 0.539223.\n",
      "Iteration 11355: Policy loss: 1.407354. Value loss: 16.320433. Entropy: 0.532654.\n",
      "episode: 4816   score: 435.0  epsilon: 1.0    steps: 619  evaluation reward: 319.2\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11356: Policy loss: -0.386478. Value loss: 28.066519. Entropy: 0.671121.\n",
      "Iteration 11357: Policy loss: -0.116472. Value loss: 14.372274. Entropy: 0.672006.\n",
      "Iteration 11358: Policy loss: -0.117505. Value loss: 13.530389. Entropy: 0.661951.\n",
      "episode: 4817   score: 285.0  epsilon: 1.0    steps: 289  evaluation reward: 318.1\n",
      "episode: 4818   score: 650.0  epsilon: 1.0    steps: 882  evaluation reward: 322.2\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11359: Policy loss: 0.201314. Value loss: 10.448811. Entropy: 0.609567.\n",
      "Iteration 11360: Policy loss: 0.199069. Value loss: 7.352451. Entropy: 0.608230.\n",
      "Iteration 11361: Policy loss: -0.003706. Value loss: 7.107457. Entropy: 0.620349.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11362: Policy loss: -0.939306. Value loss: 15.634850. Entropy: 0.399783.\n",
      "Iteration 11363: Policy loss: -1.212203. Value loss: 8.350581. Entropy: 0.428506.\n",
      "Iteration 11364: Policy loss: -0.985817. Value loss: 8.423170. Entropy: 0.421327.\n",
      "episode: 4819   score: 390.0  epsilon: 1.0    steps: 221  evaluation reward: 323.45\n",
      "episode: 4820   score: 290.0  epsilon: 1.0    steps: 762  evaluation reward: 323.75\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11365: Policy loss: -0.243979. Value loss: 23.358538. Entropy: 0.360634.\n",
      "Iteration 11366: Policy loss: -0.131551. Value loss: 12.176048. Entropy: 0.391535.\n",
      "Iteration 11367: Policy loss: -0.383605. Value loss: 11.475032. Entropy: 0.368548.\n",
      "episode: 4821   score: 240.0  epsilon: 1.0    steps: 406  evaluation reward: 323.6\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11368: Policy loss: 2.632372. Value loss: 21.051840. Entropy: 0.535019.\n",
      "Iteration 11369: Policy loss: 2.462434. Value loss: 12.885633. Entropy: 0.534662.\n",
      "Iteration 11370: Policy loss: 2.394073. Value loss: 9.345317. Entropy: 0.541227.\n",
      "episode: 4822   score: 260.0  epsilon: 1.0    steps: 25  evaluation reward: 324.1\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11371: Policy loss: 2.726871. Value loss: 39.869827. Entropy: 0.550579.\n",
      "Iteration 11372: Policy loss: 2.641031. Value loss: 23.214777. Entropy: 0.535808.\n",
      "Iteration 11373: Policy loss: 2.776235. Value loss: 20.751486. Entropy: 0.542709.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11374: Policy loss: -1.109819. Value loss: 29.250084. Entropy: 0.462520.\n",
      "Iteration 11375: Policy loss: -0.922987. Value loss: 15.903406. Entropy: 0.505358.\n",
      "Iteration 11376: Policy loss: -1.279328. Value loss: 11.860401. Entropy: 0.504445.\n",
      "episode: 4823   score: 180.0  epsilon: 1.0    steps: 226  evaluation reward: 323.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4824   score: 290.0  epsilon: 1.0    steps: 623  evaluation reward: 323.15\n",
      "episode: 4825   score: 210.0  epsilon: 1.0    steps: 835  evaluation reward: 323.15\n",
      "episode: 4826   score: 390.0  epsilon: 1.0    steps: 976  evaluation reward: 324.95\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11377: Policy loss: 1.908059. Value loss: 25.104593. Entropy: 0.680909.\n",
      "Iteration 11378: Policy loss: 1.847762. Value loss: 16.348944. Entropy: 0.655436.\n",
      "Iteration 11379: Policy loss: 1.514973. Value loss: 15.768240. Entropy: 0.661384.\n",
      "episode: 4827   score: 295.0  epsilon: 1.0    steps: 266  evaluation reward: 325.5\n",
      "episode: 4828   score: 210.0  epsilon: 1.0    steps: 475  evaluation reward: 323.4\n",
      "episode: 4829   score: 210.0  epsilon: 1.0    steps: 762  evaluation reward: 322.0\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11380: Policy loss: -4.137631. Value loss: 205.536255. Entropy: 0.592057.\n",
      "Iteration 11381: Policy loss: -3.332377. Value loss: 97.507156. Entropy: 0.549240.\n",
      "Iteration 11382: Policy loss: -4.008813. Value loss: 82.883163. Entropy: 0.541207.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11383: Policy loss: -1.434276. Value loss: 21.265743. Entropy: 0.365510.\n",
      "Iteration 11384: Policy loss: -1.224382. Value loss: 17.491751. Entropy: 0.384746.\n",
      "Iteration 11385: Policy loss: -1.398228. Value loss: 14.953719. Entropy: 0.377110.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11386: Policy loss: -1.598722. Value loss: 213.101410. Entropy: 0.667274.\n",
      "Iteration 11387: Policy loss: -1.671044. Value loss: 67.921753. Entropy: 0.663454.\n",
      "Iteration 11388: Policy loss: -1.537743. Value loss: 32.073765. Entropy: 0.632195.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11389: Policy loss: -1.723394. Value loss: 282.653137. Entropy: 0.559259.\n",
      "Iteration 11390: Policy loss: -1.268317. Value loss: 127.556389. Entropy: 0.539445.\n",
      "Iteration 11391: Policy loss: -1.502412. Value loss: 117.934273. Entropy: 0.522527.\n",
      "episode: 4830   score: 490.0  epsilon: 1.0    steps: 82  evaluation reward: 320.2\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11392: Policy loss: 0.239061. Value loss: 194.595673. Entropy: 0.345509.\n",
      "Iteration 11393: Policy loss: -0.004478. Value loss: 161.990173. Entropy: 0.336950.\n",
      "Iteration 11394: Policy loss: 0.169690. Value loss: 107.005043. Entropy: 0.305604.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11395: Policy loss: 1.170445. Value loss: 46.461243. Entropy: 0.520729.\n",
      "Iteration 11396: Policy loss: 1.404058. Value loss: 29.871475. Entropy: 0.530757.\n",
      "Iteration 11397: Policy loss: 1.267416. Value loss: 23.541801. Entropy: 0.514787.\n",
      "episode: 4831   score: 285.0  epsilon: 1.0    steps: 805  evaluation reward: 318.7\n",
      "episode: 4832   score: 285.0  epsilon: 1.0    steps: 922  evaluation reward: 318.55\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11398: Policy loss: 3.524873. Value loss: 71.697838. Entropy: 0.568689.\n",
      "Iteration 11399: Policy loss: 3.669316. Value loss: 37.971001. Entropy: 0.568122.\n",
      "Iteration 11400: Policy loss: 3.663391. Value loss: 29.986113. Entropy: 0.560071.\n",
      "episode: 4833   score: 240.0  epsilon: 1.0    steps: 444  evaluation reward: 318.35\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11401: Policy loss: 4.039874. Value loss: 56.178638. Entropy: 0.479922.\n",
      "Iteration 11402: Policy loss: 3.976646. Value loss: 26.328384. Entropy: 0.492630.\n",
      "Iteration 11403: Policy loss: 3.861786. Value loss: 20.019648. Entropy: 0.499745.\n",
      "episode: 4834   score: 390.0  epsilon: 1.0    steps: 766  evaluation reward: 319.65\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11404: Policy loss: 1.854845. Value loss: 43.962997. Entropy: 0.589665.\n",
      "Iteration 11405: Policy loss: 1.857010. Value loss: 22.488031. Entropy: 0.589405.\n",
      "Iteration 11406: Policy loss: 1.902535. Value loss: 20.899645. Entropy: 0.590559.\n",
      "episode: 4835   score: 240.0  epsilon: 1.0    steps: 8  evaluation reward: 319.4\n",
      "episode: 4836   score: 500.0  epsilon: 1.0    steps: 593  evaluation reward: 321.0\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11407: Policy loss: 1.763317. Value loss: 33.612995. Entropy: 0.534556.\n",
      "Iteration 11408: Policy loss: 1.680533. Value loss: 22.940380. Entropy: 0.535957.\n",
      "Iteration 11409: Policy loss: 1.782721. Value loss: 19.070633. Entropy: 0.540189.\n",
      "episode: 4837   score: 645.0  epsilon: 1.0    steps: 138  evaluation reward: 324.4\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11410: Policy loss: 1.645659. Value loss: 29.635841. Entropy: 0.379559.\n",
      "Iteration 11411: Policy loss: 1.562402. Value loss: 18.844290. Entropy: 0.372568.\n",
      "Iteration 11412: Policy loss: 1.618383. Value loss: 12.117014. Entropy: 0.376546.\n",
      "episode: 4838   score: 635.0  epsilon: 1.0    steps: 271  evaluation reward: 328.15\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11413: Policy loss: 1.283174. Value loss: 18.852718. Entropy: 0.285017.\n",
      "Iteration 11414: Policy loss: 1.057145. Value loss: 12.457263. Entropy: 0.313365.\n",
      "Iteration 11415: Policy loss: 1.138525. Value loss: 9.924258. Entropy: 0.323211.\n",
      "episode: 4839   score: 245.0  epsilon: 1.0    steps: 831  evaluation reward: 328.2\n",
      "episode: 4840   score: 255.0  epsilon: 1.0    steps: 983  evaluation reward: 328.15\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11416: Policy loss: 3.346527. Value loss: 32.319092. Entropy: 0.357454.\n",
      "Iteration 11417: Policy loss: 3.360387. Value loss: 18.095112. Entropy: 0.352997.\n",
      "Iteration 11418: Policy loss: 3.475970. Value loss: 13.828967. Entropy: 0.383235.\n",
      "episode: 4841   score: 240.0  epsilon: 1.0    steps: 425  evaluation reward: 327.95\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11419: Policy loss: -1.102871. Value loss: 157.219498. Entropy: 0.391155.\n",
      "Iteration 11420: Policy loss: -1.640868. Value loss: 68.203018. Entropy: 0.418852.\n",
      "Iteration 11421: Policy loss: -1.799285. Value loss: 47.375568. Entropy: 0.407939.\n",
      "episode: 4842   score: 210.0  epsilon: 1.0    steps: 22  evaluation reward: 326.55\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11422: Policy loss: 1.231340. Value loss: 34.658936. Entropy: 0.339152.\n",
      "Iteration 11423: Policy loss: 1.225978. Value loss: 23.545433. Entropy: 0.368830.\n",
      "Iteration 11424: Policy loss: 0.980721. Value loss: 17.026384. Entropy: 0.350285.\n",
      "episode: 4843   score: 240.0  epsilon: 1.0    steps: 553  evaluation reward: 326.85\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11425: Policy loss: 0.379639. Value loss: 23.816175. Entropy: 0.369893.\n",
      "Iteration 11426: Policy loss: 0.546537. Value loss: 14.992625. Entropy: 0.381272.\n",
      "Iteration 11427: Policy loss: 0.473981. Value loss: 11.939437. Entropy: 0.365360.\n",
      "episode: 4844   score: 180.0  epsilon: 1.0    steps: 865  evaluation reward: 325.8\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11428: Policy loss: 0.421663. Value loss: 32.955238. Entropy: 0.378027.\n",
      "Iteration 11429: Policy loss: 0.184521. Value loss: 22.164351. Entropy: 0.378100.\n",
      "Iteration 11430: Policy loss: 0.261582. Value loss: 15.990649. Entropy: 0.380258.\n",
      "episode: 4845   score: 545.0  epsilon: 1.0    steps: 185  evaluation reward: 329.15\n",
      "episode: 4846   score: 290.0  epsilon: 1.0    steps: 340  evaluation reward: 328.3\n",
      "episode: 4847   score: 355.0  epsilon: 1.0    steps: 663  evaluation reward: 329.45\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11431: Policy loss: 1.717262. Value loss: 47.191216. Entropy: 0.313668.\n",
      "Iteration 11432: Policy loss: 1.578910. Value loss: 18.445992. Entropy: 0.302827.\n",
      "Iteration 11433: Policy loss: 1.656140. Value loss: 15.964177. Entropy: 0.302248.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11434: Policy loss: -0.286672. Value loss: 198.517914. Entropy: 0.322360.\n",
      "Iteration 11435: Policy loss: -1.056836. Value loss: 81.781563. Entropy: 0.329992.\n",
      "Iteration 11436: Policy loss: -0.965689. Value loss: 50.437176. Entropy: 0.341826.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11437: Policy loss: -1.873749. Value loss: 21.560190. Entropy: 0.377016.\n",
      "Iteration 11438: Policy loss: -1.987700. Value loss: 15.064555. Entropy: 0.356456.\n",
      "Iteration 11439: Policy loss: -2.009596. Value loss: 11.407696. Entropy: 0.373567.\n",
      "episode: 4848   score: 285.0  epsilon: 1.0    steps: 84  evaluation reward: 327.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4849   score: 360.0  epsilon: 1.0    steps: 898  evaluation reward: 327.45\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11440: Policy loss: 0.547131. Value loss: 35.242260. Entropy: 0.345711.\n",
      "Iteration 11441: Policy loss: 0.592401. Value loss: 21.431862. Entropy: 0.326444.\n",
      "Iteration 11442: Policy loss: 0.581219. Value loss: 16.458286. Entropy: 0.324206.\n",
      "episode: 4850   score: 305.0  epsilon: 1.0    steps: 618  evaluation reward: 327.6\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11443: Policy loss: 2.428023. Value loss: 24.197075. Entropy: 0.415382.\n",
      "Iteration 11444: Policy loss: 2.153132. Value loss: 13.468915. Entropy: 0.436700.\n",
      "Iteration 11445: Policy loss: 2.313034. Value loss: 9.001195. Entropy: 0.443422.\n",
      "now time :  2019-02-25 22:13:55.159534\n",
      "episode: 4851   score: 590.0  epsilon: 1.0    steps: 386  evaluation reward: 330.9\n",
      "episode: 4852   score: 260.0  epsilon: 1.0    steps: 894  evaluation reward: 330.65\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11446: Policy loss: 0.953315. Value loss: 23.181021. Entropy: 0.501712.\n",
      "Iteration 11447: Policy loss: 0.942229. Value loss: 15.225076. Entropy: 0.484038.\n",
      "Iteration 11448: Policy loss: 1.049044. Value loss: 12.685670. Entropy: 0.495349.\n",
      "episode: 4853   score: 285.0  epsilon: 1.0    steps: 744  evaluation reward: 330.65\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11449: Policy loss: 1.496749. Value loss: 33.816730. Entropy: 0.606433.\n",
      "Iteration 11450: Policy loss: 1.231164. Value loss: 17.849058. Entropy: 0.610474.\n",
      "Iteration 11451: Policy loss: 1.196570. Value loss: 15.835890. Entropy: 0.615645.\n",
      "episode: 4854   score: 310.0  epsilon: 1.0    steps: 226  evaluation reward: 330.5\n",
      "episode: 4855   score: 335.0  epsilon: 1.0    steps: 329  evaluation reward: 330.75\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11452: Policy loss: -0.226499. Value loss: 35.153984. Entropy: 0.457903.\n",
      "Iteration 11453: Policy loss: -0.090688. Value loss: 21.790066. Entropy: 0.490653.\n",
      "Iteration 11454: Policy loss: -0.196102. Value loss: 16.806400. Entropy: 0.461261.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11455: Policy loss: -0.070470. Value loss: 14.526628. Entropy: 0.560745.\n",
      "Iteration 11456: Policy loss: -0.178565. Value loss: 9.920405. Entropy: 0.565337.\n",
      "Iteration 11457: Policy loss: -0.054585. Value loss: 8.343380. Entropy: 0.573548.\n",
      "episode: 4856   score: 260.0  epsilon: 1.0    steps: 997  evaluation reward: 329.2\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11458: Policy loss: 0.494848. Value loss: 26.680149. Entropy: 0.517325.\n",
      "Iteration 11459: Policy loss: 0.437134. Value loss: 16.883059. Entropy: 0.521462.\n",
      "Iteration 11460: Policy loss: 0.356863. Value loss: 12.396037. Entropy: 0.528923.\n",
      "episode: 4857   score: 260.0  epsilon: 1.0    steps: 506  evaluation reward: 329.7\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11461: Policy loss: 0.454473. Value loss: 14.354182. Entropy: 0.590118.\n",
      "Iteration 11462: Policy loss: 0.557433. Value loss: 10.030014. Entropy: 0.595319.\n",
      "Iteration 11463: Policy loss: 0.390047. Value loss: 8.039079. Entropy: 0.598103.\n",
      "episode: 4858   score: 300.0  epsilon: 1.0    steps: 563  evaluation reward: 327.95\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11464: Policy loss: 2.119975. Value loss: 18.481705. Entropy: 0.726065.\n",
      "Iteration 11465: Policy loss: 2.088436. Value loss: 10.288515. Entropy: 0.746630.\n",
      "Iteration 11466: Policy loss: 2.030913. Value loss: 9.991709. Entropy: 0.738967.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11467: Policy loss: -1.314714. Value loss: 33.197559. Entropy: 0.713316.\n",
      "Iteration 11468: Policy loss: -1.507813. Value loss: 18.428577. Entropy: 0.707743.\n",
      "Iteration 11469: Policy loss: -1.219290. Value loss: 11.314390. Entropy: 0.712470.\n",
      "episode: 4859   score: 305.0  epsilon: 1.0    steps: 33  evaluation reward: 325.4\n",
      "episode: 4860   score: 300.0  epsilon: 1.0    steps: 804  evaluation reward: 326.25\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11470: Policy loss: -0.816289. Value loss: 19.781479. Entropy: 0.565753.\n",
      "Iteration 11471: Policy loss: -0.801621. Value loss: 13.107217. Entropy: 0.579685.\n",
      "Iteration 11472: Policy loss: -0.831850. Value loss: 11.477932. Entropy: 0.602149.\n",
      "episode: 4861   score: 335.0  epsilon: 1.0    steps: 320  evaluation reward: 327.45\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11473: Policy loss: 2.970364. Value loss: 27.610302. Entropy: 0.509210.\n",
      "Iteration 11474: Policy loss: 3.076483. Value loss: 15.989624. Entropy: 0.520733.\n",
      "Iteration 11475: Policy loss: 2.930044. Value loss: 13.957731. Entropy: 0.510130.\n",
      "episode: 4862   score: 315.0  epsilon: 1.0    steps: 670  evaluation reward: 326.8\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11476: Policy loss: -0.555516. Value loss: 20.460569. Entropy: 0.494933.\n",
      "Iteration 11477: Policy loss: -0.474285. Value loss: 12.095863. Entropy: 0.487882.\n",
      "Iteration 11478: Policy loss: -0.556479. Value loss: 9.916605. Entropy: 0.477659.\n",
      "episode: 4863   score: 230.0  epsilon: 1.0    steps: 469  evaluation reward: 324.6\n",
      "episode: 4864   score: 260.0  epsilon: 1.0    steps: 928  evaluation reward: 324.6\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11479: Policy loss: -1.026872. Value loss: 24.636444. Entropy: 0.460421.\n",
      "Iteration 11480: Policy loss: -1.233087. Value loss: 14.870033. Entropy: 0.489949.\n",
      "Iteration 11481: Policy loss: -0.899837. Value loss: 12.137207. Entropy: 0.444284.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11482: Policy loss: 0.517859. Value loss: 23.881073. Entropy: 0.545486.\n",
      "Iteration 11483: Policy loss: 0.461630. Value loss: 13.431185. Entropy: 0.563491.\n",
      "Iteration 11484: Policy loss: 0.508501. Value loss: 10.462042. Entropy: 0.569235.\n",
      "episode: 4865   score: 455.0  epsilon: 1.0    steps: 227  evaluation reward: 325.8\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11485: Policy loss: 0.119252. Value loss: 29.655476. Entropy: 0.534342.\n",
      "Iteration 11486: Policy loss: 0.118252. Value loss: 15.018508. Entropy: 0.534086.\n",
      "Iteration 11487: Policy loss: 0.094526. Value loss: 12.767895. Entropy: 0.526882.\n",
      "episode: 4866   score: 260.0  epsilon: 1.0    steps: 806  evaluation reward: 324.05\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11488: Policy loss: -0.495346. Value loss: 157.403656. Entropy: 0.589619.\n",
      "Iteration 11489: Policy loss: -0.776771. Value loss: 107.742195. Entropy: 0.544909.\n",
      "Iteration 11490: Policy loss: -0.580113. Value loss: 82.890907. Entropy: 0.543476.\n",
      "episode: 4867   score: 350.0  epsilon: 1.0    steps: 128  evaluation reward: 321.05\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11491: Policy loss: 1.262448. Value loss: 47.577980. Entropy: 0.475019.\n",
      "Iteration 11492: Policy loss: 1.184158. Value loss: 22.450323. Entropy: 0.473589.\n",
      "Iteration 11493: Policy loss: 1.034136. Value loss: 17.701160. Entropy: 0.472416.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11494: Policy loss: 1.419402. Value loss: 35.651390. Entropy: 0.334274.\n",
      "Iteration 11495: Policy loss: 1.687995. Value loss: 18.304863. Entropy: 0.336001.\n",
      "Iteration 11496: Policy loss: 1.618699. Value loss: 14.227951. Entropy: 0.326742.\n",
      "episode: 4868   score: 180.0  epsilon: 1.0    steps: 256  evaluation reward: 317.9\n",
      "episode: 4869   score: 270.0  epsilon: 1.0    steps: 512  evaluation reward: 318.0\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11497: Policy loss: 1.732193. Value loss: 37.009899. Entropy: 0.598627.\n",
      "Iteration 11498: Policy loss: 1.721987. Value loss: 22.151402. Entropy: 0.599918.\n",
      "Iteration 11499: Policy loss: 1.930343. Value loss: 15.265875. Entropy: 0.624940.\n",
      "episode: 4870   score: 490.0  epsilon: 1.0    steps: 563  evaluation reward: 318.6\n",
      "episode: 4871   score: 565.0  epsilon: 1.0    steps: 941  evaluation reward: 321.25\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11500: Policy loss: 0.238659. Value loss: 14.142732. Entropy: 0.546047.\n",
      "Iteration 11501: Policy loss: 0.458779. Value loss: 9.439141. Entropy: 0.534945.\n",
      "Iteration 11502: Policy loss: 0.194596. Value loss: 9.703867. Entropy: 0.546522.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11503: Policy loss: 0.066521. Value loss: 25.524174. Entropy: 0.568352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11504: Policy loss: 0.124261. Value loss: 17.153952. Entropy: 0.593945.\n",
      "Iteration 11505: Policy loss: 0.102800. Value loss: 12.966431. Entropy: 0.598794.\n",
      "episode: 4872   score: 375.0  epsilon: 1.0    steps: 321  evaluation reward: 322.4\n",
      "episode: 4873   score: 475.0  epsilon: 1.0    steps: 753  evaluation reward: 320.9\n",
      "episode: 4874   score: 285.0  epsilon: 1.0    steps: 819  evaluation reward: 320.5\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11506: Policy loss: -1.167540. Value loss: 134.552628. Entropy: 0.405826.\n",
      "Iteration 11507: Policy loss: -1.174352. Value loss: 57.808075. Entropy: 0.429516.\n",
      "Iteration 11508: Policy loss: -1.184607. Value loss: 45.644672. Entropy: 0.456966.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11509: Policy loss: -0.647687. Value loss: 25.787439. Entropy: 0.474113.\n",
      "Iteration 11510: Policy loss: -0.906025. Value loss: 13.974319. Entropy: 0.477939.\n",
      "Iteration 11511: Policy loss: -0.660186. Value loss: 10.327871. Entropy: 0.470416.\n",
      "episode: 4875   score: 270.0  epsilon: 1.0    steps: 39  evaluation reward: 320.05\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11512: Policy loss: 1.685722. Value loss: 37.132442. Entropy: 0.320680.\n",
      "Iteration 11513: Policy loss: 1.810792. Value loss: 20.061472. Entropy: 0.314775.\n",
      "Iteration 11514: Policy loss: 1.738221. Value loss: 19.542152. Entropy: 0.310073.\n",
      "episode: 4876   score: 260.0  epsilon: 1.0    steps: 437  evaluation reward: 319.75\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11515: Policy loss: 2.266491. Value loss: 43.536224. Entropy: 0.548423.\n",
      "Iteration 11516: Policy loss: 1.946855. Value loss: 20.239243. Entropy: 0.572584.\n",
      "Iteration 11517: Policy loss: 2.125688. Value loss: 17.558464. Entropy: 0.578323.\n",
      "episode: 4877   score: 135.0  epsilon: 1.0    steps: 278  evaluation reward: 318.85\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11518: Policy loss: 1.819106. Value loss: 22.478725. Entropy: 0.572438.\n",
      "Iteration 11519: Policy loss: 1.942556. Value loss: 13.513295. Entropy: 0.558321.\n",
      "Iteration 11520: Policy loss: 1.875163. Value loss: 11.418742. Entropy: 0.574653.\n",
      "episode: 4878   score: 210.0  epsilon: 1.0    steps: 834  evaluation reward: 318.1\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11521: Policy loss: 1.025249. Value loss: 30.730406. Entropy: 0.503667.\n",
      "Iteration 11522: Policy loss: 0.952417. Value loss: 19.456745. Entropy: 0.516020.\n",
      "Iteration 11523: Policy loss: 1.202256. Value loss: 14.859604. Entropy: 0.525859.\n",
      "episode: 4879   score: 235.0  epsilon: 1.0    steps: 672  evaluation reward: 317.35\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11524: Policy loss: 0.249236. Value loss: 25.544701. Entropy: 0.617670.\n",
      "Iteration 11525: Policy loss: 0.287849. Value loss: 14.874619. Entropy: 0.625890.\n",
      "Iteration 11526: Policy loss: 0.224361. Value loss: 12.591358. Entropy: 0.641316.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11527: Policy loss: -0.332201. Value loss: 185.540070. Entropy: 0.682842.\n",
      "Iteration 11528: Policy loss: 0.107253. Value loss: 91.468300. Entropy: 0.690065.\n",
      "Iteration 11529: Policy loss: 0.049461. Value loss: 46.334610. Entropy: 0.677415.\n",
      "episode: 4880   score: 545.0  epsilon: 1.0    steps: 205  evaluation reward: 319.95\n",
      "episode: 4881   score: 210.0  epsilon: 1.0    steps: 262  evaluation reward: 319.65\n",
      "episode: 4882   score: 210.0  epsilon: 1.0    steps: 446  evaluation reward: 319.35\n",
      "episode: 4883   score: 455.0  epsilon: 1.0    steps: 566  evaluation reward: 320.25\n",
      "episode: 4884   score: 125.0  epsilon: 1.0    steps: 849  evaluation reward: 318.15\n",
      "episode: 4885   score: 590.0  epsilon: 1.0    steps: 989  evaluation reward: 321.05\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11530: Policy loss: 2.147326. Value loss: 28.329184. Entropy: 0.674256.\n",
      "Iteration 11531: Policy loss: 2.205023. Value loss: 21.981300. Entropy: 0.705515.\n",
      "Iteration 11532: Policy loss: 2.107612. Value loss: 16.848780. Entropy: 0.680660.\n",
      "episode: 4886   score: 505.0  epsilon: 1.0    steps: 75  evaluation reward: 323.2\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11533: Policy loss: 2.074024. Value loss: 23.755684. Entropy: 0.524294.\n",
      "Iteration 11534: Policy loss: 2.064300. Value loss: 20.606386. Entropy: 0.543379.\n",
      "Iteration 11535: Policy loss: 2.020432. Value loss: 16.946690. Entropy: 0.553980.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11536: Policy loss: -1.203108. Value loss: 26.986158. Entropy: 0.495199.\n",
      "Iteration 11537: Policy loss: -0.993332. Value loss: 17.107607. Entropy: 0.515291.\n",
      "Iteration 11538: Policy loss: -1.011726. Value loss: 13.990868. Entropy: 0.525304.\n",
      "episode: 4887   score: 210.0  epsilon: 1.0    steps: 678  evaluation reward: 322.7\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11539: Policy loss: 1.718721. Value loss: 36.304432. Entropy: 0.532181.\n",
      "Iteration 11540: Policy loss: 2.510868. Value loss: 20.859228. Entropy: 0.520292.\n",
      "Iteration 11541: Policy loss: 1.836379. Value loss: 16.498407. Entropy: 0.534219.\n",
      "episode: 4888   score: 225.0  epsilon: 1.0    steps: 366  evaluation reward: 322.35\n",
      "episode: 4889   score: 210.0  epsilon: 1.0    steps: 437  evaluation reward: 321.55\n",
      "episode: 4890   score: 180.0  epsilon: 1.0    steps: 547  evaluation reward: 320.7\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11542: Policy loss: 2.676372. Value loss: 22.448339. Entropy: 0.669038.\n",
      "Iteration 11543: Policy loss: 2.996490. Value loss: 16.406933. Entropy: 0.673932.\n",
      "Iteration 11544: Policy loss: 3.031279. Value loss: 11.191821. Entropy: 0.687347.\n",
      "episode: 4891   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 319.1\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11545: Policy loss: -0.427967. Value loss: 23.073929. Entropy: 0.423824.\n",
      "Iteration 11546: Policy loss: -0.386544. Value loss: 13.356688. Entropy: 0.439932.\n",
      "Iteration 11547: Policy loss: -0.345093. Value loss: 13.764311. Entropy: 0.408657.\n",
      "episode: 4892   score: 290.0  epsilon: 1.0    steps: 209  evaluation reward: 318.9\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11548: Policy loss: 1.788541. Value loss: 26.324848. Entropy: 0.494984.\n",
      "Iteration 11549: Policy loss: 1.602468. Value loss: 16.960802. Entropy: 0.480061.\n",
      "Iteration 11550: Policy loss: 1.650410. Value loss: 13.308545. Entropy: 0.502319.\n",
      "episode: 4893   score: 260.0  epsilon: 1.0    steps: 797  evaluation reward: 318.75\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11551: Policy loss: -1.392596. Value loss: 24.142254. Entropy: 0.685116.\n",
      "Iteration 11552: Policy loss: -1.402563. Value loss: 14.220977. Entropy: 0.686482.\n",
      "Iteration 11553: Policy loss: -1.434002. Value loss: 13.012949. Entropy: 0.673171.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11554: Policy loss: -0.484474. Value loss: 26.974115. Entropy: 0.536671.\n",
      "Iteration 11555: Policy loss: -0.509215. Value loss: 12.814683. Entropy: 0.540858.\n",
      "Iteration 11556: Policy loss: -0.511937. Value loss: 10.072261. Entropy: 0.542461.\n",
      "episode: 4894   score: 365.0  epsilon: 1.0    steps: 124  evaluation reward: 320.3\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11557: Policy loss: 0.450460. Value loss: 47.895416. Entropy: 0.557697.\n",
      "Iteration 11558: Policy loss: 0.700184. Value loss: 27.629000. Entropy: 0.533524.\n",
      "Iteration 11559: Policy loss: 0.297000. Value loss: 17.887947. Entropy: 0.533855.\n",
      "episode: 4895   score: 260.0  epsilon: 1.0    steps: 484  evaluation reward: 319.55\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11560: Policy loss: 2.214903. Value loss: 23.806522. Entropy: 0.626362.\n",
      "Iteration 11561: Policy loss: 2.456784. Value loss: 10.360709. Entropy: 0.646866.\n",
      "Iteration 11562: Policy loss: 2.123192. Value loss: 10.079041. Entropy: 0.631803.\n",
      "episode: 4896   score: 370.0  epsilon: 1.0    steps: 269  evaluation reward: 320.5\n",
      "episode: 4897   score: 310.0  epsilon: 1.0    steps: 642  evaluation reward: 321.2\n",
      "episode: 4898   score: 260.0  epsilon: 1.0    steps: 997  evaluation reward: 321.15\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11563: Policy loss: -0.451162. Value loss: 23.034920. Entropy: 0.401826.\n",
      "Iteration 11564: Policy loss: -0.476582. Value loss: 14.680437. Entropy: 0.417535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11565: Policy loss: -0.530497. Value loss: 11.526704. Entropy: 0.412873.\n",
      "episode: 4899   score: 260.0  epsilon: 1.0    steps: 229  evaluation reward: 320.35\n",
      "episode: 4900   score: 310.0  epsilon: 1.0    steps: 611  evaluation reward: 320.25\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11566: Policy loss: 2.305689. Value loss: 36.495384. Entropy: 0.513174.\n",
      "Iteration 11567: Policy loss: 2.205919. Value loss: 23.400616. Entropy: 0.533156.\n",
      "Iteration 11568: Policy loss: 2.336249. Value loss: 20.622622. Entropy: 0.522006.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11569: Policy loss: 0.322347. Value loss: 25.520294. Entropy: 0.526861.\n",
      "Iteration 11570: Policy loss: 0.415225. Value loss: 13.862970. Entropy: 0.535917.\n",
      "Iteration 11571: Policy loss: 0.304129. Value loss: 11.319913. Entropy: 0.544071.\n",
      "now time :  2019-02-25 22:16:15.727927\n",
      "episode: 4901   score: 180.0  epsilon: 1.0    steps: 475  evaluation reward: 319.65\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11572: Policy loss: -1.732901. Value loss: 25.133852. Entropy: 0.654956.\n",
      "Iteration 11573: Policy loss: -1.636404. Value loss: 14.292111. Entropy: 0.631640.\n",
      "Iteration 11574: Policy loss: -1.675104. Value loss: 11.097397. Entropy: 0.648332.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11575: Policy loss: -0.298767. Value loss: 14.557007. Entropy: 0.763422.\n",
      "Iteration 11576: Policy loss: -0.422213. Value loss: 7.260138. Entropy: 0.781957.\n",
      "Iteration 11577: Policy loss: -0.418616. Value loss: 5.991428. Entropy: 0.765770.\n",
      "episode: 4902   score: 290.0  epsilon: 1.0    steps: 33  evaluation reward: 319.55\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11578: Policy loss: -2.059928. Value loss: 37.068222. Entropy: 0.621865.\n",
      "Iteration 11579: Policy loss: -1.915082. Value loss: 21.023001. Entropy: 0.642365.\n",
      "Iteration 11580: Policy loss: -2.098440. Value loss: 16.682529. Entropy: 0.625003.\n",
      "episode: 4903   score: 395.0  epsilon: 1.0    steps: 378  evaluation reward: 318.45\n",
      "episode: 4904   score: 310.0  epsilon: 1.0    steps: 705  evaluation reward: 317.05\n",
      "episode: 4905   score: 260.0  epsilon: 1.0    steps: 951  evaluation reward: 317.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11581: Policy loss: 0.079043. Value loss: 20.722483. Entropy: 0.696706.\n",
      "Iteration 11582: Policy loss: 0.173543. Value loss: 13.919831. Entropy: 0.684950.\n",
      "Iteration 11583: Policy loss: -0.039381. Value loss: 13.656515. Entropy: 0.684691.\n",
      "episode: 4906   score: 280.0  epsilon: 1.0    steps: 604  evaluation reward: 315.2\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11584: Policy loss: 1.912198. Value loss: 24.517542. Entropy: 0.652368.\n",
      "Iteration 11585: Policy loss: 2.045407. Value loss: 17.011040. Entropy: 0.655856.\n",
      "Iteration 11586: Policy loss: 1.735357. Value loss: 12.118795. Entropy: 0.671561.\n",
      "episode: 4907   score: 360.0  epsilon: 1.0    steps: 840  evaluation reward: 316.7\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11587: Policy loss: 0.426120. Value loss: 22.300503. Entropy: 0.617214.\n",
      "Iteration 11588: Policy loss: 0.346357. Value loss: 14.737640. Entropy: 0.578617.\n",
      "Iteration 11589: Policy loss: 0.286707. Value loss: 12.589621. Entropy: 0.612416.\n",
      "episode: 4908   score: 245.0  epsilon: 1.0    steps: 438  evaluation reward: 313.95\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11590: Policy loss: -0.020961. Value loss: 20.813688. Entropy: 0.575574.\n",
      "Iteration 11591: Policy loss: 0.241275. Value loss: 17.479212. Entropy: 0.593288.\n",
      "Iteration 11592: Policy loss: 0.170185. Value loss: 12.147252. Entropy: 0.594379.\n",
      "episode: 4909   score: 260.0  epsilon: 1.0    steps: 124  evaluation reward: 311.7\n",
      "episode: 4910   score: 335.0  epsilon: 1.0    steps: 242  evaluation reward: 312.05\n",
      "episode: 4911   score: 210.0  epsilon: 1.0    steps: 1006  evaluation reward: 312.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11593: Policy loss: -1.061828. Value loss: 25.404833. Entropy: 0.577117.\n",
      "Iteration 11594: Policy loss: -0.943277. Value loss: 15.862103. Entropy: 0.578033.\n",
      "Iteration 11595: Policy loss: -1.032034. Value loss: 13.055646. Entropy: 0.589582.\n",
      "episode: 4912   score: 210.0  epsilon: 1.0    steps: 381  evaluation reward: 312.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11596: Policy loss: -1.242867. Value loss: 24.197206. Entropy: 0.604764.\n",
      "Iteration 11597: Policy loss: -1.254685. Value loss: 14.310061. Entropy: 0.610521.\n",
      "Iteration 11598: Policy loss: -1.323936. Value loss: 12.602108. Entropy: 0.625648.\n",
      "episode: 4913   score: 270.0  epsilon: 1.0    steps: 602  evaluation reward: 312.65\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11599: Policy loss: 1.173594. Value loss: 34.468273. Entropy: 0.687416.\n",
      "Iteration 11600: Policy loss: 0.800915. Value loss: 18.468784. Entropy: 0.681975.\n",
      "Iteration 11601: Policy loss: 1.199866. Value loss: 15.192347. Entropy: 0.678175.\n",
      "episode: 4914   score: 370.0  epsilon: 1.0    steps: 655  evaluation reward: 313.6\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11602: Policy loss: -0.302950. Value loss: 23.850721. Entropy: 0.605347.\n",
      "Iteration 11603: Policy loss: -0.491959. Value loss: 14.652264. Entropy: 0.608305.\n",
      "Iteration 11604: Policy loss: -0.291682. Value loss: 12.675524. Entropy: 0.619735.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11605: Policy loss: 0.457295. Value loss: 17.317547. Entropy: 0.792025.\n",
      "Iteration 11606: Policy loss: 0.515532. Value loss: 10.365724. Entropy: 0.802154.\n",
      "Iteration 11607: Policy loss: 0.445647. Value loss: 8.779490. Entropy: 0.805358.\n",
      "episode: 4915   score: 300.0  epsilon: 1.0    steps: 493  evaluation reward: 313.5\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11608: Policy loss: -3.165208. Value loss: 263.125183. Entropy: 0.671480.\n",
      "Iteration 11609: Policy loss: -2.550174. Value loss: 112.204521. Entropy: 0.608865.\n",
      "Iteration 11610: Policy loss: -2.472899. Value loss: 91.481964. Entropy: 0.611361.\n",
      "episode: 4916   score: 345.0  epsilon: 1.0    steps: 809  evaluation reward: 312.6\n",
      "episode: 4917   score: 455.0  epsilon: 1.0    steps: 944  evaluation reward: 314.3\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11611: Policy loss: 1.262600. Value loss: 22.682642. Entropy: 0.546960.\n",
      "Iteration 11612: Policy loss: 1.109034. Value loss: 11.787469. Entropy: 0.549933.\n",
      "Iteration 11613: Policy loss: 1.228212. Value loss: 11.230696. Entropy: 0.548336.\n",
      "episode: 4918   score: 325.0  epsilon: 1.0    steps: 232  evaluation reward: 311.05\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11614: Policy loss: -0.956687. Value loss: 16.540012. Entropy: 0.407071.\n",
      "Iteration 11615: Policy loss: -1.071293. Value loss: 11.381709. Entropy: 0.394410.\n",
      "Iteration 11616: Policy loss: -1.137818. Value loss: 9.397978. Entropy: 0.404756.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11617: Policy loss: -1.300586. Value loss: 31.322083. Entropy: 0.567291.\n",
      "Iteration 11618: Policy loss: -1.201333. Value loss: 17.886539. Entropy: 0.528770.\n",
      "Iteration 11619: Policy loss: -1.210451. Value loss: 14.267154. Entropy: 0.554003.\n",
      "episode: 4919   score: 330.0  epsilon: 1.0    steps: 619  evaluation reward: 310.45\n",
      "episode: 4920   score: 290.0  epsilon: 1.0    steps: 677  evaluation reward: 310.45\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11620: Policy loss: 2.146112. Value loss: 26.907356. Entropy: 0.601983.\n",
      "Iteration 11621: Policy loss: 1.928773. Value loss: 15.232905. Entropy: 0.625082.\n",
      "Iteration 11622: Policy loss: 2.102225. Value loss: 10.025671. Entropy: 0.600481.\n",
      "episode: 4921   score: 355.0  epsilon: 1.0    steps: 12  evaluation reward: 311.6\n",
      "episode: 4922   score: 290.0  epsilon: 1.0    steps: 314  evaluation reward: 311.9\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11623: Policy loss: 1.963465. Value loss: 21.855949. Entropy: 0.501507.\n",
      "Iteration 11624: Policy loss: 1.935594. Value loss: 14.130398. Entropy: 0.502037.\n",
      "Iteration 11625: Policy loss: 1.906103. Value loss: 14.976531. Entropy: 0.510621.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11626: Policy loss: 0.659900. Value loss: 25.722868. Entropy: 0.703836.\n",
      "Iteration 11627: Policy loss: 0.525727. Value loss: 16.135704. Entropy: 0.657667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11628: Policy loss: 0.611646. Value loss: 11.707225. Entropy: 0.665957.\n",
      "episode: 4923   score: 265.0  epsilon: 1.0    steps: 996  evaluation reward: 312.75\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11629: Policy loss: 2.392077. Value loss: 26.460411. Entropy: 0.437822.\n",
      "Iteration 11630: Policy loss: 2.729838. Value loss: 15.492867. Entropy: 0.444712.\n",
      "Iteration 11631: Policy loss: 2.348928. Value loss: 12.714288. Entropy: 0.442533.\n",
      "episode: 4924   score: 390.0  epsilon: 1.0    steps: 882  evaluation reward: 313.75\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11632: Policy loss: 0.396611. Value loss: 17.167509. Entropy: 0.476678.\n",
      "Iteration 11633: Policy loss: 0.182447. Value loss: 12.060001. Entropy: 0.480327.\n",
      "Iteration 11634: Policy loss: 0.081295. Value loss: 8.856897. Entropy: 0.488886.\n",
      "episode: 4925   score: 335.0  epsilon: 1.0    steps: 424  evaluation reward: 315.0\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11635: Policy loss: 0.327955. Value loss: 20.679718. Entropy: 0.436665.\n",
      "Iteration 11636: Policy loss: 0.477476. Value loss: 12.661972. Entropy: 0.424738.\n",
      "Iteration 11637: Policy loss: 0.361323. Value loss: 10.168701. Entropy: 0.436542.\n",
      "episode: 4926   score: 350.0  epsilon: 1.0    steps: 240  evaluation reward: 314.6\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11638: Policy loss: 1.907339. Value loss: 42.347923. Entropy: 0.496621.\n",
      "Iteration 11639: Policy loss: 1.954559. Value loss: 18.821390. Entropy: 0.511131.\n",
      "Iteration 11640: Policy loss: 1.855022. Value loss: 15.750897. Entropy: 0.503951.\n",
      "episode: 4927   score: 285.0  epsilon: 1.0    steps: 324  evaluation reward: 314.5\n",
      "episode: 4928   score: 305.0  epsilon: 1.0    steps: 664  evaluation reward: 315.45\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11641: Policy loss: -0.690118. Value loss: 40.551548. Entropy: 0.435668.\n",
      "Iteration 11642: Policy loss: -0.687200. Value loss: 22.828367. Entropy: 0.437810.\n",
      "Iteration 11643: Policy loss: -0.618931. Value loss: 19.289358. Entropy: 0.444457.\n",
      "episode: 4929   score: 150.0  epsilon: 1.0    steps: 844  evaluation reward: 314.85\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11644: Policy loss: 2.004173. Value loss: 28.212873. Entropy: 0.512567.\n",
      "Iteration 11645: Policy loss: 1.929715. Value loss: 20.572315. Entropy: 0.510537.\n",
      "Iteration 11646: Policy loss: 1.887589. Value loss: 15.239240. Entropy: 0.513315.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11647: Policy loss: 1.479615. Value loss: 25.491556. Entropy: 0.552202.\n",
      "Iteration 11648: Policy loss: 1.913195. Value loss: 15.788157. Entropy: 0.589589.\n",
      "Iteration 11649: Policy loss: 1.624885. Value loss: 9.976677. Entropy: 0.585090.\n",
      "episode: 4930   score: 345.0  epsilon: 1.0    steps: 124  evaluation reward: 313.4\n",
      "episode: 4931   score: 210.0  epsilon: 1.0    steps: 897  evaluation reward: 312.65\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11650: Policy loss: 0.035692. Value loss: 24.484644. Entropy: 0.616709.\n",
      "Iteration 11651: Policy loss: 0.043980. Value loss: 14.932313. Entropy: 0.613997.\n",
      "Iteration 11652: Policy loss: 0.170557. Value loss: 10.514121. Entropy: 0.623721.\n",
      "episode: 4932   score: 210.0  epsilon: 1.0    steps: 299  evaluation reward: 311.9\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11653: Policy loss: -0.769872. Value loss: 20.025574. Entropy: 0.475488.\n",
      "Iteration 11654: Policy loss: -0.693542. Value loss: 14.620549. Entropy: 0.468611.\n",
      "Iteration 11655: Policy loss: -0.754412. Value loss: 10.744882. Entropy: 0.467429.\n",
      "episode: 4933   score: 265.0  epsilon: 1.0    steps: 431  evaluation reward: 312.15\n",
      "episode: 4934   score: 500.0  epsilon: 1.0    steps: 547  evaluation reward: 313.25\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11656: Policy loss: -0.849177. Value loss: 19.167095. Entropy: 0.462717.\n",
      "Iteration 11657: Policy loss: -0.908388. Value loss: 12.278141. Entropy: 0.473393.\n",
      "Iteration 11658: Policy loss: -0.782469. Value loss: 9.476883. Entropy: 0.469153.\n",
      "episode: 4935   score: 260.0  epsilon: 1.0    steps: 677  evaluation reward: 313.45\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11659: Policy loss: -0.948583. Value loss: 22.693914. Entropy: 0.486943.\n",
      "Iteration 11660: Policy loss: -0.821883. Value loss: 14.285595. Entropy: 0.474143.\n",
      "Iteration 11661: Policy loss: -0.788368. Value loss: 12.517180. Entropy: 0.469327.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11662: Policy loss: -1.169881. Value loss: 28.149910. Entropy: 0.628513.\n",
      "Iteration 11663: Policy loss: -1.464926. Value loss: 15.291370. Entropy: 0.671651.\n",
      "Iteration 11664: Policy loss: -1.199094. Value loss: 9.748630. Entropy: 0.657249.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11665: Policy loss: 1.356592. Value loss: 20.923475. Entropy: 0.726464.\n",
      "Iteration 11666: Policy loss: 1.337669. Value loss: 8.079330. Entropy: 0.708562.\n",
      "Iteration 11667: Policy loss: 1.209932. Value loss: 7.043214. Entropy: 0.699229.\n",
      "episode: 4936   score: 370.0  epsilon: 1.0    steps: 233  evaluation reward: 312.15\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11668: Policy loss: 0.955084. Value loss: 11.822821. Entropy: 0.587646.\n",
      "Iteration 11669: Policy loss: 0.874884. Value loss: 6.451857. Entropy: 0.592622.\n",
      "Iteration 11670: Policy loss: 0.960455. Value loss: 5.081645. Entropy: 0.587306.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11671: Policy loss: -2.438287. Value loss: 219.383209. Entropy: 0.706598.\n",
      "Iteration 11672: Policy loss: -1.515651. Value loss: 57.412464. Entropy: 0.682429.\n",
      "Iteration 11673: Policy loss: -2.138582. Value loss: 25.751911. Entropy: 0.657222.\n",
      "episode: 4937   score: 240.0  epsilon: 1.0    steps: 630  evaluation reward: 308.1\n",
      "episode: 4938   score: 335.0  epsilon: 1.0    steps: 835  evaluation reward: 305.1\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11674: Policy loss: 2.976304. Value loss: 52.187263. Entropy: 0.601037.\n",
      "Iteration 11675: Policy loss: 2.794351. Value loss: 25.628386. Entropy: 0.615385.\n",
      "Iteration 11676: Policy loss: 3.324576. Value loss: 18.636591. Entropy: 0.615264.\n",
      "episode: 4939   score: 310.0  epsilon: 1.0    steps: 288  evaluation reward: 305.75\n",
      "episode: 4940   score: 455.0  epsilon: 1.0    steps: 393  evaluation reward: 307.75\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11677: Policy loss: 1.532410. Value loss: 17.286364. Entropy: 0.734048.\n",
      "Iteration 11678: Policy loss: 1.737787. Value loss: 13.682141. Entropy: 0.714023.\n",
      "Iteration 11679: Policy loss: 1.499934. Value loss: 10.898133. Entropy: 0.726167.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11680: Policy loss: 0.100800. Value loss: 25.812284. Entropy: 0.570423.\n",
      "Iteration 11681: Policy loss: 0.064417. Value loss: 16.008986. Entropy: 0.580524.\n",
      "Iteration 11682: Policy loss: 0.157651. Value loss: 11.226581. Entropy: 0.558183.\n",
      "episode: 4941   score: 380.0  epsilon: 1.0    steps: 75  evaluation reward: 309.15\n",
      "episode: 4942   score: 320.0  epsilon: 1.0    steps: 714  evaluation reward: 310.25\n",
      "episode: 4943   score: 505.0  epsilon: 1.0    steps: 934  evaluation reward: 312.9\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11683: Policy loss: 1.809633. Value loss: 25.489588. Entropy: 0.518037.\n",
      "Iteration 11684: Policy loss: 2.098178. Value loss: 12.411659. Entropy: 0.524474.\n",
      "Iteration 11685: Policy loss: 1.662764. Value loss: 12.394435. Entropy: 0.541534.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11686: Policy loss: -0.680369. Value loss: 29.885284. Entropy: 0.471654.\n",
      "Iteration 11687: Policy loss: -0.647129. Value loss: 15.722272. Entropy: 0.464688.\n",
      "Iteration 11688: Policy loss: -0.758732. Value loss: 13.048353. Entropy: 0.484866.\n",
      "episode: 4944   score: 290.0  epsilon: 1.0    steps: 200  evaluation reward: 314.0\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11689: Policy loss: 2.812889. Value loss: 30.078098. Entropy: 0.325554.\n",
      "Iteration 11690: Policy loss: 2.636012. Value loss: 21.064623. Entropy: 0.347995.\n",
      "Iteration 11691: Policy loss: 2.716857. Value loss: 15.810930. Entropy: 0.360571.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11692: Policy loss: -3.478725. Value loss: 200.540176. Entropy: 0.509609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11693: Policy loss: -3.617790. Value loss: 118.716171. Entropy: 0.486801.\n",
      "Iteration 11694: Policy loss: -3.691706. Value loss: 114.399078. Entropy: 0.433699.\n",
      "episode: 4945   score: 285.0  epsilon: 1.0    steps: 617  evaluation reward: 311.4\n",
      "episode: 4946   score: 345.0  epsilon: 1.0    steps: 838  evaluation reward: 311.95\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11695: Policy loss: 1.757690. Value loss: 29.886713. Entropy: 0.436228.\n",
      "Iteration 11696: Policy loss: 1.625820. Value loss: 17.855000. Entropy: 0.463394.\n",
      "Iteration 11697: Policy loss: 1.588992. Value loss: 13.798007. Entropy: 0.460047.\n",
      "episode: 4947   score: 270.0  epsilon: 1.0    steps: 484  evaluation reward: 311.1\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11698: Policy loss: -0.154374. Value loss: 26.906359. Entropy: 0.554893.\n",
      "Iteration 11699: Policy loss: -0.561870. Value loss: 14.026075. Entropy: 0.539211.\n",
      "Iteration 11700: Policy loss: -0.338924. Value loss: 10.932343. Entropy: 0.533897.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11701: Policy loss: -3.359477. Value loss: 179.799393. Entropy: 0.563798.\n",
      "Iteration 11702: Policy loss: -2.954936. Value loss: 75.667091. Entropy: 0.544930.\n",
      "Iteration 11703: Policy loss: -2.834111. Value loss: 46.722401. Entropy: 0.547092.\n",
      "episode: 4948   score: 320.0  epsilon: 1.0    steps: 936  evaluation reward: 311.45\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11704: Policy loss: -2.674112. Value loss: 184.882309. Entropy: 0.594954.\n",
      "Iteration 11705: Policy loss: -1.942283. Value loss: 71.327133. Entropy: 0.603126.\n",
      "Iteration 11706: Policy loss: -1.896661. Value loss: 67.322998. Entropy: 0.606892.\n",
      "episode: 4949   score: 500.0  epsilon: 1.0    steps: 46  evaluation reward: 312.85\n",
      "episode: 4950   score: 380.0  epsilon: 1.0    steps: 337  evaluation reward: 313.6\n",
      "now time :  2019-02-25 22:18:46.876590\n",
      "episode: 4951   score: 335.0  epsilon: 1.0    steps: 709  evaluation reward: 311.05\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11707: Policy loss: 6.072613. Value loss: 60.151897. Entropy: 0.501658.\n",
      "Iteration 11708: Policy loss: 6.213865. Value loss: 20.994190. Entropy: 0.490981.\n",
      "Iteration 11709: Policy loss: 6.294582. Value loss: 17.906450. Entropy: 0.504780.\n",
      "episode: 4952   score: 180.0  epsilon: 1.0    steps: 509  evaluation reward: 310.25\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11710: Policy loss: 0.757528. Value loss: 31.441193. Entropy: 0.445764.\n",
      "Iteration 11711: Policy loss: 0.729911. Value loss: 18.896749. Entropy: 0.434703.\n",
      "Iteration 11712: Policy loss: 0.900554. Value loss: 18.754490. Entropy: 0.444375.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11713: Policy loss: -0.480443. Value loss: 27.015499. Entropy: 0.446935.\n",
      "Iteration 11714: Policy loss: -0.515888. Value loss: 17.254622. Entropy: 0.474752.\n",
      "Iteration 11715: Policy loss: -0.446463. Value loss: 13.949080. Entropy: 0.451543.\n",
      "episode: 4953   score: 285.0  epsilon: 1.0    steps: 569  evaluation reward: 310.25\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11716: Policy loss: -0.083583. Value loss: 15.468210. Entropy: 0.587882.\n",
      "Iteration 11717: Policy loss: -0.211895. Value loss: 10.553811. Entropy: 0.598815.\n",
      "Iteration 11718: Policy loss: -0.097701. Value loss: 8.228419. Entropy: 0.590624.\n",
      "episode: 4954   score: 510.0  epsilon: 1.0    steps: 863  evaluation reward: 312.25\n",
      "episode: 4955   score: 240.0  epsilon: 1.0    steps: 988  evaluation reward: 311.3\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11719: Policy loss: -0.760178. Value loss: 40.200302. Entropy: 0.457399.\n",
      "Iteration 11720: Policy loss: -0.957616. Value loss: 17.545286. Entropy: 0.473803.\n",
      "Iteration 11721: Policy loss: -0.704283. Value loss: 11.513517. Entropy: 0.472026.\n",
      "episode: 4956   score: 570.0  epsilon: 1.0    steps: 184  evaluation reward: 314.4\n",
      "episode: 4957   score: 210.0  epsilon: 1.0    steps: 681  evaluation reward: 313.9\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11722: Policy loss: 2.383096. Value loss: 36.374245. Entropy: 0.436595.\n",
      "Iteration 11723: Policy loss: 2.614942. Value loss: 16.518820. Entropy: 0.471601.\n",
      "Iteration 11724: Policy loss: 2.612448. Value loss: 10.815371. Entropy: 0.448443.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11725: Policy loss: 0.731438. Value loss: 56.853958. Entropy: 0.606122.\n",
      "Iteration 11726: Policy loss: 0.895604. Value loss: 32.350365. Entropy: 0.603226.\n",
      "Iteration 11727: Policy loss: 0.653943. Value loss: 28.022369. Entropy: 0.620866.\n",
      "episode: 4958   score: 315.0  epsilon: 1.0    steps: 81  evaluation reward: 314.05\n",
      "episode: 4959   score: 265.0  epsilon: 1.0    steps: 270  evaluation reward: 313.65\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11728: Policy loss: -0.200348. Value loss: 33.742401. Entropy: 0.495906.\n",
      "Iteration 11729: Policy loss: -0.125546. Value loss: 21.948048. Entropy: 0.480512.\n",
      "Iteration 11730: Policy loss: -0.125092. Value loss: 18.396795. Entropy: 0.486122.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11731: Policy loss: -3.378485. Value loss: 123.010277. Entropy: 0.583633.\n",
      "Iteration 11732: Policy loss: -3.750026. Value loss: 59.647858. Entropy: 0.595714.\n",
      "Iteration 11733: Policy loss: -3.410825. Value loss: 32.399700. Entropy: 0.611231.\n",
      "episode: 4960   score: 215.0  epsilon: 1.0    steps: 552  evaluation reward: 312.8\n",
      "episode: 4961   score: 215.0  epsilon: 1.0    steps: 907  evaluation reward: 311.6\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11734: Policy loss: -1.110277. Value loss: 32.377998. Entropy: 0.551989.\n",
      "Iteration 11735: Policy loss: -1.365756. Value loss: 21.484737. Entropy: 0.558509.\n",
      "Iteration 11736: Policy loss: -1.365032. Value loss: 17.040476. Entropy: 0.559289.\n",
      "episode: 4962   score: 425.0  epsilon: 1.0    steps: 864  evaluation reward: 312.7\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11737: Policy loss: -1.376738. Value loss: 153.541962. Entropy: 0.464091.\n",
      "Iteration 11738: Policy loss: -1.214065. Value loss: 87.547668. Entropy: 0.484899.\n",
      "Iteration 11739: Policy loss: -0.943126. Value loss: 72.048363. Entropy: 0.500410.\n",
      "episode: 4963   score: 280.0  epsilon: 1.0    steps: 655  evaluation reward: 313.2\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11740: Policy loss: -3.085813. Value loss: 121.996574. Entropy: 0.585299.\n",
      "Iteration 11741: Policy loss: -3.426685. Value loss: 71.791885. Entropy: 0.579173.\n",
      "Iteration 11742: Policy loss: -3.382105. Value loss: 37.194012. Entropy: 0.587362.\n",
      "episode: 4964   score: 475.0  epsilon: 1.0    steps: 500  evaluation reward: 315.35\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11743: Policy loss: -0.382029. Value loss: 112.351852. Entropy: 0.631973.\n",
      "Iteration 11744: Policy loss: -0.767138. Value loss: 34.372227. Entropy: 0.616687.\n",
      "Iteration 11745: Policy loss: -0.405619. Value loss: 25.033470. Entropy: 0.619467.\n",
      "episode: 4965   score: 540.0  epsilon: 1.0    steps: 161  evaluation reward: 316.2\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11746: Policy loss: -1.477636. Value loss: 38.563828. Entropy: 0.647396.\n",
      "Iteration 11747: Policy loss: -1.322113. Value loss: 24.415644. Entropy: 0.647595.\n",
      "Iteration 11748: Policy loss: -1.636253. Value loss: 17.711163. Entropy: 0.647946.\n",
      "episode: 4966   score: 515.0  epsilon: 1.0    steps: 64  evaluation reward: 318.75\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11749: Policy loss: 3.704233. Value loss: 60.089886. Entropy: 0.612346.\n",
      "Iteration 11750: Policy loss: 3.109761. Value loss: 17.119173. Entropy: 0.590945.\n",
      "Iteration 11751: Policy loss: 3.457041. Value loss: 11.562182. Entropy: 0.588859.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11752: Policy loss: -0.971936. Value loss: 99.232307. Entropy: 0.547677.\n",
      "Iteration 11753: Policy loss: -1.643545. Value loss: 62.722530. Entropy: 0.538145.\n",
      "Iteration 11754: Policy loss: -1.011620. Value loss: 38.861153. Entropy: 0.559638.\n",
      "episode: 4967   score: 290.0  epsilon: 1.0    steps: 971  evaluation reward: 318.15\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11755: Policy loss: -2.408804. Value loss: 68.544167. Entropy: 0.688360.\n",
      "Iteration 11756: Policy loss: -2.340440. Value loss: 61.384769. Entropy: 0.662935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11757: Policy loss: -2.634926. Value loss: 40.934277. Entropy: 0.674022.\n",
      "episode: 4968   score: 315.0  epsilon: 1.0    steps: 771  evaluation reward: 319.5\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11758: Policy loss: 0.212780. Value loss: 21.958103. Entropy: 0.547789.\n",
      "Iteration 11759: Policy loss: 0.255913. Value loss: 17.002281. Entropy: 0.540890.\n",
      "Iteration 11760: Policy loss: 0.172665. Value loss: 14.561158. Entropy: 0.548614.\n",
      "episode: 4969   score: 700.0  epsilon: 1.0    steps: 285  evaluation reward: 323.8\n",
      "episode: 4970   score: 365.0  epsilon: 1.0    steps: 699  evaluation reward: 322.55\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11761: Policy loss: 0.432779. Value loss: 52.462860. Entropy: 0.535652.\n",
      "Iteration 11762: Policy loss: 0.248962. Value loss: 27.109982. Entropy: 0.545666.\n",
      "Iteration 11763: Policy loss: 0.440240. Value loss: 22.792032. Entropy: 0.542780.\n",
      "episode: 4971   score: 410.0  epsilon: 1.0    steps: 418  evaluation reward: 321.0\n",
      "episode: 4972   score: 320.0  epsilon: 1.0    steps: 596  evaluation reward: 320.45\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11764: Policy loss: 7.138551. Value loss: 125.074829. Entropy: 0.461730.\n",
      "Iteration 11765: Policy loss: 8.550119. Value loss: 30.954494. Entropy: 0.437462.\n",
      "Iteration 11766: Policy loss: 7.657778. Value loss: 18.016703. Entropy: 0.430114.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11767: Policy loss: 2.310855. Value loss: 70.677391. Entropy: 0.496135.\n",
      "Iteration 11768: Policy loss: 2.737075. Value loss: 39.776867. Entropy: 0.476134.\n",
      "Iteration 11769: Policy loss: 2.783508. Value loss: 27.053120. Entropy: 0.471618.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11770: Policy loss: 4.570426. Value loss: 63.675941. Entropy: 0.526363.\n",
      "Iteration 11771: Policy loss: 4.677515. Value loss: 25.472496. Entropy: 0.547266.\n",
      "Iteration 11772: Policy loss: 4.369071. Value loss: 18.402351. Entropy: 0.535638.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11773: Policy loss: 1.810327. Value loss: 28.442327. Entropy: 0.637075.\n",
      "Iteration 11774: Policy loss: 1.954244. Value loss: 17.715094. Entropy: 0.650041.\n",
      "Iteration 11775: Policy loss: 1.945979. Value loss: 14.563424. Entropy: 0.653337.\n",
      "episode: 4973   score: 250.0  epsilon: 1.0    steps: 900  evaluation reward: 318.2\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11776: Policy loss: 1.607878. Value loss: 35.036388. Entropy: 0.476340.\n",
      "Iteration 11777: Policy loss: 1.636686. Value loss: 15.853639. Entropy: 0.487633.\n",
      "Iteration 11778: Policy loss: 1.568681. Value loss: 12.653975. Entropy: 0.486100.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11779: Policy loss: 4.367699. Value loss: 34.211823. Entropy: 0.583972.\n",
      "Iteration 11780: Policy loss: 3.942802. Value loss: 20.416409. Entropy: 0.601047.\n",
      "Iteration 11781: Policy loss: 4.178268. Value loss: 16.225883. Entropy: 0.593063.\n",
      "episode: 4974   score: 420.0  epsilon: 1.0    steps: 97  evaluation reward: 319.55\n",
      "episode: 4975   score: 645.0  epsilon: 1.0    steps: 234  evaluation reward: 323.3\n",
      "episode: 4976   score: 260.0  epsilon: 1.0    steps: 672  evaluation reward: 323.3\n",
      "episode: 4977   score: 315.0  epsilon: 1.0    steps: 840  evaluation reward: 325.1\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11782: Policy loss: 0.498161. Value loss: 27.298668. Entropy: 0.373174.\n",
      "Iteration 11783: Policy loss: 0.406800. Value loss: 15.729775. Entropy: 0.394030.\n",
      "Iteration 11784: Policy loss: 0.656633. Value loss: 11.008470. Entropy: 0.387709.\n",
      "episode: 4978   score: 280.0  epsilon: 1.0    steps: 263  evaluation reward: 325.8\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11785: Policy loss: 1.760970. Value loss: 41.503204. Entropy: 0.597636.\n",
      "Iteration 11786: Policy loss: 1.353959. Value loss: 24.977535. Entropy: 0.615276.\n",
      "Iteration 11787: Policy loss: 1.469406. Value loss: 21.132000. Entropy: 0.641595.\n",
      "episode: 4979   score: 345.0  epsilon: 1.0    steps: 591  evaluation reward: 326.9\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11788: Policy loss: -0.838826. Value loss: 27.754070. Entropy: 0.453610.\n",
      "Iteration 11789: Policy loss: -0.719280. Value loss: 18.927200. Entropy: 0.493808.\n",
      "Iteration 11790: Policy loss: -0.822965. Value loss: 14.929463. Entropy: 0.482465.\n",
      "episode: 4980   score: 315.0  epsilon: 1.0    steps: 448  evaluation reward: 324.6\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11791: Policy loss: -0.928514. Value loss: 121.593834. Entropy: 0.510400.\n",
      "Iteration 11792: Policy loss: -1.291814. Value loss: 55.037403. Entropy: 0.495057.\n",
      "Iteration 11793: Policy loss: -0.251531. Value loss: 41.254105. Entropy: 0.501400.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11794: Policy loss: 4.668650. Value loss: 77.862602. Entropy: 0.498799.\n",
      "Iteration 11795: Policy loss: 5.044986. Value loss: 29.898653. Entropy: 0.518382.\n",
      "Iteration 11796: Policy loss: 3.811692. Value loss: 22.146988. Entropy: 0.498321.\n",
      "episode: 4981   score: 270.0  epsilon: 1.0    steps: 927  evaluation reward: 325.2\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11797: Policy loss: 1.722721. Value loss: 32.688309. Entropy: 0.403272.\n",
      "Iteration 11798: Policy loss: 1.898470. Value loss: 18.885502. Entropy: 0.406019.\n",
      "Iteration 11799: Policy loss: 1.826658. Value loss: 15.562799. Entropy: 0.421274.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11800: Policy loss: 1.369591. Value loss: 46.928280. Entropy: 0.365604.\n",
      "Iteration 11801: Policy loss: 1.777898. Value loss: 26.557770. Entropy: 0.380098.\n",
      "Iteration 11802: Policy loss: 1.626273. Value loss: 19.462994. Entropy: 0.373666.\n",
      "episode: 4982   score: 210.0  epsilon: 1.0    steps: 638  evaluation reward: 325.2\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11803: Policy loss: 3.601782. Value loss: 34.224163. Entropy: 0.375032.\n",
      "Iteration 11804: Policy loss: 3.921502. Value loss: 22.522573. Entropy: 0.384719.\n",
      "Iteration 11805: Policy loss: 3.727430. Value loss: 15.700481. Entropy: 0.397073.\n",
      "episode: 4983   score: 285.0  epsilon: 1.0    steps: 12  evaluation reward: 323.5\n",
      "episode: 4984   score: 260.0  epsilon: 1.0    steps: 218  evaluation reward: 324.85\n",
      "episode: 4985   score: 315.0  epsilon: 1.0    steps: 312  evaluation reward: 322.1\n",
      "episode: 4986   score: 420.0  epsilon: 1.0    steps: 886  evaluation reward: 321.25\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11806: Policy loss: 0.612354. Value loss: 28.861357. Entropy: 0.618562.\n",
      "Iteration 11807: Policy loss: 0.559366. Value loss: 16.195095. Entropy: 0.625299.\n",
      "Iteration 11808: Policy loss: 0.583710. Value loss: 12.861147. Entropy: 0.629509.\n",
      "episode: 4987   score: 210.0  epsilon: 1.0    steps: 1015  evaluation reward: 321.25\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11809: Policy loss: 0.062357. Value loss: 27.669380. Entropy: 0.735802.\n",
      "Iteration 11810: Policy loss: 0.040808. Value loss: 18.116318. Entropy: 0.739946.\n",
      "Iteration 11811: Policy loss: -0.233982. Value loss: 16.495386. Entropy: 0.710373.\n",
      "episode: 4988   score: 555.0  epsilon: 1.0    steps: 643  evaluation reward: 324.55\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11812: Policy loss: 1.389791. Value loss: 25.239950. Entropy: 0.539691.\n",
      "Iteration 11813: Policy loss: 1.277936. Value loss: 17.049534. Entropy: 0.516059.\n",
      "Iteration 11814: Policy loss: 1.634214. Value loss: 13.829942. Entropy: 0.540876.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11815: Policy loss: -0.941819. Value loss: 118.308640. Entropy: 0.420025.\n",
      "Iteration 11816: Policy loss: -0.774664. Value loss: 31.797722. Entropy: 0.401269.\n",
      "Iteration 11817: Policy loss: -0.691498. Value loss: 27.537210. Entropy: 0.422557.\n",
      "episode: 4989   score: 340.0  epsilon: 1.0    steps: 390  evaluation reward: 325.85\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11818: Policy loss: -1.932422. Value loss: 80.618896. Entropy: 0.537622.\n",
      "Iteration 11819: Policy loss: -1.700365. Value loss: 30.921190. Entropy: 0.523898.\n",
      "Iteration 11820: Policy loss: -1.579636. Value loss: 21.792807. Entropy: 0.527740.\n",
      "episode: 4990   score: 210.0  epsilon: 1.0    steps: 182  evaluation reward: 326.15\n",
      "episode: 4991   score: 410.0  epsilon: 1.0    steps: 368  evaluation reward: 328.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11821: Policy loss: 3.567767. Value loss: 101.604866. Entropy: 0.561719.\n",
      "Iteration 11822: Policy loss: 3.240744. Value loss: 52.639038. Entropy: 0.524355.\n",
      "Iteration 11823: Policy loss: 3.436695. Value loss: 30.617304. Entropy: 0.534538.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11824: Policy loss: 1.032296. Value loss: 48.746708. Entropy: 0.615006.\n",
      "Iteration 11825: Policy loss: 1.558237. Value loss: 25.462288. Entropy: 0.591135.\n",
      "Iteration 11826: Policy loss: 1.012731. Value loss: 18.831564. Entropy: 0.588963.\n",
      "episode: 4992   score: 560.0  epsilon: 1.0    steps: 87  evaluation reward: 330.85\n",
      "episode: 4993   score: 315.0  epsilon: 1.0    steps: 556  evaluation reward: 331.4\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11827: Policy loss: -0.640052. Value loss: 32.790905. Entropy: 0.512999.\n",
      "Iteration 11828: Policy loss: -0.603587. Value loss: 20.061260. Entropy: 0.498737.\n",
      "Iteration 11829: Policy loss: -0.531762. Value loss: 17.910416. Entropy: 0.529112.\n",
      "episode: 4994   score: 360.0  epsilon: 1.0    steps: 859  evaluation reward: 331.35\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11830: Policy loss: 1.031158. Value loss: 39.208538. Entropy: 0.546980.\n",
      "Iteration 11831: Policy loss: 1.502176. Value loss: 21.436796. Entropy: 0.554827.\n",
      "Iteration 11832: Policy loss: 1.309131. Value loss: 16.787436. Entropy: 0.533310.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11833: Policy loss: 1.770890. Value loss: 35.527126. Entropy: 0.564493.\n",
      "Iteration 11834: Policy loss: 1.820283. Value loss: 15.560024. Entropy: 0.563416.\n",
      "Iteration 11835: Policy loss: 1.720831. Value loss: 11.898742. Entropy: 0.549914.\n",
      "episode: 4995   score: 390.0  epsilon: 1.0    steps: 655  evaluation reward: 332.65\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11836: Policy loss: 0.950726. Value loss: 24.118425. Entropy: 0.556167.\n",
      "Iteration 11837: Policy loss: 1.001302. Value loss: 12.522594. Entropy: 0.546805.\n",
      "Iteration 11838: Policy loss: 1.090011. Value loss: 9.316323. Entropy: 0.545469.\n",
      "episode: 4996   score: 275.0  epsilon: 1.0    steps: 481  evaluation reward: 331.7\n",
      "episode: 4997   score: 120.0  epsilon: 1.0    steps: 592  evaluation reward: 329.8\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11839: Policy loss: -1.801560. Value loss: 92.782463. Entropy: 0.482946.\n",
      "Iteration 11840: Policy loss: -1.461377. Value loss: 38.443924. Entropy: 0.488077.\n",
      "Iteration 11841: Policy loss: -2.415991. Value loss: 37.303837. Entropy: 0.496910.\n",
      "episode: 4998   score: 225.0  epsilon: 1.0    steps: 137  evaluation reward: 329.45\n",
      "episode: 4999   score: 420.0  epsilon: 1.0    steps: 974  evaluation reward: 331.05\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11842: Policy loss: 0.394677. Value loss: 33.165291. Entropy: 0.600395.\n",
      "Iteration 11843: Policy loss: 0.569102. Value loss: 19.085037. Entropy: 0.591859.\n",
      "Iteration 11844: Policy loss: 0.195075. Value loss: 15.065055. Entropy: 0.595318.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11845: Policy loss: 0.558908. Value loss: 46.035252. Entropy: 0.606324.\n",
      "Iteration 11846: Policy loss: 0.443541. Value loss: 23.285704. Entropy: 0.607278.\n",
      "Iteration 11847: Policy loss: 0.662906. Value loss: 15.893929. Entropy: 0.589891.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11848: Policy loss: 2.402015. Value loss: 31.491398. Entropy: 0.405147.\n",
      "Iteration 11849: Policy loss: 2.068492. Value loss: 19.994646. Entropy: 0.427945.\n",
      "Iteration 11850: Policy loss: 2.272100. Value loss: 15.193262. Entropy: 0.434814.\n",
      "episode: 5000   score: 260.0  epsilon: 1.0    steps: 786  evaluation reward: 330.55\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11851: Policy loss: 1.265594. Value loss: 27.902538. Entropy: 0.456966.\n",
      "Iteration 11852: Policy loss: 1.315616. Value loss: 14.517635. Entropy: 0.483892.\n",
      "Iteration 11853: Policy loss: 1.259130. Value loss: 11.061492. Entropy: 0.459632.\n",
      "now time :  2019-02-25 22:21:30.825626\n",
      "episode: 5001   score: 190.0  epsilon: 1.0    steps: 617  evaluation reward: 330.65\n",
      "episode: 5002   score: 315.0  epsilon: 1.0    steps: 719  evaluation reward: 330.9\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11854: Policy loss: 0.277311. Value loss: 30.694561. Entropy: 0.543625.\n",
      "Iteration 11855: Policy loss: 0.241798. Value loss: 17.348970. Entropy: 0.559060.\n",
      "Iteration 11856: Policy loss: -0.059796. Value loss: 12.069391. Entropy: 0.554824.\n",
      "episode: 5003   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 328.5\n",
      "episode: 5004   score: 500.0  epsilon: 1.0    steps: 327  evaluation reward: 330.4\n",
      "episode: 5005   score: 180.0  epsilon: 1.0    steps: 957  evaluation reward: 329.6\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11857: Policy loss: 1.171861. Value loss: 27.791155. Entropy: 0.576783.\n",
      "Iteration 11858: Policy loss: 1.253302. Value loss: 15.940678. Entropy: 0.567543.\n",
      "Iteration 11859: Policy loss: 1.195284. Value loss: 12.827857. Entropy: 0.601172.\n",
      "episode: 5006   score: 650.0  epsilon: 1.0    steps: 96  evaluation reward: 333.3\n",
      "episode: 5007   score: 275.0  epsilon: 1.0    steps: 489  evaluation reward: 332.45\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11860: Policy loss: -0.359162. Value loss: 20.633656. Entropy: 0.496304.\n",
      "Iteration 11861: Policy loss: -0.392962. Value loss: 12.266606. Entropy: 0.503401.\n",
      "Iteration 11862: Policy loss: -0.630473. Value loss: 10.773977. Entropy: 0.489448.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11863: Policy loss: 4.098604. Value loss: 31.246758. Entropy: 0.560720.\n",
      "Iteration 11864: Policy loss: 4.129617. Value loss: 20.351362. Entropy: 0.530583.\n",
      "Iteration 11865: Policy loss: 4.074919. Value loss: 15.659703. Entropy: 0.536294.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11866: Policy loss: 1.806223. Value loss: 37.526657. Entropy: 0.306071.\n",
      "Iteration 11867: Policy loss: 1.624580. Value loss: 19.005548. Entropy: 0.301107.\n",
      "Iteration 11868: Policy loss: 1.680589. Value loss: 14.740633. Entropy: 0.304661.\n",
      "episode: 5008   score: 75.0  epsilon: 1.0    steps: 137  evaluation reward: 330.75\n",
      "episode: 5009   score: 285.0  epsilon: 1.0    steps: 889  evaluation reward: 331.0\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11869: Policy loss: 1.147628. Value loss: 22.336195. Entropy: 0.504653.\n",
      "Iteration 11870: Policy loss: 0.750182. Value loss: 9.804363. Entropy: 0.500516.\n",
      "Iteration 11871: Policy loss: 0.759707. Value loss: 8.394207. Entropy: 0.488822.\n",
      "episode: 5010   score: 80.0  epsilon: 1.0    steps: 322  evaluation reward: 328.45\n",
      "episode: 5011   score: 290.0  epsilon: 1.0    steps: 630  evaluation reward: 329.25\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11872: Policy loss: 0.949085. Value loss: 30.478611. Entropy: 0.384178.\n",
      "Iteration 11873: Policy loss: 0.870130. Value loss: 19.172447. Entropy: 0.387804.\n",
      "Iteration 11874: Policy loss: 0.956608. Value loss: 16.248425. Entropy: 0.380321.\n",
      "episode: 5012   score: 240.0  epsilon: 1.0    steps: 673  evaluation reward: 329.55\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11875: Policy loss: -2.513965. Value loss: 151.573349. Entropy: 0.506564.\n",
      "Iteration 11876: Policy loss: -2.151414. Value loss: 39.304821. Entropy: 0.554028.\n",
      "Iteration 11877: Policy loss: -1.832710. Value loss: 10.644923. Entropy: 0.627395.\n",
      "episode: 5013   score: 215.0  epsilon: 1.0    steps: 473  evaluation reward: 329.0\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11878: Policy loss: 3.488476. Value loss: 38.113701. Entropy: 0.365881.\n",
      "Iteration 11879: Policy loss: 3.233009. Value loss: 20.732592. Entropy: 0.392471.\n",
      "Iteration 11880: Policy loss: 3.403492. Value loss: 17.155531. Entropy: 0.397872.\n",
      "episode: 5014   score: 320.0  epsilon: 1.0    steps: 87  evaluation reward: 328.5\n",
      "episode: 5015   score: 280.0  epsilon: 1.0    steps: 924  evaluation reward: 328.3\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11881: Policy loss: 2.166912. Value loss: 41.671654. Entropy: 0.672351.\n",
      "Iteration 11882: Policy loss: 2.330344. Value loss: 16.623116. Entropy: 0.615783.\n",
      "Iteration 11883: Policy loss: 2.145255. Value loss: 13.814277. Entropy: 0.630205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11884: Policy loss: 2.994719. Value loss: 54.719959. Entropy: 0.484280.\n",
      "Iteration 11885: Policy loss: 2.612178. Value loss: 22.545206. Entropy: 0.482728.\n",
      "Iteration 11886: Policy loss: 2.401855. Value loss: 19.170879. Entropy: 0.487190.\n",
      "episode: 5016   score: 180.0  epsilon: 1.0    steps: 762  evaluation reward: 326.65\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11887: Policy loss: 2.949208. Value loss: 37.840645. Entropy: 0.473103.\n",
      "Iteration 11888: Policy loss: 3.456694. Value loss: 20.833603. Entropy: 0.481145.\n",
      "Iteration 11889: Policy loss: 2.958018. Value loss: 18.511248. Entropy: 0.476086.\n",
      "episode: 5017   score: 210.0  epsilon: 1.0    steps: 786  evaluation reward: 324.2\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11890: Policy loss: -0.506092. Value loss: 32.800022. Entropy: 0.573717.\n",
      "Iteration 11891: Policy loss: -0.277036. Value loss: 18.578672. Entropy: 0.584985.\n",
      "Iteration 11892: Policy loss: -0.238113. Value loss: 12.308785. Entropy: 0.567945.\n",
      "episode: 5018   score: 155.0  epsilon: 1.0    steps: 450  evaluation reward: 322.5\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11893: Policy loss: 0.824851. Value loss: 30.232544. Entropy: 0.594674.\n",
      "Iteration 11894: Policy loss: 0.630634. Value loss: 24.876478. Entropy: 0.576329.\n",
      "Iteration 11895: Policy loss: 0.578889. Value loss: 16.859705. Entropy: 0.579514.\n",
      "episode: 5019   score: 240.0  epsilon: 1.0    steps: 599  evaluation reward: 321.6\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11896: Policy loss: -0.012607. Value loss: 25.191086. Entropy: 0.625900.\n",
      "Iteration 11897: Policy loss: 0.025320. Value loss: 14.842484. Entropy: 0.625911.\n",
      "Iteration 11898: Policy loss: 0.027537. Value loss: 11.407536. Entropy: 0.592726.\n",
      "episode: 5020   score: 185.0  epsilon: 1.0    steps: 126  evaluation reward: 320.55\n",
      "episode: 5021   score: 405.0  epsilon: 1.0    steps: 294  evaluation reward: 321.05\n",
      "episode: 5022   score: 260.0  epsilon: 1.0    steps: 943  evaluation reward: 320.75\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11899: Policy loss: 1.342575. Value loss: 21.093996. Entropy: 0.612657.\n",
      "Iteration 11900: Policy loss: 1.239884. Value loss: 11.577639. Entropy: 0.602888.\n",
      "Iteration 11901: Policy loss: 1.211042. Value loss: 8.437578. Entropy: 0.623312.\n",
      "episode: 5023   score: 565.0  epsilon: 1.0    steps: 220  evaluation reward: 323.75\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11902: Policy loss: 0.824982. Value loss: 25.585720. Entropy: 0.444571.\n",
      "Iteration 11903: Policy loss: 0.616473. Value loss: 14.197364. Entropy: 0.469369.\n",
      "Iteration 11904: Policy loss: 0.724715. Value loss: 10.389126. Entropy: 0.446242.\n",
      "episode: 5024   score: 210.0  epsilon: 1.0    steps: 708  evaluation reward: 321.95\n",
      "episode: 5025   score: 210.0  epsilon: 1.0    steps: 803  evaluation reward: 320.7\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11905: Policy loss: 1.071721. Value loss: 29.308460. Entropy: 0.613293.\n",
      "Iteration 11906: Policy loss: 1.342267. Value loss: 11.154270. Entropy: 0.578081.\n",
      "Iteration 11907: Policy loss: 1.320288. Value loss: 11.383358. Entropy: 0.609993.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11908: Policy loss: 1.009591. Value loss: 30.247690. Entropy: 0.357172.\n",
      "Iteration 11909: Policy loss: 0.524777. Value loss: 17.368067. Entropy: 0.391512.\n",
      "Iteration 11910: Policy loss: 0.840365. Value loss: 14.287169. Entropy: 0.396311.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11911: Policy loss: -0.474708. Value loss: 25.663656. Entropy: 0.461397.\n",
      "Iteration 11912: Policy loss: -0.354937. Value loss: 15.432942. Entropy: 0.482870.\n",
      "Iteration 11913: Policy loss: -0.577821. Value loss: 12.263050. Entropy: 0.466699.\n",
      "episode: 5026   score: 210.0  epsilon: 1.0    steps: 359  evaluation reward: 319.3\n",
      "episode: 5027   score: 260.0  epsilon: 1.0    steps: 489  evaluation reward: 319.05\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11914: Policy loss: 1.125583. Value loss: 22.828997. Entropy: 0.472677.\n",
      "Iteration 11915: Policy loss: 1.235047. Value loss: 13.197894. Entropy: 0.507562.\n",
      "Iteration 11916: Policy loss: 1.117931. Value loss: 9.665549. Entropy: 0.505850.\n",
      "episode: 5028   score: 210.0  epsilon: 1.0    steps: 54  evaluation reward: 318.1\n",
      "episode: 5029   score: 210.0  epsilon: 1.0    steps: 531  evaluation reward: 318.7\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11917: Policy loss: -1.715187. Value loss: 23.937063. Entropy: 0.625603.\n",
      "Iteration 11918: Policy loss: -1.360073. Value loss: 12.432599. Entropy: 0.632897.\n",
      "Iteration 11919: Policy loss: -1.665489. Value loss: 10.142253. Entropy: 0.637977.\n",
      "episode: 5030   score: 210.0  epsilon: 1.0    steps: 789  evaluation reward: 317.35\n",
      "episode: 5031   score: 330.0  epsilon: 1.0    steps: 974  evaluation reward: 318.55\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11920: Policy loss: -0.629944. Value loss: 20.293201. Entropy: 0.670710.\n",
      "Iteration 11921: Policy loss: -0.471370. Value loss: 10.123046. Entropy: 0.699606.\n",
      "Iteration 11922: Policy loss: -0.489702. Value loss: 9.151925. Entropy: 0.685745.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11923: Policy loss: -0.210777. Value loss: 16.230000. Entropy: 0.581465.\n",
      "Iteration 11924: Policy loss: -0.156602. Value loss: 13.336007. Entropy: 0.587345.\n",
      "Iteration 11925: Policy loss: -0.078256. Value loss: 9.961480. Entropy: 0.591841.\n",
      "episode: 5032   score: 285.0  epsilon: 1.0    steps: 135  evaluation reward: 319.3\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11926: Policy loss: 0.801067. Value loss: 23.654585. Entropy: 0.567716.\n",
      "Iteration 11927: Policy loss: 0.775537. Value loss: 14.282562. Entropy: 0.524164.\n",
      "Iteration 11928: Policy loss: 0.708735. Value loss: 11.339901. Entropy: 0.551950.\n",
      "episode: 5033   score: 270.0  epsilon: 1.0    steps: 652  evaluation reward: 319.35\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11929: Policy loss: 1.040003. Value loss: 20.041943. Entropy: 0.542709.\n",
      "Iteration 11930: Policy loss: 0.890534. Value loss: 10.807552. Entropy: 0.526527.\n",
      "Iteration 11931: Policy loss: 1.294234. Value loss: 8.386376. Entropy: 0.542851.\n",
      "episode: 5034   score: 210.0  epsilon: 1.0    steps: 458  evaluation reward: 316.45\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11932: Policy loss: -2.222476. Value loss: 35.327629. Entropy: 0.711460.\n",
      "Iteration 11933: Policy loss: -2.352609. Value loss: 20.439941. Entropy: 0.715796.\n",
      "Iteration 11934: Policy loss: -2.254060. Value loss: 14.845572. Entropy: 0.722189.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11935: Policy loss: 0.920854. Value loss: 24.898926. Entropy: 0.618670.\n",
      "Iteration 11936: Policy loss: 0.851605. Value loss: 17.629742. Entropy: 0.662312.\n",
      "Iteration 11937: Policy loss: 1.146203. Value loss: 12.065613. Entropy: 0.664234.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11938: Policy loss: 1.144102. Value loss: 29.575132. Entropy: 0.610162.\n",
      "Iteration 11939: Policy loss: 1.168705. Value loss: 18.442770. Entropy: 0.595286.\n",
      "Iteration 11940: Policy loss: 1.106561. Value loss: 14.186047. Entropy: 0.620573.\n",
      "episode: 5035   score: 325.0  epsilon: 1.0    steps: 276  evaluation reward: 317.1\n",
      "episode: 5036   score: 335.0  epsilon: 1.0    steps: 623  evaluation reward: 316.75\n",
      "episode: 5037   score: 315.0  epsilon: 1.0    steps: 886  evaluation reward: 317.5\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11941: Policy loss: 0.097181. Value loss: 31.660238. Entropy: 0.729032.\n",
      "Iteration 11942: Policy loss: -0.050497. Value loss: 18.604330. Entropy: 0.719499.\n",
      "Iteration 11943: Policy loss: 0.100681. Value loss: 14.029845. Entropy: 0.700791.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11944: Policy loss: 0.341158. Value loss: 23.255733. Entropy: 0.704753.\n",
      "Iteration 11945: Policy loss: 0.307026. Value loss: 14.691700. Entropy: 0.743633.\n",
      "Iteration 11946: Policy loss: 0.603865. Value loss: 8.897037. Entropy: 0.731816.\n",
      "episode: 5038   score: 350.0  epsilon: 1.0    steps: 937  evaluation reward: 317.65\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11947: Policy loss: 0.828826. Value loss: 24.738468. Entropy: 0.613155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11948: Policy loss: 0.398842. Value loss: 16.877108. Entropy: 0.577586.\n",
      "Iteration 11949: Policy loss: 0.739010. Value loss: 12.348918. Entropy: 0.595264.\n",
      "episode: 5039   score: 295.0  epsilon: 1.0    steps: 146  evaluation reward: 317.5\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11950: Policy loss: -0.484469. Value loss: 18.294228. Entropy: 0.582240.\n",
      "Iteration 11951: Policy loss: -0.420911. Value loss: 11.982015. Entropy: 0.603314.\n",
      "Iteration 11952: Policy loss: -0.315036. Value loss: 8.494916. Entropy: 0.585997.\n",
      "episode: 5040   score: 500.0  epsilon: 1.0    steps: 96  evaluation reward: 317.95\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11953: Policy loss: -0.717817. Value loss: 24.026264. Entropy: 0.529185.\n",
      "Iteration 11954: Policy loss: -0.574170. Value loss: 13.715859. Entropy: 0.544528.\n",
      "Iteration 11955: Policy loss: -0.674207. Value loss: 11.331214. Entropy: 0.546434.\n",
      "episode: 5041   score: 210.0  epsilon: 1.0    steps: 301  evaluation reward: 316.25\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11956: Policy loss: 0.123667. Value loss: 15.173010. Entropy: 0.721451.\n",
      "Iteration 11957: Policy loss: 0.366737. Value loss: 7.963122. Entropy: 0.727536.\n",
      "Iteration 11958: Policy loss: 0.229127. Value loss: 6.248621. Entropy: 0.719460.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11959: Policy loss: -3.091308. Value loss: 272.689880. Entropy: 0.651851.\n",
      "Iteration 11960: Policy loss: -2.088958. Value loss: 137.330963. Entropy: 0.695949.\n",
      "Iteration 11961: Policy loss: -2.630413. Value loss: 99.608765. Entropy: 0.675395.\n",
      "episode: 5042   score: 165.0  epsilon: 1.0    steps: 225  evaluation reward: 314.7\n",
      "episode: 5043   score: 395.0  epsilon: 1.0    steps: 738  evaluation reward: 313.6\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11962: Policy loss: -0.085358. Value loss: 28.264208. Entropy: 0.684058.\n",
      "Iteration 11963: Policy loss: 0.179542. Value loss: 15.191170. Entropy: 0.669949.\n",
      "Iteration 11964: Policy loss: -0.071914. Value loss: 13.420468. Entropy: 0.692179.\n",
      "episode: 5044   score: 650.0  epsilon: 1.0    steps: 475  evaluation reward: 317.2\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11965: Policy loss: -0.839695. Value loss: 20.999586. Entropy: 0.639806.\n",
      "Iteration 11966: Policy loss: -0.447181. Value loss: 12.421963. Entropy: 0.621412.\n",
      "Iteration 11967: Policy loss: -0.751939. Value loss: 12.535762. Entropy: 0.624316.\n",
      "episode: 5045   score: 290.0  epsilon: 1.0    steps: 584  evaluation reward: 317.25\n",
      "episode: 5046   score: 230.0  epsilon: 1.0    steps: 824  evaluation reward: 316.1\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11968: Policy loss: 1.754086. Value loss: 20.998753. Entropy: 0.681848.\n",
      "Iteration 11969: Policy loss: 2.021156. Value loss: 13.456511. Entropy: 0.703586.\n",
      "Iteration 11970: Policy loss: 1.952749. Value loss: 11.399870. Entropy: 0.694532.\n",
      "episode: 5047   score: 240.0  epsilon: 1.0    steps: 912  evaluation reward: 315.8\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11971: Policy loss: 1.335277. Value loss: 35.381786. Entropy: 0.491587.\n",
      "Iteration 11972: Policy loss: 1.567292. Value loss: 21.478508. Entropy: 0.501164.\n",
      "Iteration 11973: Policy loss: 1.191371. Value loss: 16.980490. Entropy: 0.480794.\n",
      "episode: 5048   score: 345.0  epsilon: 1.0    steps: 124  evaluation reward: 316.05\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11974: Policy loss: 3.109055. Value loss: 20.505714. Entropy: 0.566634.\n",
      "Iteration 11975: Policy loss: 3.069071. Value loss: 13.340283. Entropy: 0.585502.\n",
      "Iteration 11976: Policy loss: 3.027011. Value loss: 9.721499. Entropy: 0.608257.\n",
      "episode: 5049   score: 120.0  epsilon: 1.0    steps: 225  evaluation reward: 312.25\n",
      "episode: 5050   score: 210.0  epsilon: 1.0    steps: 759  evaluation reward: 310.55\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11977: Policy loss: 2.609793. Value loss: 31.249254. Entropy: 0.571520.\n",
      "Iteration 11978: Policy loss: 2.566536. Value loss: 18.808746. Entropy: 0.528241.\n",
      "Iteration 11979: Policy loss: 2.476313. Value loss: 14.847013. Entropy: 0.571213.\n",
      "now time :  2019-02-25 22:23:50.659844\n",
      "episode: 5051   score: 275.0  epsilon: 1.0    steps: 336  evaluation reward: 309.95\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11980: Policy loss: -0.428331. Value loss: 19.438120. Entropy: 0.564780.\n",
      "Iteration 11981: Policy loss: -0.632260. Value loss: 11.770786. Entropy: 0.561432.\n",
      "Iteration 11982: Policy loss: -0.639437. Value loss: 10.369657. Entropy: 0.562566.\n",
      "episode: 5052   score: 210.0  epsilon: 1.0    steps: 481  evaluation reward: 310.25\n",
      "episode: 5053   score: 110.0  epsilon: 1.0    steps: 586  evaluation reward: 308.5\n",
      "episode: 5054   score: 105.0  epsilon: 1.0    steps: 933  evaluation reward: 304.45\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11983: Policy loss: 0.968271. Value loss: 22.331915. Entropy: 0.742905.\n",
      "Iteration 11984: Policy loss: 0.665253. Value loss: 11.059786. Entropy: 0.711061.\n",
      "Iteration 11985: Policy loss: 0.970937. Value loss: 8.962116. Entropy: 0.721343.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11986: Policy loss: 0.330165. Value loss: 11.406471. Entropy: 0.505278.\n",
      "Iteration 11987: Policy loss: 0.224359. Value loss: 7.082313. Entropy: 0.501933.\n",
      "Iteration 11988: Policy loss: 0.430211. Value loss: 4.674534. Entropy: 0.497895.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11989: Policy loss: -1.650965. Value loss: 187.266495. Entropy: 0.598270.\n",
      "Iteration 11990: Policy loss: -0.788249. Value loss: 29.595711. Entropy: 0.575947.\n",
      "Iteration 11991: Policy loss: -0.693772. Value loss: 16.138609. Entropy: 0.568283.\n",
      "episode: 5055   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 303.85\n",
      "episode: 5056   score: 260.0  epsilon: 1.0    steps: 789  evaluation reward: 300.75\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11992: Policy loss: 0.329055. Value loss: 15.479898. Entropy: 0.649890.\n",
      "Iteration 11993: Policy loss: 0.418615. Value loss: 10.152096. Entropy: 0.631918.\n",
      "Iteration 11994: Policy loss: 0.557782. Value loss: 8.540828. Entropy: 0.635539.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11995: Policy loss: 0.919182. Value loss: 30.829214. Entropy: 0.602373.\n",
      "Iteration 11996: Policy loss: 0.924898. Value loss: 19.246265. Entropy: 0.598347.\n",
      "Iteration 11997: Policy loss: 0.928954. Value loss: 13.453795. Entropy: 0.606838.\n",
      "episode: 5057   score: 210.0  epsilon: 1.0    steps: 998  evaluation reward: 300.75\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11998: Policy loss: -1.267544. Value loss: 23.439548. Entropy: 0.636618.\n",
      "Iteration 11999: Policy loss: -1.286899. Value loss: 14.538872. Entropy: 0.634855.\n",
      "Iteration 12000: Policy loss: -1.238386. Value loss: 11.802986. Entropy: 0.638750.\n",
      "episode: 5058   score: 260.0  epsilon: 1.0    steps: 61  evaluation reward: 300.2\n",
      "episode: 5059   score: 460.0  epsilon: 1.0    steps: 322  evaluation reward: 302.15\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12001: Policy loss: -2.120182. Value loss: 24.709316. Entropy: 0.745257.\n",
      "Iteration 12002: Policy loss: -1.867889. Value loss: 15.374535. Entropy: 0.750976.\n",
      "Iteration 12003: Policy loss: -2.164516. Value loss: 11.710680. Entropy: 0.736222.\n",
      "episode: 5060   score: 325.0  epsilon: 1.0    steps: 468  evaluation reward: 303.25\n",
      "episode: 5061   score: 285.0  epsilon: 1.0    steps: 677  evaluation reward: 303.95\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12004: Policy loss: -1.758419. Value loss: 34.266163. Entropy: 0.575799.\n",
      "Iteration 12005: Policy loss: -1.534717. Value loss: 20.326416. Entropy: 0.563983.\n",
      "Iteration 12006: Policy loss: -1.466472. Value loss: 13.114343. Entropy: 0.555012.\n",
      "episode: 5062   score: 180.0  epsilon: 1.0    steps: 133  evaluation reward: 301.5\n",
      "episode: 5063   score: 405.0  epsilon: 1.0    steps: 577  evaluation reward: 302.75\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12007: Policy loss: 0.293819. Value loss: 22.337784. Entropy: 0.477586.\n",
      "Iteration 12008: Policy loss: 0.142334. Value loss: 11.294492. Entropy: 0.487061.\n",
      "Iteration 12009: Policy loss: 0.011539. Value loss: 8.946778. Entropy: 0.505252.\n",
      "episode: 5064   score: 30.0  epsilon: 1.0    steps: 76  evaluation reward: 298.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12010: Policy loss: -0.433963. Value loss: 19.383909. Entropy: 0.448911.\n",
      "Iteration 12011: Policy loss: -0.601723. Value loss: 13.715508. Entropy: 0.418112.\n",
      "Iteration 12012: Policy loss: -0.593481. Value loss: 11.293067. Entropy: 0.454473.\n",
      "episode: 5065   score: 270.0  epsilon: 1.0    steps: 857  evaluation reward: 295.6\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12013: Policy loss: 1.147232. Value loss: 24.261370. Entropy: 0.409410.\n",
      "Iteration 12014: Policy loss: 1.007542. Value loss: 11.897675. Entropy: 0.390404.\n",
      "Iteration 12015: Policy loss: 1.076442. Value loss: 8.357045. Entropy: 0.401628.\n",
      "episode: 5066   score: 110.0  epsilon: 1.0    steps: 296  evaluation reward: 291.55\n",
      "episode: 5067   score: 105.0  epsilon: 1.0    steps: 473  evaluation reward: 289.7\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12016: Policy loss: 1.882857. Value loss: 19.755444. Entropy: 0.512801.\n",
      "Iteration 12017: Policy loss: 2.102803. Value loss: 8.227768. Entropy: 0.503590.\n",
      "Iteration 12018: Policy loss: 1.791859. Value loss: 7.334867. Entropy: 0.531277.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12019: Policy loss: 1.227298. Value loss: 21.107920. Entropy: 0.508880.\n",
      "Iteration 12020: Policy loss: 1.063071. Value loss: 12.765194. Entropy: 0.561224.\n",
      "Iteration 12021: Policy loss: 1.106570. Value loss: 8.595654. Entropy: 0.548524.\n",
      "episode: 5068   score: 75.0  epsilon: 1.0    steps: 27  evaluation reward: 287.3\n",
      "episode: 5069   score: 105.0  epsilon: 1.0    steps: 589  evaluation reward: 281.35\n",
      "episode: 5070   score: 305.0  epsilon: 1.0    steps: 993  evaluation reward: 280.75\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12022: Policy loss: 1.030109. Value loss: 22.052881. Entropy: 0.588845.\n",
      "Iteration 12023: Policy loss: 1.187394. Value loss: 10.413976. Entropy: 0.617553.\n",
      "Iteration 12024: Policy loss: 1.287722. Value loss: 8.434216. Entropy: 0.632308.\n",
      "episode: 5071   score: 260.0  epsilon: 1.0    steps: 762  evaluation reward: 279.25\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12025: Policy loss: -0.401260. Value loss: 14.080071. Entropy: 0.432754.\n",
      "Iteration 12026: Policy loss: -0.284209. Value loss: 7.372115. Entropy: 0.441846.\n",
      "Iteration 12027: Policy loss: -0.489523. Value loss: 5.776529. Entropy: 0.455547.\n",
      "episode: 5072   score: 155.0  epsilon: 1.0    steps: 378  evaluation reward: 277.6\n",
      "episode: 5073   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 277.2\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12028: Policy loss: -0.657619. Value loss: 27.032181. Entropy: 0.572884.\n",
      "Iteration 12029: Policy loss: -0.648951. Value loss: 17.813622. Entropy: 0.583686.\n",
      "Iteration 12030: Policy loss: -0.588267. Value loss: 14.567228. Entropy: 0.579574.\n",
      "episode: 5074   score: 305.0  epsilon: 1.0    steps: 244  evaluation reward: 276.05\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12031: Policy loss: 2.217774. Value loss: 29.035381. Entropy: 0.607652.\n",
      "Iteration 12032: Policy loss: 2.226566. Value loss: 14.930964. Entropy: 0.624633.\n",
      "Iteration 12033: Policy loss: 2.246923. Value loss: 9.304015. Entropy: 0.624450.\n",
      "episode: 5075   score: 105.0  epsilon: 1.0    steps: 1001  evaluation reward: 270.65\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12034: Policy loss: 1.326082. Value loss: 32.150806. Entropy: 0.544686.\n",
      "Iteration 12035: Policy loss: 1.425695. Value loss: 16.932747. Entropy: 0.574088.\n",
      "Iteration 12036: Policy loss: 1.410036. Value loss: 15.136313. Entropy: 0.546877.\n",
      "episode: 5076   score: 180.0  epsilon: 1.0    steps: 62  evaluation reward: 269.85\n",
      "episode: 5077   score: 290.0  epsilon: 1.0    steps: 490  evaluation reward: 269.6\n",
      "episode: 5078   score: 180.0  epsilon: 1.0    steps: 604  evaluation reward: 268.6\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12037: Policy loss: 0.244319. Value loss: 19.006943. Entropy: 0.551103.\n",
      "Iteration 12038: Policy loss: 0.185227. Value loss: 11.669234. Entropy: 0.543936.\n",
      "Iteration 12039: Policy loss: 0.248549. Value loss: 8.400115. Entropy: 0.549808.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12040: Policy loss: 0.428597. Value loss: 18.703985. Entropy: 0.484329.\n",
      "Iteration 12041: Policy loss: 0.347622. Value loss: 13.374057. Entropy: 0.474964.\n",
      "Iteration 12042: Policy loss: 0.257890. Value loss: 9.581980. Entropy: 0.493360.\n",
      "episode: 5079   score: 210.0  epsilon: 1.0    steps: 884  evaluation reward: 267.25\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12043: Policy loss: 0.175134. Value loss: 26.907740. Entropy: 0.346617.\n",
      "Iteration 12044: Policy loss: 0.416977. Value loss: 16.863724. Entropy: 0.326788.\n",
      "Iteration 12045: Policy loss: 0.257301. Value loss: 13.219622. Entropy: 0.336218.\n",
      "episode: 5080   score: 155.0  epsilon: 1.0    steps: 245  evaluation reward: 265.65\n",
      "episode: 5081   score: 210.0  epsilon: 1.0    steps: 353  evaluation reward: 265.05\n",
      "episode: 5082   score: 285.0  epsilon: 1.0    steps: 697  evaluation reward: 265.8\n",
      "episode: 5083   score: 75.0  epsilon: 1.0    steps: 906  evaluation reward: 263.7\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12046: Policy loss: 1.758631. Value loss: 21.512260. Entropy: 0.549597.\n",
      "Iteration 12047: Policy loss: 1.880112. Value loss: 10.183343. Entropy: 0.556847.\n",
      "Iteration 12048: Policy loss: 1.758364. Value loss: 7.769444. Entropy: 0.566739.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12049: Policy loss: -1.814543. Value loss: 21.096344. Entropy: 0.315449.\n",
      "Iteration 12050: Policy loss: -1.548954. Value loss: 13.661204. Entropy: 0.310227.\n",
      "Iteration 12051: Policy loss: -1.856410. Value loss: 10.415847. Entropy: 0.295231.\n",
      "episode: 5084   score: 180.0  epsilon: 1.0    steps: 117  evaluation reward: 262.9\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12052: Policy loss: 0.608495. Value loss: 31.221205. Entropy: 0.407166.\n",
      "Iteration 12053: Policy loss: 0.292114. Value loss: 18.901163. Entropy: 0.361809.\n",
      "Iteration 12054: Policy loss: 0.459030. Value loss: 15.910289. Entropy: 0.406687.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12055: Policy loss: -2.318603. Value loss: 21.337606. Entropy: 0.543154.\n",
      "Iteration 12056: Policy loss: -1.789754. Value loss: 10.329441. Entropy: 0.555567.\n",
      "Iteration 12057: Policy loss: -1.934350. Value loss: 10.824739. Entropy: 0.560173.\n",
      "episode: 5085   score: 75.0  epsilon: 1.0    steps: 352  evaluation reward: 260.5\n",
      "episode: 5086   score: 355.0  epsilon: 1.0    steps: 470  evaluation reward: 259.85\n",
      "episode: 5087   score: 290.0  epsilon: 1.0    steps: 584  evaluation reward: 260.65\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12058: Policy loss: 1.677745. Value loss: 26.107874. Entropy: 0.662114.\n",
      "Iteration 12059: Policy loss: 1.788067. Value loss: 13.098577. Entropy: 0.690008.\n",
      "Iteration 12060: Policy loss: 1.721989. Value loss: 10.932815. Entropy: 0.702733.\n",
      "episode: 5088   score: 210.0  epsilon: 1.0    steps: 652  evaluation reward: 257.2\n",
      "episode: 5089   score: 210.0  epsilon: 1.0    steps: 775  evaluation reward: 255.9\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12061: Policy loss: -0.152130. Value loss: 14.335381. Entropy: 0.493287.\n",
      "Iteration 12062: Policy loss: -0.264267. Value loss: 10.156219. Entropy: 0.489302.\n",
      "Iteration 12063: Policy loss: -0.207766. Value loss: 8.670515. Entropy: 0.483623.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12064: Policy loss: 1.587698. Value loss: 23.494699. Entropy: 0.444970.\n",
      "Iteration 12065: Policy loss: 1.627323. Value loss: 15.746951. Entropy: 0.473365.\n",
      "Iteration 12066: Policy loss: 1.147427. Value loss: 15.046252. Entropy: 0.493379.\n",
      "episode: 5090   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 255.9\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12067: Policy loss: -1.019380. Value loss: 13.931098. Entropy: 0.643490.\n",
      "Iteration 12068: Policy loss: -1.274602. Value loss: 8.624931. Entropy: 0.626913.\n",
      "Iteration 12069: Policy loss: -1.113574. Value loss: 7.333539. Entropy: 0.646547.\n",
      "episode: 5091   score: 265.0  epsilon: 1.0    steps: 911  evaluation reward: 254.45\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12070: Policy loss: -1.093552. Value loss: 22.335133. Entropy: 0.591677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12071: Policy loss: -1.438621. Value loss: 12.921186. Entropy: 0.572530.\n",
      "Iteration 12072: Policy loss: -1.190870. Value loss: 10.528094. Entropy: 0.580252.\n",
      "episode: 5092   score: 210.0  epsilon: 1.0    steps: 463  evaluation reward: 250.95\n",
      "episode: 5093   score: 210.0  epsilon: 1.0    steps: 553  evaluation reward: 249.9\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12073: Policy loss: -3.333889. Value loss: 186.135757. Entropy: 0.609644.\n",
      "Iteration 12074: Policy loss: -3.077235. Value loss: 94.902939. Entropy: 0.611733.\n",
      "Iteration 12075: Policy loss: -3.291081. Value loss: 51.461380. Entropy: 0.582024.\n",
      "episode: 5094   score: 225.0  epsilon: 1.0    steps: 321  evaluation reward: 248.55\n",
      "episode: 5095   score: 230.0  epsilon: 1.0    steps: 699  evaluation reward: 246.95\n",
      "episode: 5096   score: 210.0  epsilon: 1.0    steps: 796  evaluation reward: 246.3\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12076: Policy loss: 1.286437. Value loss: 11.603636. Entropy: 0.607401.\n",
      "Iteration 12077: Policy loss: 1.365746. Value loss: 7.526614. Entropy: 0.598735.\n",
      "Iteration 12078: Policy loss: 1.210616. Value loss: 6.759714. Entropy: 0.587216.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12079: Policy loss: 1.006539. Value loss: 19.969885. Entropy: 0.426948.\n",
      "Iteration 12080: Policy loss: 1.008803. Value loss: 11.792968. Entropy: 0.461813.\n",
      "Iteration 12081: Policy loss: 1.031073. Value loss: 8.378763. Entropy: 0.461779.\n",
      "episode: 5097   score: 590.0  epsilon: 1.0    steps: 225  evaluation reward: 251.0\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12082: Policy loss: 2.628558. Value loss: 25.033855. Entropy: 0.455092.\n",
      "Iteration 12083: Policy loss: 2.689934. Value loss: 11.605727. Entropy: 0.460089.\n",
      "Iteration 12084: Policy loss: 2.491393. Value loss: 9.894817. Entropy: 0.470281.\n",
      "episode: 5098   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 250.85\n",
      "episode: 5099   score: 75.0  epsilon: 1.0    steps: 747  evaluation reward: 247.4\n",
      "episode: 5100   score: 210.0  epsilon: 1.0    steps: 940  evaluation reward: 246.9\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12085: Policy loss: 2.577820. Value loss: 26.585571. Entropy: 0.596347.\n",
      "Iteration 12086: Policy loss: 2.190168. Value loss: 12.114946. Entropy: 0.620577.\n",
      "Iteration 12087: Policy loss: 2.614694. Value loss: 9.764957. Entropy: 0.592615.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12088: Policy loss: 1.034137. Value loss: 21.086140. Entropy: 0.437167.\n",
      "Iteration 12089: Policy loss: 1.063141. Value loss: 14.028256. Entropy: 0.420781.\n",
      "Iteration 12090: Policy loss: 1.043854. Value loss: 10.611782. Entropy: 0.440255.\n",
      "now time :  2019-02-25 22:25:54.519289\n",
      "episode: 5101   score: 180.0  epsilon: 1.0    steps: 295  evaluation reward: 246.8\n",
      "episode: 5102   score: 240.0  epsilon: 1.0    steps: 390  evaluation reward: 246.05\n",
      "episode: 5103   score: 180.0  epsilon: 1.0    steps: 777  evaluation reward: 246.3\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12091: Policy loss: 4.208078. Value loss: 25.592543. Entropy: 0.661350.\n",
      "Iteration 12092: Policy loss: 4.101041. Value loss: 15.069021. Entropy: 0.690903.\n",
      "Iteration 12093: Policy loss: 4.057846. Value loss: 11.173612. Entropy: 0.698007.\n",
      "episode: 5104   score: 50.0  epsilon: 1.0    steps: 127  evaluation reward: 241.8\n",
      "episode: 5105   score: 30.0  epsilon: 1.0    steps: 759  evaluation reward: 240.3\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12094: Policy loss: 2.881619. Value loss: 31.223579. Entropy: 0.535042.\n",
      "Iteration 12095: Policy loss: 2.984810. Value loss: 17.563540. Entropy: 0.535734.\n",
      "Iteration 12096: Policy loss: 3.025411. Value loss: 12.350657. Entropy: 0.516750.\n",
      "episode: 5106   score: 135.0  epsilon: 1.0    steps: 254  evaluation reward: 235.15\n",
      "episode: 5107   score: 205.0  epsilon: 1.0    steps: 522  evaluation reward: 234.45\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12097: Policy loss: 1.703131. Value loss: 18.593807. Entropy: 0.597135.\n",
      "Iteration 12098: Policy loss: 1.620760. Value loss: 13.911331. Entropy: 0.580250.\n",
      "Iteration 12099: Policy loss: 1.434084. Value loss: 10.726765. Entropy: 0.569708.\n",
      "episode: 5108   score: 75.0  epsilon: 1.0    steps: 777  evaluation reward: 234.45\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12100: Policy loss: -1.819261. Value loss: 29.469692. Entropy: 0.515715.\n",
      "Iteration 12101: Policy loss: -1.404532. Value loss: 17.688107. Entropy: 0.500006.\n",
      "Iteration 12102: Policy loss: -1.789562. Value loss: 14.214221. Entropy: 0.504095.\n",
      "episode: 5109   score: 180.0  epsilon: 1.0    steps: 379  evaluation reward: 233.4\n",
      "episode: 5110   score: 80.0  epsilon: 1.0    steps: 391  evaluation reward: 233.4\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12103: Policy loss: 0.067602. Value loss: 39.300468. Entropy: 0.436712.\n",
      "Iteration 12104: Policy loss: -0.230875. Value loss: 15.015022. Entropy: 0.415547.\n",
      "Iteration 12105: Policy loss: -0.152056. Value loss: 11.107299. Entropy: 0.413826.\n",
      "episode: 5111   score: 260.0  epsilon: 1.0    steps: 939  evaluation reward: 233.1\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12106: Policy loss: 0.802849. Value loss: 14.095860. Entropy: 0.479577.\n",
      "Iteration 12107: Policy loss: 0.689798. Value loss: 8.828048. Entropy: 0.454569.\n",
      "Iteration 12108: Policy loss: 0.749448. Value loss: 6.753548. Entropy: 0.490416.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12109: Policy loss: -0.465552. Value loss: 26.818497. Entropy: 0.307124.\n",
      "Iteration 12110: Policy loss: -0.529124. Value loss: 18.190134. Entropy: 0.301968.\n",
      "Iteration 12111: Policy loss: -0.203073. Value loss: 13.548861. Entropy: 0.346720.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12112: Policy loss: -1.658303. Value loss: 143.906433. Entropy: 0.532707.\n",
      "Iteration 12113: Policy loss: -1.968768. Value loss: 62.740017. Entropy: 0.553745.\n",
      "Iteration 12114: Policy loss: -1.830676. Value loss: 56.545261. Entropy: 0.558761.\n",
      "episode: 5112   score: 515.0  epsilon: 1.0    steps: 107  evaluation reward: 235.85\n",
      "episode: 5113   score: 210.0  epsilon: 1.0    steps: 194  evaluation reward: 235.8\n",
      "episode: 5114   score: 80.0  epsilon: 1.0    steps: 277  evaluation reward: 233.4\n",
      "episode: 5115   score: 210.0  epsilon: 1.0    steps: 506  evaluation reward: 232.7\n",
      "episode: 5116   score: 245.0  epsilon: 1.0    steps: 624  evaluation reward: 233.35\n",
      "episode: 5117   score: 225.0  epsilon: 1.0    steps: 682  evaluation reward: 233.5\n",
      "episode: 5118   score: 225.0  epsilon: 1.0    steps: 882  evaluation reward: 234.2\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12115: Policy loss: 0.983630. Value loss: 21.710703. Entropy: 0.642708.\n",
      "Iteration 12116: Policy loss: 0.861412. Value loss: 17.224653. Entropy: 0.627866.\n",
      "Iteration 12117: Policy loss: 0.966266. Value loss: 14.565102. Entropy: 0.625644.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12118: Policy loss: 0.609420. Value loss: 19.064907. Entropy: 0.453411.\n",
      "Iteration 12119: Policy loss: 0.789935. Value loss: 14.071162. Entropy: 0.428677.\n",
      "Iteration 12120: Policy loss: 0.642747. Value loss: 10.349392. Entropy: 0.457588.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12121: Policy loss: 0.339931. Value loss: 55.883858. Entropy: 0.469868.\n",
      "Iteration 12122: Policy loss: -0.013496. Value loss: 27.490479. Entropy: 0.423320.\n",
      "Iteration 12123: Policy loss: 0.220595. Value loss: 19.574345. Entropy: 0.427943.\n",
      "episode: 5119   score: 240.0  epsilon: 1.0    steps: 997  evaluation reward: 234.2\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12124: Policy loss: -0.278898. Value loss: 29.255522. Entropy: 0.529458.\n",
      "Iteration 12125: Policy loss: -0.321720. Value loss: 11.502650. Entropy: 0.539757.\n",
      "Iteration 12126: Policy loss: -0.375853. Value loss: 7.428815. Entropy: 0.540229.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12127: Policy loss: 3.841943. Value loss: 39.506493. Entropy: 0.404733.\n",
      "Iteration 12128: Policy loss: 3.630220. Value loss: 18.385038. Entropy: 0.407396.\n",
      "Iteration 12129: Policy loss: 3.602869. Value loss: 16.110001. Entropy: 0.382750.\n",
      "episode: 5120   score: 210.0  epsilon: 1.0    steps: 354  evaluation reward: 234.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5121   score: 210.0  epsilon: 1.0    steps: 725  evaluation reward: 232.5\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12130: Policy loss: 3.641014. Value loss: 30.589689. Entropy: 0.533345.\n",
      "Iteration 12131: Policy loss: 3.785156. Value loss: 23.035078. Entropy: 0.539134.\n",
      "Iteration 12132: Policy loss: 3.718445. Value loss: 18.630489. Entropy: 0.553280.\n",
      "episode: 5122   score: 210.0  epsilon: 1.0    steps: 417  evaluation reward: 232.0\n",
      "episode: 5123   score: 210.0  epsilon: 1.0    steps: 591  evaluation reward: 228.45\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12133: Policy loss: 0.556475. Value loss: 29.644060. Entropy: 0.515273.\n",
      "Iteration 12134: Policy loss: 0.737363. Value loss: 16.690449. Entropy: 0.547583.\n",
      "Iteration 12135: Policy loss: 0.503334. Value loss: 12.772456. Entropy: 0.543957.\n",
      "episode: 5124   score: 290.0  epsilon: 1.0    steps: 92  evaluation reward: 229.25\n",
      "episode: 5125   score: 155.0  epsilon: 1.0    steps: 1010  evaluation reward: 228.7\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12136: Policy loss: 0.186540. Value loss: 33.375957. Entropy: 0.664655.\n",
      "Iteration 12137: Policy loss: 0.162827. Value loss: 16.511684. Entropy: 0.660254.\n",
      "Iteration 12138: Policy loss: 0.110552. Value loss: 12.064795. Entropy: 0.635127.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12139: Policy loss: 1.269306. Value loss: 15.635265. Entropy: 0.541463.\n",
      "Iteration 12140: Policy loss: 1.427220. Value loss: 9.274773. Entropy: 0.571327.\n",
      "Iteration 12141: Policy loss: 1.333704. Value loss: 6.478011. Entropy: 0.563178.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12142: Policy loss: -0.744224. Value loss: 28.229374. Entropy: 0.541775.\n",
      "Iteration 12143: Policy loss: -0.883173. Value loss: 12.893621. Entropy: 0.534277.\n",
      "Iteration 12144: Policy loss: -0.583640. Value loss: 9.974048. Entropy: 0.528967.\n",
      "episode: 5126   score: 345.0  epsilon: 1.0    steps: 871  evaluation reward: 230.05\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12145: Policy loss: 0.449477. Value loss: 146.412521. Entropy: 0.594001.\n",
      "Iteration 12146: Policy loss: 0.397180. Value loss: 39.779747. Entropy: 0.612882.\n",
      "Iteration 12147: Policy loss: 0.776640. Value loss: 30.951063. Entropy: 0.623846.\n",
      "episode: 5127   score: 180.0  epsilon: 1.0    steps: 263  evaluation reward: 229.25\n",
      "episode: 5128   score: 210.0  epsilon: 1.0    steps: 652  evaluation reward: 229.25\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12148: Policy loss: 0.737543. Value loss: 25.065676. Entropy: 0.658341.\n",
      "Iteration 12149: Policy loss: 0.771379. Value loss: 18.211191. Entropy: 0.660479.\n",
      "Iteration 12150: Policy loss: 0.744352. Value loss: 14.327693. Entropy: 0.669985.\n",
      "episode: 5129   score: 260.0  epsilon: 1.0    steps: 633  evaluation reward: 229.75\n",
      "episode: 5130   score: 75.0  epsilon: 1.0    steps: 985  evaluation reward: 228.4\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12151: Policy loss: 1.410263. Value loss: 29.961521. Entropy: 0.643104.\n",
      "Iteration 12152: Policy loss: 1.465622. Value loss: 16.877281. Entropy: 0.633654.\n",
      "Iteration 12153: Policy loss: 1.323507. Value loss: 14.251761. Entropy: 0.643747.\n",
      "episode: 5131   score: 550.0  epsilon: 1.0    steps: 211  evaluation reward: 230.6\n",
      "episode: 5132   score: 285.0  epsilon: 1.0    steps: 474  evaluation reward: 230.6\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12154: Policy loss: 2.994878. Value loss: 29.181763. Entropy: 0.633866.\n",
      "Iteration 12155: Policy loss: 2.479629. Value loss: 11.923806. Entropy: 0.608927.\n",
      "Iteration 12156: Policy loss: 2.574616. Value loss: 9.587195. Entropy: 0.640534.\n",
      "episode: 5133   score: 285.0  epsilon: 1.0    steps: 35  evaluation reward: 230.75\n",
      "episode: 5134   score: 50.0  epsilon: 1.0    steps: 376  evaluation reward: 229.15\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12157: Policy loss: 0.305813. Value loss: 19.080910. Entropy: 0.508183.\n",
      "Iteration 12158: Policy loss: 0.262481. Value loss: 10.896899. Entropy: 0.510861.\n",
      "Iteration 12159: Policy loss: 0.227554. Value loss: 8.254631. Entropy: 0.513918.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12160: Policy loss: -1.537981. Value loss: 25.945393. Entropy: 0.385860.\n",
      "Iteration 12161: Policy loss: -1.251558. Value loss: 15.867059. Entropy: 0.387871.\n",
      "Iteration 12162: Policy loss: -1.329772. Value loss: 11.211971. Entropy: 0.397507.\n",
      "episode: 5135   score: 210.0  epsilon: 1.0    steps: 711  evaluation reward: 228.0\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12163: Policy loss: -1.290744. Value loss: 21.954321. Entropy: 0.490007.\n",
      "Iteration 12164: Policy loss: -1.501292. Value loss: 9.786276. Entropy: 0.476118.\n",
      "Iteration 12165: Policy loss: -1.317517. Value loss: 8.562645. Entropy: 0.471610.\n",
      "episode: 5136   score: 155.0  epsilon: 1.0    steps: 914  evaluation reward: 226.2\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12166: Policy loss: -1.872601. Value loss: 161.251038. Entropy: 0.555485.\n",
      "Iteration 12167: Policy loss: -1.677805. Value loss: 39.152149. Entropy: 0.584051.\n",
      "Iteration 12168: Policy loss: -2.483089. Value loss: 24.287899. Entropy: 0.576250.\n",
      "episode: 5137   score: 155.0  epsilon: 1.0    steps: 246  evaluation reward: 224.6\n",
      "episode: 5138   score: 210.0  epsilon: 1.0    steps: 535  evaluation reward: 223.2\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12169: Policy loss: 0.033916. Value loss: 16.210981. Entropy: 0.555697.\n",
      "Iteration 12170: Policy loss: 0.048264. Value loss: 7.028535. Entropy: 0.568689.\n",
      "Iteration 12171: Policy loss: 0.124505. Value loss: 5.663880. Entropy: 0.568633.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12172: Policy loss: -4.152515. Value loss: 127.826820. Entropy: 0.668307.\n",
      "Iteration 12173: Policy loss: -3.436209. Value loss: 49.372036. Entropy: 0.661160.\n",
      "Iteration 12174: Policy loss: -4.045411. Value loss: 38.241764. Entropy: 0.687019.\n",
      "episode: 5139   score: 210.0  epsilon: 1.0    steps: 267  evaluation reward: 222.35\n",
      "episode: 5140   score: 315.0  epsilon: 1.0    steps: 421  evaluation reward: 220.5\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12175: Policy loss: 2.077588. Value loss: 23.213747. Entropy: 0.639328.\n",
      "Iteration 12176: Policy loss: 2.281923. Value loss: 12.082308. Entropy: 0.648064.\n",
      "Iteration 12177: Policy loss: 2.251243. Value loss: 10.617355. Entropy: 0.646692.\n",
      "episode: 5141   score: 180.0  epsilon: 1.0    steps: 727  evaluation reward: 220.2\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12178: Policy loss: 1.510903. Value loss: 23.824184. Entropy: 0.685571.\n",
      "Iteration 12179: Policy loss: 1.596027. Value loss: 14.811116. Entropy: 0.702270.\n",
      "Iteration 12180: Policy loss: 1.550952. Value loss: 10.786312. Entropy: 0.716293.\n",
      "episode: 5142   score: 260.0  epsilon: 1.0    steps: 67  evaluation reward: 221.15\n",
      "episode: 5143   score: 805.0  epsilon: 1.0    steps: 796  evaluation reward: 225.25\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12181: Policy loss: -0.160209. Value loss: 105.886635. Entropy: 0.646004.\n",
      "Iteration 12182: Policy loss: -0.123592. Value loss: 73.757782. Entropy: 0.639826.\n",
      "Iteration 12183: Policy loss: -0.521081. Value loss: 34.950027. Entropy: 0.667184.\n",
      "episode: 5144   score: 410.0  epsilon: 1.0    steps: 602  evaluation reward: 222.85\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12184: Policy loss: 0.763845. Value loss: 136.917007. Entropy: 0.717038.\n",
      "Iteration 12185: Policy loss: 0.252076. Value loss: 58.873146. Entropy: 0.717985.\n",
      "Iteration 12186: Policy loss: 0.692155. Value loss: 39.678619. Entropy: 0.734946.\n",
      "episode: 5145   score: 210.0  epsilon: 1.0    steps: 142  evaluation reward: 222.05\n",
      "episode: 5146   score: 15.0  epsilon: 1.0    steps: 258  evaluation reward: 219.9\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12187: Policy loss: -0.706327. Value loss: 21.648266. Entropy: 0.702425.\n",
      "Iteration 12188: Policy loss: -0.849122. Value loss: 15.335566. Entropy: 0.716428.\n",
      "Iteration 12189: Policy loss: -0.719500. Value loss: 13.876860. Entropy: 0.710146.\n",
      "episode: 5147   score: 410.0  epsilon: 1.0    steps: 500  evaluation reward: 221.6\n",
      "episode: 5148   score: 230.0  epsilon: 1.0    steps: 977  evaluation reward: 220.45\n",
      "Training network. lr: 0.000157. clip: 0.062684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12190: Policy loss: -2.531227. Value loss: 113.599380. Entropy: 0.545143.\n",
      "Iteration 12191: Policy loss: -2.859607. Value loss: 57.388329. Entropy: 0.528612.\n",
      "Iteration 12192: Policy loss: -2.678123. Value loss: 27.071236. Entropy: 0.516512.\n",
      "episode: 5149   score: 210.0  epsilon: 1.0    steps: 721  evaluation reward: 221.35\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12193: Policy loss: 1.047387. Value loss: 132.192184. Entropy: 0.513782.\n",
      "Iteration 12194: Policy loss: 1.382359. Value loss: 51.585152. Entropy: 0.508886.\n",
      "Iteration 12195: Policy loss: 1.245163. Value loss: 41.319092. Entropy: 0.488714.\n",
      "episode: 5150   score: 380.0  epsilon: 1.0    steps: 110  evaluation reward: 223.05\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12196: Policy loss: -0.435343. Value loss: 74.927048. Entropy: 0.370771.\n",
      "Iteration 12197: Policy loss: -0.276946. Value loss: 30.562443. Entropy: 0.348532.\n",
      "Iteration 12198: Policy loss: -0.112326. Value loss: 25.143345. Entropy: 0.369842.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12199: Policy loss: 2.062635. Value loss: 222.121140. Entropy: 0.667028.\n",
      "Iteration 12200: Policy loss: 2.482514. Value loss: 140.180969. Entropy: 0.653963.\n",
      "Iteration 12201: Policy loss: 3.025788. Value loss: 89.599213. Entropy: 0.657154.\n",
      "now time :  2019-02-25 22:28:00.161826\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5151   score: 260.0  epsilon: 1.0    steps: 877  evaluation reward: 222.9\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12202: Policy loss: 4.981977. Value loss: 862.545776. Entropy: 0.411694.\n",
      "Iteration 12203: Policy loss: 5.932477. Value loss: 396.634003. Entropy: 0.385241.\n",
      "Iteration 12204: Policy loss: 5.975987. Value loss: 263.618256. Entropy: 0.384032.\n",
      "episode: 5152   score: 210.0  epsilon: 1.0    steps: 515  evaluation reward: 222.9\n",
      "episode: 5153   score: 135.0  epsilon: 1.0    steps: 970  evaluation reward: 223.15\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12205: Policy loss: 1.759693. Value loss: 91.792816. Entropy: 0.297268.\n",
      "Iteration 12206: Policy loss: 1.866612. Value loss: 48.852505. Entropy: 0.315677.\n",
      "Iteration 12207: Policy loss: 1.588933. Value loss: 33.066513. Entropy: 0.309335.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12208: Policy loss: 2.266338. Value loss: 41.789524. Entropy: 0.373148.\n",
      "Iteration 12209: Policy loss: 2.259560. Value loss: 25.746555. Entropy: 0.359590.\n",
      "Iteration 12210: Policy loss: 2.345219. Value loss: 22.171093. Entropy: 0.376454.\n",
      "episode: 5154   score: 540.0  epsilon: 1.0    steps: 267  evaluation reward: 227.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12211: Policy loss: -0.330802. Value loss: 51.536018. Entropy: 0.371512.\n",
      "Iteration 12212: Policy loss: -0.444865. Value loss: 29.045261. Entropy: 0.358411.\n",
      "Iteration 12213: Policy loss: -0.279329. Value loss: 22.363781. Entropy: 0.369996.\n",
      "episode: 5155   score: 445.0  epsilon: 1.0    steps: 241  evaluation reward: 230.15\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12214: Policy loss: 2.536888. Value loss: 33.831757. Entropy: 0.253919.\n",
      "Iteration 12215: Policy loss: 2.587146. Value loss: 18.173973. Entropy: 0.287944.\n",
      "Iteration 12216: Policy loss: 2.738762. Value loss: 16.521818. Entropy: 0.253392.\n",
      "episode: 5156   score: 220.0  epsilon: 1.0    steps: 44  evaluation reward: 229.75\n",
      "episode: 5157   score: 630.0  epsilon: 1.0    steps: 427  evaluation reward: 233.95\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12217: Policy loss: 1.641332. Value loss: 52.359085. Entropy: 0.181104.\n",
      "Iteration 12218: Policy loss: 1.220424. Value loss: 30.000532. Entropy: 0.196082.\n",
      "Iteration 12219: Policy loss: 1.651364. Value loss: 20.866573. Entropy: 0.180017.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12220: Policy loss: -3.310239. Value loss: 285.024994. Entropy: 0.136398.\n",
      "Iteration 12221: Policy loss: -4.492903. Value loss: 254.709229. Entropy: 0.101928.\n",
      "Iteration 12222: Policy loss: -3.891377. Value loss: 180.532974. Entropy: 0.115518.\n",
      "episode: 5158   score: 245.0  epsilon: 1.0    steps: 513  evaluation reward: 233.8\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12223: Policy loss: 0.997202. Value loss: 30.884369. Entropy: 0.137925.\n",
      "Iteration 12224: Policy loss: 1.000501. Value loss: 18.949196. Entropy: 0.119085.\n",
      "Iteration 12225: Policy loss: 1.074909. Value loss: 14.861212. Entropy: 0.129953.\n",
      "episode: 5159   score: 330.0  epsilon: 1.0    steps: 751  evaluation reward: 232.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12226: Policy loss: 1.327356. Value loss: 48.040657. Entropy: 0.099301.\n",
      "Iteration 12227: Policy loss: 1.238598. Value loss: 31.061209. Entropy: 0.090502.\n",
      "Iteration 12228: Policy loss: 0.992588. Value loss: 22.730158. Entropy: 0.088984.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12229: Policy loss: 0.156042. Value loss: 222.221924. Entropy: 0.124037.\n",
      "Iteration 12230: Policy loss: 1.096928. Value loss: 104.993065. Entropy: 0.106370.\n",
      "Iteration 12231: Policy loss: 0.571901. Value loss: 128.638321. Entropy: 0.113538.\n",
      "episode: 5160   score: 225.0  epsilon: 1.0    steps: 89  evaluation reward: 231.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12232: Policy loss: -3.789328. Value loss: 363.906036. Entropy: 0.171613.\n",
      "Iteration 12233: Policy loss: -3.857709. Value loss: 222.225021. Entropy: 0.189817.\n",
      "Iteration 12234: Policy loss: -4.390082. Value loss: 217.177414. Entropy: 0.169664.\n",
      "episode: 5161   score: 285.0  epsilon: 1.0    steps: 144  evaluation reward: 231.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12235: Policy loss: -3.368495. Value loss: 206.963852. Entropy: 0.141645.\n",
      "Iteration 12236: Policy loss: -3.050070. Value loss: 169.453598. Entropy: 0.156012.\n",
      "Iteration 12237: Policy loss: -3.320926. Value loss: 160.364136. Entropy: 0.139704.\n",
      "episode: 5162   score: 680.0  epsilon: 1.0    steps: 292  evaluation reward: 236.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12238: Policy loss: 3.379351. Value loss: 56.316147. Entropy: 0.155504.\n",
      "Iteration 12239: Policy loss: 3.074782. Value loss: 33.433853. Entropy: 0.150015.\n",
      "Iteration 12240: Policy loss: 3.030693. Value loss: 25.856628. Entropy: 0.127853.\n",
      "episode: 5163   score: 210.0  epsilon: 1.0    steps: 762  evaluation reward: 234.55\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12241: Policy loss: 1.335253. Value loss: 71.453949. Entropy: 0.147238.\n",
      "Iteration 12242: Policy loss: 1.163153. Value loss: 41.044380. Entropy: 0.125791.\n",
      "Iteration 12243: Policy loss: 1.269480. Value loss: 29.087921. Entropy: 0.132314.\n",
      "episode: 5164   score: 520.0  epsilon: 1.0    steps: 429  evaluation reward: 239.45\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12244: Policy loss: -0.875769. Value loss: 34.899132. Entropy: 0.149444.\n",
      "Iteration 12245: Policy loss: -0.974435. Value loss: 23.262630. Entropy: 0.160518.\n",
      "Iteration 12246: Policy loss: -0.983089. Value loss: 18.334463. Entropy: 0.149822.\n",
      "episode: 5165   score: 120.0  epsilon: 1.0    steps: 311  evaluation reward: 237.95\n",
      "episode: 5166   score: 830.0  epsilon: 1.0    steps: 853  evaluation reward: 245.15\n",
      "episode: 5167   score: 780.0  epsilon: 1.0    steps: 924  evaluation reward: 251.9\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12247: Policy loss: 1.075644. Value loss: 68.627693. Entropy: 0.149452.\n",
      "Iteration 12248: Policy loss: 1.878904. Value loss: 40.924660. Entropy: 0.160847.\n",
      "Iteration 12249: Policy loss: 1.403306. Value loss: 28.528656. Entropy: 0.148785.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12250: Policy loss: 5.176578. Value loss: 47.037998. Entropy: 0.134613.\n",
      "Iteration 12251: Policy loss: 4.390624. Value loss: 28.616722. Entropy: 0.210752.\n",
      "Iteration 12252: Policy loss: 4.702614. Value loss: 20.543674. Entropy: 0.282367.\n",
      "episode: 5168   score: 120.0  epsilon: 1.0    steps: 657  evaluation reward: 252.35\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12253: Policy loss: -1.784639. Value loss: 36.814800. Entropy: 0.323995.\n",
      "Iteration 12254: Policy loss: -1.735840. Value loss: 23.029219. Entropy: 0.313421.\n",
      "Iteration 12255: Policy loss: -1.530810. Value loss: 17.474813. Entropy: 0.308643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5169   score: 255.0  epsilon: 1.0    steps: 100  evaluation reward: 253.85\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12256: Policy loss: 0.285077. Value loss: 34.507450. Entropy: 0.354425.\n",
      "Iteration 12257: Policy loss: 0.149624. Value loss: 19.845474. Entropy: 0.349778.\n",
      "Iteration 12258: Policy loss: 0.189435. Value loss: 15.875266. Entropy: 0.340052.\n",
      "episode: 5170   score: 750.0  epsilon: 1.0    steps: 566  evaluation reward: 258.3\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12259: Policy loss: 1.368242. Value loss: 44.075733. Entropy: 0.244691.\n",
      "Iteration 12260: Policy loss: 1.143256. Value loss: 22.477737. Entropy: 0.212638.\n",
      "Iteration 12261: Policy loss: 1.232849. Value loss: 18.107899. Entropy: 0.231349.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12262: Policy loss: -3.666772. Value loss: 357.089722. Entropy: 0.130692.\n",
      "Iteration 12263: Policy loss: -3.059283. Value loss: 136.688553. Entropy: 0.119731.\n",
      "Iteration 12264: Policy loss: -2.482135. Value loss: 63.629223. Entropy: 0.128620.\n",
      "episode: 5171   score: 285.0  epsilon: 1.0    steps: 310  evaluation reward: 258.55\n",
      "episode: 5172   score: 260.0  epsilon: 1.0    steps: 901  evaluation reward: 259.6\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12265: Policy loss: 3.576732. Value loss: 50.430103. Entropy: 0.141319.\n",
      "Iteration 12266: Policy loss: 2.773458. Value loss: 28.443386. Entropy: 0.127962.\n",
      "Iteration 12267: Policy loss: 3.247962. Value loss: 21.227993. Entropy: 0.126136.\n",
      "episode: 5173   score: 90.0  epsilon: 1.0    steps: 582  evaluation reward: 258.4\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12268: Policy loss: 2.824271. Value loss: 33.340408. Entropy: 0.145078.\n",
      "Iteration 12269: Policy loss: 3.426978. Value loss: 19.039507. Entropy: 0.151457.\n",
      "Iteration 12270: Policy loss: 3.168340. Value loss: 15.489084. Entropy: 0.153510.\n",
      "episode: 5174   score: 465.0  epsilon: 1.0    steps: 233  evaluation reward: 260.0\n",
      "episode: 5175   score: 265.0  epsilon: 1.0    steps: 642  evaluation reward: 261.6\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12271: Policy loss: 0.079327. Value loss: 145.657654. Entropy: 0.162323.\n",
      "Iteration 12272: Policy loss: -0.050293. Value loss: 57.468826. Entropy: 0.178253.\n",
      "Iteration 12273: Policy loss: -0.279584. Value loss: 49.902561. Entropy: 0.175384.\n",
      "episode: 5176   score: 120.0  epsilon: 1.0    steps: 918  evaluation reward: 261.0\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12274: Policy loss: 0.798461. Value loss: 66.375824. Entropy: 0.128353.\n",
      "Iteration 12275: Policy loss: 1.142334. Value loss: 24.548822. Entropy: 0.103381.\n",
      "Iteration 12276: Policy loss: 0.701986. Value loss: 16.752752. Entropy: 0.125092.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12277: Policy loss: 3.169423. Value loss: 59.637783. Entropy: 0.183903.\n",
      "Iteration 12278: Policy loss: 3.232820. Value loss: 27.841288. Entropy: 0.183933.\n",
      "Iteration 12279: Policy loss: 2.845939. Value loss: 16.838036. Entropy: 0.203849.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12280: Policy loss: 0.468606. Value loss: 31.399048. Entropy: 0.147834.\n",
      "Iteration 12281: Policy loss: 0.571556. Value loss: 17.681524. Entropy: 0.153338.\n",
      "Iteration 12282: Policy loss: 0.336360. Value loss: 10.954181. Entropy: 0.159162.\n",
      "episode: 5177   score: 390.0  epsilon: 1.0    steps: 21  evaluation reward: 262.0\n",
      "episode: 5178   score: 260.0  epsilon: 1.0    steps: 315  evaluation reward: 262.8\n",
      "episode: 5179   score: 745.0  epsilon: 1.0    steps: 473  evaluation reward: 268.15\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12283: Policy loss: 1.907732. Value loss: 28.351320. Entropy: 0.184080.\n",
      "Iteration 12284: Policy loss: 1.903788. Value loss: 16.513103. Entropy: 0.184249.\n",
      "Iteration 12285: Policy loss: 1.924290. Value loss: 15.164822. Entropy: 0.197606.\n",
      "episode: 5180   score: 210.0  epsilon: 1.0    steps: 222  evaluation reward: 268.7\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12286: Policy loss: -0.080628. Value loss: 53.161026. Entropy: 0.213026.\n",
      "Iteration 12287: Policy loss: -0.484115. Value loss: 25.369465. Entropy: 0.183243.\n",
      "Iteration 12288: Policy loss: -0.108878. Value loss: 19.259497. Entropy: 0.198191.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12289: Policy loss: 0.148985. Value loss: 31.321440. Entropy: 0.248102.\n",
      "Iteration 12290: Policy loss: -0.241886. Value loss: 14.461684. Entropy: 0.235252.\n",
      "Iteration 12291: Policy loss: -0.051574. Value loss: 11.750648. Entropy: 0.221268.\n",
      "episode: 5181   score: 135.0  epsilon: 1.0    steps: 124  evaluation reward: 267.95\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12292: Policy loss: 2.310891. Value loss: 28.444778. Entropy: 0.284202.\n",
      "Iteration 12293: Policy loss: 2.676751. Value loss: 19.355263. Entropy: 0.291659.\n",
      "Iteration 12294: Policy loss: 2.213276. Value loss: 14.732215. Entropy: 0.304916.\n",
      "episode: 5182   score: 135.0  epsilon: 1.0    steps: 249  evaluation reward: 266.45\n",
      "episode: 5183   score: 155.0  epsilon: 1.0    steps: 445  evaluation reward: 267.25\n",
      "episode: 5184   score: 395.0  epsilon: 1.0    steps: 679  evaluation reward: 269.4\n",
      "episode: 5185   score: 610.0  epsilon: 1.0    steps: 828  evaluation reward: 274.75\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12295: Policy loss: 1.576901. Value loss: 161.332443. Entropy: 0.263878.\n",
      "Iteration 12296: Policy loss: 1.425395. Value loss: 164.659988. Entropy: 0.265998.\n",
      "Iteration 12297: Policy loss: 1.101351. Value loss: 115.240547. Entropy: 0.288804.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12298: Policy loss: 2.530524. Value loss: 48.592957. Entropy: 0.184140.\n",
      "Iteration 12299: Policy loss: 2.757337. Value loss: 26.919193. Entropy: 0.174663.\n",
      "Iteration 12300: Policy loss: 2.512619. Value loss: 21.464148. Entropy: 0.190121.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12301: Policy loss: 1.583753. Value loss: 38.129169. Entropy: 0.260494.\n",
      "Iteration 12302: Policy loss: 1.437600. Value loss: 19.752277. Entropy: 0.290646.\n",
      "Iteration 12303: Policy loss: 1.521083. Value loss: 14.599189. Entropy: 0.303546.\n",
      "episode: 5186   score: 90.0  epsilon: 1.0    steps: 698  evaluation reward: 272.1\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12304: Policy loss: 0.607286. Value loss: 39.660034. Entropy: 0.334700.\n",
      "Iteration 12305: Policy loss: 0.841273. Value loss: 20.753187. Entropy: 0.332752.\n",
      "Iteration 12306: Policy loss: 0.778297. Value loss: 16.719988. Entropy: 0.335050.\n",
      "episode: 5187   score: 210.0  epsilon: 1.0    steps: 97  evaluation reward: 271.3\n",
      "episode: 5188   score: 350.0  epsilon: 1.0    steps: 296  evaluation reward: 272.7\n",
      "episode: 5189   score: 270.0  epsilon: 1.0    steps: 519  evaluation reward: 273.3\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12307: Policy loss: 2.221077. Value loss: 27.511349. Entropy: 0.278314.\n",
      "Iteration 12308: Policy loss: 1.807336. Value loss: 17.533335. Entropy: 0.288566.\n",
      "Iteration 12309: Policy loss: 2.034427. Value loss: 14.204476. Entropy: 0.296078.\n",
      "episode: 5190   score: 210.0  epsilon: 1.0    steps: 431  evaluation reward: 273.3\n",
      "episode: 5191   score: 405.0  epsilon: 1.0    steps: 951  evaluation reward: 274.7\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12310: Policy loss: 0.211603. Value loss: 31.876986. Entropy: 0.268250.\n",
      "Iteration 12311: Policy loss: 0.478262. Value loss: 23.086559. Entropy: 0.296930.\n",
      "Iteration 12312: Policy loss: 0.080948. Value loss: 19.041517. Entropy: 0.283978.\n",
      "episode: 5192   score: 155.0  epsilon: 1.0    steps: 218  evaluation reward: 274.15\n",
      "episode: 5193   score: 260.0  epsilon: 1.0    steps: 821  evaluation reward: 274.65\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12313: Policy loss: 0.512103. Value loss: 21.718008. Entropy: 0.236873.\n",
      "Iteration 12314: Policy loss: 0.286386. Value loss: 13.849023. Entropy: 0.218517.\n",
      "Iteration 12315: Policy loss: 0.491118. Value loss: 11.842721. Entropy: 0.232127.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12316: Policy loss: -0.963582. Value loss: 17.772665. Entropy: 0.326227.\n",
      "Iteration 12317: Policy loss: -0.919200. Value loss: 12.370234. Entropy: 0.299862.\n",
      "Iteration 12318: Policy loss: -0.991251. Value loss: 10.710223. Entropy: 0.315652.\n",
      "episode: 5194   score: 155.0  epsilon: 1.0    steps: 50  evaluation reward: 273.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12319: Policy loss: 2.431141. Value loss: 28.246037. Entropy: 0.362582.\n",
      "Iteration 12320: Policy loss: 2.391978. Value loss: 16.538363. Entropy: 0.306053.\n",
      "Iteration 12321: Policy loss: 2.343306. Value loss: 14.478139. Entropy: 0.317914.\n",
      "episode: 5195   score: 150.0  epsilon: 1.0    steps: 444  evaluation reward: 273.15\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12322: Policy loss: -1.543373. Value loss: 19.979721. Entropy: 0.231838.\n",
      "Iteration 12323: Policy loss: -1.568164. Value loss: 15.058178. Entropy: 0.236024.\n",
      "Iteration 12324: Policy loss: -1.679838. Value loss: 11.577452. Entropy: 0.252577.\n",
      "episode: 5196   score: 260.0  epsilon: 1.0    steps: 280  evaluation reward: 273.65\n",
      "episode: 5197   score: 260.0  epsilon: 1.0    steps: 525  evaluation reward: 270.35\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12325: Policy loss: 1.220845. Value loss: 15.456151. Entropy: 0.317466.\n",
      "Iteration 12326: Policy loss: 1.444780. Value loss: 8.764166. Entropy: 0.312979.\n",
      "Iteration 12327: Policy loss: 1.454985. Value loss: 8.849631. Entropy: 0.326243.\n",
      "episode: 5198   score: 155.0  epsilon: 1.0    steps: 60  evaluation reward: 269.8\n",
      "episode: 5199   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 271.15\n",
      "episode: 5200   score: 240.0  epsilon: 1.0    steps: 1010  evaluation reward: 271.45\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12328: Policy loss: 1.448953. Value loss: 192.769043. Entropy: 0.284206.\n",
      "Iteration 12329: Policy loss: 1.650435. Value loss: 89.131897. Entropy: 0.232526.\n",
      "Iteration 12330: Policy loss: 1.419722. Value loss: 58.273277. Entropy: 0.258018.\n",
      "now time :  2019-02-25 22:30:23.447122\n",
      "episode: 5201   score: 125.0  epsilon: 1.0    steps: 638  evaluation reward: 270.9\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12331: Policy loss: 2.441768. Value loss: 42.611130. Entropy: 0.229298.\n",
      "Iteration 12332: Policy loss: 2.363101. Value loss: 20.267405. Entropy: 0.257220.\n",
      "Iteration 12333: Policy loss: 2.688194. Value loss: 17.650505. Entropy: 0.279333.\n",
      "episode: 5202   score: 440.0  epsilon: 1.0    steps: 138  evaluation reward: 272.9\n",
      "episode: 5203   score: 90.0  epsilon: 1.0    steps: 365  evaluation reward: 272.0\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12334: Policy loss: 2.857775. Value loss: 24.716816. Entropy: 0.339362.\n",
      "Iteration 12335: Policy loss: 3.014389. Value loss: 13.426271. Entropy: 0.337274.\n",
      "Iteration 12336: Policy loss: 2.853081. Value loss: 12.737177. Entropy: 0.329319.\n",
      "episode: 5204   score: 210.0  epsilon: 1.0    steps: 491  evaluation reward: 273.6\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12337: Policy loss: 0.129423. Value loss: 32.538544. Entropy: 0.271208.\n",
      "Iteration 12338: Policy loss: 0.342751. Value loss: 22.974392. Entropy: 0.277367.\n",
      "Iteration 12339: Policy loss: -0.163275. Value loss: 17.238705. Entropy: 0.259878.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12340: Policy loss: -0.730348. Value loss: 21.278112. Entropy: 0.243664.\n",
      "Iteration 12341: Policy loss: -0.619722. Value loss: 14.889357. Entropy: 0.244283.\n",
      "Iteration 12342: Policy loss: -0.721108. Value loss: 14.035295. Entropy: 0.241358.\n",
      "episode: 5205   score: 210.0  epsilon: 1.0    steps: 43  evaluation reward: 275.4\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12343: Policy loss: -2.288638. Value loss: 152.860550. Entropy: 0.297163.\n",
      "Iteration 12344: Policy loss: -1.624292. Value loss: 53.527580. Entropy: 0.289102.\n",
      "Iteration 12345: Policy loss: -2.411405. Value loss: 86.122932. Entropy: 0.318745.\n",
      "episode: 5206   score: 150.0  epsilon: 1.0    steps: 190  evaluation reward: 275.55\n",
      "episode: 5207   score: 180.0  epsilon: 1.0    steps: 354  evaluation reward: 275.3\n",
      "episode: 5208   score: 210.0  epsilon: 1.0    steps: 633  evaluation reward: 276.65\n",
      "episode: 5209   score: 380.0  epsilon: 1.0    steps: 723  evaluation reward: 278.65\n",
      "episode: 5210   score: 460.0  epsilon: 1.0    steps: 846  evaluation reward: 282.45\n",
      "episode: 5211   score: 260.0  epsilon: 1.0    steps: 995  evaluation reward: 282.45\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12346: Policy loss: 2.993477. Value loss: 46.937492. Entropy: 0.310188.\n",
      "Iteration 12347: Policy loss: 2.805303. Value loss: 27.275019. Entropy: 0.291869.\n",
      "Iteration 12348: Policy loss: 2.946311. Value loss: 23.104500. Entropy: 0.300359.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12349: Policy loss: 3.769877. Value loss: 18.356110. Entropy: 0.445883.\n",
      "Iteration 12350: Policy loss: 3.719492. Value loss: 12.795154. Entropy: 0.428327.\n",
      "Iteration 12351: Policy loss: 4.060524. Value loss: 10.299004. Entropy: 0.457845.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12352: Policy loss: 0.672722. Value loss: 37.151833. Entropy: 0.248286.\n",
      "Iteration 12353: Policy loss: 1.022325. Value loss: 24.432449. Entropy: 0.240629.\n",
      "Iteration 12354: Policy loss: 0.766428. Value loss: 20.641893. Entropy: 0.229958.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12355: Policy loss: 0.125787. Value loss: 28.975340. Entropy: 0.259973.\n",
      "Iteration 12356: Policy loss: 0.137402. Value loss: 16.299160. Entropy: 0.272175.\n",
      "Iteration 12357: Policy loss: 0.003854. Value loss: 14.537494. Entropy: 0.273304.\n",
      "episode: 5212   score: 210.0  epsilon: 1.0    steps: 39  evaluation reward: 279.4\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12358: Policy loss: -0.410949. Value loss: 34.583580. Entropy: 0.337364.\n",
      "Iteration 12359: Policy loss: -0.540114. Value loss: 15.159737. Entropy: 0.348202.\n",
      "Iteration 12360: Policy loss: -0.536411. Value loss: 11.353058. Entropy: 0.332835.\n",
      "episode: 5213   score: 210.0  epsilon: 1.0    steps: 182  evaluation reward: 279.4\n",
      "episode: 5214   score: 375.0  epsilon: 1.0    steps: 510  evaluation reward: 282.35\n",
      "episode: 5215   score: 210.0  epsilon: 1.0    steps: 604  evaluation reward: 282.35\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12361: Policy loss: 1.674300. Value loss: 21.977535. Entropy: 0.291763.\n",
      "Iteration 12362: Policy loss: 1.834103. Value loss: 12.593960. Entropy: 0.311596.\n",
      "Iteration 12363: Policy loss: 1.401351. Value loss: 10.431922. Entropy: 0.321103.\n",
      "episode: 5216   score: 215.0  epsilon: 1.0    steps: 324  evaluation reward: 282.05\n",
      "episode: 5217   score: 260.0  epsilon: 1.0    steps: 1015  evaluation reward: 282.4\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12364: Policy loss: 1.379548. Value loss: 24.963516. Entropy: 0.456000.\n",
      "Iteration 12365: Policy loss: 1.811990. Value loss: 17.094395. Entropy: 0.443894.\n",
      "Iteration 12366: Policy loss: 1.505745. Value loss: 14.158899. Entropy: 0.461162.\n",
      "episode: 5218   score: 90.0  epsilon: 1.0    steps: 59  evaluation reward: 281.05\n",
      "episode: 5219   score: 210.0  epsilon: 1.0    steps: 788  evaluation reward: 280.75\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12367: Policy loss: -0.464059. Value loss: 29.827633. Entropy: 0.350180.\n",
      "Iteration 12368: Policy loss: -0.374248. Value loss: 16.707722. Entropy: 0.350741.\n",
      "Iteration 12369: Policy loss: -0.446687. Value loss: 12.997131. Entropy: 0.361042.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12370: Policy loss: -0.819079. Value loss: 23.358263. Entropy: 0.403244.\n",
      "Iteration 12371: Policy loss: -0.646782. Value loss: 13.329705. Entropy: 0.377621.\n",
      "Iteration 12372: Policy loss: -0.611982. Value loss: 12.916842. Entropy: 0.367757.\n",
      "episode: 5220   score: 345.0  epsilon: 1.0    steps: 727  evaluation reward: 282.1\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12373: Policy loss: 2.534640. Value loss: 33.329834. Entropy: 0.255574.\n",
      "Iteration 12374: Policy loss: 2.480622. Value loss: 17.647766. Entropy: 0.228473.\n",
      "Iteration 12375: Policy loss: 2.467553. Value loss: 14.159288. Entropy: 0.235917.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12376: Policy loss: 2.489398. Value loss: 28.702553. Entropy: 0.347674.\n",
      "Iteration 12377: Policy loss: 2.396274. Value loss: 16.336226. Entropy: 0.274192.\n",
      "Iteration 12378: Policy loss: 2.501358. Value loss: 12.395368. Entropy: 0.286774.\n",
      "episode: 5221   score: 225.0  epsilon: 1.0    steps: 237  evaluation reward: 282.25\n",
      "Training network. lr: 0.000155. clip: 0.062067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12379: Policy loss: -0.326183. Value loss: 34.616211. Entropy: 0.352521.\n",
      "Iteration 12380: Policy loss: -0.758070. Value loss: 24.551256. Entropy: 0.380045.\n",
      "Iteration 12381: Policy loss: -0.344001. Value loss: 17.524426. Entropy: 0.356576.\n",
      "episode: 5222   score: 180.0  epsilon: 1.0    steps: 273  evaluation reward: 281.95\n",
      "episode: 5223   score: 260.0  epsilon: 1.0    steps: 417  evaluation reward: 282.45\n",
      "episode: 5224   score: 265.0  epsilon: 1.0    steps: 537  evaluation reward: 282.2\n",
      "episode: 5225   score: 210.0  epsilon: 1.0    steps: 805  evaluation reward: 282.75\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12382: Policy loss: 0.571219. Value loss: 11.637756. Entropy: 0.588950.\n",
      "Iteration 12383: Policy loss: 0.562609. Value loss: 8.116124. Entropy: 0.595237.\n",
      "Iteration 12384: Policy loss: 0.450152. Value loss: 8.818583. Entropy: 0.596225.\n",
      "episode: 5226   score: 260.0  epsilon: 1.0    steps: 915  evaluation reward: 281.9\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12385: Policy loss: 0.632562. Value loss: 29.040537. Entropy: 0.400277.\n",
      "Iteration 12386: Policy loss: 0.265230. Value loss: 21.492887. Entropy: 0.424334.\n",
      "Iteration 12387: Policy loss: 0.526426. Value loss: 19.029463. Entropy: 0.419884.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12388: Policy loss: 1.349980. Value loss: 40.932362. Entropy: 0.284777.\n",
      "Iteration 12389: Policy loss: 1.785510. Value loss: 24.800585. Entropy: 0.286923.\n",
      "Iteration 12390: Policy loss: 1.480204. Value loss: 20.732672. Entropy: 0.291033.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12391: Policy loss: 1.379177. Value loss: 23.398458. Entropy: 0.244389.\n",
      "Iteration 12392: Policy loss: 1.390904. Value loss: 12.644657. Entropy: 0.262050.\n",
      "Iteration 12393: Policy loss: 1.356677. Value loss: 10.386401. Entropy: 0.274295.\n",
      "episode: 5227   score: 290.0  epsilon: 1.0    steps: 39  evaluation reward: 283.0\n",
      "episode: 5228   score: 150.0  epsilon: 1.0    steps: 462  evaluation reward: 282.4\n",
      "episode: 5229   score: 135.0  epsilon: 1.0    steps: 524  evaluation reward: 281.15\n",
      "episode: 5230   score: 260.0  epsilon: 1.0    steps: 673  evaluation reward: 283.0\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12394: Policy loss: 0.958076. Value loss: 15.233029. Entropy: 0.411176.\n",
      "Iteration 12395: Policy loss: 1.247184. Value loss: 9.755319. Entropy: 0.407514.\n",
      "Iteration 12396: Policy loss: 1.061533. Value loss: 9.449732. Entropy: 0.403780.\n",
      "episode: 5231   score: 210.0  epsilon: 1.0    steps: 219  evaluation reward: 279.6\n",
      "episode: 5232   score: 230.0  epsilon: 1.0    steps: 354  evaluation reward: 279.05\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12397: Policy loss: 1.702296. Value loss: 26.104511. Entropy: 0.427686.\n",
      "Iteration 12398: Policy loss: 2.040611. Value loss: 17.409477. Entropy: 0.446609.\n",
      "Iteration 12399: Policy loss: 1.872869. Value loss: 15.222114. Entropy: 0.465532.\n",
      "episode: 5233   score: 260.0  epsilon: 1.0    steps: 824  evaluation reward: 278.8\n",
      "episode: 5234   score: 155.0  epsilon: 1.0    steps: 955  evaluation reward: 279.85\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12400: Policy loss: 0.287563. Value loss: 17.040005. Entropy: 0.339682.\n",
      "Iteration 12401: Policy loss: 0.276792. Value loss: 12.691901. Entropy: 0.333241.\n",
      "Iteration 12402: Policy loss: 0.434771. Value loss: 10.888514. Entropy: 0.332765.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12403: Policy loss: 1.412068. Value loss: 22.074268. Entropy: 0.316690.\n",
      "Iteration 12404: Policy loss: 1.106372. Value loss: 15.968328. Entropy: 0.321577.\n",
      "Iteration 12405: Policy loss: 1.209008. Value loss: 12.642595. Entropy: 0.311716.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12406: Policy loss: -2.803709. Value loss: 207.011520. Entropy: 0.333230.\n",
      "Iteration 12407: Policy loss: -2.257558. Value loss: 101.666046. Entropy: 0.311860.\n",
      "Iteration 12408: Policy loss: -3.131751. Value loss: 79.769531. Entropy: 0.317226.\n",
      "episode: 5235   score: 185.0  epsilon: 1.0    steps: 592  evaluation reward: 279.6\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12409: Policy loss: 0.619878. Value loss: 22.808752. Entropy: 0.313906.\n",
      "Iteration 12410: Policy loss: 0.834323. Value loss: 17.245174. Entropy: 0.337738.\n",
      "Iteration 12411: Policy loss: 0.721265. Value loss: 10.727846. Entropy: 0.334818.\n",
      "episode: 5236   score: 260.0  epsilon: 1.0    steps: 451  evaluation reward: 280.65\n",
      "episode: 5237   score: 270.0  epsilon: 1.0    steps: 681  evaluation reward: 281.8\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12412: Policy loss: 1.695511. Value loss: 24.425447. Entropy: 0.443855.\n",
      "Iteration 12413: Policy loss: 1.524864. Value loss: 18.372658. Entropy: 0.484148.\n",
      "Iteration 12414: Policy loss: 1.648020. Value loss: 13.277504. Entropy: 0.473814.\n",
      "episode: 5238   score: 210.0  epsilon: 1.0    steps: 158  evaluation reward: 281.8\n",
      "episode: 5239   score: 270.0  epsilon: 1.0    steps: 373  evaluation reward: 282.4\n",
      "episode: 5240   score: 155.0  epsilon: 1.0    steps: 799  evaluation reward: 280.8\n",
      "episode: 5241   score: 180.0  epsilon: 1.0    steps: 1020  evaluation reward: 280.8\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12415: Policy loss: 1.882316. Value loss: 22.727259. Entropy: 0.538417.\n",
      "Iteration 12416: Policy loss: 2.057505. Value loss: 11.757421. Entropy: 0.514448.\n",
      "Iteration 12417: Policy loss: 2.172665. Value loss: 10.163917. Entropy: 0.529230.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12418: Policy loss: 1.914243. Value loss: 35.603947. Entropy: 0.451593.\n",
      "Iteration 12419: Policy loss: 2.168210. Value loss: 18.843479. Entropy: 0.449569.\n",
      "Iteration 12420: Policy loss: 1.848725. Value loss: 15.784035. Entropy: 0.467726.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12421: Policy loss: 1.880854. Value loss: 39.589027. Entropy: 0.372054.\n",
      "Iteration 12422: Policy loss: 1.679997. Value loss: 25.385748. Entropy: 0.381048.\n",
      "Iteration 12423: Policy loss: 1.907140. Value loss: 21.768997. Entropy: 0.391793.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12424: Policy loss: 1.129228. Value loss: 19.862568. Entropy: 0.450472.\n",
      "Iteration 12425: Policy loss: 1.300452. Value loss: 14.474945. Entropy: 0.479879.\n",
      "Iteration 12426: Policy loss: 1.299499. Value loss: 9.430301. Entropy: 0.475277.\n",
      "episode: 5242   score: 180.0  epsilon: 1.0    steps: 518  evaluation reward: 280.0\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12427: Policy loss: 0.693616. Value loss: 28.811537. Entropy: 0.373804.\n",
      "Iteration 12428: Policy loss: 0.904438. Value loss: 17.633364. Entropy: 0.370454.\n",
      "Iteration 12429: Policy loss: 0.643190. Value loss: 15.910069. Entropy: 0.375001.\n",
      "episode: 5243   score: 580.0  epsilon: 1.0    steps: 53  evaluation reward: 277.75\n",
      "episode: 5244   score: 250.0  epsilon: 1.0    steps: 664  evaluation reward: 276.15\n",
      "episode: 5245   score: 180.0  epsilon: 1.0    steps: 787  evaluation reward: 275.85\n",
      "episode: 5246   score: 120.0  epsilon: 1.0    steps: 1015  evaluation reward: 276.9\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12430: Policy loss: 0.990977. Value loss: 17.381086. Entropy: 0.405604.\n",
      "Iteration 12431: Policy loss: 1.289848. Value loss: 8.205128. Entropy: 0.425441.\n",
      "Iteration 12432: Policy loss: 1.175687. Value loss: 7.262851. Entropy: 0.401450.\n",
      "episode: 5247   score: 180.0  epsilon: 1.0    steps: 309  evaluation reward: 274.6\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12433: Policy loss: 0.627810. Value loss: 22.540014. Entropy: 0.408466.\n",
      "Iteration 12434: Policy loss: 0.789852. Value loss: 16.268309. Entropy: 0.416781.\n",
      "Iteration 12435: Policy loss: 0.681420. Value loss: 11.186402. Entropy: 0.411720.\n",
      "episode: 5248   score: 255.0  epsilon: 1.0    steps: 426  evaluation reward: 274.85\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12436: Policy loss: -0.021641. Value loss: 29.045097. Entropy: 0.394764.\n",
      "Iteration 12437: Policy loss: -0.001334. Value loss: 17.602242. Entropy: 0.402466.\n",
      "Iteration 12438: Policy loss: 0.034093. Value loss: 15.255132. Entropy: 0.391281.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12439: Policy loss: 0.334323. Value loss: 20.401068. Entropy: 0.314384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12440: Policy loss: 0.217096. Value loss: 12.103096. Entropy: 0.327916.\n",
      "Iteration 12441: Policy loss: 0.199349. Value loss: 9.307557. Entropy: 0.344728.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12442: Policy loss: -0.466863. Value loss: 28.548901. Entropy: 0.450395.\n",
      "Iteration 12443: Policy loss: -0.515070. Value loss: 16.245567. Entropy: 0.453488.\n",
      "Iteration 12444: Policy loss: -0.319368. Value loss: 10.347536. Entropy: 0.467341.\n",
      "episode: 5249   score: 395.0  epsilon: 1.0    steps: 159  evaluation reward: 276.7\n",
      "episode: 5250   score: 155.0  epsilon: 1.0    steps: 768  evaluation reward: 274.45\n",
      "now time :  2019-02-25 22:32:31.583160\n",
      "episode: 5251   score: 165.0  epsilon: 1.0    steps: 796  evaluation reward: 273.5\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12445: Policy loss: 2.292376. Value loss: 25.068861. Entropy: 0.749831.\n",
      "Iteration 12446: Policy loss: 2.043333. Value loss: 13.075802. Entropy: 0.748554.\n",
      "Iteration 12447: Policy loss: 2.323013. Value loss: 9.046664. Entropy: 0.747366.\n",
      "episode: 5252   score: 180.0  epsilon: 1.0    steps: 30  evaluation reward: 273.2\n",
      "episode: 5253   score: 195.0  epsilon: 1.0    steps: 541  evaluation reward: 273.8\n",
      "episode: 5254   score: 110.0  epsilon: 1.0    steps: 984  evaluation reward: 269.5\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12448: Policy loss: 1.747640. Value loss: 11.347940. Entropy: 0.591099.\n",
      "Iteration 12449: Policy loss: 1.501650. Value loss: 6.267594. Entropy: 0.564033.\n",
      "Iteration 12450: Policy loss: 1.704261. Value loss: 5.103012. Entropy: 0.577095.\n",
      "episode: 5255   score: 215.0  epsilon: 1.0    steps: 341  evaluation reward: 267.2\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12451: Policy loss: 0.844134. Value loss: 41.714302. Entropy: 0.428474.\n",
      "Iteration 12452: Policy loss: 0.629593. Value loss: 27.045767. Entropy: 0.434108.\n",
      "Iteration 12453: Policy loss: 0.913059. Value loss: 19.897490. Entropy: 0.463563.\n",
      "episode: 5256   score: 225.0  epsilon: 1.0    steps: 479  evaluation reward: 267.25\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12454: Policy loss: 1.115124. Value loss: 28.736315. Entropy: 0.448112.\n",
      "Iteration 12455: Policy loss: 1.080581. Value loss: 18.191225. Entropy: 0.437743.\n",
      "Iteration 12456: Policy loss: 1.069006. Value loss: 16.931206. Entropy: 0.467779.\n",
      "episode: 5257   score: 80.0  epsilon: 1.0    steps: 849  evaluation reward: 261.75\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12457: Policy loss: -0.592374. Value loss: 24.784758. Entropy: 0.479071.\n",
      "Iteration 12458: Policy loss: -0.891129. Value loss: 10.855218. Entropy: 0.454261.\n",
      "Iteration 12459: Policy loss: -0.410964. Value loss: 11.174366. Entropy: 0.456466.\n",
      "episode: 5258   score: 120.0  epsilon: 1.0    steps: 5  evaluation reward: 260.5\n",
      "episode: 5259   score: 130.0  epsilon: 1.0    steps: 991  evaluation reward: 258.5\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12460: Policy loss: -1.009623. Value loss: 22.726278. Entropy: 0.398608.\n",
      "Iteration 12461: Policy loss: -1.020116. Value loss: 13.080280. Entropy: 0.391100.\n",
      "Iteration 12462: Policy loss: -0.830244. Value loss: 11.954570. Entropy: 0.415563.\n",
      "episode: 5260   score: 265.0  epsilon: 1.0    steps: 166  evaluation reward: 258.9\n",
      "episode: 5261   score: 210.0  epsilon: 1.0    steps: 659  evaluation reward: 258.15\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12463: Policy loss: -1.235178. Value loss: 18.127253. Entropy: 0.564209.\n",
      "Iteration 12464: Policy loss: -1.087838. Value loss: 10.525989. Entropy: 0.560311.\n",
      "Iteration 12465: Policy loss: -1.066107. Value loss: 7.607929. Entropy: 0.541944.\n",
      "episode: 5262   score: 90.0  epsilon: 1.0    steps: 450  evaluation reward: 252.25\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12466: Policy loss: -0.825764. Value loss: 24.142073. Entropy: 0.489216.\n",
      "Iteration 12467: Policy loss: -0.908543. Value loss: 14.636008. Entropy: 0.467626.\n",
      "Iteration 12468: Policy loss: -0.945174. Value loss: 11.796348. Entropy: 0.486894.\n",
      "episode: 5263   score: 155.0  epsilon: 1.0    steps: 281  evaluation reward: 251.7\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12469: Policy loss: 2.508597. Value loss: 32.238712. Entropy: 0.477625.\n",
      "Iteration 12470: Policy loss: 2.530177. Value loss: 13.732615. Entropy: 0.508838.\n",
      "Iteration 12471: Policy loss: 2.434700. Value loss: 10.058088. Entropy: 0.537141.\n",
      "episode: 5264   score: 290.0  epsilon: 1.0    steps: 604  evaluation reward: 249.4\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12472: Policy loss: -0.027172. Value loss: 35.884888. Entropy: 0.409786.\n",
      "Iteration 12473: Policy loss: 0.155555. Value loss: 18.321295. Entropy: 0.400445.\n",
      "Iteration 12474: Policy loss: -0.180369. Value loss: 16.434824. Entropy: 0.392429.\n",
      "episode: 5265   score: 155.0  epsilon: 1.0    steps: 425  evaluation reward: 249.75\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12475: Policy loss: -1.212769. Value loss: 29.553961. Entropy: 0.585363.\n",
      "Iteration 12476: Policy loss: -1.100412. Value loss: 18.267912. Entropy: 0.564684.\n",
      "Iteration 12477: Policy loss: -0.918411. Value loss: 14.197021. Entropy: 0.586743.\n",
      "episode: 5266   score: 285.0  epsilon: 1.0    steps: 128  evaluation reward: 244.3\n",
      "episode: 5267   score: 210.0  epsilon: 1.0    steps: 215  evaluation reward: 238.6\n",
      "episode: 5268   score: 210.0  epsilon: 1.0    steps: 687  evaluation reward: 239.5\n",
      "episode: 5269   score: 240.0  epsilon: 1.0    steps: 1020  evaluation reward: 239.35\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12478: Policy loss: -1.207993. Value loss: 25.494089. Entropy: 0.493966.\n",
      "Iteration 12479: Policy loss: -1.214734. Value loss: 14.477770. Entropy: 0.499262.\n",
      "Iteration 12480: Policy loss: -1.287799. Value loss: 11.943865. Entropy: 0.508775.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12481: Policy loss: -0.917860. Value loss: 24.286667. Entropy: 0.442863.\n",
      "Iteration 12482: Policy loss: -0.963591. Value loss: 14.828837. Entropy: 0.429903.\n",
      "Iteration 12483: Policy loss: -0.960921. Value loss: 12.157469. Entropy: 0.437594.\n",
      "episode: 5270   score: 100.0  epsilon: 1.0    steps: 404  evaluation reward: 232.85\n",
      "episode: 5271   score: 350.0  epsilon: 1.0    steps: 830  evaluation reward: 233.5\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12484: Policy loss: -0.183421. Value loss: 35.129833. Entropy: 0.552818.\n",
      "Iteration 12485: Policy loss: -0.005863. Value loss: 16.870609. Entropy: 0.566569.\n",
      "Iteration 12486: Policy loss: -0.169986. Value loss: 14.441894. Entropy: 0.580792.\n",
      "episode: 5272   score: 55.0  epsilon: 1.0    steps: 764  evaluation reward: 231.45\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12487: Policy loss: 0.856852. Value loss: 34.174232. Entropy: 0.447469.\n",
      "Iteration 12488: Policy loss: 0.899571. Value loss: 19.431259. Entropy: 0.454551.\n",
      "Iteration 12489: Policy loss: 0.891907. Value loss: 14.284350. Entropy: 0.449064.\n",
      "episode: 5273   score: 390.0  epsilon: 1.0    steps: 336  evaluation reward: 234.45\n",
      "episode: 5274   score: 260.0  epsilon: 1.0    steps: 619  evaluation reward: 232.4\n",
      "episode: 5275   score: 210.0  epsilon: 1.0    steps: 1011  evaluation reward: 231.85\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12490: Policy loss: -1.984908. Value loss: 26.957781. Entropy: 0.470159.\n",
      "Iteration 12491: Policy loss: -2.107900. Value loss: 17.546814. Entropy: 0.510557.\n",
      "Iteration 12492: Policy loss: -2.168692. Value loss: 14.595469. Entropy: 0.466046.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12493: Policy loss: -1.341258. Value loss: 29.383204. Entropy: 0.527520.\n",
      "Iteration 12494: Policy loss: -0.922987. Value loss: 16.733379. Entropy: 0.542497.\n",
      "Iteration 12495: Policy loss: -1.277334. Value loss: 13.379547. Entropy: 0.520116.\n",
      "episode: 5276   score: 240.0  epsilon: 1.0    steps: 254  evaluation reward: 233.05\n",
      "episode: 5277   score: 155.0  epsilon: 1.0    steps: 837  evaluation reward: 230.7\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12496: Policy loss: 0.860857. Value loss: 30.575706. Entropy: 0.441205.\n",
      "Iteration 12497: Policy loss: 0.739412. Value loss: 21.693312. Entropy: 0.420258.\n",
      "Iteration 12498: Policy loss: 1.064815. Value loss: 16.863136. Entropy: 0.424795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5278   score: 210.0  epsilon: 1.0    steps: 439  evaluation reward: 230.2\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12499: Policy loss: 0.853048. Value loss: 24.532686. Entropy: 0.539803.\n",
      "Iteration 12500: Policy loss: 0.419300. Value loss: 16.963736. Entropy: 0.543989.\n",
      "Iteration 12501: Policy loss: 0.811722. Value loss: 13.870691. Entropy: 0.516969.\n",
      "episode: 5279   score: 365.0  epsilon: 1.0    steps: 83  evaluation reward: 226.4\n",
      "episode: 5280   score: 155.0  epsilon: 1.0    steps: 604  evaluation reward: 225.85\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12502: Policy loss: -0.357645. Value loss: 27.325737. Entropy: 0.386816.\n",
      "Iteration 12503: Policy loss: -0.457522. Value loss: 16.402498. Entropy: 0.412477.\n",
      "Iteration 12504: Policy loss: -0.404423. Value loss: 13.529358. Entropy: 0.416986.\n",
      "episode: 5281   score: 210.0  epsilon: 1.0    steps: 661  evaluation reward: 226.6\n",
      "episode: 5282   score: 135.0  epsilon: 1.0    steps: 908  evaluation reward: 226.6\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12505: Policy loss: -0.496844. Value loss: 19.685308. Entropy: 0.382186.\n",
      "Iteration 12506: Policy loss: -0.738011. Value loss: 13.696797. Entropy: 0.370396.\n",
      "Iteration 12507: Policy loss: -0.813427. Value loss: 11.893943. Entropy: 0.375636.\n",
      "episode: 5283   score: 210.0  epsilon: 1.0    steps: 205  evaluation reward: 227.15\n",
      "episode: 5284   score: 290.0  epsilon: 1.0    steps: 334  evaluation reward: 226.1\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12508: Policy loss: -0.345482. Value loss: 30.914427. Entropy: 0.429919.\n",
      "Iteration 12509: Policy loss: -0.457755. Value loss: 19.380407. Entropy: 0.411952.\n",
      "Iteration 12510: Policy loss: -0.437381. Value loss: 17.404020. Entropy: 0.409636.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12511: Policy loss: -1.131916. Value loss: 20.270796. Entropy: 0.392650.\n",
      "Iteration 12512: Policy loss: -1.238559. Value loss: 12.231994. Entropy: 0.375465.\n",
      "Iteration 12513: Policy loss: -1.191029. Value loss: 11.113610. Entropy: 0.368274.\n",
      "episode: 5285   score: 270.0  epsilon: 1.0    steps: 841  evaluation reward: 222.7\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12514: Policy loss: -0.616598. Value loss: 15.214308. Entropy: 0.418800.\n",
      "Iteration 12515: Policy loss: -0.592412. Value loss: 11.076723. Entropy: 0.433530.\n",
      "Iteration 12516: Policy loss: -0.672736. Value loss: 9.581274. Entropy: 0.427988.\n",
      "episode: 5286   score: 150.0  epsilon: 1.0    steps: 365  evaluation reward: 223.3\n",
      "episode: 5287   score: 230.0  epsilon: 1.0    steps: 448  evaluation reward: 223.5\n",
      "episode: 5288   score: 210.0  epsilon: 1.0    steps: 615  evaluation reward: 222.1\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12517: Policy loss: 1.606447. Value loss: 15.471372. Entropy: 0.344571.\n",
      "Iteration 12518: Policy loss: 1.739721. Value loss: 9.706491. Entropy: 0.346695.\n",
      "Iteration 12519: Policy loss: 1.635199. Value loss: 9.399429. Entropy: 0.349967.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12520: Policy loss: 0.279565. Value loss: 17.818073. Entropy: 0.461619.\n",
      "Iteration 12521: Policy loss: 0.222773. Value loss: 12.254283. Entropy: 0.456793.\n",
      "Iteration 12522: Policy loss: 0.233692. Value loss: 8.461796. Entropy: 0.458030.\n",
      "episode: 5289   score: 240.0  epsilon: 1.0    steps: 947  evaluation reward: 221.8\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12523: Policy loss: 2.187805. Value loss: 37.056519. Entropy: 0.530334.\n",
      "Iteration 12524: Policy loss: 2.070288. Value loss: 19.649206. Entropy: 0.546738.\n",
      "Iteration 12525: Policy loss: 2.152252. Value loss: 16.711050. Entropy: 0.533363.\n",
      "episode: 5290   score: 320.0  epsilon: 1.0    steps: 94  evaluation reward: 222.9\n",
      "episode: 5291   score: 245.0  epsilon: 1.0    steps: 147  evaluation reward: 221.3\n",
      "episode: 5292   score: 245.0  epsilon: 1.0    steps: 739  evaluation reward: 222.2\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12526: Policy loss: -0.565572. Value loss: 23.470509. Entropy: 0.452422.\n",
      "Iteration 12527: Policy loss: -0.624906. Value loss: 14.659773. Entropy: 0.445228.\n",
      "Iteration 12528: Policy loss: -0.546140. Value loss: 13.451967. Entropy: 0.429866.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12529: Policy loss: -0.691210. Value loss: 15.380522. Entropy: 0.278371.\n",
      "Iteration 12530: Policy loss: -0.844963. Value loss: 10.655569. Entropy: 0.266209.\n",
      "Iteration 12531: Policy loss: -0.944850. Value loss: 8.175278. Entropy: 0.272776.\n",
      "episode: 5293   score: 210.0  epsilon: 1.0    steps: 437  evaluation reward: 221.7\n",
      "episode: 5294   score: 240.0  epsilon: 1.0    steps: 822  evaluation reward: 222.55\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12532: Policy loss: 1.974427. Value loss: 17.439304. Entropy: 0.432261.\n",
      "Iteration 12533: Policy loss: 1.731774. Value loss: 13.126368. Entropy: 0.437905.\n",
      "Iteration 12534: Policy loss: 2.032923. Value loss: 8.432661. Entropy: 0.434036.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12535: Policy loss: -4.053087. Value loss: 271.254242. Entropy: 0.432063.\n",
      "Iteration 12536: Policy loss: -5.406247. Value loss: 208.145447. Entropy: 0.367804.\n",
      "Iteration 12537: Policy loss: -4.510513. Value loss: 153.064850. Entropy: 0.358861.\n",
      "episode: 5295   score: 180.0  epsilon: 1.0    steps: 64  evaluation reward: 222.85\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12538: Policy loss: -1.952740. Value loss: 26.380156. Entropy: 0.480374.\n",
      "Iteration 12539: Policy loss: -1.865858. Value loss: 14.111205. Entropy: 0.485271.\n",
      "Iteration 12540: Policy loss: -1.837122. Value loss: 10.917896. Entropy: 0.490314.\n",
      "episode: 5296   score: 445.0  epsilon: 1.0    steps: 320  evaluation reward: 224.7\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12541: Policy loss: 1.233192. Value loss: 35.553520. Entropy: 0.361011.\n",
      "Iteration 12542: Policy loss: 1.164380. Value loss: 17.779083. Entropy: 0.367980.\n",
      "Iteration 12543: Policy loss: 1.137288. Value loss: 11.967202. Entropy: 0.369415.\n",
      "episode: 5297   score: 445.0  epsilon: 1.0    steps: 526  evaluation reward: 226.55\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12544: Policy loss: 0.508995. Value loss: 23.240765. Entropy: 0.463600.\n",
      "Iteration 12545: Policy loss: 0.451967. Value loss: 13.987009. Entropy: 0.450168.\n",
      "Iteration 12546: Policy loss: 0.389730. Value loss: 8.570840. Entropy: 0.454573.\n",
      "episode: 5298   score: 275.0  epsilon: 1.0    steps: 148  evaluation reward: 227.75\n",
      "episode: 5299   score: 210.0  epsilon: 1.0    steps: 867  evaluation reward: 227.75\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12547: Policy loss: 0.988947. Value loss: 17.503143. Entropy: 0.474624.\n",
      "Iteration 12548: Policy loss: 0.605351. Value loss: 12.193020. Entropy: 0.492587.\n",
      "Iteration 12549: Policy loss: 0.919465. Value loss: 11.042992. Entropy: 0.488103.\n",
      "episode: 5300   score: 225.0  epsilon: 1.0    steps: 426  evaluation reward: 227.6\n",
      "now time :  2019-02-25 22:34:29.479459\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5301   score: 255.0  epsilon: 1.0    steps: 722  evaluation reward: 228.9\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12550: Policy loss: -0.166933. Value loss: 33.670837. Entropy: 0.318126.\n",
      "Iteration 12551: Policy loss: 0.038960. Value loss: 19.806713. Entropy: 0.342173.\n",
      "Iteration 12552: Policy loss: -0.041098. Value loss: 15.906960. Entropy: 0.353134.\n",
      "episode: 5302   score: 270.0  epsilon: 1.0    steps: 915  evaluation reward: 227.2\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12553: Policy loss: 0.692314. Value loss: 30.087709. Entropy: 0.320731.\n",
      "Iteration 12554: Policy loss: 0.519045. Value loss: 16.292196. Entropy: 0.327669.\n",
      "Iteration 12555: Policy loss: 0.610956. Value loss: 12.386715. Entropy: 0.346755.\n",
      "episode: 5303   score: 240.0  epsilon: 1.0    steps: 21  evaluation reward: 228.7\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12556: Policy loss: -2.956345. Value loss: 196.799805. Entropy: 0.245273.\n",
      "Iteration 12557: Policy loss: -3.217850. Value loss: 115.958099. Entropy: 0.241603.\n",
      "Iteration 12558: Policy loss: -2.662263. Value loss: 82.178780. Entropy: 0.252492.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12559: Policy loss: 0.660122. Value loss: 22.789446. Entropy: 0.198436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12560: Policy loss: 0.473993. Value loss: 16.469248. Entropy: 0.189329.\n",
      "Iteration 12561: Policy loss: 0.724469. Value loss: 10.477229. Entropy: 0.189395.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12562: Policy loss: 0.426907. Value loss: 37.552860. Entropy: 0.295326.\n",
      "Iteration 12563: Policy loss: 0.653308. Value loss: 18.364510. Entropy: 0.287730.\n",
      "Iteration 12564: Policy loss: 0.996525. Value loss: 12.627386. Entropy: 0.274908.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12565: Policy loss: -0.432568. Value loss: 46.275139. Entropy: 0.270651.\n",
      "Iteration 12566: Policy loss: -0.133638. Value loss: 27.477537. Entropy: 0.269892.\n",
      "Iteration 12567: Policy loss: -0.202734. Value loss: 20.848869. Entropy: 0.299361.\n",
      "episode: 5304   score: 265.0  epsilon: 1.0    steps: 434  evaluation reward: 229.25\n",
      "episode: 5305   score: 355.0  epsilon: 1.0    steps: 639  evaluation reward: 230.7\n",
      "episode: 5306   score: 260.0  epsilon: 1.0    steps: 729  evaluation reward: 231.8\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12568: Policy loss: 1.091894. Value loss: 32.335114. Entropy: 0.354556.\n",
      "Iteration 12569: Policy loss: 0.813610. Value loss: 20.686077. Entropy: 0.363012.\n",
      "Iteration 12570: Policy loss: 0.879106. Value loss: 15.394567. Entropy: 0.362531.\n",
      "episode: 5307   score: 295.0  epsilon: 1.0    steps: 147  evaluation reward: 232.95\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12571: Policy loss: 1.222284. Value loss: 28.609283. Entropy: 0.402052.\n",
      "Iteration 12572: Policy loss: 1.392702. Value loss: 17.533220. Entropy: 0.409092.\n",
      "Iteration 12573: Policy loss: 1.369267. Value loss: 12.620043. Entropy: 0.409358.\n",
      "episode: 5308   score: 570.0  epsilon: 1.0    steps: 852  evaluation reward: 236.55\n",
      "episode: 5309   score: 310.0  epsilon: 1.0    steps: 908  evaluation reward: 235.85\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12574: Policy loss: -2.273034. Value loss: 333.440308. Entropy: 0.411238.\n",
      "Iteration 12575: Policy loss: -0.194502. Value loss: 97.108528. Entropy: 0.395507.\n",
      "Iteration 12576: Policy loss: -1.836790. Value loss: 123.528107. Entropy: 0.359002.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12577: Policy loss: 0.898623. Value loss: 34.231148. Entropy: 0.237852.\n",
      "Iteration 12578: Policy loss: 0.706106. Value loss: 17.905565. Entropy: 0.227747.\n",
      "Iteration 12579: Policy loss: 0.788434. Value loss: 17.746286. Entropy: 0.234879.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12580: Policy loss: -2.781782. Value loss: 259.412781. Entropy: 0.166825.\n",
      "Iteration 12581: Policy loss: -1.822942. Value loss: 118.997887. Entropy: 0.157706.\n",
      "Iteration 12582: Policy loss: -2.649494. Value loss: 88.729340. Entropy: 0.164957.\n",
      "episode: 5310   score: 405.0  epsilon: 1.0    steps: 21  evaluation reward: 235.3\n",
      "episode: 5311   score: 560.0  epsilon: 1.0    steps: 369  evaluation reward: 238.3\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12583: Policy loss: 2.652145. Value loss: 42.035137. Entropy: 0.250150.\n",
      "Iteration 12584: Policy loss: 2.595058. Value loss: 24.437159. Entropy: 0.242397.\n",
      "Iteration 12585: Policy loss: 2.582842. Value loss: 17.906115. Entropy: 0.243308.\n",
      "episode: 5312   score: 285.0  epsilon: 1.0    steps: 737  evaluation reward: 239.05\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12586: Policy loss: 2.917309. Value loss: 36.423840. Entropy: 0.408740.\n",
      "Iteration 12587: Policy loss: 3.012554. Value loss: 19.389685. Entropy: 0.408626.\n",
      "Iteration 12588: Policy loss: 3.050372. Value loss: 14.870935. Entropy: 0.418235.\n",
      "episode: 5313   score: 240.0  epsilon: 1.0    steps: 583  evaluation reward: 239.35\n",
      "episode: 5314   score: 180.0  epsilon: 1.0    steps: 909  evaluation reward: 237.4\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12589: Policy loss: 1.122416. Value loss: 39.097622. Entropy: 0.445885.\n",
      "Iteration 12590: Policy loss: 1.032048. Value loss: 21.409937. Entropy: 0.444534.\n",
      "Iteration 12591: Policy loss: 1.051064. Value loss: 16.681278. Entropy: 0.459968.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12592: Policy loss: 3.253807. Value loss: 70.411201. Entropy: 0.386460.\n",
      "Iteration 12593: Policy loss: 3.336987. Value loss: 37.505817. Entropy: 0.399656.\n",
      "Iteration 12594: Policy loss: 3.013328. Value loss: 26.996605. Entropy: 0.387710.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12595: Policy loss: 3.485970. Value loss: 38.424564. Entropy: 0.374390.\n",
      "Iteration 12596: Policy loss: 3.406672. Value loss: 27.031233. Entropy: 0.388648.\n",
      "Iteration 12597: Policy loss: 3.368078. Value loss: 19.642309. Entropy: 0.372073.\n",
      "episode: 5315   score: 565.0  epsilon: 1.0    steps: 479  evaluation reward: 240.95\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12598: Policy loss: 3.874315. Value loss: 31.506996. Entropy: 0.326515.\n",
      "Iteration 12599: Policy loss: 3.637045. Value loss: 19.812304. Entropy: 0.328688.\n",
      "Iteration 12600: Policy loss: 3.804163. Value loss: 15.686357. Entropy: 0.342278.\n",
      "episode: 5316   score: 260.0  epsilon: 1.0    steps: 98  evaluation reward: 241.4\n",
      "episode: 5317   score: 380.0  epsilon: 1.0    steps: 216  evaluation reward: 242.6\n",
      "episode: 5318   score: 285.0  epsilon: 1.0    steps: 381  evaluation reward: 244.55\n",
      "episode: 5319   score: 330.0  epsilon: 1.0    steps: 801  evaluation reward: 245.75\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12601: Policy loss: -0.129290. Value loss: 256.739410. Entropy: 0.416501.\n",
      "Iteration 12602: Policy loss: -0.134544. Value loss: 168.020203. Entropy: 0.423427.\n",
      "Iteration 12603: Policy loss: -0.054812. Value loss: 86.115189. Entropy: 0.448615.\n",
      "episode: 5320   score: 225.0  epsilon: 1.0    steps: 713  evaluation reward: 244.55\n",
      "episode: 5321   score: 210.0  epsilon: 1.0    steps: 924  evaluation reward: 244.4\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12604: Policy loss: 1.182537. Value loss: 38.253765. Entropy: 0.388953.\n",
      "Iteration 12605: Policy loss: 1.040346. Value loss: 21.919712. Entropy: 0.366265.\n",
      "Iteration 12606: Policy loss: 1.164848. Value loss: 21.272232. Entropy: 0.394033.\n",
      "episode: 5322   score: 355.0  epsilon: 1.0    steps: 588  evaluation reward: 246.15\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12607: Policy loss: 0.290312. Value loss: 36.261272. Entropy: 0.357335.\n",
      "Iteration 12608: Policy loss: 0.146209. Value loss: 23.425720. Entropy: 0.342304.\n",
      "Iteration 12609: Policy loss: 0.334502. Value loss: 18.405195. Entropy: 0.354950.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12610: Policy loss: -1.448278. Value loss: 191.953094. Entropy: 0.262295.\n",
      "Iteration 12611: Policy loss: -2.075665. Value loss: 150.783005. Entropy: 0.267951.\n",
      "Iteration 12612: Policy loss: -1.794032. Value loss: 94.675377. Entropy: 0.259384.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12613: Policy loss: 3.002427. Value loss: 59.245953. Entropy: 0.260503.\n",
      "Iteration 12614: Policy loss: 2.946794. Value loss: 22.192642. Entropy: 0.248728.\n",
      "Iteration 12615: Policy loss: 2.734048. Value loss: 15.043469. Entropy: 0.248997.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12616: Policy loss: 1.007161. Value loss: 35.005039. Entropy: 0.371411.\n",
      "Iteration 12617: Policy loss: 1.078966. Value loss: 22.132082. Entropy: 0.376387.\n",
      "Iteration 12618: Policy loss: 1.095964. Value loss: 18.118187. Entropy: 0.363010.\n",
      "episode: 5323   score: 460.0  epsilon: 1.0    steps: 420  evaluation reward: 248.15\n",
      "episode: 5324   score: 305.0  epsilon: 1.0    steps: 876  evaluation reward: 248.55\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12619: Policy loss: -1.965639. Value loss: 325.037720. Entropy: 0.577491.\n",
      "Iteration 12620: Policy loss: -2.591687. Value loss: 224.209702. Entropy: 0.616286.\n",
      "Iteration 12621: Policy loss: -1.660647. Value loss: 183.261917. Entropy: 0.614804.\n",
      "episode: 5325   score: 415.0  epsilon: 1.0    steps: 302  evaluation reward: 250.6\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12622: Policy loss: 0.682296. Value loss: 25.694365. Entropy: 0.561876.\n",
      "Iteration 12623: Policy loss: 0.386253. Value loss: 17.974907. Entropy: 0.545446.\n",
      "Iteration 12624: Policy loss: 0.645882. Value loss: 13.180163. Entropy: 0.578066.\n",
      "episode: 5326   score: 265.0  epsilon: 1.0    steps: 205  evaluation reward: 250.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5327   score: 410.0  epsilon: 1.0    steps: 641  evaluation reward: 251.85\n",
      "episode: 5328   score: 290.0  epsilon: 1.0    steps: 906  evaluation reward: 253.25\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12625: Policy loss: -0.955764. Value loss: 207.794662. Entropy: 0.491969.\n",
      "Iteration 12626: Policy loss: -0.365323. Value loss: 104.520897. Entropy: 0.479095.\n",
      "Iteration 12627: Policy loss: -1.489520. Value loss: 65.380692. Entropy: 0.468166.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12628: Policy loss: -0.412801. Value loss: 44.035252. Entropy: 0.307402.\n",
      "Iteration 12629: Policy loss: -0.404773. Value loss: 26.790457. Entropy: 0.304191.\n",
      "Iteration 12630: Policy loss: -0.393268. Value loss: 23.001629. Entropy: 0.307705.\n",
      "episode: 5329   score: 135.0  epsilon: 1.0    steps: 373  evaluation reward: 253.25\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12631: Policy loss: 0.892396. Value loss: 64.533714. Entropy: 0.288317.\n",
      "Iteration 12632: Policy loss: 0.973128. Value loss: 31.264515. Entropy: 0.280230.\n",
      "Iteration 12633: Policy loss: 1.172723. Value loss: 21.682505. Entropy: 0.289401.\n",
      "episode: 5330   score: 745.0  epsilon: 1.0    steps: 98  evaluation reward: 258.1\n",
      "episode: 5331   score: 135.0  epsilon: 1.0    steps: 912  evaluation reward: 257.35\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12634: Policy loss: 6.269712. Value loss: 117.338570. Entropy: 0.343146.\n",
      "Iteration 12635: Policy loss: 5.798965. Value loss: 41.674816. Entropy: 0.339254.\n",
      "Iteration 12636: Policy loss: 5.375349. Value loss: 28.102547. Entropy: 0.337599.\n",
      "episode: 5332   score: 435.0  epsilon: 1.0    steps: 551  evaluation reward: 259.4\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12637: Policy loss: 2.035849. Value loss: 39.368317. Entropy: 0.355202.\n",
      "Iteration 12638: Policy loss: 2.318341. Value loss: 26.584747. Entropy: 0.327747.\n",
      "Iteration 12639: Policy loss: 2.018599. Value loss: 20.137657. Entropy: 0.350675.\n",
      "episode: 5333   score: 210.0  epsilon: 1.0    steps: 244  evaluation reward: 258.9\n",
      "episode: 5334   score: 210.0  epsilon: 1.0    steps: 385  evaluation reward: 259.45\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12640: Policy loss: 0.926957. Value loss: 38.169411. Entropy: 0.325845.\n",
      "Iteration 12641: Policy loss: 0.945967. Value loss: 19.588358. Entropy: 0.306410.\n",
      "Iteration 12642: Policy loss: 0.955575. Value loss: 18.324926. Entropy: 0.323541.\n",
      "episode: 5335   score: 260.0  epsilon: 1.0    steps: 891  evaluation reward: 260.2\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12643: Policy loss: -0.376379. Value loss: 35.070065. Entropy: 0.403339.\n",
      "Iteration 12644: Policy loss: -0.547916. Value loss: 23.110899. Entropy: 0.401896.\n",
      "Iteration 12645: Policy loss: -0.518572. Value loss: 19.125708. Entropy: 0.410513.\n",
      "episode: 5336   score: 180.0  epsilon: 1.0    steps: 257  evaluation reward: 259.4\n",
      "episode: 5337   score: 365.0  epsilon: 1.0    steps: 751  evaluation reward: 260.35\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12646: Policy loss: 1.221389. Value loss: 28.043730. Entropy: 0.286061.\n",
      "Iteration 12647: Policy loss: 1.152596. Value loss: 15.938063. Entropy: 0.299241.\n",
      "Iteration 12648: Policy loss: 1.003703. Value loss: 12.595677. Entropy: 0.298582.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12649: Policy loss: -0.024015. Value loss: 169.990723. Entropy: 0.353829.\n",
      "Iteration 12650: Policy loss: 0.062724. Value loss: 58.527637. Entropy: 0.361133.\n",
      "Iteration 12651: Policy loss: 0.341753. Value loss: 57.396717. Entropy: 0.338462.\n",
      "episode: 5338   score: 410.0  epsilon: 1.0    steps: 952  evaluation reward: 262.35\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12652: Policy loss: 0.893762. Value loss: 28.626894. Entropy: 0.340610.\n",
      "Iteration 12653: Policy loss: 0.811565. Value loss: 19.640268. Entropy: 0.340219.\n",
      "Iteration 12654: Policy loss: 0.957282. Value loss: 14.284369. Entropy: 0.361993.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12655: Policy loss: 1.931156. Value loss: 31.963617. Entropy: 0.267556.\n",
      "Iteration 12656: Policy loss: 2.016381. Value loss: 16.289967. Entropy: 0.260333.\n",
      "Iteration 12657: Policy loss: 2.368854. Value loss: 11.986463. Entropy: 0.253642.\n",
      "episode: 5339   score: 335.0  epsilon: 1.0    steps: 110  evaluation reward: 263.0\n",
      "episode: 5340   score: 285.0  epsilon: 1.0    steps: 448  evaluation reward: 264.3\n",
      "episode: 5341   score: 290.0  epsilon: 1.0    steps: 570  evaluation reward: 265.4\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12658: Policy loss: -0.011658. Value loss: 321.458923. Entropy: 0.371555.\n",
      "Iteration 12659: Policy loss: -0.401625. Value loss: 183.499130. Entropy: 0.362895.\n",
      "Iteration 12660: Policy loss: 0.235426. Value loss: 121.036591. Entropy: 0.356634.\n",
      "episode: 5342   score: 240.0  epsilon: 1.0    steps: 191  evaluation reward: 266.0\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12661: Policy loss: 0.595317. Value loss: 58.170921. Entropy: 0.353472.\n",
      "Iteration 12662: Policy loss: 0.538707. Value loss: 35.694832. Entropy: 0.347678.\n",
      "Iteration 12663: Policy loss: 0.521369. Value loss: 28.724913. Entropy: 0.348487.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12664: Policy loss: 0.406958. Value loss: 237.757904. Entropy: 0.379851.\n",
      "Iteration 12665: Policy loss: 0.792167. Value loss: 55.094242. Entropy: 0.357942.\n",
      "Iteration 12666: Policy loss: 0.478243. Value loss: 80.230415. Entropy: 0.373389.\n",
      "episode: 5343   score: 290.0  epsilon: 1.0    steps: 258  evaluation reward: 263.1\n",
      "episode: 5344   score: 260.0  epsilon: 1.0    steps: 736  evaluation reward: 263.2\n",
      "episode: 5345   score: 210.0  epsilon: 1.0    steps: 943  evaluation reward: 263.5\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12667: Policy loss: 2.554266. Value loss: 33.234322. Entropy: 0.314498.\n",
      "Iteration 12668: Policy loss: 2.269163. Value loss: 17.752913. Entropy: 0.318234.\n",
      "Iteration 12669: Policy loss: 1.961685. Value loss: 16.550491. Entropy: 0.309642.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12670: Policy loss: 0.016676. Value loss: 50.438007. Entropy: 0.298422.\n",
      "Iteration 12671: Policy loss: 0.542512. Value loss: 23.678772. Entropy: 0.312956.\n",
      "Iteration 12672: Policy loss: 0.562007. Value loss: 18.665239. Entropy: 0.296463.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12673: Policy loss: -2.903450. Value loss: 160.367157. Entropy: 0.319386.\n",
      "Iteration 12674: Policy loss: -3.592056. Value loss: 99.921265. Entropy: 0.310101.\n",
      "Iteration 12675: Policy loss: -2.893384. Value loss: 54.608532. Entropy: 0.302284.\n",
      "episode: 5346   score: 210.0  epsilon: 1.0    steps: 177  evaluation reward: 264.4\n",
      "episode: 5347   score: 210.0  epsilon: 1.0    steps: 503  evaluation reward: 264.7\n",
      "episode: 5348   score: 925.0  epsilon: 1.0    steps: 846  evaluation reward: 271.4\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12676: Policy loss: -0.339191. Value loss: 81.823059. Entropy: 0.388059.\n",
      "Iteration 12677: Policy loss: -0.305906. Value loss: 37.543495. Entropy: 0.395095.\n",
      "Iteration 12678: Policy loss: 0.124900. Value loss: 28.415533. Entropy: 0.410463.\n",
      "episode: 5349   score: 495.0  epsilon: 1.0    steps: 40  evaluation reward: 272.4\n",
      "episode: 5350   score: 325.0  epsilon: 1.0    steps: 561  evaluation reward: 274.1\n",
      "now time :  2019-02-25 22:36:53.422929\n",
      "episode: 5351   score: 210.0  epsilon: 1.0    steps: 699  evaluation reward: 274.55\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12679: Policy loss: 4.825572. Value loss: 62.900345. Entropy: 0.313863.\n",
      "Iteration 12680: Policy loss: 4.159459. Value loss: 30.516117. Entropy: 0.336468.\n",
      "Iteration 12681: Policy loss: 4.233082. Value loss: 23.026083. Entropy: 0.329767.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12682: Policy loss: -0.582963. Value loss: 38.222874. Entropy: 0.318642.\n",
      "Iteration 12683: Policy loss: -0.937050. Value loss: 26.403345. Entropy: 0.321905.\n",
      "Iteration 12684: Policy loss: -0.390500. Value loss: 19.308214. Entropy: 0.328568.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12685: Policy loss: -2.009084. Value loss: 54.568295. Entropy: 0.377969.\n",
      "Iteration 12686: Policy loss: -1.781086. Value loss: 25.112764. Entropy: 0.356505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12687: Policy loss: -2.240170. Value loss: 19.187962. Entropy: 0.355285.\n",
      "episode: 5352   score: 350.0  epsilon: 1.0    steps: 308  evaluation reward: 276.25\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12688: Policy loss: -1.403138. Value loss: 200.102432. Entropy: 0.316905.\n",
      "Iteration 12689: Policy loss: -1.130115. Value loss: 84.960831. Entropy: 0.293247.\n",
      "Iteration 12690: Policy loss: -1.775915. Value loss: 62.295341. Entropy: 0.313714.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12691: Policy loss: 3.091743. Value loss: 78.074120. Entropy: 0.184119.\n",
      "Iteration 12692: Policy loss: 2.915453. Value loss: 33.848179. Entropy: 0.189312.\n",
      "Iteration 12693: Policy loss: 2.909777. Value loss: 22.488058. Entropy: 0.203069.\n",
      "episode: 5353   score: 210.0  epsilon: 1.0    steps: 707  evaluation reward: 276.4\n",
      "episode: 5354   score: 375.0  epsilon: 1.0    steps: 988  evaluation reward: 279.05\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12694: Policy loss: 0.659102. Value loss: 41.243050. Entropy: 0.274405.\n",
      "Iteration 12695: Policy loss: 0.719671. Value loss: 27.696810. Entropy: 0.280579.\n",
      "Iteration 12696: Policy loss: 0.954594. Value loss: 20.498766. Entropy: 0.282453.\n",
      "episode: 5355   score: 285.0  epsilon: 1.0    steps: 386  evaluation reward: 279.75\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12697: Policy loss: 3.856564. Value loss: 65.634689. Entropy: 0.364429.\n",
      "Iteration 12698: Policy loss: 4.218486. Value loss: 33.096478. Entropy: 0.397691.\n",
      "Iteration 12699: Policy loss: 4.149394. Value loss: 23.538385. Entropy: 0.379040.\n",
      "episode: 5356   score: 305.0  epsilon: 1.0    steps: 590  evaluation reward: 280.55\n",
      "episode: 5357   score: 245.0  epsilon: 1.0    steps: 776  evaluation reward: 282.2\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12700: Policy loss: 1.678510. Value loss: 36.591805. Entropy: 0.384336.\n",
      "Iteration 12701: Policy loss: 1.751887. Value loss: 18.485451. Entropy: 0.384154.\n",
      "Iteration 12702: Policy loss: 1.977256. Value loss: 15.888460. Entropy: 0.384795.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12703: Policy loss: 2.311173. Value loss: 26.972696. Entropy: 0.364186.\n",
      "Iteration 12704: Policy loss: 2.167624. Value loss: 13.920243. Entropy: 0.334466.\n",
      "Iteration 12705: Policy loss: 2.278141. Value loss: 11.148413. Entropy: 0.356818.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12706: Policy loss: 1.714993. Value loss: 38.602066. Entropy: 0.456782.\n",
      "Iteration 12707: Policy loss: 1.746573. Value loss: 19.251272. Entropy: 0.453845.\n",
      "Iteration 12708: Policy loss: 1.888315. Value loss: 14.526070. Entropy: 0.447729.\n",
      "episode: 5358   score: 455.0  epsilon: 1.0    steps: 53  evaluation reward: 285.55\n",
      "episode: 5359   score: 640.0  epsilon: 1.0    steps: 148  evaluation reward: 290.65\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12709: Policy loss: 1.473718. Value loss: 32.368305. Entropy: 0.451652.\n",
      "Iteration 12710: Policy loss: 2.162417. Value loss: 16.305841. Entropy: 0.470427.\n",
      "Iteration 12711: Policy loss: 1.379689. Value loss: 13.934337. Entropy: 0.474410.\n",
      "episode: 5360   score: 320.0  epsilon: 1.0    steps: 381  evaluation reward: 291.2\n",
      "episode: 5361   score: 260.0  epsilon: 1.0    steps: 1002  evaluation reward: 291.7\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12712: Policy loss: -1.998448. Value loss: 40.558048. Entropy: 0.383630.\n",
      "Iteration 12713: Policy loss: -1.742249. Value loss: 26.961529. Entropy: 0.367529.\n",
      "Iteration 12714: Policy loss: -2.299592. Value loss: 22.507004. Entropy: 0.349452.\n",
      "episode: 5362   score: 300.0  epsilon: 1.0    steps: 698  evaluation reward: 293.8\n",
      "episode: 5363   score: 210.0  epsilon: 1.0    steps: 820  evaluation reward: 294.35\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12715: Policy loss: -0.929258. Value loss: 26.825346. Entropy: 0.317976.\n",
      "Iteration 12716: Policy loss: -1.014756. Value loss: 16.022429. Entropy: 0.309942.\n",
      "Iteration 12717: Policy loss: -1.003238. Value loss: 13.378417. Entropy: 0.317211.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12718: Policy loss: 2.687827. Value loss: 40.004028. Entropy: 0.401148.\n",
      "Iteration 12719: Policy loss: 2.667951. Value loss: 25.484768. Entropy: 0.389324.\n",
      "Iteration 12720: Policy loss: 2.681697. Value loss: 19.635788. Entropy: 0.392911.\n",
      "episode: 5364   score: 135.0  epsilon: 1.0    steps: 30  evaluation reward: 292.8\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12721: Policy loss: 1.257955. Value loss: 34.293301. Entropy: 0.465903.\n",
      "Iteration 12722: Policy loss: 1.587330. Value loss: 14.110979. Entropy: 0.462794.\n",
      "Iteration 12723: Policy loss: 1.169891. Value loss: 12.258028. Entropy: 0.443171.\n",
      "episode: 5365   score: 180.0  epsilon: 1.0    steps: 384  evaluation reward: 293.05\n",
      "episode: 5366   score: 355.0  epsilon: 1.0    steps: 584  evaluation reward: 293.75\n",
      "episode: 5367   score: 135.0  epsilon: 1.0    steps: 952  evaluation reward: 293.0\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12724: Policy loss: -3.723956. Value loss: 153.834778. Entropy: 0.412419.\n",
      "Iteration 12725: Policy loss: -3.791769. Value loss: 83.335098. Entropy: 0.407726.\n",
      "Iteration 12726: Policy loss: -4.336375. Value loss: 83.409676. Entropy: 0.425370.\n",
      "episode: 5368   score: 395.0  epsilon: 1.0    steps: 394  evaluation reward: 294.85\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12727: Policy loss: 2.990329. Value loss: 42.484303. Entropy: 0.294396.\n",
      "Iteration 12728: Policy loss: 2.329020. Value loss: 20.949024. Entropy: 0.309521.\n",
      "Iteration 12729: Policy loss: 2.696364. Value loss: 17.926016. Entropy: 0.299444.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12730: Policy loss: 2.352311. Value loss: 52.059265. Entropy: 0.342649.\n",
      "Iteration 12731: Policy loss: 2.532380. Value loss: 22.281956. Entropy: 0.323232.\n",
      "Iteration 12732: Policy loss: 2.721282. Value loss: 17.040510. Entropy: 0.351928.\n",
      "episode: 5369   score: 135.0  epsilon: 1.0    steps: 1  evaluation reward: 293.8\n",
      "episode: 5370   score: 600.0  epsilon: 1.0    steps: 179  evaluation reward: 298.8\n",
      "episode: 5371   score: 120.0  epsilon: 1.0    steps: 973  evaluation reward: 296.5\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12733: Policy loss: 0.902269. Value loss: 25.857210. Entropy: 0.322145.\n",
      "Iteration 12734: Policy loss: 1.070762. Value loss: 17.613092. Entropy: 0.364042.\n",
      "Iteration 12735: Policy loss: 0.886362. Value loss: 15.811847. Entropy: 0.353763.\n",
      "episode: 5372   score: 210.0  epsilon: 1.0    steps: 773  evaluation reward: 298.05\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12736: Policy loss: 1.656561. Value loss: 32.064606. Entropy: 0.399304.\n",
      "Iteration 12737: Policy loss: 1.518847. Value loss: 25.056318. Entropy: 0.421056.\n",
      "Iteration 12738: Policy loss: 1.833632. Value loss: 18.121378. Entropy: 0.426420.\n",
      "episode: 5373   score: 315.0  epsilon: 1.0    steps: 680  evaluation reward: 297.3\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12739: Policy loss: -1.737025. Value loss: 230.630951. Entropy: 0.429681.\n",
      "Iteration 12740: Policy loss: -2.382637. Value loss: 133.383514. Entropy: 0.434724.\n",
      "Iteration 12741: Policy loss: -1.950259. Value loss: 83.124260. Entropy: 0.421753.\n",
      "episode: 5374   score: 210.0  epsilon: 1.0    steps: 301  evaluation reward: 296.8\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12742: Policy loss: 1.315630. Value loss: 54.960163. Entropy: 0.426400.\n",
      "Iteration 12743: Policy loss: 1.581692. Value loss: 25.747025. Entropy: 0.392837.\n",
      "Iteration 12744: Policy loss: 1.226584. Value loss: 19.618139. Entropy: 0.400355.\n",
      "episode: 5375   score: 260.0  epsilon: 1.0    steps: 581  evaluation reward: 297.3\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12745: Policy loss: 1.777407. Value loss: 46.710239. Entropy: 0.599623.\n",
      "Iteration 12746: Policy loss: 1.088907. Value loss: 26.080168. Entropy: 0.615792.\n",
      "Iteration 12747: Policy loss: 1.374738. Value loss: 23.149313. Entropy: 0.625332.\n",
      "episode: 5376   score: 210.0  epsilon: 1.0    steps: 66  evaluation reward: 297.0\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12748: Policy loss: -1.850073. Value loss: 142.246048. Entropy: 0.595743.\n",
      "Iteration 12749: Policy loss: -1.732193. Value loss: 71.202957. Entropy: 0.572280.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12750: Policy loss: -1.990929. Value loss: 54.103897. Entropy: 0.565077.\n",
      "episode: 5377   score: 155.0  epsilon: 1.0    steps: 288  evaluation reward: 297.0\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12751: Policy loss: 2.343469. Value loss: 61.351383. Entropy: 0.468370.\n",
      "Iteration 12752: Policy loss: 2.386430. Value loss: 28.657682. Entropy: 0.467865.\n",
      "Iteration 12753: Policy loss: 2.034532. Value loss: 24.842731. Entropy: 0.473827.\n",
      "episode: 5378   score: 460.0  epsilon: 1.0    steps: 978  evaluation reward: 299.5\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12754: Policy loss: 4.132251. Value loss: 48.804001. Entropy: 0.456300.\n",
      "Iteration 12755: Policy loss: 4.405505. Value loss: 24.894567. Entropy: 0.488692.\n",
      "Iteration 12756: Policy loss: 4.325032. Value loss: 17.167913. Entropy: 0.498952.\n",
      "episode: 5379   score: 310.0  epsilon: 1.0    steps: 164  evaluation reward: 298.95\n",
      "episode: 5380   score: 180.0  epsilon: 1.0    steps: 622  evaluation reward: 299.2\n",
      "episode: 5381   score: 265.0  epsilon: 1.0    steps: 836  evaluation reward: 299.75\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12757: Policy loss: 1.733841. Value loss: 40.378353. Entropy: 0.476542.\n",
      "Iteration 12758: Policy loss: 2.023360. Value loss: 21.493366. Entropy: 0.525941.\n",
      "Iteration 12759: Policy loss: 1.757440. Value loss: 15.204735. Entropy: 0.560592.\n",
      "episode: 5382   score: 125.0  epsilon: 1.0    steps: 55  evaluation reward: 299.65\n",
      "episode: 5383   score: 670.0  epsilon: 1.0    steps: 458  evaluation reward: 304.25\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12760: Policy loss: 3.123761. Value loss: 37.027699. Entropy: 0.419177.\n",
      "Iteration 12761: Policy loss: 2.844451. Value loss: 21.998762. Entropy: 0.380717.\n",
      "Iteration 12762: Policy loss: 3.049434. Value loss: 16.406258. Entropy: 0.416095.\n",
      "episode: 5384   score: 125.0  epsilon: 1.0    steps: 980  evaluation reward: 302.6\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12763: Policy loss: -0.882827. Value loss: 21.262209. Entropy: 0.299286.\n",
      "Iteration 12764: Policy loss: -0.858693. Value loss: 14.397312. Entropy: 0.300038.\n",
      "Iteration 12765: Policy loss: -0.945053. Value loss: 11.597912. Entropy: 0.298771.\n",
      "episode: 5385   score: 135.0  epsilon: 1.0    steps: 187  evaluation reward: 301.25\n",
      "episode: 5386   score: 210.0  epsilon: 1.0    steps: 340  evaluation reward: 301.85\n",
      "episode: 5387   score: 315.0  epsilon: 1.0    steps: 756  evaluation reward: 302.7\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12766: Policy loss: 0.585380. Value loss: 26.068426. Entropy: 0.202063.\n",
      "Iteration 12767: Policy loss: 0.569462. Value loss: 16.039831. Entropy: 0.197708.\n",
      "Iteration 12768: Policy loss: 0.648391. Value loss: 12.989315. Entropy: 0.206764.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12769: Policy loss: 0.117126. Value loss: 31.821518. Entropy: 0.283962.\n",
      "Iteration 12770: Policy loss: 0.066783. Value loss: 17.968184. Entropy: 0.276301.\n",
      "Iteration 12771: Policy loss: -0.230678. Value loss: 14.103420. Entropy: 0.308995.\n",
      "episode: 5388   score: 155.0  epsilon: 1.0    steps: 412  evaluation reward: 302.15\n",
      "episode: 5389   score: 130.0  epsilon: 1.0    steps: 538  evaluation reward: 301.05\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12772: Policy loss: -0.239643. Value loss: 25.341103. Entropy: 0.363573.\n",
      "Iteration 12773: Policy loss: -0.158256. Value loss: 15.002666. Entropy: 0.383213.\n",
      "Iteration 12774: Policy loss: -0.129361. Value loss: 14.184000. Entropy: 0.377169.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12775: Policy loss: -4.466928. Value loss: 185.803360. Entropy: 0.281263.\n",
      "Iteration 12776: Policy loss: -3.556169. Value loss: 78.492302. Entropy: 0.251475.\n",
      "Iteration 12777: Policy loss: -4.042835. Value loss: 75.454285. Entropy: 0.235758.\n",
      "episode: 5390   score: 180.0  epsilon: 1.0    steps: 341  evaluation reward: 299.65\n",
      "episode: 5391   score: 265.0  epsilon: 1.0    steps: 821  evaluation reward: 299.85\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12778: Policy loss: 0.215823. Value loss: 35.569408. Entropy: 0.524829.\n",
      "Iteration 12779: Policy loss: 0.545007. Value loss: 20.352165. Entropy: 0.524231.\n",
      "Iteration 12780: Policy loss: 0.370856. Value loss: 15.301060. Entropy: 0.521015.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12781: Policy loss: -1.218418. Value loss: 48.186203. Entropy: 0.473182.\n",
      "Iteration 12782: Policy loss: -1.120693. Value loss: 26.201565. Entropy: 0.461342.\n",
      "Iteration 12783: Policy loss: -1.236675. Value loss: 15.597860. Entropy: 0.475508.\n",
      "episode: 5392   score: 345.0  epsilon: 1.0    steps: 991  evaluation reward: 300.85\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12784: Policy loss: 0.371281. Value loss: 35.054073. Entropy: 0.566591.\n",
      "Iteration 12785: Policy loss: 0.267102. Value loss: 17.986792. Entropy: 0.587952.\n",
      "Iteration 12786: Policy loss: 0.416480. Value loss: 13.195742. Entropy: 0.584722.\n",
      "episode: 5393   score: 210.0  epsilon: 1.0    steps: 487  evaluation reward: 300.85\n",
      "episode: 5394   score: 315.0  epsilon: 1.0    steps: 692  evaluation reward: 301.6\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12787: Policy loss: 0.598253. Value loss: 169.213364. Entropy: 0.472584.\n",
      "Iteration 12788: Policy loss: 1.193153. Value loss: 88.571655. Entropy: 0.427561.\n",
      "Iteration 12789: Policy loss: 0.676743. Value loss: 90.881096. Entropy: 0.441302.\n",
      "episode: 5395   score: 590.0  epsilon: 1.0    steps: 85  evaluation reward: 305.7\n",
      "episode: 5396   score: 600.0  epsilon: 1.0    steps: 154  evaluation reward: 307.25\n",
      "episode: 5397   score: 80.0  epsilon: 1.0    steps: 868  evaluation reward: 303.6\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12790: Policy loss: 3.287294. Value loss: 66.982452. Entropy: 0.349684.\n",
      "Iteration 12791: Policy loss: 3.206178. Value loss: 39.677467. Entropy: 0.376203.\n",
      "Iteration 12792: Policy loss: 3.145148. Value loss: 31.133022. Entropy: 0.323775.\n",
      "episode: 5398   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 302.95\n",
      "episode: 5399   score: 365.0  epsilon: 1.0    steps: 602  evaluation reward: 304.5\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12793: Policy loss: 4.974523. Value loss: 81.119751. Entropy: 0.352605.\n",
      "Iteration 12794: Policy loss: 4.450677. Value loss: 31.450302. Entropy: 0.341173.\n",
      "Iteration 12795: Policy loss: 4.807813. Value loss: 21.647812. Entropy: 0.347697.\n",
      "episode: 5400   score: 125.0  epsilon: 1.0    steps: 245  evaluation reward: 303.5\n",
      "now time :  2019-02-25 22:39:04.553273\n",
      "episode: 5401   score: 125.0  epsilon: 1.0    steps: 501  evaluation reward: 302.2\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12796: Policy loss: 0.588651. Value loss: 32.200748. Entropy: 0.261475.\n",
      "Iteration 12797: Policy loss: 0.767103. Value loss: 20.938738. Entropy: 0.249685.\n",
      "Iteration 12798: Policy loss: 0.717279. Value loss: 15.885992. Entropy: 0.240965.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12799: Policy loss: 1.997180. Value loss: 25.379156. Entropy: 0.309503.\n",
      "Iteration 12800: Policy loss: 2.021358. Value loss: 15.014647. Entropy: 0.317692.\n",
      "Iteration 12801: Policy loss: 1.963519. Value loss: 10.591932. Entropy: 0.311737.\n",
      "episode: 5402   score: 125.0  epsilon: 1.0    steps: 317  evaluation reward: 300.75\n",
      "episode: 5403   score: 125.0  epsilon: 1.0    steps: 573  evaluation reward: 299.6\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12802: Policy loss: 0.633931. Value loss: 33.115108. Entropy: 0.238806.\n",
      "Iteration 12803: Policy loss: 0.279713. Value loss: 14.855103. Entropy: 0.239479.\n",
      "Iteration 12804: Policy loss: 0.624516. Value loss: 13.402275. Entropy: 0.243177.\n",
      "episode: 5404   score: 290.0  epsilon: 1.0    steps: 1000  evaluation reward: 299.85\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12805: Policy loss: 0.425241. Value loss: 39.680202. Entropy: 0.357124.\n",
      "Iteration 12806: Policy loss: 0.704439. Value loss: 21.069986. Entropy: 0.322376.\n",
      "Iteration 12807: Policy loss: 0.500303. Value loss: 15.210898. Entropy: 0.343229.\n",
      "episode: 5405   score: 290.0  epsilon: 1.0    steps: 741  evaluation reward: 299.2\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12808: Policy loss: 0.684330. Value loss: 34.497395. Entropy: 0.312146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12809: Policy loss: 0.781906. Value loss: 16.492525. Entropy: 0.303514.\n",
      "Iteration 12810: Policy loss: 0.832739. Value loss: 13.460719. Entropy: 0.315132.\n",
      "episode: 5406   score: 120.0  epsilon: 1.0    steps: 590  evaluation reward: 297.8\n",
      "episode: 5407   score: 260.0  epsilon: 1.0    steps: 883  evaluation reward: 297.45\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12811: Policy loss: 1.815544. Value loss: 27.356464. Entropy: 0.337736.\n",
      "Iteration 12812: Policy loss: 2.090455. Value loss: 16.924162. Entropy: 0.364629.\n",
      "Iteration 12813: Policy loss: 1.869382. Value loss: 15.070192. Entropy: 0.361891.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12814: Policy loss: -0.872468. Value loss: 24.745333. Entropy: 0.336918.\n",
      "Iteration 12815: Policy loss: -0.991991. Value loss: 10.687421. Entropy: 0.358696.\n",
      "Iteration 12816: Policy loss: -1.132465. Value loss: 9.589161. Entropy: 0.332300.\n",
      "episode: 5408   score: 390.0  epsilon: 1.0    steps: 96  evaluation reward: 295.65\n",
      "episode: 5409   score: 260.0  epsilon: 1.0    steps: 201  evaluation reward: 295.15\n",
      "episode: 5410   score: 240.0  epsilon: 1.0    steps: 464  evaluation reward: 293.5\n",
      "episode: 5411   score: 105.0  epsilon: 1.0    steps: 717  evaluation reward: 288.95\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12817: Policy loss: -0.530651. Value loss: 27.974401. Entropy: 0.280553.\n",
      "Iteration 12818: Policy loss: -0.156002. Value loss: 15.915546. Entropy: 0.303167.\n",
      "Iteration 12819: Policy loss: -0.369859. Value loss: 15.239962. Entropy: 0.280284.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12820: Policy loss: -1.824233. Value loss: 31.961414. Entropy: 0.344309.\n",
      "Iteration 12821: Policy loss: -1.599779. Value loss: 19.285791. Entropy: 0.324062.\n",
      "Iteration 12822: Policy loss: -1.465820. Value loss: 15.445156. Entropy: 0.328173.\n",
      "episode: 5412   score: 265.0  epsilon: 1.0    steps: 330  evaluation reward: 288.75\n",
      "episode: 5413   score: 210.0  epsilon: 1.0    steps: 916  evaluation reward: 288.45\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12823: Policy loss: 0.347503. Value loss: 27.765709. Entropy: 0.423062.\n",
      "Iteration 12824: Policy loss: 0.592529. Value loss: 18.245182. Entropy: 0.430343.\n",
      "Iteration 12825: Policy loss: 0.286918. Value loss: 15.145632. Entropy: 0.440238.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12826: Policy loss: 0.003452. Value loss: 24.781115. Entropy: 0.415755.\n",
      "Iteration 12827: Policy loss: 0.043564. Value loss: 11.181119. Entropy: 0.425319.\n",
      "Iteration 12828: Policy loss: 0.038910. Value loss: 9.079046. Entropy: 0.412068.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12829: Policy loss: -2.331112. Value loss: 113.930443. Entropy: 0.467127.\n",
      "Iteration 12830: Policy loss: -2.398696. Value loss: 36.629227. Entropy: 0.458121.\n",
      "Iteration 12831: Policy loss: -2.093423. Value loss: 21.841852. Entropy: 0.489412.\n",
      "episode: 5414   score: 155.0  epsilon: 1.0    steps: 104  evaluation reward: 288.2\n",
      "episode: 5415   score: 180.0  epsilon: 1.0    steps: 204  evaluation reward: 284.35\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12832: Policy loss: 0.594252. Value loss: 29.821358. Entropy: 0.659274.\n",
      "Iteration 12833: Policy loss: 0.239963. Value loss: 17.337389. Entropy: 0.664359.\n",
      "Iteration 12834: Policy loss: 0.303292. Value loss: 13.914783. Entropy: 0.652766.\n",
      "episode: 5416   score: 395.0  epsilon: 1.0    steps: 622  evaluation reward: 285.7\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12835: Policy loss: 3.873545. Value loss: 35.878296. Entropy: 0.466966.\n",
      "Iteration 12836: Policy loss: 3.485379. Value loss: 13.770156. Entropy: 0.453263.\n",
      "Iteration 12837: Policy loss: 3.786479. Value loss: 10.957119. Entropy: 0.478103.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12838: Policy loss: -1.485824. Value loss: 38.226540. Entropy: 0.631524.\n",
      "Iteration 12839: Policy loss: -1.349010. Value loss: 23.579256. Entropy: 0.614952.\n",
      "Iteration 12840: Policy loss: -1.371350. Value loss: 18.428879. Entropy: 0.612499.\n",
      "episode: 5417   score: 90.0  epsilon: 1.0    steps: 69  evaluation reward: 282.8\n",
      "episode: 5418   score: 290.0  epsilon: 1.0    steps: 644  evaluation reward: 282.85\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12841: Policy loss: -1.754199. Value loss: 189.745422. Entropy: 0.503325.\n",
      "Iteration 12842: Policy loss: -1.804199. Value loss: 71.035484. Entropy: 0.493106.\n",
      "Iteration 12843: Policy loss: -1.446536. Value loss: 49.128075. Entropy: 0.509485.\n",
      "episode: 5419   score: 820.0  epsilon: 1.0    steps: 824  evaluation reward: 287.75\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12844: Policy loss: -0.471264. Value loss: 120.746574. Entropy: 0.540518.\n",
      "Iteration 12845: Policy loss: -0.014682. Value loss: 68.748695. Entropy: 0.527023.\n",
      "Iteration 12846: Policy loss: -0.372498. Value loss: 65.323738. Entropy: 0.544726.\n",
      "episode: 5420   score: 270.0  epsilon: 1.0    steps: 269  evaluation reward: 288.2\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12847: Policy loss: -0.585237. Value loss: 21.797182. Entropy: 0.444890.\n",
      "Iteration 12848: Policy loss: -0.747168. Value loss: 13.128546. Entropy: 0.454852.\n",
      "Iteration 12849: Policy loss: -0.540307. Value loss: 8.981909. Entropy: 0.450901.\n",
      "episode: 5421   score: 670.0  epsilon: 1.0    steps: 444  evaluation reward: 292.8\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12850: Policy loss: -6.725409. Value loss: 262.400543. Entropy: 0.366234.\n",
      "Iteration 12851: Policy loss: -7.005264. Value loss: 152.527878. Entropy: 0.381640.\n",
      "Iteration 12852: Policy loss: -6.098974. Value loss: 90.058395. Entropy: 0.390554.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12853: Policy loss: 4.762732. Value loss: 100.505440. Entropy: 0.484074.\n",
      "Iteration 12854: Policy loss: 4.503871. Value loss: 31.813173. Entropy: 0.468315.\n",
      "Iteration 12855: Policy loss: 4.603963. Value loss: 24.107975. Entropy: 0.475689.\n",
      "episode: 5422   score: 440.0  epsilon: 1.0    steps: 171  evaluation reward: 293.65\n",
      "episode: 5423   score: 210.0  epsilon: 1.0    steps: 741  evaluation reward: 291.15\n",
      "episode: 5424   score: 650.0  epsilon: 1.0    steps: 917  evaluation reward: 294.6\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12856: Policy loss: 2.751152. Value loss: 24.045866. Entropy: 0.414913.\n",
      "Iteration 12857: Policy loss: 2.695890. Value loss: 16.209261. Entropy: 0.418424.\n",
      "Iteration 12858: Policy loss: 2.523966. Value loss: 15.264532. Entropy: 0.437082.\n",
      "episode: 5425   score: 180.0  epsilon: 1.0    steps: 34  evaluation reward: 292.25\n",
      "episode: 5426   score: 295.0  epsilon: 1.0    steps: 604  evaluation reward: 292.55\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12859: Policy loss: -0.692241. Value loss: 38.757767. Entropy: 0.422218.\n",
      "Iteration 12860: Policy loss: -0.562733. Value loss: 23.084389. Entropy: 0.426782.\n",
      "Iteration 12861: Policy loss: -0.648810. Value loss: 19.191292. Entropy: 0.438394.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12862: Policy loss: 1.463561. Value loss: 66.727440. Entropy: 0.425251.\n",
      "Iteration 12863: Policy loss: 0.751837. Value loss: 34.620346. Entropy: 0.375118.\n",
      "Iteration 12864: Policy loss: 0.724374. Value loss: 21.876755. Entropy: 0.405162.\n",
      "episode: 5427   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 290.0\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12865: Policy loss: 0.428565. Value loss: 47.830471. Entropy: 0.408400.\n",
      "Iteration 12866: Policy loss: 0.441299. Value loss: 24.036608. Entropy: 0.379076.\n",
      "Iteration 12867: Policy loss: 0.646047. Value loss: 17.261642. Entropy: 0.359293.\n",
      "episode: 5428   score: 265.0  epsilon: 1.0    steps: 787  evaluation reward: 289.75\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12868: Policy loss: -3.109219. Value loss: 237.113754. Entropy: 0.459856.\n",
      "Iteration 12869: Policy loss: -3.088272. Value loss: 109.912498. Entropy: 0.421931.\n",
      "Iteration 12870: Policy loss: -2.794829. Value loss: 76.268402. Entropy: 0.419405.\n",
      "episode: 5429   score: 180.0  epsilon: 1.0    steps: 930  evaluation reward: 290.2\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12871: Policy loss: -0.117116. Value loss: 44.628269. Entropy: 0.294706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12872: Policy loss: -0.087866. Value loss: 27.203066. Entropy: 0.295697.\n",
      "Iteration 12873: Policy loss: 0.007945. Value loss: 26.567924. Entropy: 0.301149.\n",
      "episode: 5430   score: 210.0  epsilon: 1.0    steps: 188  evaluation reward: 284.85\n",
      "episode: 5431   score: 210.0  epsilon: 1.0    steps: 668  evaluation reward: 285.6\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12874: Policy loss: 0.849812. Value loss: 38.863415. Entropy: 0.402921.\n",
      "Iteration 12875: Policy loss: 0.862830. Value loss: 19.977037. Entropy: 0.390136.\n",
      "Iteration 12876: Policy loss: 0.829565. Value loss: 15.067241. Entropy: 0.404235.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12877: Policy loss: -0.457308. Value loss: 120.506386. Entropy: 0.416586.\n",
      "Iteration 12878: Policy loss: -0.028956. Value loss: 73.727448. Entropy: 0.401048.\n",
      "Iteration 12879: Policy loss: -0.631352. Value loss: 41.261871. Entropy: 0.416503.\n",
      "episode: 5432   score: 575.0  epsilon: 1.0    steps: 277  evaluation reward: 287.0\n",
      "episode: 5433   score: 275.0  epsilon: 1.0    steps: 630  evaluation reward: 287.65\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12880: Policy loss: 2.426983. Value loss: 23.181410. Entropy: 0.321975.\n",
      "Iteration 12881: Policy loss: 2.209034. Value loss: 13.867292. Entropy: 0.342170.\n",
      "Iteration 12882: Policy loss: 2.358342. Value loss: 12.128679. Entropy: 0.340149.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12883: Policy loss: -0.241518. Value loss: 40.511852. Entropy: 0.350506.\n",
      "Iteration 12884: Policy loss: -0.232173. Value loss: 21.763090. Entropy: 0.359880.\n",
      "Iteration 12885: Policy loss: -0.249723. Value loss: 17.635363. Entropy: 0.355121.\n",
      "episode: 5434   score: 210.0  epsilon: 1.0    steps: 455  evaluation reward: 287.65\n",
      "episode: 5435   score: 180.0  epsilon: 1.0    steps: 991  evaluation reward: 286.85\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12886: Policy loss: 5.022165. Value loss: 78.009346. Entropy: 0.447655.\n",
      "Iteration 12887: Policy loss: 4.906898. Value loss: 32.312824. Entropy: 0.437618.\n",
      "Iteration 12888: Policy loss: 4.810866. Value loss: 22.501740. Entropy: 0.464251.\n",
      "episode: 5436   score: 265.0  epsilon: 1.0    steps: 877  evaluation reward: 287.7\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12889: Policy loss: -2.538693. Value loss: 159.460403. Entropy: 0.526466.\n",
      "Iteration 12890: Policy loss: -2.391534. Value loss: 101.073944. Entropy: 0.533757.\n",
      "Iteration 12891: Policy loss: -2.149778. Value loss: 60.890072. Entropy: 0.538543.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12892: Policy loss: 2.660967. Value loss: 58.605415. Entropy: 0.440530.\n",
      "Iteration 12893: Policy loss: 3.108685. Value loss: 24.052753. Entropy: 0.459054.\n",
      "Iteration 12894: Policy loss: 2.668422. Value loss: 19.481960. Entropy: 0.466776.\n",
      "episode: 5437   score: 110.0  epsilon: 1.0    steps: 582  evaluation reward: 285.15\n",
      "episode: 5438   score: 260.0  epsilon: 1.0    steps: 680  evaluation reward: 283.65\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12895: Policy loss: 1.266834. Value loss: 25.870747. Entropy: 0.446009.\n",
      "Iteration 12896: Policy loss: 1.440300. Value loss: 12.786502. Entropy: 0.465255.\n",
      "Iteration 12897: Policy loss: 1.282144. Value loss: 14.589279. Entropy: 0.450266.\n",
      "episode: 5439   score: 260.0  epsilon: 1.0    steps: 138  evaluation reward: 282.9\n",
      "episode: 5440   score: 460.0  epsilon: 1.0    steps: 360  evaluation reward: 284.65\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12898: Policy loss: -0.083642. Value loss: 10.514686. Entropy: 0.331826.\n",
      "Iteration 12899: Policy loss: -0.173739. Value loss: 8.749324. Entropy: 0.317389.\n",
      "Iteration 12900: Policy loss: -0.173127. Value loss: 7.740363. Entropy: 0.326616.\n",
      "episode: 5441   score: 650.0  epsilon: 1.0    steps: 5  evaluation reward: 288.25\n",
      "episode: 5442   score: 155.0  epsilon: 1.0    steps: 460  evaluation reward: 287.4\n",
      "episode: 5443   score: 180.0  epsilon: 1.0    steps: 1018  evaluation reward: 286.3\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12901: Policy loss: -0.071497. Value loss: 15.520804. Entropy: 0.193171.\n",
      "Iteration 12902: Policy loss: -0.077564. Value loss: 11.885926. Entropy: 0.203743.\n",
      "Iteration 12903: Policy loss: -0.048595. Value loss: 9.717384. Entropy: 0.188131.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12904: Policy loss: -0.865661. Value loss: 9.391669. Entropy: 0.234713.\n",
      "Iteration 12905: Policy loss: -0.938081. Value loss: 7.723513. Entropy: 0.228042.\n",
      "Iteration 12906: Policy loss: -0.695992. Value loss: 5.534559. Entropy: 0.228241.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12907: Policy loss: -0.520910. Value loss: 16.512911. Entropy: 0.259129.\n",
      "Iteration 12908: Policy loss: -0.189443. Value loss: 8.586206. Entropy: 0.267367.\n",
      "Iteration 12909: Policy loss: -0.317943. Value loss: 7.879344. Entropy: 0.239571.\n",
      "episode: 5444   score: 210.0  epsilon: 1.0    steps: 613  evaluation reward: 285.8\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12910: Policy loss: -0.598412. Value loss: 23.675795. Entropy: 0.273936.\n",
      "Iteration 12911: Policy loss: -1.040057. Value loss: 16.236307. Entropy: 0.301670.\n",
      "Iteration 12912: Policy loss: -0.844939. Value loss: 12.724127. Entropy: 0.290490.\n",
      "episode: 5445   score: 210.0  epsilon: 1.0    steps: 171  evaluation reward: 285.8\n",
      "episode: 5446   score: 290.0  epsilon: 1.0    steps: 887  evaluation reward: 286.6\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12913: Policy loss: 0.843681. Value loss: 19.028250. Entropy: 0.396991.\n",
      "Iteration 12914: Policy loss: 0.797404. Value loss: 9.926971. Entropy: 0.373843.\n",
      "Iteration 12915: Policy loss: 0.854726. Value loss: 8.566978. Entropy: 0.400586.\n",
      "episode: 5447   score: 155.0  epsilon: 1.0    steps: 285  evaluation reward: 286.05\n",
      "episode: 5448   score: 155.0  epsilon: 1.0    steps: 482  evaluation reward: 278.35\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12916: Policy loss: 1.038138. Value loss: 28.182030. Entropy: 0.374976.\n",
      "Iteration 12917: Policy loss: 0.943281. Value loss: 16.465019. Entropy: 0.360520.\n",
      "Iteration 12918: Policy loss: 1.073229. Value loss: 12.081345. Entropy: 0.364494.\n",
      "episode: 5449   score: 295.0  epsilon: 1.0    steps: 663  evaluation reward: 276.35\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12919: Policy loss: -0.020374. Value loss: 10.252472. Entropy: 0.315388.\n",
      "Iteration 12920: Policy loss: 0.069180. Value loss: 5.832174. Entropy: 0.322517.\n",
      "Iteration 12921: Policy loss: 0.034172. Value loss: 4.772276. Entropy: 0.298809.\n",
      "episode: 5450   score: 240.0  epsilon: 1.0    steps: 63  evaluation reward: 275.5\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12922: Policy loss: 0.966901. Value loss: 8.457354. Entropy: 0.281606.\n",
      "Iteration 12923: Policy loss: 0.899217. Value loss: 5.048888. Entropy: 0.276405.\n",
      "Iteration 12924: Policy loss: 0.892159. Value loss: 4.124486. Entropy: 0.300528.\n",
      "now time :  2019-02-25 22:41:31.302821\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5451   score: 240.0  epsilon: 1.0    steps: 937  evaluation reward: 275.8\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12925: Policy loss: -0.149303. Value loss: 37.543579. Entropy: 0.369814.\n",
      "Iteration 12926: Policy loss: -0.372164. Value loss: 11.816073. Entropy: 0.404360.\n",
      "Iteration 12927: Policy loss: -0.397923. Value loss: 7.699252. Entropy: 0.400651.\n",
      "episode: 5452   score: 155.0  epsilon: 1.0    steps: 175  evaluation reward: 273.85\n",
      "episode: 5453   score: 165.0  epsilon: 1.0    steps: 522  evaluation reward: 273.4\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12928: Policy loss: -1.332802. Value loss: 39.284664. Entropy: 0.271981.\n",
      "Iteration 12929: Policy loss: -1.102268. Value loss: 24.663460. Entropy: 0.283168.\n",
      "Iteration 12930: Policy loss: -1.252951. Value loss: 20.045986. Entropy: 0.286960.\n",
      "episode: 5454   score: 210.0  epsilon: 1.0    steps: 300  evaluation reward: 271.75\n",
      "episode: 5455   score: 210.0  epsilon: 1.0    steps: 755  evaluation reward: 271.0\n",
      "episode: 5456   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 270.05\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12931: Policy loss: 0.833823. Value loss: 32.711216. Entropy: 0.285891.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12932: Policy loss: 0.960500. Value loss: 20.403833. Entropy: 0.305269.\n",
      "Iteration 12933: Policy loss: 0.878708. Value loss: 15.906100. Entropy: 0.303272.\n",
      "episode: 5457   score: 290.0  epsilon: 1.0    steps: 491  evaluation reward: 270.5\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12934: Policy loss: -0.301997. Value loss: 28.484047. Entropy: 0.253992.\n",
      "Iteration 12935: Policy loss: -0.382465. Value loss: 14.992136. Entropy: 0.244872.\n",
      "Iteration 12936: Policy loss: -0.191142. Value loss: 13.722949. Entropy: 0.255283.\n",
      "episode: 5458   score: 180.0  epsilon: 1.0    steps: 17  evaluation reward: 267.75\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12937: Policy loss: 1.107911. Value loss: 30.136265. Entropy: 0.247939.\n",
      "Iteration 12938: Policy loss: 0.708818. Value loss: 16.639650. Entropy: 0.214959.\n",
      "Iteration 12939: Policy loss: 0.487849. Value loss: 11.444020. Entropy: 0.237204.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12940: Policy loss: -0.472352. Value loss: 35.058426. Entropy: 0.328122.\n",
      "Iteration 12941: Policy loss: -0.557408. Value loss: 23.451677. Entropy: 0.322222.\n",
      "Iteration 12942: Policy loss: -0.464810. Value loss: 17.377764. Entropy: 0.315135.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12943: Policy loss: 0.235766. Value loss: 23.667009. Entropy: 0.248599.\n",
      "Iteration 12944: Policy loss: 0.607926. Value loss: 12.553543. Entropy: 0.258898.\n",
      "Iteration 12945: Policy loss: 0.427752. Value loss: 8.937034. Entropy: 0.262137.\n",
      "episode: 5459   score: 275.0  epsilon: 1.0    steps: 553  evaluation reward: 264.1\n",
      "episode: 5460   score: 335.0  epsilon: 1.0    steps: 920  evaluation reward: 264.25\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12946: Policy loss: -0.960409. Value loss: 17.933447. Entropy: 0.325971.\n",
      "Iteration 12947: Policy loss: -0.815031. Value loss: 10.973399. Entropy: 0.323088.\n",
      "Iteration 12948: Policy loss: -1.033903. Value loss: 10.743627. Entropy: 0.328205.\n",
      "episode: 5461   score: 285.0  epsilon: 1.0    steps: 734  evaluation reward: 264.5\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12949: Policy loss: 3.028058. Value loss: 35.340233. Entropy: 0.188500.\n",
      "Iteration 12950: Policy loss: 3.179379. Value loss: 18.783749. Entropy: 0.204227.\n",
      "Iteration 12951: Policy loss: 2.977282. Value loss: 15.215363. Entropy: 0.211463.\n",
      "episode: 5462   score: 480.0  epsilon: 1.0    steps: 252  evaluation reward: 266.3\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12952: Policy loss: 1.152571. Value loss: 34.719425. Entropy: 0.310541.\n",
      "Iteration 12953: Policy loss: 1.478896. Value loss: 19.084139. Entropy: 0.297623.\n",
      "Iteration 12954: Policy loss: 1.371454. Value loss: 14.389109. Entropy: 0.308071.\n",
      "episode: 5463   score: 260.0  epsilon: 1.0    steps: 32  evaluation reward: 266.8\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12955: Policy loss: -0.453013. Value loss: 20.489735. Entropy: 0.231942.\n",
      "Iteration 12956: Policy loss: -0.515795. Value loss: 18.517456. Entropy: 0.231769.\n",
      "Iteration 12957: Policy loss: -0.698427. Value loss: 11.853655. Entropy: 0.226975.\n",
      "episode: 5464   score: 385.0  epsilon: 1.0    steps: 291  evaluation reward: 269.3\n",
      "episode: 5465   score: 355.0  epsilon: 1.0    steps: 788  evaluation reward: 271.05\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12958: Policy loss: -1.691553. Value loss: 29.420212. Entropy: 0.270913.\n",
      "Iteration 12959: Policy loss: -1.826370. Value loss: 15.149477. Entropy: 0.270151.\n",
      "Iteration 12960: Policy loss: -1.589682. Value loss: 13.044618. Entropy: 0.278756.\n",
      "episode: 5466   score: 210.0  epsilon: 1.0    steps: 579  evaluation reward: 269.6\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12961: Policy loss: 1.465226. Value loss: 24.132179. Entropy: 0.295224.\n",
      "Iteration 12962: Policy loss: 1.600018. Value loss: 16.567610. Entropy: 0.285342.\n",
      "Iteration 12963: Policy loss: 1.416434. Value loss: 13.604710. Entropy: 0.308301.\n",
      "episode: 5467   score: 380.0  epsilon: 1.0    steps: 411  evaluation reward: 272.05\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12964: Policy loss: -0.959371. Value loss: 19.404736. Entropy: 0.386252.\n",
      "Iteration 12965: Policy loss: -0.919569. Value loss: 10.540575. Entropy: 0.399398.\n",
      "Iteration 12966: Policy loss: -0.815408. Value loss: 10.671700. Entropy: 0.386503.\n",
      "episode: 5468   score: 260.0  epsilon: 1.0    steps: 996  evaluation reward: 270.7\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12967: Policy loss: -4.448838. Value loss: 297.447327. Entropy: 0.326137.\n",
      "Iteration 12968: Policy loss: -3.265862. Value loss: 113.340546. Entropy: 0.334420.\n",
      "Iteration 12969: Policy loss: -4.225478. Value loss: 58.663731. Entropy: 0.373975.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12970: Policy loss: 0.219269. Value loss: 29.689333. Entropy: 0.264569.\n",
      "Iteration 12971: Policy loss: 0.067731. Value loss: 15.342621. Entropy: 0.273784.\n",
      "Iteration 12972: Policy loss: 0.044210. Value loss: 12.234401. Entropy: 0.272318.\n",
      "episode: 5469   score: 270.0  epsilon: 1.0    steps: 163  evaluation reward: 272.05\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12973: Policy loss: -2.061694. Value loss: 237.454208. Entropy: 0.167972.\n",
      "Iteration 12974: Policy loss: -1.851959. Value loss: 86.113487. Entropy: 0.157338.\n",
      "Iteration 12975: Policy loss: -1.535641. Value loss: 71.597870. Entropy: 0.153188.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12976: Policy loss: -1.668953. Value loss: 113.730324. Entropy: 0.156845.\n",
      "Iteration 12977: Policy loss: -1.495247. Value loss: 46.131641. Entropy: 0.159427.\n",
      "Iteration 12978: Policy loss: -1.779399. Value loss: 25.827652. Entropy: 0.152198.\n",
      "episode: 5470   score: 275.0  epsilon: 1.0    steps: 630  evaluation reward: 268.8\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12979: Policy loss: 1.777129. Value loss: 27.041077. Entropy: 0.208267.\n",
      "Iteration 12980: Policy loss: 1.846462. Value loss: 16.829533. Entropy: 0.213013.\n",
      "Iteration 12981: Policy loss: 1.837669. Value loss: 14.197552. Entropy: 0.241465.\n",
      "episode: 5471   score: 390.0  epsilon: 1.0    steps: 428  evaluation reward: 271.5\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12982: Policy loss: -0.006000. Value loss: 53.866486. Entropy: 0.160920.\n",
      "Iteration 12983: Policy loss: 0.004368. Value loss: 27.730408. Entropy: 0.158853.\n",
      "Iteration 12984: Policy loss: -0.077090. Value loss: 16.869703. Entropy: 0.151343.\n",
      "episode: 5472   score: 665.0  epsilon: 1.0    steps: 310  evaluation reward: 276.05\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12985: Policy loss: 5.308032. Value loss: 86.210754. Entropy: 0.196742.\n",
      "Iteration 12986: Policy loss: 6.163056. Value loss: 33.868889. Entropy: 0.178075.\n",
      "Iteration 12987: Policy loss: 5.768693. Value loss: 19.392279. Entropy: 0.217203.\n",
      "episode: 5473   score: 210.0  epsilon: 1.0    steps: 202  evaluation reward: 275.0\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12988: Policy loss: 1.133203. Value loss: 29.950230. Entropy: 0.119677.\n",
      "Iteration 12989: Policy loss: 1.243520. Value loss: 12.810542. Entropy: 0.123020.\n",
      "Iteration 12990: Policy loss: 1.338909. Value loss: 10.965492. Entropy: 0.127172.\n",
      "episode: 5474   score: 695.0  epsilon: 1.0    steps: 739  evaluation reward: 279.85\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12991: Policy loss: 0.957899. Value loss: 36.060108. Entropy: 0.165064.\n",
      "Iteration 12992: Policy loss: 0.878227. Value loss: 20.729704. Entropy: 0.156060.\n",
      "Iteration 12993: Policy loss: 1.238047. Value loss: 15.987178. Entropy: 0.174726.\n",
      "episode: 5475   score: 250.0  epsilon: 1.0    steps: 868  evaluation reward: 279.75\n",
      "episode: 5476   score: 320.0  epsilon: 1.0    steps: 1014  evaluation reward: 280.85\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12994: Policy loss: -0.299349. Value loss: 40.132477. Entropy: 0.313544.\n",
      "Iteration 12995: Policy loss: 0.192524. Value loss: 24.240221. Entropy: 0.332579.\n",
      "Iteration 12996: Policy loss: -0.301800. Value loss: 20.538460. Entropy: 0.311795.\n",
      "episode: 5477   score: 800.0  epsilon: 1.0    steps: 32  evaluation reward: 287.3\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12997: Policy loss: 1.059886. Value loss: 40.803684. Entropy: 0.268938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12998: Policy loss: 1.178206. Value loss: 23.999268. Entropy: 0.283296.\n",
      "Iteration 12999: Policy loss: 1.026852. Value loss: 18.211830. Entropy: 0.287838.\n",
      "episode: 5478   score: 260.0  epsilon: 1.0    steps: 455  evaluation reward: 285.3\n",
      "episode: 5479   score: 310.0  epsilon: 1.0    steps: 611  evaluation reward: 285.3\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 13000: Policy loss: 1.027356. Value loss: 23.497377. Entropy: 0.363210.\n",
      "Iteration 13001: Policy loss: 1.180309. Value loss: 15.807240. Entropy: 0.355726.\n",
      "Iteration 13002: Policy loss: 0.843050. Value loss: 11.786574. Entropy: 0.363393.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13003: Policy loss: -0.297845. Value loss: 15.093898. Entropy: 0.190625.\n",
      "Iteration 13004: Policy loss: -0.205323. Value loss: 8.384995. Entropy: 0.200810.\n",
      "Iteration 13005: Policy loss: -0.330372. Value loss: 6.072470. Entropy: 0.190285.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13006: Policy loss: -2.874831. Value loss: 185.162476. Entropy: 0.305602.\n",
      "Iteration 13007: Policy loss: -3.029784. Value loss: 92.815659. Entropy: 0.295388.\n",
      "Iteration 13008: Policy loss: -2.900816. Value loss: 61.036560. Entropy: 0.296576.\n",
      "episode: 5480   score: 290.0  epsilon: 1.0    steps: 161  evaluation reward: 286.4\n",
      "episode: 5481   score: 260.0  epsilon: 1.0    steps: 744  evaluation reward: 286.35\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13009: Policy loss: -0.666645. Value loss: 29.720539. Entropy: 0.334942.\n",
      "Iteration 13010: Policy loss: -0.884110. Value loss: 20.374146. Entropy: 0.313488.\n",
      "Iteration 13011: Policy loss: -0.846591. Value loss: 16.796818. Entropy: 0.346692.\n",
      "episode: 5482   score: 305.0  epsilon: 1.0    steps: 949  evaluation reward: 288.15\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13012: Policy loss: -2.358742. Value loss: 158.287155. Entropy: 0.282508.\n",
      "Iteration 13013: Policy loss: -2.066226. Value loss: 81.659866. Entropy: 0.254744.\n",
      "Iteration 13014: Policy loss: -2.661701. Value loss: 65.460594. Entropy: 0.268001.\n",
      "episode: 5483   score: 405.0  epsilon: 1.0    steps: 893  evaluation reward: 285.5\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13015: Policy loss: 1.691643. Value loss: 28.220434. Entropy: 0.209254.\n",
      "Iteration 13016: Policy loss: 1.822793. Value loss: 20.809397. Entropy: 0.198163.\n",
      "Iteration 13017: Policy loss: 1.749760. Value loss: 17.258572. Entropy: 0.211082.\n",
      "episode: 5484   score: 270.0  epsilon: 1.0    steps: 21  evaluation reward: 286.95\n",
      "episode: 5485   score: 290.0  epsilon: 1.0    steps: 474  evaluation reward: 288.5\n",
      "episode: 5486   score: 260.0  epsilon: 1.0    steps: 634  evaluation reward: 289.0\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13018: Policy loss: 0.104761. Value loss: 31.039446. Entropy: 0.400237.\n",
      "Iteration 13019: Policy loss: 0.083920. Value loss: 19.500484. Entropy: 0.404418.\n",
      "Iteration 13020: Policy loss: 0.026068. Value loss: 16.938597. Entropy: 0.399687.\n",
      "episode: 5487   score: 895.0  epsilon: 1.0    steps: 333  evaluation reward: 294.8\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13021: Policy loss: -0.956880. Value loss: 25.595367. Entropy: 0.338512.\n",
      "Iteration 13022: Policy loss: -0.972269. Value loss: 17.606817. Entropy: 0.338817.\n",
      "Iteration 13023: Policy loss: -0.968240. Value loss: 14.449363. Entropy: 0.340931.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13024: Policy loss: -0.277182. Value loss: 26.012348. Entropy: 0.283696.\n",
      "Iteration 13025: Policy loss: -0.269936. Value loss: 14.859480. Entropy: 0.285377.\n",
      "Iteration 13026: Policy loss: -0.416796. Value loss: 12.219072. Entropy: 0.287401.\n",
      "episode: 5488   score: 225.0  epsilon: 1.0    steps: 767  evaluation reward: 295.5\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13027: Policy loss: 1.549193. Value loss: 23.075325. Entropy: 0.377303.\n",
      "Iteration 13028: Policy loss: 1.629487. Value loss: 12.064181. Entropy: 0.387904.\n",
      "Iteration 13029: Policy loss: 1.808029. Value loss: 9.850604. Entropy: 0.398966.\n",
      "episode: 5489   score: 215.0  epsilon: 1.0    steps: 977  evaluation reward: 296.35\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13030: Policy loss: 1.775686. Value loss: 21.275196. Entropy: 0.288053.\n",
      "Iteration 13031: Policy loss: 1.992899. Value loss: 14.257503. Entropy: 0.308533.\n",
      "Iteration 13032: Policy loss: 1.872850. Value loss: 8.960289. Entropy: 0.305209.\n",
      "episode: 5490   score: 315.0  epsilon: 1.0    steps: 159  evaluation reward: 297.7\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13033: Policy loss: 2.257934. Value loss: 25.710957. Entropy: 0.251691.\n",
      "Iteration 13034: Policy loss: 2.213506. Value loss: 14.631567. Entropy: 0.245257.\n",
      "Iteration 13035: Policy loss: 2.256381. Value loss: 12.023893. Entropy: 0.251813.\n",
      "episode: 5491   score: 260.0  epsilon: 1.0    steps: 80  evaluation reward: 297.65\n",
      "episode: 5492   score: 265.0  epsilon: 1.0    steps: 587  evaluation reward: 296.85\n",
      "episode: 5493   score: 240.0  epsilon: 1.0    steps: 800  evaluation reward: 297.15\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13036: Policy loss: 1.184612. Value loss: 20.538424. Entropy: 0.350711.\n",
      "Iteration 13037: Policy loss: 1.315917. Value loss: 13.385432. Entropy: 0.338338.\n",
      "Iteration 13038: Policy loss: 1.174762. Value loss: 13.607087. Entropy: 0.346162.\n",
      "episode: 5494   score: 240.0  epsilon: 1.0    steps: 339  evaluation reward: 296.4\n",
      "episode: 5495   score: 270.0  epsilon: 1.0    steps: 385  evaluation reward: 293.2\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13039: Policy loss: -1.351135. Value loss: 19.198061. Entropy: 0.302505.\n",
      "Iteration 13040: Policy loss: -1.449439. Value loss: 12.973479. Entropy: 0.321184.\n",
      "Iteration 13041: Policy loss: -1.192569. Value loss: 9.364346. Entropy: 0.333494.\n",
      "episode: 5496   score: 210.0  epsilon: 1.0    steps: 244  evaluation reward: 289.3\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13042: Policy loss: 0.438851. Value loss: 20.445240. Entropy: 0.374923.\n",
      "Iteration 13043: Policy loss: 0.586016. Value loss: 13.216029. Entropy: 0.365937.\n",
      "Iteration 13044: Policy loss: 0.531840. Value loss: 11.962078. Entropy: 0.358569.\n",
      "episode: 5497   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 290.6\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13045: Policy loss: 0.464330. Value loss: 16.394995. Entropy: 0.405884.\n",
      "Iteration 13046: Policy loss: 0.571629. Value loss: 12.011681. Entropy: 0.434170.\n",
      "Iteration 13047: Policy loss: 0.549928. Value loss: 9.701946. Entropy: 0.421282.\n",
      "episode: 5498   score: 260.0  epsilon: 1.0    steps: 643  evaluation reward: 291.1\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13048: Policy loss: -1.073950. Value loss: 15.358773. Entropy: 0.254080.\n",
      "Iteration 13049: Policy loss: -0.960148. Value loss: 11.569005. Entropy: 0.257636.\n",
      "Iteration 13050: Policy loss: -1.129364. Value loss: 10.828968. Entropy: 0.257517.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13051: Policy loss: 0.092960. Value loss: 22.921370. Entropy: 0.279923.\n",
      "Iteration 13052: Policy loss: 0.109871. Value loss: 16.709248. Entropy: 0.291641.\n",
      "Iteration 13053: Policy loss: 0.173414. Value loss: 12.400108. Entropy: 0.289304.\n",
      "episode: 5499   score: 210.0  epsilon: 1.0    steps: 446  evaluation reward: 289.55\n",
      "episode: 5500   score: 260.0  epsilon: 1.0    steps: 820  evaluation reward: 290.9\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13054: Policy loss: -0.124445. Value loss: 25.533146. Entropy: 0.340337.\n",
      "Iteration 13055: Policy loss: 0.255476. Value loss: 12.418708. Entropy: 0.346415.\n",
      "Iteration 13056: Policy loss: 0.053555. Value loss: 13.803488. Entropy: 0.369409.\n",
      "now time :  2019-02-25 22:43:58.906652\n",
      "episode: 5501   score: 285.0  epsilon: 1.0    steps: 340  evaluation reward: 292.5\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13057: Policy loss: -0.856769. Value loss: 24.621155. Entropy: 0.456839.\n",
      "Iteration 13058: Policy loss: -1.082569. Value loss: 15.677322. Entropy: 0.457131.\n",
      "Iteration 13059: Policy loss: -1.133634. Value loss: 13.669161. Entropy: 0.457994.\n",
      "episode: 5502   score: 365.0  epsilon: 1.0    steps: 37  evaluation reward: 294.9\n",
      "episode: 5503   score: 365.0  epsilon: 1.0    steps: 526  evaluation reward: 297.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13060: Policy loss: -1.134454. Value loss: 20.154362. Entropy: 0.359752.\n",
      "Iteration 13061: Policy loss: -0.985317. Value loss: 10.073955. Entropy: 0.371108.\n",
      "Iteration 13062: Policy loss: -1.026089. Value loss: 10.886744. Entropy: 0.358296.\n",
      "episode: 5504   score: 300.0  epsilon: 1.0    steps: 225  evaluation reward: 297.4\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13063: Policy loss: -0.991894. Value loss: 46.437881. Entropy: 0.524723.\n",
      "Iteration 13064: Policy loss: -1.031848. Value loss: 22.065359. Entropy: 0.515986.\n",
      "Iteration 13065: Policy loss: -1.286238. Value loss: 16.407806. Entropy: 0.518580.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13066: Policy loss: -0.328074. Value loss: 22.891806. Entropy: 0.477525.\n",
      "Iteration 13067: Policy loss: -0.044386. Value loss: 13.697516. Entropy: 0.481299.\n",
      "Iteration 13068: Policy loss: -0.489725. Value loss: 10.174489. Entropy: 0.491083.\n",
      "episode: 5505   score: 180.0  epsilon: 1.0    steps: 892  evaluation reward: 296.3\n",
      "episode: 5506   score: 345.0  epsilon: 1.0    steps: 979  evaluation reward: 298.55\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13069: Policy loss: 1.860758. Value loss: 21.827299. Entropy: 0.263742.\n",
      "Iteration 13070: Policy loss: 2.090793. Value loss: 10.810230. Entropy: 0.246392.\n",
      "Iteration 13071: Policy loss: 2.161681. Value loss: 9.854644. Entropy: 0.270031.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13072: Policy loss: -0.434524. Value loss: 20.118319. Entropy: 0.340309.\n",
      "Iteration 13073: Policy loss: -0.705155. Value loss: 10.325529. Entropy: 0.339213.\n",
      "Iteration 13074: Policy loss: -0.443826. Value loss: 8.097225. Entropy: 0.327034.\n",
      "episode: 5507   score: 260.0  epsilon: 1.0    steps: 367  evaluation reward: 298.55\n",
      "episode: 5508   score: 380.0  epsilon: 1.0    steps: 726  evaluation reward: 298.45\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13075: Policy loss: 1.513945. Value loss: 26.094320. Entropy: 0.330090.\n",
      "Iteration 13076: Policy loss: 1.998139. Value loss: 21.140591. Entropy: 0.304635.\n",
      "Iteration 13077: Policy loss: 1.759355. Value loss: 15.361369. Entropy: 0.301618.\n",
      "episode: 5509   score: 285.0  epsilon: 1.0    steps: 48  evaluation reward: 298.7\n",
      "episode: 5510   score: 260.0  epsilon: 1.0    steps: 603  evaluation reward: 298.9\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13078: Policy loss: -1.134221. Value loss: 25.907471. Entropy: 0.418194.\n",
      "Iteration 13079: Policy loss: -1.219105. Value loss: 15.537987. Entropy: 0.408400.\n",
      "Iteration 13080: Policy loss: -1.221071. Value loss: 12.808302. Entropy: 0.424907.\n",
      "episode: 5511   score: 285.0  epsilon: 1.0    steps: 227  evaluation reward: 300.7\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13081: Policy loss: -3.096514. Value loss: 31.654236. Entropy: 0.344266.\n",
      "Iteration 13082: Policy loss: -2.821149. Value loss: 20.675980. Entropy: 0.337964.\n",
      "Iteration 13083: Policy loss: -3.291528. Value loss: 16.958433. Entropy: 0.315781.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13084: Policy loss: -3.728344. Value loss: 231.215561. Entropy: 0.382614.\n",
      "Iteration 13085: Policy loss: -4.508362. Value loss: 133.605545. Entropy: 0.419178.\n",
      "Iteration 13086: Policy loss: -4.266622. Value loss: 86.282669. Entropy: 0.404542.\n",
      "episode: 5512   score: 320.0  epsilon: 1.0    steps: 929  evaluation reward: 301.25\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13087: Policy loss: 0.217661. Value loss: 19.282564. Entropy: 0.358565.\n",
      "Iteration 13088: Policy loss: 0.144953. Value loss: 14.549995. Entropy: 0.371115.\n",
      "Iteration 13089: Policy loss: 0.311759. Value loss: 9.911901. Entropy: 0.380063.\n",
      "episode: 5513   score: 260.0  epsilon: 1.0    steps: 792  evaluation reward: 301.75\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13090: Policy loss: -3.019223. Value loss: 179.552155. Entropy: 0.336079.\n",
      "Iteration 13091: Policy loss: -2.257433. Value loss: 64.886581. Entropy: 0.354195.\n",
      "Iteration 13092: Policy loss: -2.961246. Value loss: 47.381958. Entropy: 0.338754.\n",
      "episode: 5514   score: 575.0  epsilon: 1.0    steps: 388  evaluation reward: 305.95\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13093: Policy loss: 0.353073. Value loss: 22.961329. Entropy: 0.351632.\n",
      "Iteration 13094: Policy loss: 0.267324. Value loss: 15.461910. Entropy: 0.356029.\n",
      "Iteration 13095: Policy loss: 0.293617. Value loss: 12.965255. Entropy: 0.347666.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13096: Policy loss: -1.342931. Value loss: 31.398941. Entropy: 0.443970.\n",
      "Iteration 13097: Policy loss: -1.395461. Value loss: 16.987070. Entropy: 0.433011.\n",
      "Iteration 13098: Policy loss: -1.537166. Value loss: 12.633594. Entropy: 0.430634.\n",
      "episode: 5515   score: 320.0  epsilon: 1.0    steps: 23  evaluation reward: 307.35\n",
      "episode: 5516   score: 285.0  epsilon: 1.0    steps: 227  evaluation reward: 306.25\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13099: Policy loss: 0.882111. Value loss: 44.679501. Entropy: 0.321664.\n",
      "Iteration 13100: Policy loss: 0.378069. Value loss: 25.580679. Entropy: 0.320991.\n",
      "Iteration 13101: Policy loss: 0.843731. Value loss: 22.232908. Entropy: 0.314125.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13102: Policy loss: 1.138721. Value loss: 29.435350. Entropy: 0.355740.\n",
      "Iteration 13103: Policy loss: 1.249632. Value loss: 13.623219. Entropy: 0.323088.\n",
      "Iteration 13104: Policy loss: 1.124454. Value loss: 9.863243. Entropy: 0.361804.\n",
      "episode: 5517   score: 560.0  epsilon: 1.0    steps: 587  evaluation reward: 310.95\n",
      "episode: 5518   score: 805.0  epsilon: 1.0    steps: 664  evaluation reward: 316.1\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13105: Policy loss: 2.214948. Value loss: 24.874670. Entropy: 0.318795.\n",
      "Iteration 13106: Policy loss: 1.940120. Value loss: 13.759866. Entropy: 0.317022.\n",
      "Iteration 13107: Policy loss: 2.002501. Value loss: 11.273720. Entropy: 0.339846.\n",
      "episode: 5519   score: 290.0  epsilon: 1.0    steps: 869  evaluation reward: 310.8\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13108: Policy loss: -0.306341. Value loss: 22.683765. Entropy: 0.337077.\n",
      "Iteration 13109: Policy loss: -0.607862. Value loss: 12.233145. Entropy: 0.338446.\n",
      "Iteration 13110: Policy loss: -0.518676. Value loss: 9.116037. Entropy: 0.341965.\n",
      "episode: 5520   score: 495.0  epsilon: 1.0    steps: 284  evaluation reward: 313.05\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13111: Policy loss: 4.062327. Value loss: 26.387428. Entropy: 0.330338.\n",
      "Iteration 13112: Policy loss: 3.855972. Value loss: 10.392120. Entropy: 0.356179.\n",
      "Iteration 13113: Policy loss: 3.809568. Value loss: 9.970661. Entropy: 0.346635.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13114: Policy loss: -0.903071. Value loss: 243.829727. Entropy: 0.376649.\n",
      "Iteration 13115: Policy loss: -0.473806. Value loss: 65.090927. Entropy: 0.422923.\n",
      "Iteration 13116: Policy loss: -0.020282. Value loss: 45.611130. Entropy: 0.416192.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13117: Policy loss: 0.253182. Value loss: 20.572086. Entropy: 0.468741.\n",
      "Iteration 13118: Policy loss: 0.150673. Value loss: 15.424600. Entropy: 0.498655.\n",
      "Iteration 13119: Policy loss: 0.195169. Value loss: 10.496215. Entropy: 0.485605.\n",
      "episode: 5521   score: 265.0  epsilon: 1.0    steps: 32  evaluation reward: 309.0\n",
      "episode: 5522   score: 285.0  epsilon: 1.0    steps: 168  evaluation reward: 307.45\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13120: Policy loss: 2.280237. Value loss: 27.496634. Entropy: 0.375003.\n",
      "Iteration 13121: Policy loss: 2.205833. Value loss: 16.685503. Entropy: 0.398699.\n",
      "Iteration 13122: Policy loss: 1.997285. Value loss: 14.218182. Entropy: 0.381698.\n",
      "episode: 5523   score: 345.0  epsilon: 1.0    steps: 405  evaluation reward: 308.8\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13123: Policy loss: -0.305507. Value loss: 21.492811. Entropy: 0.435446.\n",
      "Iteration 13124: Policy loss: -0.301641. Value loss: 13.793818. Entropy: 0.443494.\n",
      "Iteration 13125: Policy loss: -0.483252. Value loss: 11.398423. Entropy: 0.436903.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13126: Policy loss: 1.793496. Value loss: 32.604145. Entropy: 0.581106.\n",
      "Iteration 13127: Policy loss: 1.760361. Value loss: 17.912624. Entropy: 0.590912.\n",
      "Iteration 13128: Policy loss: 1.844746. Value loss: 14.074750. Entropy: 0.584680.\n",
      "episode: 5524   score: 530.0  epsilon: 1.0    steps: 606  evaluation reward: 307.6\n",
      "episode: 5525   score: 365.0  epsilon: 1.0    steps: 987  evaluation reward: 309.45\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13129: Policy loss: 1.828465. Value loss: 49.864052. Entropy: 0.534540.\n",
      "Iteration 13130: Policy loss: 2.047574. Value loss: 18.267302. Entropy: 0.557647.\n",
      "Iteration 13131: Policy loss: 2.246702. Value loss: 11.336874. Entropy: 0.560151.\n",
      "episode: 5526   score: 370.0  epsilon: 1.0    steps: 760  evaluation reward: 310.2\n",
      "episode: 5527   score: 280.0  epsilon: 1.0    steps: 865  evaluation reward: 311.45\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13132: Policy loss: 1.627757. Value loss: 32.862206. Entropy: 0.540405.\n",
      "Iteration 13133: Policy loss: 1.408447. Value loss: 19.434662. Entropy: 0.518278.\n",
      "Iteration 13134: Policy loss: 1.553806. Value loss: 14.602451. Entropy: 0.522597.\n",
      "episode: 5528   score: 310.0  epsilon: 1.0    steps: 350  evaluation reward: 311.9\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13135: Policy loss: 0.452278. Value loss: 18.864368. Entropy: 0.528944.\n",
      "Iteration 13136: Policy loss: 0.429074. Value loss: 13.695663. Entropy: 0.522814.\n",
      "Iteration 13137: Policy loss: 0.632659. Value loss: 11.435616. Entropy: 0.512598.\n",
      "episode: 5529   score: 260.0  epsilon: 1.0    steps: 60  evaluation reward: 312.7\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13138: Policy loss: 1.459194. Value loss: 38.740776. Entropy: 0.573783.\n",
      "Iteration 13139: Policy loss: 1.743410. Value loss: 19.065102. Entropy: 0.578716.\n",
      "Iteration 13140: Policy loss: 1.578710. Value loss: 15.211231. Entropy: 0.583160.\n",
      "episode: 5530   score: 260.0  epsilon: 1.0    steps: 434  evaluation reward: 313.2\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13141: Policy loss: -0.421983. Value loss: 17.255184. Entropy: 0.442001.\n",
      "Iteration 13142: Policy loss: -0.370816. Value loss: 12.207002. Entropy: 0.446461.\n",
      "Iteration 13143: Policy loss: -0.451635. Value loss: 11.538885. Entropy: 0.431550.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13144: Policy loss: 0.813011. Value loss: 15.113148. Entropy: 0.416447.\n",
      "Iteration 13145: Policy loss: 0.501502. Value loss: 7.602073. Entropy: 0.410451.\n",
      "Iteration 13146: Policy loss: 0.662213. Value loss: 7.849388. Entropy: 0.419030.\n",
      "episode: 5531   score: 110.0  epsilon: 1.0    steps: 369  evaluation reward: 312.2\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13147: Policy loss: 3.808308. Value loss: 25.393530. Entropy: 0.562829.\n",
      "Iteration 13148: Policy loss: 3.780921. Value loss: 13.145548. Entropy: 0.545627.\n",
      "Iteration 13149: Policy loss: 3.820904. Value loss: 8.681381. Entropy: 0.569195.\n",
      "episode: 5532   score: 375.0  epsilon: 1.0    steps: 153  evaluation reward: 310.2\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13150: Policy loss: 2.100970. Value loss: 17.572203. Entropy: 0.485261.\n",
      "Iteration 13151: Policy loss: 2.083737. Value loss: 9.542464. Entropy: 0.486768.\n",
      "Iteration 13152: Policy loss: 2.243238. Value loss: 6.142016. Entropy: 0.527736.\n",
      "episode: 5533   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 309.55\n",
      "episode: 5534   score: 215.0  epsilon: 1.0    steps: 792  evaluation reward: 309.6\n",
      "episode: 5535   score: 260.0  epsilon: 1.0    steps: 920  evaluation reward: 310.4\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13153: Policy loss: 0.587133. Value loss: 16.158148. Entropy: 0.580377.\n",
      "Iteration 13154: Policy loss: 0.444998. Value loss: 9.798429. Entropy: 0.582614.\n",
      "Iteration 13155: Policy loss: 0.517427. Value loss: 8.120457. Entropy: 0.585370.\n",
      "episode: 5536   score: 210.0  epsilon: 1.0    steps: 459  evaluation reward: 309.85\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13156: Policy loss: -1.114840. Value loss: 14.885235. Entropy: 0.612623.\n",
      "Iteration 13157: Policy loss: -1.239837. Value loss: 9.900263. Entropy: 0.609144.\n",
      "Iteration 13158: Policy loss: -1.045856. Value loss: 8.220141. Entropy: 0.591816.\n",
      "episode: 5537   score: 345.0  epsilon: 1.0    steps: 630  evaluation reward: 312.2\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13159: Policy loss: 1.212038. Value loss: 15.690050. Entropy: 0.561834.\n",
      "Iteration 13160: Policy loss: 1.558408. Value loss: 8.622847. Entropy: 0.584985.\n",
      "Iteration 13161: Policy loss: 1.391248. Value loss: 7.833573. Entropy: 0.593835.\n",
      "episode: 5538   score: 260.0  epsilon: 1.0    steps: 23  evaluation reward: 312.2\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13162: Policy loss: 2.073266. Value loss: 22.692692. Entropy: 0.638841.\n",
      "Iteration 13163: Policy loss: 1.929407. Value loss: 11.703535. Entropy: 0.621232.\n",
      "Iteration 13164: Policy loss: 1.696305. Value loss: 11.721466. Entropy: 0.634749.\n",
      "episode: 5539   score: 210.0  epsilon: 1.0    steps: 207  evaluation reward: 311.7\n",
      "episode: 5540   score: 210.0  epsilon: 1.0    steps: 268  evaluation reward: 309.2\n",
      "episode: 5541   score: 90.0  epsilon: 1.0    steps: 966  evaluation reward: 303.6\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13165: Policy loss: 2.659887. Value loss: 25.074554. Entropy: 0.621347.\n",
      "Iteration 13166: Policy loss: 2.672799. Value loss: 11.754359. Entropy: 0.652593.\n",
      "Iteration 13167: Policy loss: 2.578813. Value loss: 9.039078. Entropy: 0.673784.\n",
      "episode: 5542   score: 210.0  epsilon: 1.0    steps: 833  evaluation reward: 304.15\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13168: Policy loss: 1.019266. Value loss: 20.695648. Entropy: 0.774131.\n",
      "Iteration 13169: Policy loss: 1.003920. Value loss: 11.794835. Entropy: 0.781893.\n",
      "Iteration 13170: Policy loss: 1.074202. Value loss: 10.149006. Entropy: 0.779721.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13171: Policy loss: -0.180619. Value loss: 18.084089. Entropy: 0.640680.\n",
      "Iteration 13172: Policy loss: -0.292861. Value loss: 11.006417. Entropy: 0.650944.\n",
      "Iteration 13173: Policy loss: -0.137422. Value loss: 8.130813. Entropy: 0.661624.\n",
      "episode: 5543   score: 210.0  epsilon: 1.0    steps: 669  evaluation reward: 304.45\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13174: Policy loss: -1.268784. Value loss: 18.823807. Entropy: 0.552617.\n",
      "Iteration 13175: Policy loss: -0.908084. Value loss: 9.027017. Entropy: 0.548613.\n",
      "Iteration 13176: Policy loss: -1.208471. Value loss: 9.009407. Entropy: 0.551631.\n",
      "episode: 5544   score: 210.0  epsilon: 1.0    steps: 34  evaluation reward: 304.45\n",
      "episode: 5545   score: 155.0  epsilon: 1.0    steps: 534  evaluation reward: 303.9\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13177: Policy loss: -0.157542. Value loss: 22.715017. Entropy: 0.535945.\n",
      "Iteration 13178: Policy loss: -0.107068. Value loss: 9.415782. Entropy: 0.513963.\n",
      "Iteration 13179: Policy loss: -0.197505. Value loss: 8.714861. Entropy: 0.532407.\n",
      "episode: 5546   score: 260.0  epsilon: 1.0    steps: 415  evaluation reward: 303.6\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13180: Policy loss: -2.616278. Value loss: 376.342102. Entropy: 0.702226.\n",
      "Iteration 13181: Policy loss: -1.410486. Value loss: 127.824234. Entropy: 0.625671.\n",
      "Iteration 13182: Policy loss: -2.465149. Value loss: 181.319046. Entropy: 0.623689.\n",
      "episode: 5547   score: 210.0  epsilon: 1.0    steps: 357  evaluation reward: 304.15\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13183: Policy loss: -0.583135. Value loss: 24.079565. Entropy: 0.658819.\n",
      "Iteration 13184: Policy loss: -0.656914. Value loss: 14.626152. Entropy: 0.649682.\n",
      "Iteration 13185: Policy loss: -0.670633. Value loss: 10.751678. Entropy: 0.677122.\n",
      "episode: 5548   score: 215.0  epsilon: 1.0    steps: 184  evaluation reward: 304.75\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13186: Policy loss: 0.326402. Value loss: 17.962326. Entropy: 0.569497.\n",
      "Iteration 13187: Policy loss: 0.218088. Value loss: 9.229503. Entropy: 0.568056.\n",
      "Iteration 13188: Policy loss: 0.221165. Value loss: 6.495840. Entropy: 0.554829.\n",
      "episode: 5549   score: 485.0  epsilon: 1.0    steps: 935  evaluation reward: 306.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13189: Policy loss: -1.179573. Value loss: 34.142868. Entropy: 0.589588.\n",
      "Iteration 13190: Policy loss: -1.261462. Value loss: 16.960274. Entropy: 0.601792.\n",
      "Iteration 13191: Policy loss: -1.294684. Value loss: 13.172438. Entropy: 0.611415.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13192: Policy loss: -0.080673. Value loss: 22.873158. Entropy: 0.665626.\n",
      "Iteration 13193: Policy loss: 0.192230. Value loss: 12.412015. Entropy: 0.667335.\n",
      "Iteration 13194: Policy loss: -0.125889. Value loss: 9.535265. Entropy: 0.681196.\n",
      "episode: 5550   score: 290.0  epsilon: 1.0    steps: 835  evaluation reward: 307.15\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13195: Policy loss: 2.600715. Value loss: 22.546856. Entropy: 0.701268.\n",
      "Iteration 13196: Policy loss: 2.556466. Value loss: 9.969723. Entropy: 0.692602.\n",
      "Iteration 13197: Policy loss: 2.392936. Value loss: 8.458195. Entropy: 0.688309.\n",
      "now time :  2019-02-25 22:46:36.401414\n",
      "episode: 5551   score: 210.0  epsilon: 1.0    steps: 33  evaluation reward: 306.85\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13198: Policy loss: 1.730471. Value loss: 22.458487. Entropy: 0.750109.\n",
      "Iteration 13199: Policy loss: 1.699189. Value loss: 11.407062. Entropy: 0.778448.\n",
      "Iteration 13200: Policy loss: 1.717786. Value loss: 9.096682. Entropy: 0.763780.\n",
      "episode: 5552   score: 285.0  epsilon: 1.0    steps: 572  evaluation reward: 308.15\n",
      "episode: 5553   score: 120.0  epsilon: 1.0    steps: 981  evaluation reward: 307.7\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13201: Policy loss: 0.484428. Value loss: 22.548298. Entropy: 0.783020.\n",
      "Iteration 13202: Policy loss: 0.453774. Value loss: 13.272052. Entropy: 0.787528.\n",
      "Iteration 13203: Policy loss: 0.295192. Value loss: 11.883654. Entropy: 0.796201.\n",
      "episode: 5554   score: 280.0  epsilon: 1.0    steps: 392  evaluation reward: 308.4\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13204: Policy loss: -0.468107. Value loss: 21.241129. Entropy: 0.644620.\n",
      "Iteration 13205: Policy loss: -0.618138. Value loss: 10.214251. Entropy: 0.639602.\n",
      "Iteration 13206: Policy loss: -0.631929. Value loss: 8.197492. Entropy: 0.620727.\n",
      "episode: 5555   score: 210.0  epsilon: 1.0    steps: 139  evaluation reward: 308.4\n",
      "episode: 5556   score: 260.0  epsilon: 1.0    steps: 293  evaluation reward: 308.9\n",
      "episode: 5557   score: 310.0  epsilon: 1.0    steps: 665  evaluation reward: 309.1\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13207: Policy loss: 0.592922. Value loss: 20.621906. Entropy: 0.648265.\n",
      "Iteration 13208: Policy loss: 0.430266. Value loss: 11.632026. Entropy: 0.631221.\n",
      "Iteration 13209: Policy loss: 0.397830. Value loss: 10.130223. Entropy: 0.649252.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13210: Policy loss: 0.152689. Value loss: 16.691666. Entropy: 0.725585.\n",
      "Iteration 13211: Policy loss: -0.145682. Value loss: 11.431460. Entropy: 0.729077.\n",
      "Iteration 13212: Policy loss: 0.312739. Value loss: 9.710634. Entropy: 0.717272.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13213: Policy loss: -0.303527. Value loss: 16.632702. Entropy: 0.635190.\n",
      "Iteration 13214: Policy loss: -0.330895. Value loss: 9.581851. Entropy: 0.638863.\n",
      "Iteration 13215: Policy loss: -0.329159. Value loss: 7.943796. Entropy: 0.638614.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13216: Policy loss: 0.763804. Value loss: 16.777979. Entropy: 0.608460.\n",
      "Iteration 13217: Policy loss: 0.856178. Value loss: 9.939988. Entropy: 0.589387.\n",
      "Iteration 13218: Policy loss: 0.752958. Value loss: 7.707286. Entropy: 0.599517.\n",
      "episode: 5558   score: 210.0  epsilon: 1.0    steps: 17  evaluation reward: 309.4\n",
      "episode: 5559   score: 260.0  epsilon: 1.0    steps: 782  evaluation reward: 309.25\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13219: Policy loss: -0.027986. Value loss: 15.030004. Entropy: 0.646055.\n",
      "Iteration 13220: Policy loss: -0.082951. Value loss: 9.064537. Entropy: 0.648258.\n",
      "Iteration 13221: Policy loss: 0.224194. Value loss: 6.444503. Entropy: 0.640793.\n",
      "episode: 5560   score: 210.0  epsilon: 1.0    steps: 534  evaluation reward: 308.0\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13222: Policy loss: -1.857904. Value loss: 17.686335. Entropy: 0.582036.\n",
      "Iteration 13223: Policy loss: -1.596530. Value loss: 14.637650. Entropy: 0.608345.\n",
      "Iteration 13224: Policy loss: -1.942273. Value loss: 10.266963. Entropy: 0.574989.\n",
      "episode: 5561   score: 180.0  epsilon: 1.0    steps: 234  evaluation reward: 306.95\n",
      "episode: 5562   score: 290.0  epsilon: 1.0    steps: 502  evaluation reward: 305.05\n",
      "episode: 5563   score: 265.0  epsilon: 1.0    steps: 944  evaluation reward: 305.1\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13225: Policy loss: 1.053487. Value loss: 19.611811. Entropy: 0.722644.\n",
      "Iteration 13226: Policy loss: 1.168786. Value loss: 10.882390. Entropy: 0.720651.\n",
      "Iteration 13227: Policy loss: 1.193993. Value loss: 10.455556. Entropy: 0.727377.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13228: Policy loss: 0.067725. Value loss: 25.624664. Entropy: 0.694568.\n",
      "Iteration 13229: Policy loss: 0.004420. Value loss: 11.146658. Entropy: 0.690719.\n",
      "Iteration 13230: Policy loss: 0.005341. Value loss: 8.165579. Entropy: 0.687724.\n",
      "episode: 5564   score: 355.0  epsilon: 1.0    steps: 763  evaluation reward: 304.8\n",
      "episode: 5565   score: 80.0  epsilon: 1.0    steps: 841  evaluation reward: 302.05\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13231: Policy loss: 1.252146. Value loss: 26.506485. Entropy: 0.669922.\n",
      "Iteration 13232: Policy loss: 1.463374. Value loss: 14.384786. Entropy: 0.654772.\n",
      "Iteration 13233: Policy loss: 1.208927. Value loss: 12.950318. Entropy: 0.665086.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13234: Policy loss: -0.339000. Value loss: 11.372245. Entropy: 0.590828.\n",
      "Iteration 13235: Policy loss: -0.365848. Value loss: 6.509101. Entropy: 0.598476.\n",
      "Iteration 13236: Policy loss: -0.481171. Value loss: 6.698523. Entropy: 0.594097.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13237: Policy loss: -1.004148. Value loss: 19.210299. Entropy: 0.684260.\n",
      "Iteration 13238: Policy loss: -0.948642. Value loss: 12.995916. Entropy: 0.717411.\n",
      "Iteration 13239: Policy loss: -1.080412. Value loss: 10.842204. Entropy: 0.675537.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13240: Policy loss: -1.428415. Value loss: 17.307728. Entropy: 0.567172.\n",
      "Iteration 13241: Policy loss: -1.278297. Value loss: 7.590384. Entropy: 0.556361.\n",
      "Iteration 13242: Policy loss: -1.486878. Value loss: 6.865501. Entropy: 0.545821.\n",
      "episode: 5566   score: 275.0  epsilon: 1.0    steps: 90  evaluation reward: 302.7\n",
      "episode: 5567   score: 385.0  epsilon: 1.0    steps: 314  evaluation reward: 302.75\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13243: Policy loss: 3.071472. Value loss: 26.920704. Entropy: 0.631583.\n",
      "Iteration 13244: Policy loss: 3.293366. Value loss: 12.545734. Entropy: 0.629982.\n",
      "Iteration 13245: Policy loss: 3.169600. Value loss: 9.316978. Entropy: 0.644870.\n",
      "episode: 5568   score: 180.0  epsilon: 1.0    steps: 164  evaluation reward: 301.95\n",
      "episode: 5569   score: 260.0  epsilon: 1.0    steps: 975  evaluation reward: 301.85\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13246: Policy loss: -1.368946. Value loss: 225.486313. Entropy: 0.734291.\n",
      "Iteration 13247: Policy loss: -1.200606. Value loss: 83.437965. Entropy: 0.744313.\n",
      "Iteration 13248: Policy loss: -0.969285. Value loss: 45.385109. Entropy: 0.731089.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13249: Policy loss: 0.267945. Value loss: 19.793837. Entropy: 0.884835.\n",
      "Iteration 13250: Policy loss: 0.252357. Value loss: 14.657295. Entropy: 0.871633.\n",
      "Iteration 13251: Policy loss: 0.167946. Value loss: 10.932292. Entropy: 0.872854.\n",
      "episode: 5570   score: 290.0  epsilon: 1.0    steps: 415  evaluation reward: 302.0\n",
      "episode: 5571   score: 180.0  epsilon: 1.0    steps: 786  evaluation reward: 299.9\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13252: Policy loss: -0.117466. Value loss: 12.421880. Entropy: 0.653491.\n",
      "Iteration 13253: Policy loss: -0.219551. Value loss: 7.343239. Entropy: 0.667722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13254: Policy loss: 0.109560. Value loss: 4.953964. Entropy: 0.639189.\n",
      "episode: 5572   score: 110.0  epsilon: 1.0    steps: 199  evaluation reward: 294.35\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13255: Policy loss: 0.142587. Value loss: 19.870640. Entropy: 0.656437.\n",
      "Iteration 13256: Policy loss: 0.014786. Value loss: 11.298643. Entropy: 0.627977.\n",
      "Iteration 13257: Policy loss: 0.116237. Value loss: 9.317983. Entropy: 0.636489.\n",
      "episode: 5573   score: 335.0  epsilon: 1.0    steps: 539  evaluation reward: 295.6\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13258: Policy loss: -0.450959. Value loss: 20.640503. Entropy: 0.458903.\n",
      "Iteration 13259: Policy loss: -0.627366. Value loss: 10.489180. Entropy: 0.437303.\n",
      "Iteration 13260: Policy loss: -0.536341. Value loss: 6.789019. Entropy: 0.456550.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13261: Policy loss: -0.157082. Value loss: 18.499603. Entropy: 0.539184.\n",
      "Iteration 13262: Policy loss: -0.056421. Value loss: 9.304597. Entropy: 0.548542.\n",
      "Iteration 13263: Policy loss: -0.149256. Value loss: 6.208056. Entropy: 0.537914.\n",
      "episode: 5574   score: 180.0  epsilon: 1.0    steps: 60  evaluation reward: 290.45\n",
      "episode: 5575   score: 180.0  epsilon: 1.0    steps: 301  evaluation reward: 289.75\n",
      "episode: 5576   score: 545.0  epsilon: 1.0    steps: 724  evaluation reward: 292.0\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13264: Policy loss: 1.229827. Value loss: 24.668114. Entropy: 0.557254.\n",
      "Iteration 13265: Policy loss: 1.257043. Value loss: 13.541088. Entropy: 0.564983.\n",
      "Iteration 13266: Policy loss: 1.534036. Value loss: 8.397954. Entropy: 0.598664.\n",
      "episode: 5577   score: 210.0  epsilon: 1.0    steps: 453  evaluation reward: 286.1\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13267: Policy loss: 0.699842. Value loss: 16.027794. Entropy: 0.445838.\n",
      "Iteration 13268: Policy loss: 0.467984. Value loss: 7.412289. Entropy: 0.454986.\n",
      "Iteration 13269: Policy loss: 0.701209. Value loss: 6.092754. Entropy: 0.470846.\n",
      "episode: 5578   score: 210.0  epsilon: 1.0    steps: 926  evaluation reward: 285.6\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13270: Policy loss: 1.309945. Value loss: 16.304068. Entropy: 0.775111.\n",
      "Iteration 13271: Policy loss: 1.302773. Value loss: 11.378460. Entropy: 0.760192.\n",
      "Iteration 13272: Policy loss: 1.420427. Value loss: 8.236633. Entropy: 0.781125.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13273: Policy loss: 1.039005. Value loss: 19.006893. Entropy: 0.581717.\n",
      "Iteration 13274: Policy loss: 0.878911. Value loss: 11.141466. Entropy: 0.575544.\n",
      "Iteration 13275: Policy loss: 1.004707. Value loss: 7.801871. Entropy: 0.562614.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13276: Policy loss: -1.298564. Value loss: 39.970726. Entropy: 0.452250.\n",
      "Iteration 13277: Policy loss: -1.382092. Value loss: 19.948946. Entropy: 0.454871.\n",
      "Iteration 13278: Policy loss: -1.192381. Value loss: 16.058289. Entropy: 0.447409.\n",
      "episode: 5579   score: 210.0  epsilon: 1.0    steps: 101  evaluation reward: 284.6\n",
      "episode: 5580   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 283.5\n",
      "episode: 5581   score: 210.0  epsilon: 1.0    steps: 754  evaluation reward: 283.0\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13279: Policy loss: 0.066616. Value loss: 16.956322. Entropy: 0.553372.\n",
      "Iteration 13280: Policy loss: 0.464941. Value loss: 8.021168. Entropy: 0.529079.\n",
      "Iteration 13281: Policy loss: 0.015270. Value loss: 5.454750. Entropy: 0.556876.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13282: Policy loss: -0.629783. Value loss: 25.808596. Entropy: 0.659748.\n",
      "Iteration 13283: Policy loss: -0.411925. Value loss: 13.272283. Entropy: 0.688464.\n",
      "Iteration 13284: Policy loss: -0.658170. Value loss: 9.400517. Entropy: 0.662999.\n",
      "episode: 5582   score: 210.0  epsilon: 1.0    steps: 911  evaluation reward: 282.05\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13285: Policy loss: 1.035093. Value loss: 20.269152. Entropy: 0.888768.\n",
      "Iteration 13286: Policy loss: 0.583245. Value loss: 10.920217. Entropy: 0.911193.\n",
      "Iteration 13287: Policy loss: 0.864968. Value loss: 8.768035. Entropy: 0.906906.\n",
      "episode: 5583   score: 240.0  epsilon: 1.0    steps: 275  evaluation reward: 280.4\n",
      "episode: 5584   score: 330.0  epsilon: 1.0    steps: 859  evaluation reward: 281.0\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13288: Policy loss: 0.213696. Value loss: 12.527233. Entropy: 0.754014.\n",
      "Iteration 13289: Policy loss: 0.200807. Value loss: 8.179020. Entropy: 0.760058.\n",
      "Iteration 13290: Policy loss: 0.334410. Value loss: 6.973953. Entropy: 0.760000.\n",
      "episode: 5585   score: 330.0  epsilon: 1.0    steps: 132  evaluation reward: 281.4\n",
      "episode: 5586   score: 290.0  epsilon: 1.0    steps: 394  evaluation reward: 281.7\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13291: Policy loss: 0.381526. Value loss: 22.224003. Entropy: 0.787279.\n",
      "Iteration 13292: Policy loss: 0.312638. Value loss: 15.060736. Entropy: 0.765812.\n",
      "Iteration 13293: Policy loss: 0.480632. Value loss: 13.986912. Entropy: 0.779855.\n",
      "episode: 5587   score: 185.0  epsilon: 1.0    steps: 623  evaluation reward: 274.6\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13294: Policy loss: -0.090193. Value loss: 19.006571. Entropy: 0.683575.\n",
      "Iteration 13295: Policy loss: -0.035767. Value loss: 11.914206. Entropy: 0.678370.\n",
      "Iteration 13296: Policy loss: 0.132611. Value loss: 10.305105. Entropy: 0.658748.\n",
      "episode: 5588   score: 210.0  epsilon: 1.0    steps: 654  evaluation reward: 274.45\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13297: Policy loss: 0.333319. Value loss: 10.673254. Entropy: 0.664682.\n",
      "Iteration 13298: Policy loss: 0.213283. Value loss: 7.301760. Entropy: 0.667439.\n",
      "Iteration 13299: Policy loss: 0.314182. Value loss: 5.430340. Entropy: 0.669497.\n",
      "episode: 5589   score: 110.0  epsilon: 1.0    steps: 164  evaluation reward: 273.4\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13300: Policy loss: 4.012642. Value loss: 30.081161. Entropy: 0.658602.\n",
      "Iteration 13301: Policy loss: 3.682006. Value loss: 15.443414. Entropy: 0.672510.\n",
      "Iteration 13302: Policy loss: 3.834842. Value loss: 16.527885. Entropy: 0.678953.\n",
      "episode: 5590   score: 270.0  epsilon: 1.0    steps: 128  evaluation reward: 272.95\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13303: Policy loss: -0.989302. Value loss: 20.800503. Entropy: 0.820873.\n",
      "Iteration 13304: Policy loss: -0.975939. Value loss: 11.103643. Entropy: 0.837251.\n",
      "Iteration 13305: Policy loss: -1.074323. Value loss: 7.958175. Entropy: 0.833968.\n",
      "episode: 5591   score: 155.0  epsilon: 1.0    steps: 439  evaluation reward: 271.9\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13306: Policy loss: 0.199958. Value loss: 14.483096. Entropy: 0.648404.\n",
      "Iteration 13307: Policy loss: 0.198966. Value loss: 8.043953. Entropy: 0.646367.\n",
      "Iteration 13308: Policy loss: 0.223163. Value loss: 7.957793. Entropy: 0.650986.\n",
      "episode: 5592   score: 155.0  epsilon: 1.0    steps: 586  evaluation reward: 270.8\n",
      "episode: 5593   score: 275.0  epsilon: 1.0    steps: 957  evaluation reward: 271.15\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13309: Policy loss: 0.999517. Value loss: 19.996473. Entropy: 0.572700.\n",
      "Iteration 13310: Policy loss: 1.251287. Value loss: 14.256031. Entropy: 0.610567.\n",
      "Iteration 13311: Policy loss: 0.822294. Value loss: 8.174447. Entropy: 0.607830.\n",
      "episode: 5594   score: 210.0  epsilon: 1.0    steps: 663  evaluation reward: 270.85\n",
      "episode: 5595   score: 240.0  epsilon: 1.0    steps: 776  evaluation reward: 270.55\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13312: Policy loss: -0.400358. Value loss: 12.828752. Entropy: 0.597818.\n",
      "Iteration 13313: Policy loss: -0.344006. Value loss: 8.858683. Entropy: 0.629568.\n",
      "Iteration 13314: Policy loss: -0.330801. Value loss: 7.082510. Entropy: 0.623999.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13315: Policy loss: -0.936424. Value loss: 17.635300. Entropy: 0.615956.\n",
      "Iteration 13316: Policy loss: -1.067273. Value loss: 9.593900. Entropy: 0.678674.\n",
      "Iteration 13317: Policy loss: -0.972213. Value loss: 6.748945. Entropy: 0.660328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5596   score: 110.0  epsilon: 1.0    steps: 447  evaluation reward: 269.55\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13318: Policy loss: 1.118154. Value loss: 11.496012. Entropy: 0.519577.\n",
      "Iteration 13319: Policy loss: 1.083111. Value loss: 7.397561. Entropy: 0.477266.\n",
      "Iteration 13320: Policy loss: 1.075691. Value loss: 5.026290. Entropy: 0.481375.\n",
      "episode: 5597   score: 320.0  epsilon: 1.0    steps: 285  evaluation reward: 270.65\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13321: Policy loss: -0.468022. Value loss: 22.765053. Entropy: 0.492575.\n",
      "Iteration 13322: Policy loss: -0.610000. Value loss: 12.723584. Entropy: 0.501247.\n",
      "Iteration 13323: Policy loss: -0.558711. Value loss: 10.711246. Entropy: 0.499165.\n",
      "episode: 5598   score: 210.0  epsilon: 1.0    steps: 996  evaluation reward: 270.15\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13324: Policy loss: 0.223133. Value loss: 17.592484. Entropy: 0.618173.\n",
      "Iteration 13325: Policy loss: 0.266084. Value loss: 10.605491. Entropy: 0.630455.\n",
      "Iteration 13326: Policy loss: 0.175535. Value loss: 8.959109. Entropy: 0.589913.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13327: Policy loss: 0.403139. Value loss: 15.473023. Entropy: 0.708247.\n",
      "Iteration 13328: Policy loss: 0.384449. Value loss: 7.931068. Entropy: 0.682371.\n",
      "Iteration 13329: Policy loss: 0.272743. Value loss: 7.052421. Entropy: 0.700829.\n",
      "episode: 5599   score: 320.0  epsilon: 1.0    steps: 249  evaluation reward: 271.25\n",
      "episode: 5600   score: 260.0  epsilon: 1.0    steps: 660  evaluation reward: 271.25\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13330: Policy loss: 0.227701. Value loss: 17.849901. Entropy: 0.764461.\n",
      "Iteration 13331: Policy loss: 0.132558. Value loss: 12.254725. Entropy: 0.754032.\n",
      "Iteration 13332: Policy loss: 0.181184. Value loss: 9.304989. Entropy: 0.738713.\n",
      "now time :  2019-02-25 22:49:09.543922\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5601   score: 260.0  epsilon: 1.0    steps: 533  evaluation reward: 271.0\n",
      "episode: 5602   score: 275.0  epsilon: 1.0    steps: 871  evaluation reward: 270.1\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13333: Policy loss: 2.120739. Value loss: 58.915024. Entropy: 0.306053.\n",
      "Iteration 13334: Policy loss: 1.846045. Value loss: 21.634325. Entropy: 0.351355.\n",
      "Iteration 13335: Policy loss: 1.640298. Value loss: 16.213705. Entropy: 0.338194.\n",
      "episode: 5603   score: 180.0  epsilon: 1.0    steps: 270  evaluation reward: 268.25\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13336: Policy loss: 1.174452. Value loss: 28.537264. Entropy: 0.406507.\n",
      "Iteration 13337: Policy loss: 1.031459. Value loss: 18.893564. Entropy: 0.390866.\n",
      "Iteration 13338: Policy loss: 1.348764. Value loss: 16.148241. Entropy: 0.395258.\n",
      "episode: 5604   score: 210.0  epsilon: 1.0    steps: 411  evaluation reward: 267.35\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13339: Policy loss: -0.391174. Value loss: 20.693670. Entropy: 0.299235.\n",
      "Iteration 13340: Policy loss: -0.661912. Value loss: 13.587190. Entropy: 0.298754.\n",
      "Iteration 13341: Policy loss: -0.340344. Value loss: 10.048800. Entropy: 0.328830.\n",
      "episode: 5605   score: 400.0  epsilon: 1.0    steps: 93  evaluation reward: 269.55\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13342: Policy loss: -0.686034. Value loss: 31.654930. Entropy: 0.169455.\n",
      "Iteration 13343: Policy loss: -0.497728. Value loss: 21.112774. Entropy: 0.174578.\n",
      "Iteration 13344: Policy loss: -1.051060. Value loss: 17.089340. Entropy: 0.162703.\n",
      "episode: 5606   score: 265.0  epsilon: 1.0    steps: 731  evaluation reward: 268.75\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13345: Policy loss: -0.540840. Value loss: 17.658514. Entropy: 0.313323.\n",
      "Iteration 13346: Policy loss: -0.278882. Value loss: 9.714682. Entropy: 0.306139.\n",
      "Iteration 13347: Policy loss: -0.333153. Value loss: 7.002206. Entropy: 0.307066.\n",
      "episode: 5607   score: 210.0  epsilon: 1.0    steps: 565  evaluation reward: 268.25\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13348: Policy loss: -2.123704. Value loss: 104.623276. Entropy: 0.195481.\n",
      "Iteration 13349: Policy loss: -2.356798. Value loss: 95.298309. Entropy: 0.193208.\n",
      "Iteration 13350: Policy loss: -2.204177. Value loss: 55.649723. Entropy: 0.183184.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13351: Policy loss: 1.796268. Value loss: 60.109531. Entropy: 0.127717.\n",
      "Iteration 13352: Policy loss: 1.829357. Value loss: 29.101982. Entropy: 0.127989.\n",
      "Iteration 13353: Policy loss: 1.757039. Value loss: 20.267826. Entropy: 0.135920.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13354: Policy loss: 0.576635. Value loss: 47.840542. Entropy: 0.238256.\n",
      "Iteration 13355: Policy loss: 0.481327. Value loss: 24.278269. Entropy: 0.251634.\n",
      "Iteration 13356: Policy loss: 0.346864. Value loss: 15.236327. Entropy: 0.255528.\n",
      "episode: 5608   score: 265.0  epsilon: 1.0    steps: 469  evaluation reward: 267.1\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13357: Policy loss: 1.620152. Value loss: 26.410801. Entropy: 0.283736.\n",
      "Iteration 13358: Policy loss: 1.927765. Value loss: 13.288294. Entropy: 0.314772.\n",
      "Iteration 13359: Policy loss: 1.878489. Value loss: 11.727633. Entropy: 0.309292.\n",
      "episode: 5609   score: 345.0  epsilon: 1.0    steps: 826  evaluation reward: 267.7\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13360: Policy loss: -0.537330. Value loss: 22.927265. Entropy: 0.210855.\n",
      "Iteration 13361: Policy loss: -0.549389. Value loss: 12.702503. Entropy: 0.222043.\n",
      "Iteration 13362: Policy loss: -0.393153. Value loss: 9.541163. Entropy: 0.211132.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13363: Policy loss: -3.606066. Value loss: 202.191971. Entropy: 0.197526.\n",
      "Iteration 13364: Policy loss: -3.063186. Value loss: 79.677872. Entropy: 0.194275.\n",
      "Iteration 13365: Policy loss: -3.582347. Value loss: 74.276222. Entropy: 0.214075.\n",
      "episode: 5610   score: 400.0  epsilon: 1.0    steps: 43  evaluation reward: 269.1\n",
      "episode: 5611   score: 320.0  epsilon: 1.0    steps: 617  evaluation reward: 269.45\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13366: Policy loss: 0.312486. Value loss: 37.231918. Entropy: 0.261587.\n",
      "Iteration 13367: Policy loss: 0.360715. Value loss: 20.114159. Entropy: 0.278275.\n",
      "Iteration 13368: Policy loss: 0.359139. Value loss: 16.862677. Entropy: 0.280102.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13369: Policy loss: -0.306835. Value loss: 46.331467. Entropy: 0.329389.\n",
      "Iteration 13370: Policy loss: -0.301991. Value loss: 25.852848. Entropy: 0.304782.\n",
      "Iteration 13371: Policy loss: -0.216287. Value loss: 17.081837. Entropy: 0.308612.\n",
      "episode: 5612   score: 460.0  epsilon: 1.0    steps: 218  evaluation reward: 270.85\n",
      "episode: 5613   score: 365.0  epsilon: 1.0    steps: 734  evaluation reward: 271.9\n",
      "episode: 5614   score: 450.0  epsilon: 1.0    steps: 898  evaluation reward: 270.65\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13372: Policy loss: 3.177365. Value loss: 36.697853. Entropy: 0.250028.\n",
      "Iteration 13373: Policy loss: 2.803139. Value loss: 15.154168. Entropy: 0.265820.\n",
      "Iteration 13374: Policy loss: 2.995811. Value loss: 14.474442. Entropy: 0.288137.\n",
      "episode: 5615   score: 535.0  epsilon: 1.0    steps: 372  evaluation reward: 272.8\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13375: Policy loss: 0.499746. Value loss: 21.539705. Entropy: 0.290515.\n",
      "Iteration 13376: Policy loss: 0.548789. Value loss: 12.865816. Entropy: 0.305193.\n",
      "Iteration 13377: Policy loss: 0.643346. Value loss: 11.648311. Entropy: 0.301655.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13378: Policy loss: 2.896817. Value loss: 22.099463. Entropy: 0.299176.\n",
      "Iteration 13379: Policy loss: 2.931638. Value loss: 12.916880. Entropy: 0.318861.\n",
      "Iteration 13380: Policy loss: 2.911831. Value loss: 9.799376. Entropy: 0.311254.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13381: Policy loss: 2.309092. Value loss: 31.122355. Entropy: 0.259140.\n",
      "Iteration 13382: Policy loss: 2.523160. Value loss: 13.549196. Entropy: 0.279246.\n",
      "Iteration 13383: Policy loss: 2.251381. Value loss: 13.268683. Entropy: 0.247445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5616   score: 335.0  epsilon: 1.0    steps: 421  evaluation reward: 273.3\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13384: Policy loss: -0.112345. Value loss: 19.662487. Entropy: 0.158024.\n",
      "Iteration 13385: Policy loss: -0.044075. Value loss: 13.702959. Entropy: 0.179344.\n",
      "Iteration 13386: Policy loss: 0.163845. Value loss: 9.815686. Entropy: 0.179950.\n",
      "episode: 5617   score: 210.0  epsilon: 1.0    steps: 213  evaluation reward: 269.8\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13387: Policy loss: 1.208562. Value loss: 25.910666. Entropy: 0.207912.\n",
      "Iteration 13388: Policy loss: 1.160485. Value loss: 10.646902. Entropy: 0.221400.\n",
      "Iteration 13389: Policy loss: 1.139950. Value loss: 8.652035. Entropy: 0.211278.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13390: Policy loss: -0.920717. Value loss: 169.449631. Entropy: 0.174499.\n",
      "Iteration 13391: Policy loss: -1.379078. Value loss: 81.320511. Entropy: 0.144254.\n",
      "Iteration 13392: Policy loss: -0.154604. Value loss: 62.361725. Entropy: 0.152120.\n",
      "episode: 5618   score: 405.0  epsilon: 1.0    steps: 524  evaluation reward: 265.8\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13393: Policy loss: -5.400667. Value loss: 327.832397. Entropy: 0.094909.\n",
      "Iteration 13394: Policy loss: -5.236203. Value loss: 222.821716. Entropy: 0.101136.\n",
      "Iteration 13395: Policy loss: -5.080000. Value loss: 173.676376. Entropy: 0.096282.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13396: Policy loss: 2.445160. Value loss: 26.513788. Entropy: 0.092299.\n",
      "Iteration 13397: Policy loss: 2.400727. Value loss: 10.624209. Entropy: 0.084817.\n",
      "Iteration 13398: Policy loss: 2.282837. Value loss: 8.878462. Entropy: 0.097710.\n",
      "episode: 5619   score: 275.0  epsilon: 1.0    steps: 384  evaluation reward: 265.65\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13399: Policy loss: -0.220358. Value loss: 55.456661. Entropy: 0.083118.\n",
      "Iteration 13400: Policy loss: -0.720958. Value loss: 25.931892. Entropy: 0.065870.\n",
      "Iteration 13401: Policy loss: -0.644867. Value loss: 16.381102. Entropy: 0.074739.\n",
      "episode: 5620   score: 210.0  epsilon: 1.0    steps: 252  evaluation reward: 262.8\n",
      "episode: 5621   score: 345.0  epsilon: 1.0    steps: 816  evaluation reward: 263.6\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13402: Policy loss: 4.133099. Value loss: 27.404003. Entropy: 0.146089.\n",
      "Iteration 13403: Policy loss: 4.033013. Value loss: 13.887519. Entropy: 0.155721.\n",
      "Iteration 13404: Policy loss: 3.966072. Value loss: 9.259692. Entropy: 0.146387.\n",
      "episode: 5622   score: 445.0  epsilon: 1.0    steps: 28  evaluation reward: 265.2\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13405: Policy loss: -0.077105. Value loss: 32.440849. Entropy: 0.160696.\n",
      "Iteration 13406: Policy loss: 0.261596. Value loss: 12.904052. Entropy: 0.158678.\n",
      "Iteration 13407: Policy loss: 0.045271. Value loss: 9.369409. Entropy: 0.164820.\n",
      "episode: 5623   score: 245.0  epsilon: 1.0    steps: 627  evaluation reward: 264.2\n",
      "episode: 5624   score: 465.0  epsilon: 1.0    steps: 969  evaluation reward: 263.55\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13408: Policy loss: 0.679842. Value loss: 17.162386. Entropy: 0.169173.\n",
      "Iteration 13409: Policy loss: 0.899506. Value loss: 13.510460. Entropy: 0.197120.\n",
      "Iteration 13410: Policy loss: 0.742272. Value loss: 11.980742. Entropy: 0.202088.\n",
      "episode: 5625   score: 480.0  epsilon: 1.0    steps: 657  evaluation reward: 264.7\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13411: Policy loss: -1.887647. Value loss: 20.509506. Entropy: 0.223497.\n",
      "Iteration 13412: Policy loss: -1.859753. Value loss: 12.806316. Entropy: 0.245816.\n",
      "Iteration 13413: Policy loss: -1.865157. Value loss: 10.028907. Entropy: 0.232577.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13414: Policy loss: -1.742128. Value loss: 249.406219. Entropy: 0.197355.\n",
      "Iteration 13415: Policy loss: -1.160990. Value loss: 129.747620. Entropy: 0.161226.\n",
      "Iteration 13416: Policy loss: -1.664808. Value loss: 99.013092. Entropy: 0.179160.\n",
      "episode: 5626   score: 285.0  epsilon: 1.0    steps: 375  evaluation reward: 263.85\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13417: Policy loss: -3.046824. Value loss: 58.766438. Entropy: 0.406790.\n",
      "Iteration 13418: Policy loss: -2.423300. Value loss: 29.011492. Entropy: 0.424413.\n",
      "Iteration 13419: Policy loss: -3.007795. Value loss: 20.801723. Entropy: 0.389908.\n",
      "episode: 5627   score: 245.0  epsilon: 1.0    steps: 204  evaluation reward: 263.5\n",
      "episode: 5628   score: 530.0  epsilon: 1.0    steps: 799  evaluation reward: 265.7\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13420: Policy loss: 4.363405. Value loss: 83.887794. Entropy: 0.319306.\n",
      "Iteration 13421: Policy loss: 3.589825. Value loss: 34.157360. Entropy: 0.355034.\n",
      "Iteration 13422: Policy loss: 4.509294. Value loss: 20.937498. Entropy: 0.322900.\n",
      "episode: 5629   score: 530.0  epsilon: 1.0    steps: 72  evaluation reward: 268.4\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13423: Policy loss: 0.748477. Value loss: 25.449274. Entropy: 0.233254.\n",
      "Iteration 13424: Policy loss: 0.829776. Value loss: 13.834375. Entropy: 0.254599.\n",
      "Iteration 13425: Policy loss: 0.790363. Value loss: 11.822900. Entropy: 0.250602.\n",
      "episode: 5630   score: 555.0  epsilon: 1.0    steps: 458  evaluation reward: 271.35\n",
      "episode: 5631   score: 235.0  epsilon: 1.0    steps: 558  evaluation reward: 272.6\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13426: Policy loss: 0.513789. Value loss: 34.324825. Entropy: 0.172687.\n",
      "Iteration 13427: Policy loss: 0.729760. Value loss: 19.247494. Entropy: 0.169327.\n",
      "Iteration 13428: Policy loss: 0.645404. Value loss: 17.231071. Entropy: 0.182866.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13429: Policy loss: -2.412396. Value loss: 177.206665. Entropy: 0.235534.\n",
      "Iteration 13430: Policy loss: -2.040098. Value loss: 82.513382. Entropy: 0.235088.\n",
      "Iteration 13431: Policy loss: -2.780101. Value loss: 73.425209. Entropy: 0.234592.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13432: Policy loss: -1.317672. Value loss: 43.523155. Entropy: 0.294239.\n",
      "Iteration 13433: Policy loss: -1.156069. Value loss: 28.000580. Entropy: 0.303328.\n",
      "Iteration 13434: Policy loss: -1.615861. Value loss: 21.169254. Entropy: 0.302621.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13435: Policy loss: 0.354239. Value loss: 102.894531. Entropy: 0.310270.\n",
      "Iteration 13436: Policy loss: -0.108755. Value loss: 67.695602. Entropy: 0.302692.\n",
      "Iteration 13437: Policy loss: 0.161681. Value loss: 41.224625. Entropy: 0.306202.\n",
      "episode: 5632   score: 305.0  epsilon: 1.0    steps: 185  evaluation reward: 271.9\n",
      "episode: 5633   score: 490.0  epsilon: 1.0    steps: 728  evaluation reward: 274.7\n",
      "episode: 5634   score: 300.0  epsilon: 1.0    steps: 780  evaluation reward: 275.55\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13438: Policy loss: 1.214023. Value loss: 58.526245. Entropy: 0.137787.\n",
      "Iteration 13439: Policy loss: 1.119178. Value loss: 36.333679. Entropy: 0.147829.\n",
      "Iteration 13440: Policy loss: 1.041155. Value loss: 28.158512. Entropy: 0.152467.\n",
      "episode: 5635   score: 610.0  epsilon: 1.0    steps: 354  evaluation reward: 279.05\n",
      "episode: 5636   score: 230.0  epsilon: 1.0    steps: 465  evaluation reward: 279.25\n",
      "episode: 5637   score: 520.0  epsilon: 1.0    steps: 978  evaluation reward: 281.0\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13441: Policy loss: -0.078285. Value loss: 182.090576. Entropy: 0.264821.\n",
      "Iteration 13442: Policy loss: 0.565958. Value loss: 93.477081. Entropy: 0.258948.\n",
      "Iteration 13443: Policy loss: -0.326849. Value loss: 90.152351. Entropy: 0.260428.\n",
      "episode: 5638   score: 240.0  epsilon: 1.0    steps: 535  evaluation reward: 280.8\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13444: Policy loss: 0.038243. Value loss: 28.215506. Entropy: 0.261557.\n",
      "Iteration 13445: Policy loss: 0.332222. Value loss: 16.047329. Entropy: 0.269894.\n",
      "Iteration 13446: Policy loss: 0.184741. Value loss: 16.107138. Entropy: 0.275392.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13447: Policy loss: -0.505533. Value loss: 37.646214. Entropy: 0.250213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13448: Policy loss: -0.608391. Value loss: 26.236834. Entropy: 0.243119.\n",
      "Iteration 13449: Policy loss: -0.391424. Value loss: 19.351107. Entropy: 0.262677.\n",
      "episode: 5639   score: 915.0  epsilon: 1.0    steps: 85  evaluation reward: 287.85\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13450: Policy loss: 1.195371. Value loss: 39.955673. Entropy: 0.218287.\n",
      "Iteration 13451: Policy loss: 1.360860. Value loss: 20.405180. Entropy: 0.253802.\n",
      "Iteration 13452: Policy loss: 1.110610. Value loss: 15.373368. Entropy: 0.246159.\n",
      "episode: 5640   score: 210.0  epsilon: 1.0    steps: 182  evaluation reward: 287.85\n",
      "episode: 5641   score: 220.0  epsilon: 1.0    steps: 830  evaluation reward: 289.15\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13453: Policy loss: 0.209283. Value loss: 98.672394. Entropy: 0.200963.\n",
      "Iteration 13454: Policy loss: 0.033471. Value loss: 74.786217. Entropy: 0.203687.\n",
      "Iteration 13455: Policy loss: -0.102035. Value loss: 73.889923. Entropy: 0.207772.\n",
      "episode: 5642   score: 210.0  epsilon: 1.0    steps: 375  evaluation reward: 289.15\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13456: Policy loss: -0.213148. Value loss: 104.536728. Entropy: 0.288143.\n",
      "Iteration 13457: Policy loss: -0.188328. Value loss: 65.703522. Entropy: 0.293356.\n",
      "Iteration 13458: Policy loss: -0.141365. Value loss: 54.483475. Entropy: 0.284257.\n",
      "episode: 5643   score: 285.0  epsilon: 1.0    steps: 598  evaluation reward: 289.9\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13459: Policy loss: 2.695716. Value loss: 59.372734. Entropy: 0.198794.\n",
      "Iteration 13460: Policy loss: 2.292340. Value loss: 28.671793. Entropy: 0.196587.\n",
      "Iteration 13461: Policy loss: 2.413362. Value loss: 20.222607. Entropy: 0.199274.\n",
      "episode: 5644   score: 360.0  epsilon: 1.0    steps: 705  evaluation reward: 291.4\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13462: Policy loss: 3.017924. Value loss: 66.104225. Entropy: 0.195036.\n",
      "Iteration 13463: Policy loss: 2.328789. Value loss: 28.797112. Entropy: 0.197764.\n",
      "Iteration 13464: Policy loss: 2.567463. Value loss: 19.942596. Entropy: 0.199685.\n",
      "episode: 5645   score: 590.0  epsilon: 1.0    steps: 434  evaluation reward: 295.75\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13465: Policy loss: 2.682929. Value loss: 72.285316. Entropy: 0.223018.\n",
      "Iteration 13466: Policy loss: 3.448609. Value loss: 31.471569. Entropy: 0.230907.\n",
      "Iteration 13467: Policy loss: 2.410760. Value loss: 27.231108. Entropy: 0.217866.\n",
      "episode: 5646   score: 210.0  epsilon: 1.0    steps: 802  evaluation reward: 295.25\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13468: Policy loss: -1.327118. Value loss: 44.259773. Entropy: 0.280813.\n",
      "Iteration 13469: Policy loss: -1.241284. Value loss: 23.731882. Entropy: 0.280548.\n",
      "Iteration 13470: Policy loss: -1.195434. Value loss: 18.685774. Entropy: 0.283510.\n",
      "episode: 5647   score: 260.0  epsilon: 1.0    steps: 185  evaluation reward: 295.75\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13471: Policy loss: -0.100485. Value loss: 73.131920. Entropy: 0.119139.\n",
      "Iteration 13472: Policy loss: -0.390690. Value loss: 29.885233. Entropy: 0.120120.\n",
      "Iteration 13473: Policy loss: -0.343689. Value loss: 17.738407. Entropy: 0.120075.\n",
      "episode: 5648   score: 575.0  epsilon: 1.0    steps: 62  evaluation reward: 299.35\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13474: Policy loss: -3.835234. Value loss: 208.685654. Entropy: 0.228299.\n",
      "Iteration 13475: Policy loss: -3.914250. Value loss: 137.444458. Entropy: 0.199453.\n",
      "Iteration 13476: Policy loss: -3.609208. Value loss: 124.395897. Entropy: 0.198312.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13477: Policy loss: 0.308813. Value loss: 77.185478. Entropy: 0.168378.\n",
      "Iteration 13478: Policy loss: 0.811804. Value loss: 35.070820. Entropy: 0.174559.\n",
      "Iteration 13479: Policy loss: 0.206569. Value loss: 22.834694. Entropy: 0.152885.\n",
      "episode: 5649   score: 400.0  epsilon: 1.0    steps: 600  evaluation reward: 298.5\n",
      "episode: 5650   score: 500.0  epsilon: 1.0    steps: 676  evaluation reward: 300.6\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13480: Policy loss: 0.316095. Value loss: 56.417290. Entropy: 0.116943.\n",
      "Iteration 13481: Policy loss: 0.499110. Value loss: 33.850456. Entropy: 0.101582.\n",
      "Iteration 13482: Policy loss: 0.511234. Value loss: 19.253674. Entropy: 0.111511.\n",
      "now time :  2019-02-25 22:51:58.138347\n",
      "episode: 5651   score: 535.0  epsilon: 1.0    steps: 990  evaluation reward: 303.85\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13483: Policy loss: 2.031424. Value loss: 72.000618. Entropy: 0.262470.\n",
      "Iteration 13484: Policy loss: 2.043864. Value loss: 25.994316. Entropy: 0.257954.\n",
      "Iteration 13485: Policy loss: 1.988068. Value loss: 17.755646. Entropy: 0.266450.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13486: Policy loss: -3.805083. Value loss: 102.353760. Entropy: 0.227680.\n",
      "Iteration 13487: Policy loss: -3.549539. Value loss: 43.145569. Entropy: 0.237065.\n",
      "Iteration 13488: Policy loss: -3.666519. Value loss: 38.374481. Entropy: 0.236761.\n",
      "episode: 5652   score: 415.0  epsilon: 1.0    steps: 851  evaluation reward: 305.15\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13489: Policy loss: 2.691929. Value loss: 71.445953. Entropy: 0.174223.\n",
      "Iteration 13490: Policy loss: 3.254975. Value loss: 33.778942. Entropy: 0.186178.\n",
      "Iteration 13491: Policy loss: 3.036103. Value loss: 24.389252. Entropy: 0.181326.\n",
      "episode: 5653   score: 335.0  epsilon: 1.0    steps: 405  evaluation reward: 307.3\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13492: Policy loss: -0.284365. Value loss: 64.782562. Entropy: 0.177171.\n",
      "Iteration 13493: Policy loss: 0.138051. Value loss: 22.153933. Entropy: 0.171344.\n",
      "Iteration 13494: Policy loss: -0.164043. Value loss: 21.481165. Entropy: 0.169109.\n",
      "episode: 5654   score: 440.0  epsilon: 1.0    steps: 240  evaluation reward: 308.9\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13495: Policy loss: 0.283836. Value loss: 57.440601. Entropy: 0.282719.\n",
      "Iteration 13496: Policy loss: 0.866864. Value loss: 30.764605. Entropy: 0.289047.\n",
      "Iteration 13497: Policy loss: 0.517934. Value loss: 24.686346. Entropy: 0.292059.\n",
      "episode: 5655   score: 300.0  epsilon: 1.0    steps: 333  evaluation reward: 309.8\n",
      "episode: 5656   score: 365.0  epsilon: 1.0    steps: 751  evaluation reward: 310.85\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13498: Policy loss: -0.065883. Value loss: 43.483940. Entropy: 0.352609.\n",
      "Iteration 13499: Policy loss: 0.080537. Value loss: 29.825434. Entropy: 0.339667.\n",
      "Iteration 13500: Policy loss: -0.017611. Value loss: 23.955282. Entropy: 0.352502.\n",
      "episode: 5657   score: 615.0  epsilon: 1.0    steps: 18  evaluation reward: 313.9\n",
      "episode: 5658   score: 355.0  epsilon: 1.0    steps: 624  evaluation reward: 315.35\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13501: Policy loss: 2.738902. Value loss: 51.995953. Entropy: 0.131355.\n",
      "Iteration 13502: Policy loss: 2.777842. Value loss: 26.432318. Entropy: 0.131104.\n",
      "Iteration 13503: Policy loss: 2.594471. Value loss: 18.082382. Entropy: 0.125672.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13504: Policy loss: 2.221524. Value loss: 37.291248. Entropy: 0.275915.\n",
      "Iteration 13505: Policy loss: 2.161379. Value loss: 19.590565. Entropy: 0.261787.\n",
      "Iteration 13506: Policy loss: 2.231038. Value loss: 18.261219. Entropy: 0.273629.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13507: Policy loss: 5.222239. Value loss: 63.248768. Entropy: 0.296408.\n",
      "Iteration 13508: Policy loss: 5.337825. Value loss: 31.087017. Entropy: 0.283295.\n",
      "Iteration 13509: Policy loss: 5.312945. Value loss: 26.328529. Entropy: 0.316129.\n",
      "episode: 5659   score: 210.0  epsilon: 1.0    steps: 234  evaluation reward: 314.85\n",
      "episode: 5660   score: 425.0  epsilon: 1.0    steps: 916  evaluation reward: 317.0\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13510: Policy loss: 1.461506. Value loss: 35.117565. Entropy: 0.425619.\n",
      "Iteration 13511: Policy loss: 1.289718. Value loss: 19.506886. Entropy: 0.435150.\n",
      "Iteration 13512: Policy loss: 1.356295. Value loss: 13.250187. Entropy: 0.420338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13513: Policy loss: 0.181781. Value loss: 41.232681. Entropy: 0.495008.\n",
      "Iteration 13514: Policy loss: 0.373945. Value loss: 19.480534. Entropy: 0.498439.\n",
      "Iteration 13515: Policy loss: 0.054704. Value loss: 13.419709. Entropy: 0.509473.\n",
      "episode: 5661   score: 395.0  epsilon: 1.0    steps: 456  evaluation reward: 319.15\n",
      "episode: 5662   score: 430.0  epsilon: 1.0    steps: 843  evaluation reward: 320.55\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13516: Policy loss: 3.723570. Value loss: 34.411255. Entropy: 0.324963.\n",
      "Iteration 13517: Policy loss: 3.614481. Value loss: 15.485767. Entropy: 0.340233.\n",
      "Iteration 13518: Policy loss: 3.593913. Value loss: 14.447801. Entropy: 0.331091.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13519: Policy loss: -1.719169. Value loss: 164.510620. Entropy: 0.238303.\n",
      "Iteration 13520: Policy loss: -0.508310. Value loss: 48.524536. Entropy: 0.246537.\n",
      "Iteration 13521: Policy loss: -1.476056. Value loss: 31.593493. Entropy: 0.247402.\n",
      "episode: 5663   score: 325.0  epsilon: 1.0    steps: 110  evaluation reward: 321.15\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13522: Policy loss: 0.664728. Value loss: 28.222286. Entropy: 0.414636.\n",
      "Iteration 13523: Policy loss: 0.609294. Value loss: 18.804667. Entropy: 0.437840.\n",
      "Iteration 13524: Policy loss: 0.781863. Value loss: 16.343477. Entropy: 0.412373.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13525: Policy loss: 0.485972. Value loss: 30.214218. Entropy: 0.390854.\n",
      "Iteration 13526: Policy loss: 0.756991. Value loss: 15.965498. Entropy: 0.407767.\n",
      "Iteration 13527: Policy loss: 0.885689. Value loss: 13.827859. Entropy: 0.415807.\n",
      "episode: 5664   score: 230.0  epsilon: 1.0    steps: 285  evaluation reward: 319.9\n",
      "episode: 5665   score: 320.0  epsilon: 1.0    steps: 648  evaluation reward: 322.3\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13528: Policy loss: 0.977948. Value loss: 46.137611. Entropy: 0.277249.\n",
      "Iteration 13529: Policy loss: 1.119180. Value loss: 26.312654. Entropy: 0.289516.\n",
      "Iteration 13530: Policy loss: 1.160774. Value loss: 19.128664. Entropy: 0.282294.\n",
      "episode: 5666   score: 335.0  epsilon: 1.0    steps: 567  evaluation reward: 322.9\n",
      "episode: 5667   score: 480.0  epsilon: 1.0    steps: 973  evaluation reward: 323.85\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13531: Policy loss: -0.339073. Value loss: 23.281786. Entropy: 0.397165.\n",
      "Iteration 13532: Policy loss: -0.342909. Value loss: 14.617848. Entropy: 0.396171.\n",
      "Iteration 13533: Policy loss: -0.455615. Value loss: 11.030785. Entropy: 0.405816.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13534: Policy loss: -1.007994. Value loss: 230.184891. Entropy: 0.379409.\n",
      "Iteration 13535: Policy loss: -1.437164. Value loss: 154.640594. Entropy: 0.365028.\n",
      "Iteration 13536: Policy loss: -0.478619. Value loss: 87.139008. Entropy: 0.366812.\n",
      "episode: 5668   score: 275.0  epsilon: 1.0    steps: 848  evaluation reward: 324.8\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13537: Policy loss: 4.188986. Value loss: 44.730614. Entropy: 0.262208.\n",
      "Iteration 13538: Policy loss: 4.033447. Value loss: 18.914766. Entropy: 0.262830.\n",
      "Iteration 13539: Policy loss: 3.868383. Value loss: 12.952755. Entropy: 0.261108.\n",
      "episode: 5669   score: 285.0  epsilon: 1.0    steps: 421  evaluation reward: 325.05\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13540: Policy loss: 0.394166. Value loss: 18.877035. Entropy: 0.304295.\n",
      "Iteration 13541: Policy loss: 0.157452. Value loss: 15.810136. Entropy: 0.320585.\n",
      "Iteration 13542: Policy loss: 0.222208. Value loss: 9.093210. Entropy: 0.324221.\n",
      "episode: 5670   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 324.25\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13543: Policy loss: -2.541234. Value loss: 262.040436. Entropy: 0.575936.\n",
      "Iteration 13544: Policy loss: -1.336314. Value loss: 113.468391. Entropy: 0.530418.\n",
      "Iteration 13545: Policy loss: -2.277320. Value loss: 119.872154. Entropy: 0.502400.\n",
      "episode: 5671   score: 880.0  epsilon: 1.0    steps: 219  evaluation reward: 331.25\n",
      "episode: 5672   score: 340.0  epsilon: 1.0    steps: 305  evaluation reward: 333.55\n",
      "episode: 5673   score: 225.0  epsilon: 1.0    steps: 631  evaluation reward: 332.45\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13546: Policy loss: 1.401062. Value loss: 36.743523. Entropy: 0.427741.\n",
      "Iteration 13547: Policy loss: 1.602203. Value loss: 24.133732. Entropy: 0.442659.\n",
      "Iteration 13548: Policy loss: 1.329843. Value loss: 18.877428. Entropy: 0.446451.\n",
      "episode: 5674   score: 235.0  epsilon: 1.0    steps: 905  evaluation reward: 333.0\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13549: Policy loss: -0.945901. Value loss: 43.143250. Entropy: 0.306526.\n",
      "Iteration 13550: Policy loss: -0.689993. Value loss: 21.482790. Entropy: 0.325719.\n",
      "Iteration 13551: Policy loss: -0.702124. Value loss: 15.468530. Entropy: 0.325546.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13552: Policy loss: -2.420620. Value loss: 109.313828. Entropy: 0.262232.\n",
      "Iteration 13553: Policy loss: -4.218162. Value loss: 139.560532. Entropy: 0.256047.\n",
      "Iteration 13554: Policy loss: -3.707920. Value loss: 130.384949. Entropy: 0.250749.\n",
      "episode: 5675   score: 590.0  epsilon: 1.0    steps: 49  evaluation reward: 337.1\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13555: Policy loss: 3.333046. Value loss: 86.366066. Entropy: 0.260198.\n",
      "Iteration 13556: Policy loss: 3.925959. Value loss: 48.710976. Entropy: 0.256749.\n",
      "Iteration 13557: Policy loss: 3.236244. Value loss: 37.989815. Entropy: 0.272233.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13558: Policy loss: 4.249052. Value loss: 62.349804. Entropy: 0.298778.\n",
      "Iteration 13559: Policy loss: 4.227007. Value loss: 29.990295. Entropy: 0.309922.\n",
      "Iteration 13560: Policy loss: 4.117927. Value loss: 22.602911. Entropy: 0.315020.\n",
      "episode: 5676   score: 350.0  epsilon: 1.0    steps: 842  evaluation reward: 335.15\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13561: Policy loss: -1.219781. Value loss: 48.524685. Entropy: 0.423530.\n",
      "Iteration 13562: Policy loss: -1.261596. Value loss: 26.428841. Entropy: 0.430730.\n",
      "Iteration 13563: Policy loss: -1.167147. Value loss: 21.380077. Entropy: 0.421803.\n",
      "episode: 5677   score: 210.0  epsilon: 1.0    steps: 268  evaluation reward: 335.15\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13564: Policy loss: -0.269662. Value loss: 34.006508. Entropy: 0.280088.\n",
      "Iteration 13565: Policy loss: -0.614070. Value loss: 17.312244. Entropy: 0.293389.\n",
      "Iteration 13566: Policy loss: -0.249784. Value loss: 13.149287. Entropy: 0.280846.\n",
      "episode: 5678   score: 260.0  epsilon: 1.0    steps: 570  evaluation reward: 335.65\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13567: Policy loss: 6.704644. Value loss: 137.949173. Entropy: 0.285241.\n",
      "Iteration 13568: Policy loss: 7.321143. Value loss: 57.012169. Entropy: 0.314272.\n",
      "Iteration 13569: Policy loss: 6.274908. Value loss: 43.953365. Entropy: 0.308215.\n",
      "episode: 5679   score: 180.0  epsilon: 1.0    steps: 87  evaluation reward: 335.35\n",
      "episode: 5680   score: 455.0  epsilon: 1.0    steps: 391  evaluation reward: 338.1\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13570: Policy loss: 1.068962. Value loss: 26.503548. Entropy: 0.365174.\n",
      "Iteration 13571: Policy loss: 1.385100. Value loss: 14.872129. Entropy: 0.356694.\n",
      "Iteration 13572: Policy loss: 1.121345. Value loss: 11.723458. Entropy: 0.353050.\n",
      "episode: 5681   score: 410.0  epsilon: 1.0    steps: 907  evaluation reward: 340.1\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13573: Policy loss: 2.519434. Value loss: 24.723730. Entropy: 0.395240.\n",
      "Iteration 13574: Policy loss: 2.491836. Value loss: 17.016506. Entropy: 0.407326.\n",
      "Iteration 13575: Policy loss: 2.469991. Value loss: 13.234482. Entropy: 0.425028.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13576: Policy loss: -2.250994. Value loss: 53.298771. Entropy: 0.397414.\n",
      "Iteration 13577: Policy loss: -2.366571. Value loss: 30.582008. Entropy: 0.380837.\n",
      "Iteration 13578: Policy loss: -2.342155. Value loss: 24.782040. Entropy: 0.399415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5682   score: 560.0  epsilon: 1.0    steps: 694  evaluation reward: 343.6\n",
      "episode: 5683   score: 240.0  epsilon: 1.0    steps: 845  evaluation reward: 343.6\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13579: Policy loss: 0.883617. Value loss: 54.355206. Entropy: 0.400237.\n",
      "Iteration 13580: Policy loss: 0.986155. Value loss: 25.006203. Entropy: 0.380605.\n",
      "Iteration 13581: Policy loss: 1.114056. Value loss: 20.108789. Entropy: 0.383387.\n",
      "episode: 5684   score: 210.0  epsilon: 1.0    steps: 283  evaluation reward: 342.4\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13582: Policy loss: -0.994079. Value loss: 161.896179. Entropy: 0.357981.\n",
      "Iteration 13583: Policy loss: -0.891014. Value loss: 62.953281. Entropy: 0.363104.\n",
      "Iteration 13584: Policy loss: -0.878238. Value loss: 55.870033. Entropy: 0.352323.\n",
      "episode: 5685   score: 255.0  epsilon: 1.0    steps: 635  evaluation reward: 341.65\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13585: Policy loss: 2.180541. Value loss: 39.272636. Entropy: 0.568080.\n",
      "Iteration 13586: Policy loss: 2.523770. Value loss: 21.737152. Entropy: 0.551161.\n",
      "Iteration 13587: Policy loss: 2.484120. Value loss: 19.636835. Entropy: 0.562693.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13588: Policy loss: -0.021550. Value loss: 33.309029. Entropy: 0.307537.\n",
      "Iteration 13589: Policy loss: 0.166659. Value loss: 18.414011. Entropy: 0.307344.\n",
      "Iteration 13590: Policy loss: 0.315466. Value loss: 13.924412. Entropy: 0.319035.\n",
      "episode: 5686   score: 645.0  epsilon: 1.0    steps: 172  evaluation reward: 345.2\n",
      "episode: 5687   score: 455.0  epsilon: 1.0    steps: 950  evaluation reward: 347.9\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13591: Policy loss: 0.825229. Value loss: 28.043283. Entropy: 0.292202.\n",
      "Iteration 13592: Policy loss: 0.793806. Value loss: 16.649677. Entropy: 0.297674.\n",
      "Iteration 13593: Policy loss: 0.780271. Value loss: 14.315491. Entropy: 0.280764.\n",
      "episode: 5688   score: 230.0  epsilon: 1.0    steps: 112  evaluation reward: 348.1\n",
      "episode: 5689   score: 180.0  epsilon: 1.0    steps: 671  evaluation reward: 348.8\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13594: Policy loss: 1.938676. Value loss: 26.719936. Entropy: 0.336799.\n",
      "Iteration 13595: Policy loss: 2.448608. Value loss: 16.462206. Entropy: 0.330448.\n",
      "Iteration 13596: Policy loss: 1.962200. Value loss: 12.229613. Entropy: 0.337416.\n",
      "episode: 5690   score: 330.0  epsilon: 1.0    steps: 408  evaluation reward: 349.4\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13597: Policy loss: 0.894060. Value loss: 31.268412. Entropy: 0.403764.\n",
      "Iteration 13598: Policy loss: 0.616025. Value loss: 17.316753. Entropy: 0.416048.\n",
      "Iteration 13599: Policy loss: 0.785737. Value loss: 12.179937. Entropy: 0.399018.\n",
      "episode: 5691   score: 260.0  epsilon: 1.0    steps: 325  evaluation reward: 350.45\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13600: Policy loss: 2.588694. Value loss: 27.054392. Entropy: 0.456733.\n",
      "Iteration 13601: Policy loss: 2.391469. Value loss: 15.637414. Entropy: 0.445927.\n",
      "Iteration 13602: Policy loss: 2.352891. Value loss: 13.422920. Entropy: 0.470858.\n",
      "episode: 5692   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 351.0\n",
      "episode: 5693   score: 310.0  epsilon: 1.0    steps: 878  evaluation reward: 351.35\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13603: Policy loss: -0.101015. Value loss: 40.171192. Entropy: 0.313822.\n",
      "Iteration 13604: Policy loss: -0.184315. Value loss: 21.134743. Entropy: 0.321845.\n",
      "Iteration 13605: Policy loss: -0.222638. Value loss: 15.300874. Entropy: 0.331552.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13606: Policy loss: -2.564478. Value loss: 92.496719. Entropy: 0.396240.\n",
      "Iteration 13607: Policy loss: -2.611235. Value loss: 29.642876. Entropy: 0.393447.\n",
      "Iteration 13608: Policy loss: -2.700900. Value loss: 24.281883. Entropy: 0.392749.\n",
      "episode: 5694   score: 190.0  epsilon: 1.0    steps: 135  evaluation reward: 351.15\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13609: Policy loss: -1.433305. Value loss: 31.446033. Entropy: 0.402874.\n",
      "Iteration 13610: Policy loss: -1.448617. Value loss: 20.599943. Entropy: 0.396182.\n",
      "Iteration 13611: Policy loss: -1.060327. Value loss: 14.879400. Entropy: 0.405043.\n",
      "episode: 5695   score: 210.0  epsilon: 1.0    steps: 641  evaluation reward: 350.85\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13612: Policy loss: 0.093268. Value loss: 48.032669. Entropy: 0.384138.\n",
      "Iteration 13613: Policy loss: -0.192047. Value loss: 25.281891. Entropy: 0.387312.\n",
      "Iteration 13614: Policy loss: -0.277576. Value loss: 18.665045. Entropy: 0.379750.\n",
      "episode: 5696   score: 290.0  epsilon: 1.0    steps: 421  evaluation reward: 352.65\n",
      "episode: 5697   score: 320.0  epsilon: 1.0    steps: 926  evaluation reward: 352.65\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13615: Policy loss: 1.340181. Value loss: 18.561766. Entropy: 0.416848.\n",
      "Iteration 13616: Policy loss: 1.260664. Value loss: 12.487318. Entropy: 0.433544.\n",
      "Iteration 13617: Policy loss: 1.323989. Value loss: 10.413055. Entropy: 0.419267.\n",
      "episode: 5698   score: 275.0  epsilon: 1.0    steps: 356  evaluation reward: 353.3\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13618: Policy loss: 2.901649. Value loss: 39.196915. Entropy: 0.404989.\n",
      "Iteration 13619: Policy loss: 2.834542. Value loss: 18.135006. Entropy: 0.431875.\n",
      "Iteration 13620: Policy loss: 2.841171. Value loss: 14.258676. Entropy: 0.446002.\n",
      "episode: 5699   score: 590.0  epsilon: 1.0    steps: 56  evaluation reward: 356.0\n",
      "episode: 5700   score: 215.0  epsilon: 1.0    steps: 628  evaluation reward: 355.55\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13621: Policy loss: 0.089036. Value loss: 19.388151. Entropy: 0.543344.\n",
      "Iteration 13622: Policy loss: 0.138968. Value loss: 12.577272. Entropy: 0.541359.\n",
      "Iteration 13623: Policy loss: 0.142462. Value loss: 10.927919. Entropy: 0.549502.\n",
      "now time :  2019-02-25 22:54:33.911219\n",
      "episode: 5701   score: 240.0  epsilon: 1.0    steps: 235  evaluation reward: 355.35\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13624: Policy loss: 0.516627. Value loss: 20.403921. Entropy: 0.479936.\n",
      "Iteration 13625: Policy loss: 0.437321. Value loss: 13.162120. Entropy: 0.480823.\n",
      "Iteration 13626: Policy loss: 0.598499. Value loss: 10.866768. Entropy: 0.496079.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13627: Policy loss: 1.242697. Value loss: 20.587395. Entropy: 0.534133.\n",
      "Iteration 13628: Policy loss: 1.433282. Value loss: 11.809788. Entropy: 0.519033.\n",
      "Iteration 13629: Policy loss: 1.097375. Value loss: 8.388877. Entropy: 0.515319.\n",
      "episode: 5702   score: 200.0  epsilon: 1.0    steps: 451  evaluation reward: 354.6\n",
      "episode: 5703   score: 180.0  epsilon: 1.0    steps: 924  evaluation reward: 354.6\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13630: Policy loss: 1.136793. Value loss: 21.863211. Entropy: 0.333728.\n",
      "Iteration 13631: Policy loss: 0.808462. Value loss: 9.586754. Entropy: 0.338125.\n",
      "Iteration 13632: Policy loss: 0.785004. Value loss: 9.000552. Entropy: 0.340204.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13633: Policy loss: 0.922332. Value loss: 25.284285. Entropy: 0.336933.\n",
      "Iteration 13634: Policy loss: 0.962604. Value loss: 11.596087. Entropy: 0.347439.\n",
      "Iteration 13635: Policy loss: 0.854011. Value loss: 9.262091. Entropy: 0.350872.\n",
      "episode: 5704   score: 220.0  epsilon: 1.0    steps: 91  evaluation reward: 354.7\n",
      "episode: 5705   score: 195.0  epsilon: 1.0    steps: 281  evaluation reward: 352.65\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13636: Policy loss: 0.265591. Value loss: 19.897772. Entropy: 0.329819.\n",
      "Iteration 13637: Policy loss: 0.290129. Value loss: 12.183510. Entropy: 0.332708.\n",
      "Iteration 13638: Policy loss: 0.152529. Value loss: 10.594423. Entropy: 0.332880.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13639: Policy loss: 1.083049. Value loss: 28.942259. Entropy: 0.300772.\n",
      "Iteration 13640: Policy loss: 0.971945. Value loss: 14.657097. Entropy: 0.300086.\n",
      "Iteration 13641: Policy loss: 0.831385. Value loss: 11.981947. Entropy: 0.317355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5706   score: 155.0  epsilon: 1.0    steps: 981  evaluation reward: 351.55\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13642: Policy loss: 1.553153. Value loss: 34.861748. Entropy: 0.360145.\n",
      "Iteration 13643: Policy loss: 1.345546. Value loss: 17.027058. Entropy: 0.356725.\n",
      "Iteration 13644: Policy loss: 1.601834. Value loss: 13.911285. Entropy: 0.347091.\n",
      "episode: 5707   score: 225.0  epsilon: 1.0    steps: 138  evaluation reward: 351.7\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13645: Policy loss: 2.902571. Value loss: 22.206673. Entropy: 0.408446.\n",
      "Iteration 13646: Policy loss: 3.225436. Value loss: 12.074162. Entropy: 0.390537.\n",
      "Iteration 13647: Policy loss: 3.225204. Value loss: 9.285871. Entropy: 0.402662.\n",
      "episode: 5708   score: 120.0  epsilon: 1.0    steps: 347  evaluation reward: 350.25\n",
      "episode: 5709   score: 225.0  epsilon: 1.0    steps: 410  evaluation reward: 349.05\n",
      "episode: 5710   score: 290.0  epsilon: 1.0    steps: 702  evaluation reward: 347.95\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13648: Policy loss: 0.268529. Value loss: 33.018486. Entropy: 0.374509.\n",
      "Iteration 13649: Policy loss: 0.290195. Value loss: 18.779984. Entropy: 0.385308.\n",
      "Iteration 13650: Policy loss: 0.391099. Value loss: 13.923906. Entropy: 0.388628.\n",
      "episode: 5711   score: 230.0  epsilon: 1.0    steps: 76  evaluation reward: 347.05\n",
      "episode: 5712   score: 440.0  epsilon: 1.0    steps: 846  evaluation reward: 346.85\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13651: Policy loss: 2.170449. Value loss: 24.348978. Entropy: 0.397967.\n",
      "Iteration 13652: Policy loss: 2.092489. Value loss: 15.033792. Entropy: 0.380135.\n",
      "Iteration 13653: Policy loss: 2.053546. Value loss: 12.499067. Entropy: 0.378519.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13654: Policy loss: 1.956438. Value loss: 19.850567. Entropy: 0.469111.\n",
      "Iteration 13655: Policy loss: 2.071983. Value loss: 10.593995. Entropy: 0.481505.\n",
      "Iteration 13656: Policy loss: 1.964121. Value loss: 9.050207. Entropy: 0.477391.\n",
      "episode: 5713   score: 385.0  epsilon: 1.0    steps: 561  evaluation reward: 347.05\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13657: Policy loss: 1.442727. Value loss: 17.869787. Entropy: 0.434971.\n",
      "Iteration 13658: Policy loss: 1.457326. Value loss: 11.604714. Entropy: 0.445956.\n",
      "Iteration 13659: Policy loss: 1.371912. Value loss: 10.024415. Entropy: 0.459554.\n",
      "episode: 5714   score: 210.0  epsilon: 1.0    steps: 131  evaluation reward: 344.65\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13660: Policy loss: -1.366595. Value loss: 19.386438. Entropy: 0.457037.\n",
      "Iteration 13661: Policy loss: -0.949375. Value loss: 12.322673. Entropy: 0.472134.\n",
      "Iteration 13662: Policy loss: -1.559440. Value loss: 9.882553. Entropy: 0.452350.\n",
      "episode: 5715   score: 185.0  epsilon: 1.0    steps: 364  evaluation reward: 341.15\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13663: Policy loss: -1.312794. Value loss: 31.589828. Entropy: 0.451319.\n",
      "Iteration 13664: Policy loss: -1.251952. Value loss: 15.367929. Entropy: 0.474342.\n",
      "Iteration 13665: Policy loss: -1.261909. Value loss: 12.108356. Entropy: 0.445744.\n",
      "episode: 5716   score: 210.0  epsilon: 1.0    steps: 837  evaluation reward: 339.9\n",
      "episode: 5717   score: 285.0  epsilon: 1.0    steps: 964  evaluation reward: 340.65\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13666: Policy loss: -2.016041. Value loss: 210.600998. Entropy: 0.359740.\n",
      "Iteration 13667: Policy loss: -2.381428. Value loss: 118.230583. Entropy: 0.306442.\n",
      "Iteration 13668: Policy loss: -1.639729. Value loss: 74.796196. Entropy: 0.302852.\n",
      "episode: 5718   score: 465.0  epsilon: 1.0    steps: 462  evaluation reward: 341.25\n",
      "episode: 5719   score: 255.0  epsilon: 1.0    steps: 655  evaluation reward: 341.05\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13669: Policy loss: 0.654859. Value loss: 25.773415. Entropy: 0.435133.\n",
      "Iteration 13670: Policy loss: 0.465975. Value loss: 16.766874. Entropy: 0.414247.\n",
      "Iteration 13671: Policy loss: 0.717562. Value loss: 14.061391. Entropy: 0.419550.\n",
      "episode: 5720   score: 240.0  epsilon: 1.0    steps: 13  evaluation reward: 341.35\n",
      "episode: 5721   score: 155.0  epsilon: 1.0    steps: 549  evaluation reward: 339.45\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13672: Policy loss: 0.981657. Value loss: 20.087109. Entropy: 0.457999.\n",
      "Iteration 13673: Policy loss: 0.870374. Value loss: 10.842166. Entropy: 0.481240.\n",
      "Iteration 13674: Policy loss: 0.810372. Value loss: 9.994576. Entropy: 0.452245.\n",
      "episode: 5722   score: 185.0  epsilon: 1.0    steps: 180  evaluation reward: 336.85\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13675: Policy loss: -0.830685. Value loss: 27.399996. Entropy: 0.387462.\n",
      "Iteration 13676: Policy loss: -0.774614. Value loss: 14.795496. Entropy: 0.399070.\n",
      "Iteration 13677: Policy loss: -0.757980. Value loss: 10.134291. Entropy: 0.380810.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13678: Policy loss: 2.630676. Value loss: 40.456108. Entropy: 0.294126.\n",
      "Iteration 13679: Policy loss: 2.742518. Value loss: 17.299984. Entropy: 0.319852.\n",
      "Iteration 13680: Policy loss: 2.768186. Value loss: 13.775426. Entropy: 0.314660.\n",
      "episode: 5723   score: 155.0  epsilon: 1.0    steps: 728  evaluation reward: 335.95\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13681: Policy loss: 0.616938. Value loss: 15.806405. Entropy: 0.436183.\n",
      "Iteration 13682: Policy loss: 0.602819. Value loss: 10.459511. Entropy: 0.435837.\n",
      "Iteration 13683: Policy loss: 0.559281. Value loss: 9.106881. Entropy: 0.454603.\n",
      "episode: 5724   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 333.4\n",
      "episode: 5725   score: 255.0  epsilon: 1.0    steps: 280  evaluation reward: 331.15\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13684: Policy loss: -2.904865. Value loss: 32.039242. Entropy: 0.424687.\n",
      "Iteration 13685: Policy loss: -2.848664. Value loss: 15.215371. Entropy: 0.407690.\n",
      "Iteration 13686: Policy loss: -2.977804. Value loss: 11.724247. Entropy: 0.421226.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13687: Policy loss: -0.727994. Value loss: 35.250275. Entropy: 0.458920.\n",
      "Iteration 13688: Policy loss: -0.879633. Value loss: 16.234516. Entropy: 0.451208.\n",
      "Iteration 13689: Policy loss: -0.769955. Value loss: 10.409651. Entropy: 0.441020.\n",
      "episode: 5726   score: 185.0  epsilon: 1.0    steps: 177  evaluation reward: 330.15\n",
      "episode: 5727   score: 375.0  epsilon: 1.0    steps: 834  evaluation reward: 331.45\n",
      "episode: 5728   score: 360.0  epsilon: 1.0    steps: 978  evaluation reward: 329.75\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13690: Policy loss: 0.991654. Value loss: 22.254990. Entropy: 0.422930.\n",
      "Iteration 13691: Policy loss: 1.067471. Value loss: 12.419849. Entropy: 0.438424.\n",
      "Iteration 13692: Policy loss: 1.170330. Value loss: 9.799528. Entropy: 0.415055.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13693: Policy loss: -0.310414. Value loss: 22.751205. Entropy: 0.419355.\n",
      "Iteration 13694: Policy loss: -0.112418. Value loss: 13.623507. Entropy: 0.403102.\n",
      "Iteration 13695: Policy loss: -0.276366. Value loss: 9.841148. Entropy: 0.420626.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13696: Policy loss: 0.036562. Value loss: 44.328457. Entropy: 0.438183.\n",
      "Iteration 13697: Policy loss: -0.034672. Value loss: 24.967125. Entropy: 0.447392.\n",
      "Iteration 13698: Policy loss: 0.051222. Value loss: 15.484407. Entropy: 0.462102.\n",
      "episode: 5729   score: 180.0  epsilon: 1.0    steps: 269  evaluation reward: 326.25\n",
      "episode: 5730   score: 345.0  epsilon: 1.0    steps: 480  evaluation reward: 324.15\n",
      "episode: 5731   score: 230.0  epsilon: 1.0    steps: 719  evaluation reward: 324.1\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13699: Policy loss: 1.562218. Value loss: 19.212345. Entropy: 0.330507.\n",
      "Iteration 13700: Policy loss: 1.690541. Value loss: 11.373331. Entropy: 0.328843.\n",
      "Iteration 13701: Policy loss: 1.345914. Value loss: 9.784788. Entropy: 0.351982.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13702: Policy loss: 1.250208. Value loss: 19.689875. Entropy: 0.431967.\n",
      "Iteration 13703: Policy loss: 1.156390. Value loss: 12.629873. Entropy: 0.445561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13704: Policy loss: 1.012813. Value loss: 10.520186. Entropy: 0.442881.\n",
      "episode: 5732   score: 185.0  epsilon: 1.0    steps: 234  evaluation reward: 322.9\n",
      "episode: 5733   score: 410.0  epsilon: 1.0    steps: 600  evaluation reward: 322.1\n",
      "episode: 5734   score: 155.0  epsilon: 1.0    steps: 779  evaluation reward: 320.65\n",
      "episode: 5735   score: 155.0  epsilon: 1.0    steps: 915  evaluation reward: 316.1\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13705: Policy loss: 0.774059. Value loss: 18.723835. Entropy: 0.607860.\n",
      "Iteration 13706: Policy loss: 0.591167. Value loss: 13.406001. Entropy: 0.573146.\n",
      "Iteration 13707: Policy loss: 0.441895. Value loss: 9.826462. Entropy: 0.609492.\n",
      "episode: 5736   score: 365.0  epsilon: 1.0    steps: 50  evaluation reward: 317.45\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13708: Policy loss: 1.198171. Value loss: 18.181995. Entropy: 0.512530.\n",
      "Iteration 13709: Policy loss: 1.293522. Value loss: 11.544807. Entropy: 0.527703.\n",
      "Iteration 13710: Policy loss: 1.278006. Value loss: 9.214315. Entropy: 0.527148.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13711: Policy loss: -0.802509. Value loss: 23.482956. Entropy: 0.613353.\n",
      "Iteration 13712: Policy loss: -0.700334. Value loss: 15.591398. Entropy: 0.627049.\n",
      "Iteration 13713: Policy loss: -0.982651. Value loss: 12.403764. Entropy: 0.628271.\n",
      "episode: 5737   score: 210.0  epsilon: 1.0    steps: 486  evaluation reward: 314.35\n",
      "episode: 5738   score: 230.0  epsilon: 1.0    steps: 733  evaluation reward: 314.25\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13714: Policy loss: -3.195884. Value loss: 206.713455. Entropy: 0.310252.\n",
      "Iteration 13715: Policy loss: -3.992599. Value loss: 106.246651. Entropy: 0.259005.\n",
      "Iteration 13716: Policy loss: -3.235168. Value loss: 65.212929. Entropy: 0.276413.\n",
      "episode: 5739   score: 105.0  epsilon: 1.0    steps: 231  evaluation reward: 306.15\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13717: Policy loss: 0.876757. Value loss: 70.680725. Entropy: 0.381696.\n",
      "Iteration 13718: Policy loss: 1.143494. Value loss: 37.737438. Entropy: 0.359153.\n",
      "Iteration 13719: Policy loss: 0.732184. Value loss: 22.452686. Entropy: 0.335765.\n",
      "episode: 5740   score: 135.0  epsilon: 1.0    steps: 102  evaluation reward: 305.4\n",
      "episode: 5741   score: 205.0  epsilon: 1.0    steps: 622  evaluation reward: 305.25\n",
      "episode: 5742   score: 235.0  epsilon: 1.0    steps: 826  evaluation reward: 305.5\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13720: Policy loss: 4.015411. Value loss: 36.907085. Entropy: 0.550510.\n",
      "Iteration 13721: Policy loss: 3.929392. Value loss: 13.789058. Entropy: 0.601049.\n",
      "Iteration 13722: Policy loss: 4.119381. Value loss: 10.121215. Entropy: 0.559395.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13723: Policy loss: -0.224093. Value loss: 39.003342. Entropy: 0.421043.\n",
      "Iteration 13724: Policy loss: -0.268257. Value loss: 17.904602. Entropy: 0.425024.\n",
      "Iteration 13725: Policy loss: 0.015683. Value loss: 12.640985. Entropy: 0.428200.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13726: Policy loss: 3.624120. Value loss: 34.060776. Entropy: 0.485063.\n",
      "Iteration 13727: Policy loss: 3.575362. Value loss: 16.070749. Entropy: 0.476322.\n",
      "Iteration 13728: Policy loss: 3.859708. Value loss: 13.658381. Entropy: 0.499650.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13729: Policy loss: 1.302655. Value loss: 28.415125. Entropy: 0.376451.\n",
      "Iteration 13730: Policy loss: 1.274909. Value loss: 14.584098. Entropy: 0.400104.\n",
      "Iteration 13731: Policy loss: 1.529385. Value loss: 11.153815. Entropy: 0.380964.\n",
      "episode: 5743   score: 170.0  epsilon: 1.0    steps: 387  evaluation reward: 304.35\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13732: Policy loss: -2.663528. Value loss: 180.060959. Entropy: 0.316652.\n",
      "Iteration 13733: Policy loss: -2.759140. Value loss: 106.858902. Entropy: 0.326057.\n",
      "Iteration 13734: Policy loss: -2.836329. Value loss: 67.412392. Entropy: 0.328760.\n",
      "episode: 5744   score: 210.0  epsilon: 1.0    steps: 220  evaluation reward: 302.85\n",
      "episode: 5745   score: 445.0  epsilon: 1.0    steps: 335  evaluation reward: 301.4\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13735: Policy loss: 1.391274. Value loss: 43.058083. Entropy: 0.506875.\n",
      "Iteration 13736: Policy loss: 2.014053. Value loss: 19.075678. Entropy: 0.486533.\n",
      "Iteration 13737: Policy loss: 1.573702. Value loss: 14.048697. Entropy: 0.487749.\n",
      "episode: 5746   score: 185.0  epsilon: 1.0    steps: 15  evaluation reward: 301.15\n",
      "episode: 5747   score: 290.0  epsilon: 1.0    steps: 860  evaluation reward: 301.45\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13738: Policy loss: 0.935227. Value loss: 124.914131. Entropy: 0.553946.\n",
      "Iteration 13739: Policy loss: 0.981210. Value loss: 68.704521. Entropy: 0.573318.\n",
      "Iteration 13740: Policy loss: 0.937488. Value loss: 63.782669. Entropy: 0.577854.\n",
      "episode: 5748   score: 525.0  epsilon: 1.0    steps: 754  evaluation reward: 300.95\n",
      "episode: 5749   score: 810.0  epsilon: 1.0    steps: 988  evaluation reward: 305.05\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13741: Policy loss: -0.637957. Value loss: 69.818199. Entropy: 0.472741.\n",
      "Iteration 13742: Policy loss: -0.227059. Value loss: 25.877634. Entropy: 0.447352.\n",
      "Iteration 13743: Policy loss: -0.811204. Value loss: 21.653437. Entropy: 0.428125.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13744: Policy loss: 1.781158. Value loss: 15.977750. Entropy: 0.480836.\n",
      "Iteration 13745: Policy loss: 1.911980. Value loss: 8.000394. Entropy: 0.463472.\n",
      "Iteration 13746: Policy loss: 1.747579. Value loss: 4.941279. Entropy: 0.481504.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13747: Policy loss: 1.174378. Value loss: 19.258677. Entropy: 0.399965.\n",
      "Iteration 13748: Policy loss: 0.858667. Value loss: 10.923601. Entropy: 0.403616.\n",
      "Iteration 13749: Policy loss: 1.186849. Value loss: 7.900930. Entropy: 0.407459.\n",
      "episode: 5750   score: 160.0  epsilon: 1.0    steps: 318  evaluation reward: 301.65\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13750: Policy loss: 3.182581. Value loss: 40.392227. Entropy: 0.338127.\n",
      "Iteration 13751: Policy loss: 2.759951. Value loss: 30.574024. Entropy: 0.334254.\n",
      "Iteration 13752: Policy loss: 3.027746. Value loss: 20.220427. Entropy: 0.335224.\n",
      "now time :  2019-02-25 22:56:58.992064\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5751   score: 300.0  epsilon: 1.0    steps: 601  evaluation reward: 299.3\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13753: Policy loss: 3.544130. Value loss: 130.754089. Entropy: 0.181452.\n",
      "Iteration 13754: Policy loss: 2.894563. Value loss: 35.227497. Entropy: 0.193562.\n",
      "Iteration 13755: Policy loss: 3.522484. Value loss: 23.740362. Entropy: 0.197467.\n",
      "episode: 5752   score: 210.0  epsilon: 1.0    steps: 18  evaluation reward: 297.25\n",
      "episode: 5753   score: 200.0  epsilon: 1.0    steps: 155  evaluation reward: 295.9\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13756: Policy loss: 1.266171. Value loss: 25.492575. Entropy: 0.174523.\n",
      "Iteration 13757: Policy loss: 1.517387. Value loss: 12.784801. Entropy: 0.181880.\n",
      "Iteration 13758: Policy loss: 1.409444. Value loss: 10.161787. Entropy: 0.188040.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13759: Policy loss: 3.295652. Value loss: 52.641701. Entropy: 0.212657.\n",
      "Iteration 13760: Policy loss: 3.335460. Value loss: 30.735340. Entropy: 0.227320.\n",
      "Iteration 13761: Policy loss: 3.059510. Value loss: 23.991728. Entropy: 0.262796.\n",
      "episode: 5754   score: 290.0  epsilon: 1.0    steps: 733  evaluation reward: 294.4\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13762: Policy loss: 3.754870. Value loss: 51.715034. Entropy: 0.174960.\n",
      "Iteration 13763: Policy loss: 3.439041. Value loss: 30.296854. Entropy: 0.175853.\n",
      "Iteration 13764: Policy loss: 3.151396. Value loss: 22.593578. Entropy: 0.175871.\n",
      "episode: 5755   score: 475.0  epsilon: 1.0    steps: 420  evaluation reward: 296.15\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13765: Policy loss: 1.989295. Value loss: 44.217449. Entropy: 0.288360.\n",
      "Iteration 13766: Policy loss: 2.271103. Value loss: 24.607115. Entropy: 0.253875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13767: Policy loss: 2.293307. Value loss: 17.677803. Entropy: 0.270832.\n",
      "episode: 5756   score: 265.0  epsilon: 1.0    steps: 889  evaluation reward: 295.15\n",
      "episode: 5757   score: 245.0  epsilon: 1.0    steps: 923  evaluation reward: 291.45\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13768: Policy loss: 1.158701. Value loss: 33.506966. Entropy: 0.327845.\n",
      "Iteration 13769: Policy loss: 1.110704. Value loss: 16.901028. Entropy: 0.330422.\n",
      "Iteration 13770: Policy loss: 1.134182. Value loss: 13.165373. Entropy: 0.312576.\n",
      "episode: 5758   score: 215.0  epsilon: 1.0    steps: 291  evaluation reward: 290.05\n",
      "episode: 5759   score: 260.0  epsilon: 1.0    steps: 591  evaluation reward: 290.55\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13771: Policy loss: -4.794556. Value loss: 255.182755. Entropy: 0.291515.\n",
      "Iteration 13772: Policy loss: -4.671909. Value loss: 154.862305. Entropy: 0.265075.\n",
      "Iteration 13773: Policy loss: -5.053753. Value loss: 176.281876. Entropy: 0.238575.\n",
      "episode: 5760   score: 255.0  epsilon: 1.0    steps: 168  evaluation reward: 288.85\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13774: Policy loss: 3.495504. Value loss: 39.955013. Entropy: 0.287023.\n",
      "Iteration 13775: Policy loss: 3.761168. Value loss: 21.636330. Entropy: 0.291275.\n",
      "Iteration 13776: Policy loss: 3.800726. Value loss: 15.849885. Entropy: 0.298269.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13777: Policy loss: -0.707038. Value loss: 32.502357. Entropy: 0.318901.\n",
      "Iteration 13778: Policy loss: -0.841142. Value loss: 19.832428. Entropy: 0.333204.\n",
      "Iteration 13779: Policy loss: -0.579703. Value loss: 15.371568. Entropy: 0.320710.\n",
      "episode: 5761   score: 300.0  epsilon: 1.0    steps: 71  evaluation reward: 287.9\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13780: Policy loss: 0.532815. Value loss: 26.959633. Entropy: 0.389931.\n",
      "Iteration 13781: Policy loss: 0.526822. Value loss: 16.256456. Entropy: 0.354643.\n",
      "Iteration 13782: Policy loss: 0.724844. Value loss: 12.993718. Entropy: 0.383213.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13783: Policy loss: -0.841053. Value loss: 198.743118. Entropy: 0.330489.\n",
      "Iteration 13784: Policy loss: -0.148916. Value loss: 115.687210. Entropy: 0.293431.\n",
      "Iteration 13785: Policy loss: -0.499428. Value loss: 64.964790. Entropy: 0.305742.\n",
      "episode: 5762   score: 340.0  epsilon: 1.0    steps: 394  evaluation reward: 287.0\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13786: Policy loss: -2.763852. Value loss: 211.107849. Entropy: 0.305552.\n",
      "Iteration 13787: Policy loss: -1.933362. Value loss: 76.003036. Entropy: 0.301043.\n",
      "Iteration 13788: Policy loss: -2.441289. Value loss: 51.764263. Entropy: 0.304208.\n",
      "episode: 5763   score: 260.0  epsilon: 1.0    steps: 613  evaluation reward: 286.35\n",
      "episode: 5764   score: 500.0  epsilon: 1.0    steps: 665  evaluation reward: 289.05\n",
      "episode: 5765   score: 260.0  epsilon: 1.0    steps: 788  evaluation reward: 288.45\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13789: Policy loss: 1.249484. Value loss: 24.627165. Entropy: 0.265517.\n",
      "Iteration 13790: Policy loss: 0.623617. Value loss: 15.705812. Entropy: 0.276514.\n",
      "Iteration 13791: Policy loss: 1.149263. Value loss: 11.355674. Entropy: 0.270458.\n",
      "episode: 5766   score: 355.0  epsilon: 1.0    steps: 946  evaluation reward: 288.65\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13792: Policy loss: -1.570097. Value loss: 57.092964. Entropy: 0.261118.\n",
      "Iteration 13793: Policy loss: -1.161843. Value loss: 31.147169. Entropy: 0.257217.\n",
      "Iteration 13794: Policy loss: -1.488571. Value loss: 26.006472. Entropy: 0.261102.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13795: Policy loss: 1.075113. Value loss: 53.846851. Entropy: 0.222333.\n",
      "Iteration 13796: Policy loss: 1.387116. Value loss: 20.638512. Entropy: 0.229344.\n",
      "Iteration 13797: Policy loss: 0.867527. Value loss: 16.433311. Entropy: 0.239947.\n",
      "episode: 5767   score: 535.0  epsilon: 1.0    steps: 187  evaluation reward: 289.2\n",
      "episode: 5768   score: 545.0  epsilon: 1.0    steps: 347  evaluation reward: 291.9\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13798: Policy loss: 1.879739. Value loss: 55.035919. Entropy: 0.313771.\n",
      "Iteration 13799: Policy loss: 2.073281. Value loss: 27.539640. Entropy: 0.313636.\n",
      "Iteration 13800: Policy loss: 2.377440. Value loss: 20.561289. Entropy: 0.309861.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13801: Policy loss: 3.058295. Value loss: 40.620747. Entropy: 0.150931.\n",
      "Iteration 13802: Policy loss: 2.835666. Value loss: 20.719542. Entropy: 0.174475.\n",
      "Iteration 13803: Policy loss: 3.164338. Value loss: 15.893307. Entropy: 0.167875.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13804: Policy loss: 1.734635. Value loss: 24.667091. Entropy: 0.207655.\n",
      "Iteration 13805: Policy loss: 1.986622. Value loss: 12.608866. Entropy: 0.227863.\n",
      "Iteration 13806: Policy loss: 1.830716. Value loss: 8.698102. Entropy: 0.226100.\n",
      "episode: 5769   score: 305.0  epsilon: 1.0    steps: 80  evaluation reward: 292.1\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13807: Policy loss: -0.114078. Value loss: 156.969193. Entropy: 0.315402.\n",
      "Iteration 13808: Policy loss: 0.076095. Value loss: 60.640541. Entropy: 0.331488.\n",
      "Iteration 13809: Policy loss: 0.426890. Value loss: 68.640572. Entropy: 0.314797.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13810: Policy loss: 0.214682. Value loss: 80.556946. Entropy: 0.209133.\n",
      "Iteration 13811: Policy loss: -0.177599. Value loss: 32.156578. Entropy: 0.197450.\n",
      "Iteration 13812: Policy loss: -0.262040. Value loss: 21.756924. Entropy: 0.186025.\n",
      "episode: 5770   score: 470.0  epsilon: 1.0    steps: 454  evaluation reward: 294.7\n",
      "episode: 5771   score: 395.0  epsilon: 1.0    steps: 540  evaluation reward: 289.85\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13813: Policy loss: 0.268537. Value loss: 25.741671. Entropy: 0.328678.\n",
      "Iteration 13814: Policy loss: 0.259800. Value loss: 16.070692. Entropy: 0.335356.\n",
      "Iteration 13815: Policy loss: 0.351398. Value loss: 13.842408. Entropy: 0.342824.\n",
      "episode: 5772   score: 460.0  epsilon: 1.0    steps: 334  evaluation reward: 291.05\n",
      "episode: 5773   score: 370.0  epsilon: 1.0    steps: 813  evaluation reward: 292.5\n",
      "episode: 5774   score: 360.0  epsilon: 1.0    steps: 930  evaluation reward: 293.75\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13816: Policy loss: 2.066725. Value loss: 60.600384. Entropy: 0.240182.\n",
      "Iteration 13817: Policy loss: 1.989776. Value loss: 27.109409. Entropy: 0.229082.\n",
      "Iteration 13818: Policy loss: 2.559702. Value loss: 17.093540. Entropy: 0.246739.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13819: Policy loss: -0.357753. Value loss: 137.121552. Entropy: 0.309063.\n",
      "Iteration 13820: Policy loss: -0.281813. Value loss: 53.363564. Entropy: 0.295675.\n",
      "Iteration 13821: Policy loss: -0.782542. Value loss: 38.831631. Entropy: 0.309035.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13822: Policy loss: -0.642195. Value loss: 48.105347. Entropy: 0.218307.\n",
      "Iteration 13823: Policy loss: -0.021486. Value loss: 28.803156. Entropy: 0.198674.\n",
      "Iteration 13824: Policy loss: -0.030331. Value loss: 24.414249. Entropy: 0.203222.\n",
      "episode: 5775   score: 345.0  epsilon: 1.0    steps: 192  evaluation reward: 291.3\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13825: Policy loss: 3.881017. Value loss: 53.375332. Entropy: 0.260501.\n",
      "Iteration 13826: Policy loss: 4.834232. Value loss: 25.616014. Entropy: 0.257980.\n",
      "Iteration 13827: Policy loss: 4.127262. Value loss: 17.398420. Entropy: 0.280618.\n",
      "episode: 5776   score: 260.0  epsilon: 1.0    steps: 546  evaluation reward: 290.4\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13828: Policy loss: 4.598615. Value loss: 45.506977. Entropy: 0.315596.\n",
      "Iteration 13829: Policy loss: 4.566282. Value loss: 25.869579. Entropy: 0.313249.\n",
      "Iteration 13830: Policy loss: 4.558600. Value loss: 19.305141. Entropy: 0.319909.\n",
      "episode: 5777   score: 575.0  epsilon: 1.0    steps: 63  evaluation reward: 294.05\n",
      "episode: 5778   score: 365.0  epsilon: 1.0    steps: 690  evaluation reward: 295.1\n",
      "Training network. lr: 0.000144. clip: 0.057616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13831: Policy loss: 0.193660. Value loss: 34.048531. Entropy: 0.404351.\n",
      "Iteration 13832: Policy loss: 0.025977. Value loss: 20.182835. Entropy: 0.399250.\n",
      "Iteration 13833: Policy loss: 0.093103. Value loss: 17.361126. Entropy: 0.401768.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13834: Policy loss: 1.209727. Value loss: 32.780777. Entropy: 0.218555.\n",
      "Iteration 13835: Policy loss: 1.119864. Value loss: 20.402771. Entropy: 0.238442.\n",
      "Iteration 13836: Policy loss: 1.028008. Value loss: 15.643209. Entropy: 0.246906.\n",
      "episode: 5779   score: 375.0  epsilon: 1.0    steps: 921  evaluation reward: 297.05\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13837: Policy loss: 0.169570. Value loss: 34.920849. Entropy: 0.273763.\n",
      "Iteration 13838: Policy loss: 0.551550. Value loss: 16.451241. Entropy: 0.262495.\n",
      "Iteration 13839: Policy loss: 0.335712. Value loss: 12.640949. Entropy: 0.257747.\n",
      "episode: 5780   score: 420.0  epsilon: 1.0    steps: 788  evaluation reward: 296.7\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13840: Policy loss: 2.464979. Value loss: 36.756794. Entropy: 0.242982.\n",
      "Iteration 13841: Policy loss: 2.008564. Value loss: 19.044283. Entropy: 0.230554.\n",
      "Iteration 13842: Policy loss: 1.818358. Value loss: 15.234416. Entropy: 0.230354.\n",
      "episode: 5781   score: 210.0  epsilon: 1.0    steps: 26  evaluation reward: 294.7\n",
      "episode: 5782   score: 210.0  epsilon: 1.0    steps: 579  evaluation reward: 291.2\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13843: Policy loss: 1.024132. Value loss: 24.354004. Entropy: 0.246829.\n",
      "Iteration 13844: Policy loss: 0.824362. Value loss: 16.491796. Entropy: 0.265210.\n",
      "Iteration 13845: Policy loss: 0.945882. Value loss: 12.066807. Entropy: 0.248685.\n",
      "episode: 5783   score: 210.0  epsilon: 1.0    steps: 665  evaluation reward: 290.9\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13846: Policy loss: 3.124267. Value loss: 34.686893. Entropy: 0.142295.\n",
      "Iteration 13847: Policy loss: 3.176559. Value loss: 17.115204. Entropy: 0.128717.\n",
      "Iteration 13848: Policy loss: 3.262867. Value loss: 15.452803. Entropy: 0.139423.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13849: Policy loss: 0.642073. Value loss: 34.598038. Entropy: 0.262373.\n",
      "Iteration 13850: Policy loss: 0.550264. Value loss: 20.053400. Entropy: 0.260804.\n",
      "Iteration 13851: Policy loss: 0.966296. Value loss: 13.108957. Entropy: 0.264039.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13852: Policy loss: 0.736938. Value loss: 34.466549. Entropy: 0.145522.\n",
      "Iteration 13853: Policy loss: 0.738924. Value loss: 15.574543. Entropy: 0.152693.\n",
      "Iteration 13854: Policy loss: 0.542996. Value loss: 9.979741. Entropy: 0.153501.\n",
      "episode: 5784   score: 285.0  epsilon: 1.0    steps: 423  evaluation reward: 291.65\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13855: Policy loss: 0.557884. Value loss: 21.138079. Entropy: 0.109486.\n",
      "Iteration 13856: Policy loss: 0.623760. Value loss: 13.929892. Entropy: 0.112027.\n",
      "Iteration 13857: Policy loss: 0.667184. Value loss: 9.553121. Entropy: 0.111640.\n",
      "episode: 5785   score: 220.0  epsilon: 1.0    steps: 633  evaluation reward: 291.3\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13858: Policy loss: -3.137283. Value loss: 58.498039. Entropy: 0.163088.\n",
      "Iteration 13859: Policy loss: -3.292482. Value loss: 30.821335. Entropy: 0.142923.\n",
      "Iteration 13860: Policy loss: -3.139518. Value loss: 21.382942. Entropy: 0.147793.\n",
      "episode: 5786   score: 630.0  epsilon: 1.0    steps: 311  evaluation reward: 291.15\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13861: Policy loss: -0.312212. Value loss: 24.950718. Entropy: 0.208304.\n",
      "Iteration 13862: Policy loss: -0.046964. Value loss: 15.395384. Entropy: 0.203780.\n",
      "Iteration 13863: Policy loss: -0.442684. Value loss: 10.514355. Entropy: 0.209823.\n",
      "episode: 5787   score: 395.0  epsilon: 1.0    steps: 94  evaluation reward: 290.55\n",
      "episode: 5788   score: 335.0  epsilon: 1.0    steps: 851  evaluation reward: 291.6\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13864: Policy loss: 1.379077. Value loss: 27.944912. Entropy: 0.149670.\n",
      "Iteration 13865: Policy loss: 1.311246. Value loss: 15.643317. Entropy: 0.188396.\n",
      "Iteration 13866: Policy loss: 1.173796. Value loss: 15.231231. Entropy: 0.180862.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13867: Policy loss: 0.346940. Value loss: 24.298038. Entropy: 0.209968.\n",
      "Iteration 13868: Policy loss: 0.395717. Value loss: 13.593050. Entropy: 0.218485.\n",
      "Iteration 13869: Policy loss: 0.221660. Value loss: 10.485273. Entropy: 0.225041.\n",
      "episode: 5789   score: 250.0  epsilon: 1.0    steps: 404  evaluation reward: 292.3\n",
      "episode: 5790   score: 315.0  epsilon: 1.0    steps: 760  evaluation reward: 292.15\n",
      "episode: 5791   score: 475.0  epsilon: 1.0    steps: 955  evaluation reward: 294.3\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13870: Policy loss: 0.770604. Value loss: 41.948029. Entropy: 0.228711.\n",
      "Iteration 13871: Policy loss: 0.531012. Value loss: 22.682838. Entropy: 0.223297.\n",
      "Iteration 13872: Policy loss: 1.041984. Value loss: 16.227512. Entropy: 0.227751.\n",
      "episode: 5792   score: 380.0  epsilon: 1.0    steps: 170  evaluation reward: 296.0\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13873: Policy loss: -0.049731. Value loss: 25.568417. Entropy: 0.292275.\n",
      "Iteration 13874: Policy loss: -0.039518. Value loss: 17.134451. Entropy: 0.290976.\n",
      "Iteration 13875: Policy loss: -0.012906. Value loss: 14.236720. Entropy: 0.310433.\n",
      "episode: 5793   score: 230.0  epsilon: 1.0    steps: 344  evaluation reward: 295.2\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13876: Policy loss: 3.774475. Value loss: 41.603252. Entropy: 0.328480.\n",
      "Iteration 13877: Policy loss: 3.457243. Value loss: 21.641548. Entropy: 0.340349.\n",
      "Iteration 13878: Policy loss: 3.808199. Value loss: 17.906464. Entropy: 0.362903.\n",
      "episode: 5794   score: 380.0  epsilon: 1.0    steps: 628  evaluation reward: 297.1\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13879: Policy loss: -1.156383. Value loss: 37.631012. Entropy: 0.246936.\n",
      "Iteration 13880: Policy loss: -1.665410. Value loss: 20.484224. Entropy: 0.259937.\n",
      "Iteration 13881: Policy loss: -1.636886. Value loss: 18.183733. Entropy: 0.251550.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13882: Policy loss: -4.208892. Value loss: 317.433533. Entropy: 0.176285.\n",
      "Iteration 13883: Policy loss: -2.674570. Value loss: 110.870087. Entropy: 0.182900.\n",
      "Iteration 13884: Policy loss: -3.500441. Value loss: 100.692963. Entropy: 0.162943.\n",
      "episode: 5795   score: 210.0  epsilon: 1.0    steps: 428  evaluation reward: 297.1\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13885: Policy loss: -2.003311. Value loss: 239.915817. Entropy: 0.227035.\n",
      "Iteration 13886: Policy loss: -1.859360. Value loss: 141.488312. Entropy: 0.218961.\n",
      "Iteration 13887: Policy loss: -2.611485. Value loss: 166.024673. Entropy: 0.215240.\n",
      "episode: 5796   score: 650.0  epsilon: 1.0    steps: 802  evaluation reward: 300.7\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13888: Policy loss: 1.330084. Value loss: 37.769711. Entropy: 0.112358.\n",
      "Iteration 13889: Policy loss: 1.181877. Value loss: 16.415934. Entropy: 0.131875.\n",
      "Iteration 13890: Policy loss: 1.287232. Value loss: 14.538627. Entropy: 0.122355.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13891: Policy loss: 1.150813. Value loss: 35.203175. Entropy: 0.105475.\n",
      "Iteration 13892: Policy loss: 1.077172. Value loss: 18.972286. Entropy: 0.109481.\n",
      "Iteration 13893: Policy loss: 1.520415. Value loss: 15.381794. Entropy: 0.108143.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13894: Policy loss: 2.783561. Value loss: 34.891655. Entropy: 0.159209.\n",
      "Iteration 13895: Policy loss: 2.810296. Value loss: 15.157505. Entropy: 0.182307.\n",
      "Iteration 13896: Policy loss: 2.982336. Value loss: 10.310699. Entropy: 0.162425.\n",
      "episode: 5797   score: 560.0  epsilon: 1.0    steps: 171  evaluation reward: 303.1\n",
      "episode: 5798   score: 355.0  epsilon: 1.0    steps: 934  evaluation reward: 303.9\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13897: Policy loss: 3.166516. Value loss: 31.385754. Entropy: 0.215634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13898: Policy loss: 3.421375. Value loss: 11.425086. Entropy: 0.198831.\n",
      "Iteration 13899: Policy loss: 3.248226. Value loss: 10.028264. Entropy: 0.208876.\n",
      "episode: 5799   score: 320.0  epsilon: 1.0    steps: 608  evaluation reward: 301.2\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13900: Policy loss: -1.077812. Value loss: 187.395645. Entropy: 0.093729.\n",
      "Iteration 13901: Policy loss: -1.344424. Value loss: 96.917496. Entropy: 0.129937.\n",
      "Iteration 13902: Policy loss: -1.227372. Value loss: 41.562641. Entropy: 0.150556.\n",
      "episode: 5800   score: 260.0  epsilon: 1.0    steps: 415  evaluation reward: 301.65\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13903: Policy loss: -4.463035. Value loss: 353.465912. Entropy: 0.270908.\n",
      "Iteration 13904: Policy loss: -3.934826. Value loss: 180.452942. Entropy: 0.260849.\n",
      "Iteration 13905: Policy loss: -4.442397. Value loss: 144.970337. Entropy: 0.271840.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13906: Policy loss: 1.808235. Value loss: 104.886047. Entropy: 0.283643.\n",
      "Iteration 13907: Policy loss: 1.907949. Value loss: 36.328789. Entropy: 0.278917.\n",
      "Iteration 13908: Policy loss: 1.889205. Value loss: 23.847109. Entropy: 0.286632.\n",
      "now time :  2019-02-25 22:59:53.024585\n",
      "episode: 5801   score: 785.0  epsilon: 1.0    steps: 101  evaluation reward: 307.1\n",
      "episode: 5802   score: 465.0  epsilon: 1.0    steps: 697  evaluation reward: 309.75\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13909: Policy loss: 2.809245. Value loss: 87.036880. Entropy: 0.174626.\n",
      "Iteration 13910: Policy loss: 2.672237. Value loss: 43.275906. Entropy: 0.159653.\n",
      "Iteration 13911: Policy loss: 2.817299. Value loss: 27.726423. Entropy: 0.164737.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13912: Policy loss: 4.094466. Value loss: 67.876991. Entropy: 0.224564.\n",
      "Iteration 13913: Policy loss: 4.067315. Value loss: 28.248220. Entropy: 0.200268.\n",
      "Iteration 13914: Policy loss: 4.419372. Value loss: 22.488920. Entropy: 0.214776.\n",
      "episode: 5803   score: 265.0  epsilon: 1.0    steps: 160  evaluation reward: 310.6\n",
      "episode: 5804   score: 465.0  epsilon: 1.0    steps: 286  evaluation reward: 313.05\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13915: Policy loss: 1.144228. Value loss: 38.998966. Entropy: 0.207452.\n",
      "Iteration 13916: Policy loss: 1.089445. Value loss: 21.833574. Entropy: 0.220028.\n",
      "Iteration 13917: Policy loss: 1.363340. Value loss: 17.060747. Entropy: 0.216186.\n",
      "episode: 5805   score: 225.0  epsilon: 1.0    steps: 532  evaluation reward: 313.35\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13918: Policy loss: -0.147897. Value loss: 46.256615. Entropy: 0.245414.\n",
      "Iteration 13919: Policy loss: -0.276269. Value loss: 26.009245. Entropy: 0.237958.\n",
      "Iteration 13920: Policy loss: -0.346919. Value loss: 19.253620. Entropy: 0.247322.\n",
      "episode: 5806   score: 240.0  epsilon: 1.0    steps: 907  evaluation reward: 314.2\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13921: Policy loss: -0.652011. Value loss: 33.668751. Entropy: 0.259675.\n",
      "Iteration 13922: Policy loss: -0.662241. Value loss: 22.660875. Entropy: 0.253743.\n",
      "Iteration 13923: Policy loss: -0.794881. Value loss: 16.577389. Entropy: 0.246529.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13924: Policy loss: 1.511061. Value loss: 35.171864. Entropy: 0.221667.\n",
      "Iteration 13925: Policy loss: 1.118965. Value loss: 17.806881. Entropy: 0.205198.\n",
      "Iteration 13926: Policy loss: 1.149889. Value loss: 13.299089. Entropy: 0.207484.\n",
      "episode: 5807   score: 240.0  epsilon: 1.0    steps: 100  evaluation reward: 314.35\n",
      "episode: 5808   score: 285.0  epsilon: 1.0    steps: 689  evaluation reward: 316.0\n",
      "episode: 5809   score: 490.0  epsilon: 1.0    steps: 871  evaluation reward: 318.65\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13927: Policy loss: -0.263846. Value loss: 40.065952. Entropy: 0.264631.\n",
      "Iteration 13928: Policy loss: 0.000629. Value loss: 23.632036. Entropy: 0.257678.\n",
      "Iteration 13929: Policy loss: -0.248260. Value loss: 17.154421. Entropy: 0.267247.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13930: Policy loss: 0.438837. Value loss: 23.700569. Entropy: 0.257031.\n",
      "Iteration 13931: Policy loss: 0.259697. Value loss: 14.123429. Entropy: 0.253792.\n",
      "Iteration 13932: Policy loss: 0.235082. Value loss: 10.412019. Entropy: 0.239256.\n",
      "episode: 5810   score: 310.0  epsilon: 1.0    steps: 327  evaluation reward: 318.85\n",
      "episode: 5811   score: 440.0  epsilon: 1.0    steps: 387  evaluation reward: 320.95\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13933: Policy loss: -0.729656. Value loss: 25.562946. Entropy: 0.233703.\n",
      "Iteration 13934: Policy loss: -0.671100. Value loss: 17.431870. Entropy: 0.221437.\n",
      "Iteration 13935: Policy loss: -0.745099. Value loss: 14.853574. Entropy: 0.224276.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13936: Policy loss: 1.593465. Value loss: 20.539661. Entropy: 0.232299.\n",
      "Iteration 13937: Policy loss: 1.646272. Value loss: 11.340278. Entropy: 0.233265.\n",
      "Iteration 13938: Policy loss: 1.785645. Value loss: 10.080595. Entropy: 0.234044.\n",
      "episode: 5812   score: 260.0  epsilon: 1.0    steps: 928  evaluation reward: 319.15\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13939: Policy loss: 0.088136. Value loss: 37.652142. Entropy: 0.216787.\n",
      "Iteration 13940: Policy loss: 0.028883. Value loss: 20.242004. Entropy: 0.218265.\n",
      "Iteration 13941: Policy loss: 0.313572. Value loss: 14.451014. Entropy: 0.223685.\n",
      "episode: 5813   score: 250.0  epsilon: 1.0    steps: 521  evaluation reward: 317.8\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13942: Policy loss: -1.098098. Value loss: 27.647547. Entropy: 0.275502.\n",
      "Iteration 13943: Policy loss: -1.061823. Value loss: 15.133954. Entropy: 0.267625.\n",
      "Iteration 13944: Policy loss: -0.861156. Value loss: 14.331588. Entropy: 0.248195.\n",
      "episode: 5814   score: 260.0  epsilon: 1.0    steps: 894  evaluation reward: 318.3\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13945: Policy loss: 2.091885. Value loss: 36.837036. Entropy: 0.278667.\n",
      "Iteration 13946: Policy loss: 2.322527. Value loss: 19.733780. Entropy: 0.277339.\n",
      "Iteration 13947: Policy loss: 2.509154. Value loss: 18.219156. Entropy: 0.287703.\n",
      "episode: 5815   score: 315.0  epsilon: 1.0    steps: 21  evaluation reward: 319.6\n",
      "episode: 5816   score: 310.0  epsilon: 1.0    steps: 227  evaluation reward: 320.6\n",
      "episode: 5817   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 320.35\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13948: Policy loss: 1.444196. Value loss: 22.318615. Entropy: 0.341313.\n",
      "Iteration 13949: Policy loss: 1.269235. Value loss: 16.441919. Entropy: 0.340331.\n",
      "Iteration 13950: Policy loss: 1.221173. Value loss: 12.789873. Entropy: 0.342501.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13951: Policy loss: -1.715377. Value loss: 35.262520. Entropy: 0.187439.\n",
      "Iteration 13952: Policy loss: -1.943434. Value loss: 20.666527. Entropy: 0.176811.\n",
      "Iteration 13953: Policy loss: -1.745948. Value loss: 15.431968. Entropy: 0.175668.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13954: Policy loss: -2.854177. Value loss: 170.959991. Entropy: 0.194748.\n",
      "Iteration 13955: Policy loss: -3.191634. Value loss: 176.590393. Entropy: 0.177677.\n",
      "Iteration 13956: Policy loss: -2.885690. Value loss: 99.256866. Entropy: 0.171724.\n",
      "episode: 5818   score: 340.0  epsilon: 1.0    steps: 1019  evaluation reward: 319.1\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13957: Policy loss: 0.024847. Value loss: 29.893988. Entropy: 0.230913.\n",
      "Iteration 13958: Policy loss: 0.141523. Value loss: 21.251593. Entropy: 0.239692.\n",
      "Iteration 13959: Policy loss: -0.214924. Value loss: 15.750984. Entropy: 0.223328.\n",
      "episode: 5819   score: 505.0  epsilon: 1.0    steps: 593  evaluation reward: 321.6\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13960: Policy loss: 2.319671. Value loss: 34.237022. Entropy: 0.210718.\n",
      "Iteration 13961: Policy loss: 2.624376. Value loss: 18.511147. Entropy: 0.206186.\n",
      "Iteration 13962: Policy loss: 2.469267. Value loss: 16.160339. Entropy: 0.224241.\n",
      "episode: 5820   score: 405.0  epsilon: 1.0    steps: 278  evaluation reward: 323.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13963: Policy loss: -0.845750. Value loss: 17.022091. Entropy: 0.169289.\n",
      "Iteration 13964: Policy loss: -0.905762. Value loss: 14.460450. Entropy: 0.170466.\n",
      "Iteration 13965: Policy loss: -0.843679. Value loss: 11.571091. Entropy: 0.169448.\n",
      "episode: 5821   score: 295.0  epsilon: 1.0    steps: 64  evaluation reward: 324.65\n",
      "episode: 5822   score: 275.0  epsilon: 1.0    steps: 500  evaluation reward: 325.55\n",
      "episode: 5823   score: 260.0  epsilon: 1.0    steps: 815  evaluation reward: 326.6\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13966: Policy loss: 1.087360. Value loss: 28.207127. Entropy: 0.324585.\n",
      "Iteration 13967: Policy loss: 1.126216. Value loss: 17.529114. Entropy: 0.318245.\n",
      "Iteration 13968: Policy loss: 1.043654. Value loss: 15.021281. Entropy: 0.319386.\n",
      "episode: 5824   score: 350.0  epsilon: 1.0    steps: 732  evaluation reward: 328.0\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13969: Policy loss: -3.231277. Value loss: 258.281128. Entropy: 0.321959.\n",
      "Iteration 13970: Policy loss: -3.688773. Value loss: 97.773964. Entropy: 0.301984.\n",
      "Iteration 13971: Policy loss: -3.404010. Value loss: 49.803829. Entropy: 0.290809.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13972: Policy loss: -0.822277. Value loss: 30.592825. Entropy: 0.211614.\n",
      "Iteration 13973: Policy loss: -0.932237. Value loss: 17.845688. Entropy: 0.230175.\n",
      "Iteration 13974: Policy loss: -0.933520. Value loss: 18.543964. Entropy: 0.208317.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13975: Policy loss: 1.224430. Value loss: 39.774044. Entropy: 0.248128.\n",
      "Iteration 13976: Policy loss: 1.248292. Value loss: 19.924212. Entropy: 0.235130.\n",
      "Iteration 13977: Policy loss: 0.908579. Value loss: 16.861830. Entropy: 0.243058.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13978: Policy loss: -1.283866. Value loss: 57.233826. Entropy: 0.283906.\n",
      "Iteration 13979: Policy loss: -1.062726. Value loss: 30.065397. Entropy: 0.272823.\n",
      "Iteration 13980: Policy loss: -1.030223. Value loss: 19.725500. Entropy: 0.273744.\n",
      "episode: 5825   score: 515.0  epsilon: 1.0    steps: 146  evaluation reward: 330.6\n",
      "episode: 5826   score: 485.0  epsilon: 1.0    steps: 546  evaluation reward: 333.6\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13981: Policy loss: 1.287924. Value loss: 28.689581. Entropy: 0.148026.\n",
      "Iteration 13982: Policy loss: 1.331991. Value loss: 20.123589. Entropy: 0.150227.\n",
      "Iteration 13983: Policy loss: 1.283067. Value loss: 14.953267. Entropy: 0.146113.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13984: Policy loss: 1.914181. Value loss: 28.376104. Entropy: 0.096192.\n",
      "Iteration 13985: Policy loss: 1.802110. Value loss: 11.550811. Entropy: 0.089710.\n",
      "Iteration 13986: Policy loss: 1.743995. Value loss: 10.419420. Entropy: 0.089562.\n",
      "episode: 5827   score: 440.0  epsilon: 1.0    steps: 278  evaluation reward: 334.25\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13987: Policy loss: 3.016298. Value loss: 19.394939. Entropy: 0.256758.\n",
      "Iteration 13988: Policy loss: 3.056223. Value loss: 11.039603. Entropy: 0.242368.\n",
      "Iteration 13989: Policy loss: 3.199549. Value loss: 12.085855. Entropy: 0.255742.\n",
      "episode: 5828   score: 360.0  epsilon: 1.0    steps: 866  evaluation reward: 334.25\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13990: Policy loss: 0.907200. Value loss: 232.802109. Entropy: 0.163443.\n",
      "Iteration 13991: Policy loss: 0.644286. Value loss: 85.978973. Entropy: 0.159355.\n",
      "Iteration 13992: Policy loss: 0.922299. Value loss: 54.997437. Entropy: 0.163743.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13993: Policy loss: 3.756536. Value loss: 62.771942. Entropy: 0.159284.\n",
      "Iteration 13994: Policy loss: 3.616410. Value loss: 22.907192. Entropy: 0.165999.\n",
      "Iteration 13995: Policy loss: 3.724059. Value loss: 14.492839. Entropy: 0.148021.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13996: Policy loss: 0.764153. Value loss: 11.126490. Entropy: 0.050033.\n",
      "Iteration 13997: Policy loss: 0.880083. Value loss: 6.392976. Entropy: 0.051877.\n",
      "Iteration 13998: Policy loss: 0.885453. Value loss: 5.103644. Entropy: 0.057014.\n",
      "episode: 5829   score: 260.0  epsilon: 1.0    steps: 555  evaluation reward: 335.05\n",
      "episode: 5830   score: 340.0  epsilon: 1.0    steps: 1024  evaluation reward: 335.0\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13999: Policy loss: -1.867504. Value loss: 232.097137. Entropy: 0.216039.\n",
      "Iteration 14000: Policy loss: -1.871235. Value loss: 156.175064. Entropy: 0.219562.\n",
      "Iteration 14001: Policy loss: -1.608645. Value loss: 93.611671. Entropy: 0.227761.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14002: Policy loss: -0.564041. Value loss: 39.590538. Entropy: 0.087184.\n",
      "Iteration 14003: Policy loss: -0.349176. Value loss: 21.623032. Entropy: 0.086727.\n",
      "Iteration 14004: Policy loss: -0.561456. Value loss: 15.029960. Entropy: 0.086380.\n",
      "episode: 5831   score: 435.0  epsilon: 1.0    steps: 204  evaluation reward: 337.05\n",
      "episode: 5832   score: 260.0  epsilon: 1.0    steps: 307  evaluation reward: 337.8\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14005: Policy loss: 0.609572. Value loss: 20.048071. Entropy: 0.182899.\n",
      "Iteration 14006: Policy loss: 0.415032. Value loss: 14.170428. Entropy: 0.191585.\n",
      "Iteration 14007: Policy loss: 0.556855. Value loss: 11.808375. Entropy: 0.195865.\n",
      "episode: 5833   score: 670.0  epsilon: 1.0    steps: 71  evaluation reward: 340.4\n",
      "episode: 5834   score: 295.0  epsilon: 1.0    steps: 469  evaluation reward: 341.8\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14008: Policy loss: -0.342353. Value loss: 241.182816. Entropy: 0.257590.\n",
      "Iteration 14009: Policy loss: 0.261425. Value loss: 123.010475. Entropy: 0.257069.\n",
      "Iteration 14010: Policy loss: 0.060689. Value loss: 95.828163. Entropy: 0.251195.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14011: Policy loss: 1.399529. Value loss: 28.145004. Entropy: 0.267322.\n",
      "Iteration 14012: Policy loss: 1.266226. Value loss: 19.291555. Entropy: 0.265675.\n",
      "Iteration 14013: Policy loss: 1.456325. Value loss: 14.844349. Entropy: 0.276058.\n",
      "episode: 5835   score: 415.0  epsilon: 1.0    steps: 648  evaluation reward: 344.4\n",
      "episode: 5836   score: 285.0  epsilon: 1.0    steps: 887  evaluation reward: 343.6\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14014: Policy loss: 2.367134. Value loss: 54.624588. Entropy: 0.267178.\n",
      "Iteration 14015: Policy loss: 2.440863. Value loss: 29.153973. Entropy: 0.268032.\n",
      "Iteration 14016: Policy loss: 2.274438. Value loss: 17.788311. Entropy: 0.261823.\n",
      "episode: 5837   score: 180.0  epsilon: 1.0    steps: 530  evaluation reward: 343.3\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14017: Policy loss: -0.343290. Value loss: 21.245054. Entropy: 0.200051.\n",
      "Iteration 14018: Policy loss: -0.562874. Value loss: 15.118568. Entropy: 0.194502.\n",
      "Iteration 14019: Policy loss: -0.385057. Value loss: 11.476190. Entropy: 0.199913.\n",
      "episode: 5838   score: 475.0  epsilon: 1.0    steps: 1023  evaluation reward: 345.75\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14020: Policy loss: 0.672079. Value loss: 28.984531. Entropy: 0.239583.\n",
      "Iteration 14021: Policy loss: 0.373512. Value loss: 16.912899. Entropy: 0.245193.\n",
      "Iteration 14022: Policy loss: 0.555890. Value loss: 13.149806. Entropy: 0.243774.\n",
      "episode: 5839   score: 265.0  epsilon: 1.0    steps: 224  evaluation reward: 347.35\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14023: Policy loss: -0.066936. Value loss: 34.120323. Entropy: 0.260787.\n",
      "Iteration 14024: Policy loss: -0.009012. Value loss: 18.397860. Entropy: 0.249482.\n",
      "Iteration 14025: Policy loss: -0.015627. Value loss: 14.775117. Entropy: 0.239320.\n",
      "episode: 5840   score: 255.0  epsilon: 1.0    steps: 470  evaluation reward: 348.55\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14026: Policy loss: -2.212503. Value loss: 198.027176. Entropy: 0.230611.\n",
      "Iteration 14027: Policy loss: -1.828579. Value loss: 102.066719. Entropy: 0.231846.\n",
      "Iteration 14028: Policy loss: -2.267237. Value loss: 74.583046. Entropy: 0.225885.\n",
      "episode: 5841   score: 295.0  epsilon: 1.0    steps: 16  evaluation reward: 349.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14029: Policy loss: 0.436743. Value loss: 31.607908. Entropy: 0.185202.\n",
      "Iteration 14030: Policy loss: 0.739126. Value loss: 21.557793. Entropy: 0.188229.\n",
      "Iteration 14031: Policy loss: 0.719623. Value loss: 15.778063. Entropy: 0.183357.\n",
      "episode: 5842   score: 375.0  epsilon: 1.0    steps: 731  evaluation reward: 350.85\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14032: Policy loss: 1.026710. Value loss: 38.282368. Entropy: 0.168983.\n",
      "Iteration 14033: Policy loss: 0.955900. Value loss: 17.972439. Entropy: 0.185027.\n",
      "Iteration 14034: Policy loss: 1.055892. Value loss: 14.563956. Entropy: 0.172739.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14035: Policy loss: -0.470979. Value loss: 36.304893. Entropy: 0.084223.\n",
      "Iteration 14036: Policy loss: -0.785769. Value loss: 22.386400. Entropy: 0.080251.\n",
      "Iteration 14037: Policy loss: -0.727419. Value loss: 15.931402. Entropy: 0.083510.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14038: Policy loss: 1.846150. Value loss: 34.021969. Entropy: 0.158398.\n",
      "Iteration 14039: Policy loss: 2.156464. Value loss: 20.441296. Entropy: 0.151520.\n",
      "Iteration 14040: Policy loss: 1.897964. Value loss: 14.806160. Entropy: 0.155668.\n",
      "episode: 5843   score: 260.0  epsilon: 1.0    steps: 231  evaluation reward: 351.75\n",
      "episode: 5844   score: 695.0  epsilon: 1.0    steps: 376  evaluation reward: 356.6\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14041: Policy loss: -0.239717. Value loss: 40.695137. Entropy: 0.196152.\n",
      "Iteration 14042: Policy loss: -0.034976. Value loss: 20.141800. Entropy: 0.205425.\n",
      "Iteration 14043: Policy loss: -0.299180. Value loss: 18.248447. Entropy: 0.216806.\n",
      "episode: 5845   score: 280.0  epsilon: 1.0    steps: 70  evaluation reward: 354.95\n",
      "episode: 5846   score: 275.0  epsilon: 1.0    steps: 395  evaluation reward: 355.85\n",
      "episode: 5847   score: 505.0  epsilon: 1.0    steps: 784  evaluation reward: 358.0\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14044: Policy loss: 0.179701. Value loss: 25.402348. Entropy: 0.185176.\n",
      "Iteration 14045: Policy loss: 0.233814. Value loss: 17.186617. Entropy: 0.198322.\n",
      "Iteration 14046: Policy loss: 0.327565. Value loss: 14.693898. Entropy: 0.194904.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14047: Policy loss: -0.187529. Value loss: 25.522942. Entropy: 0.241423.\n",
      "Iteration 14048: Policy loss: 0.017729. Value loss: 18.364759. Entropy: 0.245928.\n",
      "Iteration 14049: Policy loss: -0.226516. Value loss: 13.942162. Entropy: 0.259610.\n",
      "episode: 5848   score: 530.0  epsilon: 1.0    steps: 604  evaluation reward: 358.05\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14050: Policy loss: -3.180585. Value loss: 224.029419. Entropy: 0.156045.\n",
      "Iteration 14051: Policy loss: -2.623791. Value loss: 99.647758. Entropy: 0.166532.\n",
      "Iteration 14052: Policy loss: -2.394312. Value loss: 83.850578. Entropy: 0.168163.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14053: Policy loss: -0.281180. Value loss: 16.110878. Entropy: 0.127925.\n",
      "Iteration 14054: Policy loss: -0.121874. Value loss: 11.229850. Entropy: 0.120777.\n",
      "Iteration 14055: Policy loss: -0.162483. Value loss: 8.816583. Entropy: 0.122251.\n",
      "episode: 5849   score: 320.0  epsilon: 1.0    steps: 707  evaluation reward: 353.15\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14056: Policy loss: 1.577737. Value loss: 29.885342. Entropy: 0.195875.\n",
      "Iteration 14057: Policy loss: 1.648809. Value loss: 15.522545. Entropy: 0.188856.\n",
      "Iteration 14058: Policy loss: 1.414145. Value loss: 12.440474. Entropy: 0.226039.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14059: Policy loss: -2.004553. Value loss: 154.248947. Entropy: 0.174607.\n",
      "Iteration 14060: Policy loss: -1.206133. Value loss: 71.077454. Entropy: 0.169870.\n",
      "Iteration 14061: Policy loss: -2.097253. Value loss: 46.450790. Entropy: 0.152606.\n",
      "episode: 5850   score: 285.0  epsilon: 1.0    steps: 194  evaluation reward: 354.4\n",
      "now time :  2019-02-25 23:02:44.344762\n",
      "episode: 5851   score: 280.0  epsilon: 1.0    steps: 339  evaluation reward: 354.2\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14062: Policy loss: 0.177239. Value loss: 29.396889. Entropy: 0.151203.\n",
      "Iteration 14063: Policy loss: 0.145551. Value loss: 17.326578. Entropy: 0.151954.\n",
      "Iteration 14064: Policy loss: 0.248122. Value loss: 14.176373. Entropy: 0.167146.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14065: Policy loss: 2.511323. Value loss: 94.999428. Entropy: 0.156273.\n",
      "Iteration 14066: Policy loss: 2.605519. Value loss: 29.625885. Entropy: 0.149408.\n",
      "Iteration 14067: Policy loss: 2.121593. Value loss: 22.047186. Entropy: 0.174185.\n",
      "episode: 5852   score: 250.0  epsilon: 1.0    steps: 617  evaluation reward: 354.6\n",
      "episode: 5853   score: 590.0  epsilon: 1.0    steps: 916  evaluation reward: 358.5\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14068: Policy loss: 1.987802. Value loss: 40.651844. Entropy: 0.220057.\n",
      "Iteration 14069: Policy loss: 2.042910. Value loss: 22.973984. Entropy: 0.221431.\n",
      "Iteration 14070: Policy loss: 2.096186. Value loss: 15.560519. Entropy: 0.212896.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14071: Policy loss: -3.656013. Value loss: 252.718246. Entropy: 0.195870.\n",
      "Iteration 14072: Policy loss: -3.941388. Value loss: 182.337112. Entropy: 0.185179.\n",
      "Iteration 14073: Policy loss: -3.771183. Value loss: 113.518539. Entropy: 0.184083.\n",
      "episode: 5854   score: 515.0  epsilon: 1.0    steps: 452  evaluation reward: 360.75\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14074: Policy loss: 3.081299. Value loss: 37.405334. Entropy: 0.111062.\n",
      "Iteration 14075: Policy loss: 3.102579. Value loss: 21.647024. Entropy: 0.117528.\n",
      "Iteration 14076: Policy loss: 3.283870. Value loss: 18.928543. Entropy: 0.119655.\n",
      "episode: 5855   score: 580.0  epsilon: 1.0    steps: 87  evaluation reward: 361.8\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14077: Policy loss: 0.608742. Value loss: 25.064438. Entropy: 0.155022.\n",
      "Iteration 14078: Policy loss: 0.583501. Value loss: 14.447351. Entropy: 0.134356.\n",
      "Iteration 14079: Policy loss: 0.785581. Value loss: 10.699990. Entropy: 0.155508.\n",
      "episode: 5856   score: 280.0  epsilon: 1.0    steps: 142  evaluation reward: 361.95\n",
      "episode: 5857   score: 285.0  epsilon: 1.0    steps: 345  evaluation reward: 362.35\n",
      "episode: 5858   score: 745.0  epsilon: 1.0    steps: 883  evaluation reward: 367.65\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14080: Policy loss: 3.272534. Value loss: 60.441284. Entropy: 0.188379.\n",
      "Iteration 14081: Policy loss: 3.318152. Value loss: 34.871761. Entropy: 0.205401.\n",
      "Iteration 14082: Policy loss: 3.110521. Value loss: 28.678528. Entropy: 0.195007.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14083: Policy loss: 1.409588. Value loss: 41.790226. Entropy: 0.195944.\n",
      "Iteration 14084: Policy loss: 1.567446. Value loss: 27.657459. Entropy: 0.194991.\n",
      "Iteration 14085: Policy loss: 1.510398. Value loss: 22.182856. Entropy: 0.204773.\n",
      "episode: 5859   score: 455.0  epsilon: 1.0    steps: 668  evaluation reward: 369.6\n",
      "episode: 5860   score: 260.0  epsilon: 1.0    steps: 924  evaluation reward: 369.65\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14086: Policy loss: 1.078330. Value loss: 27.455946. Entropy: 0.177021.\n",
      "Iteration 14087: Policy loss: 0.979952. Value loss: 17.390638. Entropy: 0.173421.\n",
      "Iteration 14088: Policy loss: 1.172039. Value loss: 12.974734. Entropy: 0.180356.\n",
      "episode: 5861   score: 350.0  epsilon: 1.0    steps: 594  evaluation reward: 370.15\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14089: Policy loss: 0.345391. Value loss: 33.396965. Entropy: 0.154940.\n",
      "Iteration 14090: Policy loss: 0.337104. Value loss: 24.474117. Entropy: 0.153525.\n",
      "Iteration 14091: Policy loss: 0.561068. Value loss: 22.526102. Entropy: 0.164812.\n",
      "episode: 5862   score: 260.0  epsilon: 1.0    steps: 505  evaluation reward: 369.35\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14092: Policy loss: -0.561174. Value loss: 212.055542. Entropy: 0.297363.\n",
      "Iteration 14093: Policy loss: -0.786155. Value loss: 155.845001. Entropy: 0.290606.\n",
      "Iteration 14094: Policy loss: -0.875914. Value loss: 111.419609. Entropy: 0.292387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5863   score: 460.0  epsilon: 1.0    steps: 114  evaluation reward: 371.35\n",
      "episode: 5864   score: 210.0  epsilon: 1.0    steps: 337  evaluation reward: 368.45\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14095: Policy loss: 0.511058. Value loss: 27.348551. Entropy: 0.234136.\n",
      "Iteration 14096: Policy loss: 0.515590. Value loss: 16.534611. Entropy: 0.240212.\n",
      "Iteration 14097: Policy loss: 0.251885. Value loss: 14.934755. Entropy: 0.213290.\n",
      "episode: 5865   score: 265.0  epsilon: 1.0    steps: 164  evaluation reward: 368.5\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14098: Policy loss: -0.613490. Value loss: 35.773010. Entropy: 0.138087.\n",
      "Iteration 14099: Policy loss: -0.783291. Value loss: 19.909460. Entropy: 0.143144.\n",
      "Iteration 14100: Policy loss: -0.748253. Value loss: 14.709541. Entropy: 0.146294.\n",
      "episode: 5866   score: 240.0  epsilon: 1.0    steps: 796  evaluation reward: 367.35\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14101: Policy loss: 1.834584. Value loss: 21.812725. Entropy: 0.175133.\n",
      "Iteration 14102: Policy loss: 1.979733. Value loss: 12.204719. Entropy: 0.186638.\n",
      "Iteration 14103: Policy loss: 1.759877. Value loss: 9.835667. Entropy: 0.209484.\n",
      "episode: 5867   score: 220.0  epsilon: 1.0    steps: 970  evaluation reward: 364.2\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14104: Policy loss: 0.649027. Value loss: 14.784851. Entropy: 0.303552.\n",
      "Iteration 14105: Policy loss: 0.832434. Value loss: 11.917302. Entropy: 0.315312.\n",
      "Iteration 14106: Policy loss: 0.766784. Value loss: 7.818653. Entropy: 0.323056.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14107: Policy loss: -0.099912. Value loss: 27.914072. Entropy: 0.296167.\n",
      "Iteration 14108: Policy loss: -0.503536. Value loss: 15.731865. Entropy: 0.300669.\n",
      "Iteration 14109: Policy loss: -0.207629. Value loss: 13.276865. Entropy: 0.286401.\n",
      "episode: 5868   score: 240.0  epsilon: 1.0    steps: 502  evaluation reward: 361.15\n",
      "episode: 5869   score: 330.0  epsilon: 1.0    steps: 571  evaluation reward: 361.4\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14110: Policy loss: 0.221159. Value loss: 27.324718. Entropy: 0.256843.\n",
      "Iteration 14111: Policy loss: 0.384697. Value loss: 17.410934. Entropy: 0.284548.\n",
      "Iteration 14112: Policy loss: 0.316799. Value loss: 14.204899. Entropy: 0.284564.\n",
      "episode: 5870   score: 285.0  epsilon: 1.0    steps: 316  evaluation reward: 359.55\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14113: Policy loss: -0.909759. Value loss: 22.787182. Entropy: 0.251859.\n",
      "Iteration 14114: Policy loss: -0.872172. Value loss: 16.466208. Entropy: 0.280211.\n",
      "Iteration 14115: Policy loss: -0.825555. Value loss: 13.091796. Entropy: 0.278169.\n",
      "episode: 5871   score: 390.0  epsilon: 1.0    steps: 94  evaluation reward: 359.5\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14116: Policy loss: 2.719584. Value loss: 32.846264. Entropy: 0.258894.\n",
      "Iteration 14117: Policy loss: 2.789754. Value loss: 21.806944. Entropy: 0.249326.\n",
      "Iteration 14118: Policy loss: 2.733346. Value loss: 17.382708. Entropy: 0.267246.\n",
      "episode: 5872   score: 315.0  epsilon: 1.0    steps: 669  evaluation reward: 358.05\n",
      "episode: 5873   score: 265.0  epsilon: 1.0    steps: 828  evaluation reward: 357.0\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14119: Policy loss: 1.826817. Value loss: 25.950438. Entropy: 0.320831.\n",
      "Iteration 14120: Policy loss: 1.872797. Value loss: 15.595456. Entropy: 0.304545.\n",
      "Iteration 14121: Policy loss: 1.649685. Value loss: 12.758513. Entropy: 0.303603.\n",
      "episode: 5874   score: 260.0  epsilon: 1.0    steps: 979  evaluation reward: 356.0\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14122: Policy loss: -0.017218. Value loss: 29.905956. Entropy: 0.340898.\n",
      "Iteration 14123: Policy loss: -0.277113. Value loss: 21.754335. Entropy: 0.355164.\n",
      "Iteration 14124: Policy loss: 0.045607. Value loss: 15.420684. Entropy: 0.336912.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14125: Policy loss: 0.992246. Value loss: 29.120089. Entropy: 0.271187.\n",
      "Iteration 14126: Policy loss: 0.907623. Value loss: 16.003633. Entropy: 0.271657.\n",
      "Iteration 14127: Policy loss: 0.727341. Value loss: 11.520324. Entropy: 0.271260.\n",
      "episode: 5875   score: 470.0  epsilon: 1.0    steps: 223  evaluation reward: 357.25\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14128: Policy loss: 1.463934. Value loss: 29.433207. Entropy: 0.416812.\n",
      "Iteration 14129: Policy loss: 1.514377. Value loss: 18.021088. Entropy: 0.420053.\n",
      "Iteration 14130: Policy loss: 1.484135. Value loss: 11.933475. Entropy: 0.438524.\n",
      "episode: 5876   score: 260.0  epsilon: 1.0    steps: 406  evaluation reward: 357.25\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14131: Policy loss: -0.506029. Value loss: 46.639191. Entropy: 0.450624.\n",
      "Iteration 14132: Policy loss: -0.500085. Value loss: 22.400703. Entropy: 0.441410.\n",
      "Iteration 14133: Policy loss: -0.414266. Value loss: 16.050386. Entropy: 0.446062.\n",
      "episode: 5877   score: 240.0  epsilon: 1.0    steps: 55  evaluation reward: 353.9\n",
      "episode: 5878   score: 325.0  epsilon: 1.0    steps: 288  evaluation reward: 353.5\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14134: Policy loss: 3.090334. Value loss: 29.697353. Entropy: 0.341238.\n",
      "Iteration 14135: Policy loss: 2.953273. Value loss: 18.090950. Entropy: 0.356014.\n",
      "Iteration 14136: Policy loss: 3.379426. Value loss: 12.087922. Entropy: 0.341107.\n",
      "episode: 5879   score: 255.0  epsilon: 1.0    steps: 763  evaluation reward: 352.3\n",
      "episode: 5880   score: 245.0  epsilon: 1.0    steps: 892  evaluation reward: 350.55\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14137: Policy loss: 0.116341. Value loss: 25.912823. Entropy: 0.361713.\n",
      "Iteration 14138: Policy loss: 0.065924. Value loss: 16.715771. Entropy: 0.377232.\n",
      "Iteration 14139: Policy loss: 0.207292. Value loss: 11.384548. Entropy: 0.385596.\n",
      "episode: 5881   score: 395.0  epsilon: 1.0    steps: 529  evaluation reward: 352.4\n",
      "episode: 5882   score: 225.0  epsilon: 1.0    steps: 971  evaluation reward: 352.55\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14140: Policy loss: 0.999982. Value loss: 22.844618. Entropy: 0.248999.\n",
      "Iteration 14141: Policy loss: 1.314138. Value loss: 18.082012. Entropy: 0.279641.\n",
      "Iteration 14142: Policy loss: 0.965924. Value loss: 12.506652. Entropy: 0.281479.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14143: Policy loss: -2.400908. Value loss: 237.159454. Entropy: 0.273606.\n",
      "Iteration 14144: Policy loss: -2.533476. Value loss: 169.108887. Entropy: 0.282044.\n",
      "Iteration 14145: Policy loss: -2.029118. Value loss: 117.786911. Entropy: 0.256336.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14146: Policy loss: 1.330070. Value loss: 21.227999. Entropy: 0.246208.\n",
      "Iteration 14147: Policy loss: 1.273282. Value loss: 14.403438. Entropy: 0.244580.\n",
      "Iteration 14148: Policy loss: 1.130291. Value loss: 12.849123. Entropy: 0.241434.\n",
      "episode: 5883   score: 245.0  epsilon: 1.0    steps: 132  evaluation reward: 352.9\n",
      "episode: 5884   score: 240.0  epsilon: 1.0    steps: 480  evaluation reward: 352.45\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14149: Policy loss: 0.388093. Value loss: 21.735046. Entropy: 0.195033.\n",
      "Iteration 14150: Policy loss: 0.183343. Value loss: 14.224041. Entropy: 0.197475.\n",
      "Iteration 14151: Policy loss: 0.352176. Value loss: 8.793985. Entropy: 0.202579.\n",
      "episode: 5885   score: 230.0  epsilon: 1.0    steps: 343  evaluation reward: 352.55\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14152: Policy loss: 1.400742. Value loss: 29.614925. Entropy: 0.273755.\n",
      "Iteration 14153: Policy loss: 1.289309. Value loss: 14.891352. Entropy: 0.268427.\n",
      "Iteration 14154: Policy loss: 1.646183. Value loss: 10.958847. Entropy: 0.272375.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14155: Policy loss: -0.498946. Value loss: 36.857170. Entropy: 0.254121.\n",
      "Iteration 14156: Policy loss: -1.188495. Value loss: 17.778683. Entropy: 0.222544.\n",
      "Iteration 14157: Policy loss: -0.891044. Value loss: 13.468616. Entropy: 0.236567.\n",
      "episode: 5886   score: 290.0  epsilon: 1.0    steps: 626  evaluation reward: 349.15\n",
      "episode: 5887   score: 285.0  epsilon: 1.0    steps: 664  evaluation reward: 348.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5888   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 346.8\n",
      "episode: 5889   score: 225.0  epsilon: 1.0    steps: 976  evaluation reward: 346.55\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14158: Policy loss: -0.513461. Value loss: 22.683619. Entropy: 0.243295.\n",
      "Iteration 14159: Policy loss: -0.540229. Value loss: 16.767958. Entropy: 0.239930.\n",
      "Iteration 14160: Policy loss: -0.545014. Value loss: 13.018690. Entropy: 0.256845.\n",
      "episode: 5890   score: 630.0  epsilon: 1.0    steps: 121  evaluation reward: 349.7\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14161: Policy loss: -0.251478. Value loss: 19.053757. Entropy: 0.363841.\n",
      "Iteration 14162: Policy loss: -0.370506. Value loss: 13.510262. Entropy: 0.365271.\n",
      "Iteration 14163: Policy loss: -0.178977. Value loss: 10.518099. Entropy: 0.349931.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14164: Policy loss: -1.474700. Value loss: 21.911882. Entropy: 0.211157.\n",
      "Iteration 14165: Policy loss: -1.493995. Value loss: 11.883705. Entropy: 0.223909.\n",
      "Iteration 14166: Policy loss: -1.455043. Value loss: 9.523191. Entropy: 0.215389.\n",
      "episode: 5891   score: 260.0  epsilon: 1.0    steps: 199  evaluation reward: 347.55\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14167: Policy loss: 1.281371. Value loss: 22.383167. Entropy: 0.377441.\n",
      "Iteration 14168: Policy loss: 1.351858. Value loss: 12.690824. Entropy: 0.374391.\n",
      "Iteration 14169: Policy loss: 1.448327. Value loss: 9.082717. Entropy: 0.391961.\n",
      "episode: 5892   score: 230.0  epsilon: 1.0    steps: 408  evaluation reward: 346.05\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14170: Policy loss: -0.852287. Value loss: 18.190702. Entropy: 0.362694.\n",
      "Iteration 14171: Policy loss: -0.750274. Value loss: 11.877047. Entropy: 0.351357.\n",
      "Iteration 14172: Policy loss: -0.953290. Value loss: 12.165996. Entropy: 0.365706.\n",
      "episode: 5893   score: 305.0  epsilon: 1.0    steps: 283  evaluation reward: 346.8\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14173: Policy loss: -0.340330. Value loss: 18.434433. Entropy: 0.270731.\n",
      "Iteration 14174: Policy loss: -0.424106. Value loss: 10.068575. Entropy: 0.272434.\n",
      "Iteration 14175: Policy loss: -0.363700. Value loss: 9.209406. Entropy: 0.285738.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14176: Policy loss: -0.747907. Value loss: 26.112761. Entropy: 0.365707.\n",
      "Iteration 14177: Policy loss: -0.899401. Value loss: 12.043659. Entropy: 0.363140.\n",
      "Iteration 14178: Policy loss: -1.047740. Value loss: 9.869740. Entropy: 0.371164.\n",
      "episode: 5894   score: 315.0  epsilon: 1.0    steps: 850  evaluation reward: 346.15\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14179: Policy loss: 0.420908. Value loss: 42.103085. Entropy: 0.325284.\n",
      "Iteration 14180: Policy loss: 0.358997. Value loss: 19.750908. Entropy: 0.314662.\n",
      "Iteration 14181: Policy loss: 0.585142. Value loss: 15.108222. Entropy: 0.326360.\n",
      "episode: 5895   score: 400.0  epsilon: 1.0    steps: 705  evaluation reward: 348.05\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14182: Policy loss: 1.951205. Value loss: 31.969687. Entropy: 0.347802.\n",
      "Iteration 14183: Policy loss: 1.917365. Value loss: 14.580042. Entropy: 0.339870.\n",
      "Iteration 14184: Policy loss: 2.058414. Value loss: 13.009495. Entropy: 0.395329.\n",
      "episode: 5896   score: 230.0  epsilon: 1.0    steps: 24  evaluation reward: 343.85\n",
      "episode: 5897   score: 260.0  epsilon: 1.0    steps: 246  evaluation reward: 340.85\n",
      "episode: 5898   score: 375.0  epsilon: 1.0    steps: 903  evaluation reward: 341.05\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14185: Policy loss: 2.420590. Value loss: 25.480179. Entropy: 0.381782.\n",
      "Iteration 14186: Policy loss: 2.496972. Value loss: 14.008626. Entropy: 0.378011.\n",
      "Iteration 14187: Policy loss: 2.313393. Value loss: 13.155633. Entropy: 0.389031.\n",
      "episode: 5899   score: 370.0  epsilon: 1.0    steps: 568  evaluation reward: 341.55\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14188: Policy loss: -0.827276. Value loss: 25.003395. Entropy: 0.314965.\n",
      "Iteration 14189: Policy loss: -0.638390. Value loss: 13.927919. Entropy: 0.317678.\n",
      "Iteration 14190: Policy loss: -0.754396. Value loss: 11.610008. Entropy: 0.315047.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14191: Policy loss: -0.606796. Value loss: 31.904587. Entropy: 0.343611.\n",
      "Iteration 14192: Policy loss: -0.780756. Value loss: 17.158400. Entropy: 0.350825.\n",
      "Iteration 14193: Policy loss: -0.554551. Value loss: 13.026886. Entropy: 0.349135.\n",
      "episode: 5900   score: 350.0  epsilon: 1.0    steps: 336  evaluation reward: 342.45\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14194: Policy loss: 1.010949. Value loss: 29.325239. Entropy: 0.315064.\n",
      "Iteration 14195: Policy loss: 0.846925. Value loss: 16.639557. Entropy: 0.297607.\n",
      "Iteration 14196: Policy loss: 0.868797. Value loss: 13.770983. Entropy: 0.306437.\n",
      "now time :  2019-02-25 23:05:15.000554\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 5901   score: 380.0  epsilon: 1.0    steps: 490  evaluation reward: 338.4\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14197: Policy loss: -1.596275. Value loss: 274.495331. Entropy: 0.296645.\n",
      "Iteration 14198: Policy loss: -1.131889. Value loss: 68.451683. Entropy: 0.295884.\n",
      "Iteration 14199: Policy loss: -1.701595. Value loss: 61.629921. Entropy: 0.296752.\n",
      "episode: 5902   score: 260.0  epsilon: 1.0    steps: 1018  evaluation reward: 336.35\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14200: Policy loss: 2.677928. Value loss: 19.763210. Entropy: 0.267932.\n",
      "Iteration 14201: Policy loss: 2.583036. Value loss: 10.283703. Entropy: 0.309468.\n",
      "Iteration 14202: Policy loss: 2.764234. Value loss: 8.403881. Entropy: 0.306675.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14203: Policy loss: -0.220321. Value loss: 43.823502. Entropy: 0.450738.\n",
      "Iteration 14204: Policy loss: 0.002025. Value loss: 26.598240. Entropy: 0.447535.\n",
      "Iteration 14205: Policy loss: -0.134053. Value loss: 23.092943. Entropy: 0.464278.\n",
      "episode: 5903   score: 360.0  epsilon: 1.0    steps: 39  evaluation reward: 337.3\n",
      "episode: 5904   score: 260.0  epsilon: 1.0    steps: 562  evaluation reward: 335.25\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14206: Policy loss: 3.311887. Value loss: 62.438755. Entropy: 0.389250.\n",
      "Iteration 14207: Policy loss: 3.274870. Value loss: 26.534037. Entropy: 0.378959.\n",
      "Iteration 14208: Policy loss: 2.896751. Value loss: 18.900417. Entropy: 0.383651.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14209: Policy loss: 3.610663. Value loss: 23.441200. Entropy: 0.375731.\n",
      "Iteration 14210: Policy loss: 3.632869. Value loss: 13.100061. Entropy: 0.389655.\n",
      "Iteration 14211: Policy loss: 3.542292. Value loss: 10.564592. Entropy: 0.361347.\n",
      "episode: 5905   score: 315.0  epsilon: 1.0    steps: 247  evaluation reward: 336.15\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14212: Policy loss: 2.461429. Value loss: 39.491894. Entropy: 0.228979.\n",
      "Iteration 14213: Policy loss: 2.606609. Value loss: 22.712883. Entropy: 0.228628.\n",
      "Iteration 14214: Policy loss: 2.683661. Value loss: 14.937629. Entropy: 0.230736.\n",
      "episode: 5906   score: 240.0  epsilon: 1.0    steps: 495  evaluation reward: 336.15\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14215: Policy loss: 0.483823. Value loss: 19.874447. Entropy: 0.268463.\n",
      "Iteration 14216: Policy loss: 0.486614. Value loss: 12.766906. Entropy: 0.255451.\n",
      "Iteration 14217: Policy loss: 0.393322. Value loss: 9.568537. Entropy: 0.245924.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14218: Policy loss: -2.024046. Value loss: 186.650177. Entropy: 0.359831.\n",
      "Iteration 14219: Policy loss: -2.291478. Value loss: 56.068409. Entropy: 0.327759.\n",
      "Iteration 14220: Policy loss: -2.120918. Value loss: 52.866665. Entropy: 0.334757.\n",
      "episode: 5907   score: 260.0  epsilon: 1.0    steps: 902  evaluation reward: 336.35\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14221: Policy loss: -1.401858. Value loss: 23.964457. Entropy: 0.280769.\n",
      "Iteration 14222: Policy loss: -1.216262. Value loss: 14.355158. Entropy: 0.292834.\n",
      "Iteration 14223: Policy loss: -1.336343. Value loss: 10.664446. Entropy: 0.291504.\n",
      "episode: 5908   score: 275.0  epsilon: 1.0    steps: 52  evaluation reward: 336.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5909   score: 510.0  epsilon: 1.0    steps: 675  evaluation reward: 336.45\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14224: Policy loss: -2.268697. Value loss: 255.839218. Entropy: 0.354924.\n",
      "Iteration 14225: Policy loss: -2.419009. Value loss: 112.169464. Entropy: 0.300988.\n",
      "Iteration 14226: Policy loss: -1.258945. Value loss: 54.322639. Entropy: 0.292817.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14227: Policy loss: 1.694480. Value loss: 44.340130. Entropy: 0.244531.\n",
      "Iteration 14228: Policy loss: 2.243868. Value loss: 23.836460. Entropy: 0.249487.\n",
      "Iteration 14229: Policy loss: 2.234396. Value loss: 16.930494. Entropy: 0.248246.\n",
      "episode: 5910   score: 265.0  epsilon: 1.0    steps: 232  evaluation reward: 336.0\n",
      "episode: 5911   score: 275.0  epsilon: 1.0    steps: 612  evaluation reward: 334.35\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14230: Policy loss: 0.071416. Value loss: 35.785782. Entropy: 0.286840.\n",
      "Iteration 14231: Policy loss: 0.151756. Value loss: 22.218273. Entropy: 0.284340.\n",
      "Iteration 14232: Policy loss: -0.229370. Value loss: 17.348598. Entropy: 0.290937.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14233: Policy loss: 0.807312. Value loss: 38.792984. Entropy: 0.255984.\n",
      "Iteration 14234: Policy loss: 0.766404. Value loss: 24.897911. Entropy: 0.238771.\n",
      "Iteration 14235: Policy loss: 0.519424. Value loss: 21.048899. Entropy: 0.258586.\n",
      "episode: 5912   score: 475.0  epsilon: 1.0    steps: 332  evaluation reward: 336.5\n",
      "episode: 5913   score: 350.0  epsilon: 1.0    steps: 511  evaluation reward: 337.5\n",
      "episode: 5914   score: 260.0  epsilon: 1.0    steps: 1000  evaluation reward: 337.5\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14236: Policy loss: -0.351201. Value loss: 32.721279. Entropy: 0.407700.\n",
      "Iteration 14237: Policy loss: -0.146801. Value loss: 18.450096. Entropy: 0.409078.\n",
      "Iteration 14238: Policy loss: -0.277108. Value loss: 17.169048. Entropy: 0.405722.\n",
      "episode: 5915   score: 265.0  epsilon: 1.0    steps: 743  evaluation reward: 337.0\n",
      "episode: 5916   score: 935.0  epsilon: 1.0    steps: 840  evaluation reward: 343.25\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14239: Policy loss: 0.143550. Value loss: 21.962683. Entropy: 0.390059.\n",
      "Iteration 14240: Policy loss: -0.306899. Value loss: 19.837921. Entropy: 0.370756.\n",
      "Iteration 14241: Policy loss: 0.084065. Value loss: 14.107492. Entropy: 0.364099.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14242: Policy loss: 0.629133. Value loss: 26.778397. Entropy: 0.414830.\n",
      "Iteration 14243: Policy loss: 0.670824. Value loss: 17.446363. Entropy: 0.422485.\n",
      "Iteration 14244: Policy loss: 0.526053. Value loss: 12.443709. Entropy: 0.408964.\n",
      "episode: 5917   score: 295.0  epsilon: 1.0    steps: 22  evaluation reward: 343.6\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14245: Policy loss: 0.075838. Value loss: 21.714304. Entropy: 0.269580.\n",
      "Iteration 14246: Policy loss: 0.005015. Value loss: 12.894787. Entropy: 0.280782.\n",
      "Iteration 14247: Policy loss: 0.263085. Value loss: 9.658355. Entropy: 0.281313.\n",
      "episode: 5918   score: 260.0  epsilon: 1.0    steps: 573  evaluation reward: 342.8\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14248: Policy loss: 0.814762. Value loss: 25.178362. Entropy: 0.247444.\n",
      "Iteration 14249: Policy loss: 0.640921. Value loss: 14.633180. Entropy: 0.246966.\n",
      "Iteration 14250: Policy loss: 0.745319. Value loss: 12.335234. Entropy: 0.249631.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14251: Policy loss: -0.136699. Value loss: 29.911886. Entropy: 0.193542.\n",
      "Iteration 14252: Policy loss: -0.069451. Value loss: 14.084797. Entropy: 0.198543.\n",
      "Iteration 14253: Policy loss: -0.259523. Value loss: 10.943334. Entropy: 0.212985.\n",
      "episode: 5919   score: 285.0  epsilon: 1.0    steps: 1000  evaluation reward: 340.6\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14254: Policy loss: 1.663800. Value loss: 25.378529. Entropy: 0.310792.\n",
      "Iteration 14255: Policy loss: 1.560245. Value loss: 15.548181. Entropy: 0.293938.\n",
      "Iteration 14256: Policy loss: 1.556910. Value loss: 13.579050. Entropy: 0.312198.\n",
      "episode: 5920   score: 410.0  epsilon: 1.0    steps: 184  evaluation reward: 340.65\n",
      "episode: 5921   score: 245.0  epsilon: 1.0    steps: 403  evaluation reward: 340.15\n",
      "episode: 5922   score: 260.0  epsilon: 1.0    steps: 720  evaluation reward: 340.0\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14257: Policy loss: -1.796137. Value loss: 24.974905. Entropy: 0.319553.\n",
      "Iteration 14258: Policy loss: -1.678065. Value loss: 12.731901. Entropy: 0.302895.\n",
      "Iteration 14259: Policy loss: -2.059930. Value loss: 11.467253. Entropy: 0.304863.\n",
      "episode: 5923   score: 270.0  epsilon: 1.0    steps: 302  evaluation reward: 340.1\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14260: Policy loss: 4.742542. Value loss: 36.609829. Entropy: 0.433629.\n",
      "Iteration 14261: Policy loss: 4.506929. Value loss: 18.133646. Entropy: 0.441815.\n",
      "Iteration 14262: Policy loss: 4.696252. Value loss: 18.153723. Entropy: 0.453271.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14263: Policy loss: 0.675290. Value loss: 26.083591. Entropy: 0.430929.\n",
      "Iteration 14264: Policy loss: 0.516250. Value loss: 17.648468. Entropy: 0.416477.\n",
      "Iteration 14265: Policy loss: 0.555369. Value loss: 14.573117. Entropy: 0.422027.\n",
      "episode: 5924   score: 280.0  epsilon: 1.0    steps: 552  evaluation reward: 339.4\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14266: Policy loss: 0.363695. Value loss: 26.074944. Entropy: 0.228663.\n",
      "Iteration 14267: Policy loss: 0.305960. Value loss: 16.529593. Entropy: 0.230560.\n",
      "Iteration 14268: Policy loss: 0.390156. Value loss: 12.631946. Entropy: 0.232759.\n",
      "episode: 5925   score: 390.0  epsilon: 1.0    steps: 33  evaluation reward: 338.15\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14269: Policy loss: -1.499029. Value loss: 196.781372. Entropy: 0.277658.\n",
      "Iteration 14270: Policy loss: -1.402788. Value loss: 107.322662. Entropy: 0.260419.\n",
      "Iteration 14271: Policy loss: -2.334938. Value loss: 62.622295. Entropy: 0.273725.\n",
      "episode: 5926   score: 245.0  epsilon: 1.0    steps: 469  evaluation reward: 335.75\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14272: Policy loss: 1.206708. Value loss: 28.777466. Entropy: 0.284689.\n",
      "Iteration 14273: Policy loss: 1.333073. Value loss: 13.101457. Entropy: 0.281904.\n",
      "Iteration 14274: Policy loss: 1.385814. Value loss: 10.374242. Entropy: 0.281273.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14275: Policy loss: -1.728494. Value loss: 29.767193. Entropy: 0.119838.\n",
      "Iteration 14276: Policy loss: -1.639377. Value loss: 15.296717. Entropy: 0.117563.\n",
      "Iteration 14277: Policy loss: -1.828060. Value loss: 13.568465. Entropy: 0.132179.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14278: Policy loss: 0.871086. Value loss: 30.326427. Entropy: 0.260372.\n",
      "Iteration 14279: Policy loss: 0.764667. Value loss: 16.413996. Entropy: 0.270810.\n",
      "Iteration 14280: Policy loss: 0.764007. Value loss: 12.346466. Entropy: 0.267194.\n",
      "episode: 5927   score: 420.0  epsilon: 1.0    steps: 604  evaluation reward: 335.55\n",
      "episode: 5928   score: 495.0  epsilon: 1.0    steps: 815  evaluation reward: 336.9\n",
      "episode: 5929   score: 385.0  epsilon: 1.0    steps: 931  evaluation reward: 338.15\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14281: Policy loss: -0.413112. Value loss: 25.854170. Entropy: 0.239857.\n",
      "Iteration 14282: Policy loss: -0.655686. Value loss: 14.469330. Entropy: 0.231667.\n",
      "Iteration 14283: Policy loss: -0.469903. Value loss: 11.484492. Entropy: 0.238416.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14284: Policy loss: -0.852669. Value loss: 197.953629. Entropy: 0.264421.\n",
      "Iteration 14285: Policy loss: -0.717736. Value loss: 138.911499. Entropy: 0.258376.\n",
      "Iteration 14286: Policy loss: -0.892888. Value loss: 87.619255. Entropy: 0.257570.\n",
      "episode: 5930   score: 490.0  epsilon: 1.0    steps: 641  evaluation reward: 339.65\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14287: Policy loss: -0.468851. Value loss: 35.207680. Entropy: 0.269746.\n",
      "Iteration 14288: Policy loss: -0.488639. Value loss: 17.871881. Entropy: 0.248145.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14289: Policy loss: -0.380399. Value loss: 15.472457. Entropy: 0.263423.\n",
      "episode: 5931   score: 260.0  epsilon: 1.0    steps: 429  evaluation reward: 337.9\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14290: Policy loss: 0.273325. Value loss: 25.810801. Entropy: 0.101142.\n",
      "Iteration 14291: Policy loss: 0.543668. Value loss: 15.234637. Entropy: 0.113151.\n",
      "Iteration 14292: Policy loss: 0.436465. Value loss: 12.108715. Entropy: 0.102537.\n",
      "episode: 5932   score: 390.0  epsilon: 1.0    steps: 50  evaluation reward: 339.2\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14293: Policy loss: -1.834109. Value loss: 203.555176. Entropy: 0.294284.\n",
      "Iteration 14294: Policy loss: -2.170357. Value loss: 114.979996. Entropy: 0.298590.\n",
      "Iteration 14295: Policy loss: -2.067190. Value loss: 86.845352. Entropy: 0.303744.\n",
      "episode: 5933   score: 660.0  epsilon: 1.0    steps: 142  evaluation reward: 339.1\n",
      "episode: 5934   score: 280.0  epsilon: 1.0    steps: 873  evaluation reward: 338.95\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14296: Policy loss: 2.525913. Value loss: 55.826099. Entropy: 0.401826.\n",
      "Iteration 14297: Policy loss: 2.559517. Value loss: 24.584663. Entropy: 0.395888.\n",
      "Iteration 14298: Policy loss: 2.459544. Value loss: 19.337410. Entropy: 0.393502.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14299: Policy loss: -1.463863. Value loss: 38.296780. Entropy: 0.245305.\n",
      "Iteration 14300: Policy loss: -1.473802. Value loss: 27.520926. Entropy: 0.230734.\n",
      "Iteration 14301: Policy loss: -1.535904. Value loss: 20.035566. Entropy: 0.220608.\n",
      "episode: 5935   score: 225.0  epsilon: 1.0    steps: 727  evaluation reward: 337.05\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14302: Policy loss: -1.662097. Value loss: 219.310852. Entropy: 0.164129.\n",
      "Iteration 14303: Policy loss: -1.648644. Value loss: 77.578293. Entropy: 0.158618.\n",
      "Iteration 14304: Policy loss: -1.502608. Value loss: 93.139473. Entropy: 0.151044.\n",
      "episode: 5936   score: 555.0  epsilon: 1.0    steps: 284  evaluation reward: 339.75\n",
      "episode: 5937   score: 360.0  epsilon: 1.0    steps: 514  evaluation reward: 341.55\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14305: Policy loss: 0.211364. Value loss: 28.741144. Entropy: 0.196678.\n",
      "Iteration 14306: Policy loss: 0.250889. Value loss: 15.206043. Entropy: 0.190482.\n",
      "Iteration 14307: Policy loss: 0.146492. Value loss: 10.114551. Entropy: 0.188738.\n",
      "episode: 5938   score: 210.0  epsilon: 1.0    steps: 77  evaluation reward: 338.9\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14308: Policy loss: -4.190739. Value loss: 262.617798. Entropy: 0.259360.\n",
      "Iteration 14309: Policy loss: -3.823693. Value loss: 150.263382. Entropy: 0.246787.\n",
      "Iteration 14310: Policy loss: -3.907108. Value loss: 130.634918. Entropy: 0.243838.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14311: Policy loss: 3.776028. Value loss: 57.863701. Entropy: 0.200801.\n",
      "Iteration 14312: Policy loss: 3.563496. Value loss: 21.161037. Entropy: 0.190164.\n",
      "Iteration 14313: Policy loss: 3.618438. Value loss: 14.857618. Entropy: 0.185492.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14314: Policy loss: 1.210621. Value loss: 17.142492. Entropy: 0.116001.\n",
      "Iteration 14315: Policy loss: 1.365613. Value loss: 9.106812. Entropy: 0.130208.\n",
      "Iteration 14316: Policy loss: 1.531610. Value loss: 7.409141. Entropy: 0.133834.\n",
      "episode: 5939   score: 475.0  epsilon: 1.0    steps: 1004  evaluation reward: 341.0\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14317: Policy loss: 0.591541. Value loss: 38.073090. Entropy: 0.095778.\n",
      "Iteration 14318: Policy loss: 0.586939. Value loss: 20.962040. Entropy: 0.112807.\n",
      "Iteration 14319: Policy loss: 0.723186. Value loss: 15.291557. Entropy: 0.117786.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14320: Policy loss: -1.869031. Value loss: 313.636841. Entropy: 0.126925.\n",
      "Iteration 14321: Policy loss: -1.840758. Value loss: 183.673370. Entropy: 0.121872.\n",
      "Iteration 14322: Policy loss: -1.945623. Value loss: 144.183746. Entropy: 0.123303.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14323: Policy loss: 1.181086. Value loss: 21.651361. Entropy: 0.175156.\n",
      "Iteration 14324: Policy loss: 1.222697. Value loss: 14.214508. Entropy: 0.171387.\n",
      "Iteration 14325: Policy loss: 1.292660. Value loss: 11.709052. Entropy: 0.170595.\n",
      "episode: 5940   score: 420.0  epsilon: 1.0    steps: 1  evaluation reward: 342.65\n",
      "episode: 5941   score: 275.0  epsilon: 1.0    steps: 730  evaluation reward: 342.45\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14326: Policy loss: 0.276989. Value loss: 53.122047. Entropy: 0.118297.\n",
      "Iteration 14327: Policy loss: -0.112653. Value loss: 24.510021. Entropy: 0.110258.\n",
      "Iteration 14328: Policy loss: 0.704372. Value loss: 17.159464. Entropy: 0.116835.\n",
      "episode: 5942   score: 345.0  epsilon: 1.0    steps: 324  evaluation reward: 342.15\n",
      "episode: 5943   score: 460.0  epsilon: 1.0    steps: 392  evaluation reward: 344.15\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14329: Policy loss: -3.445055. Value loss: 258.593719. Entropy: 0.174012.\n",
      "Iteration 14330: Policy loss: -4.103621. Value loss: 177.791382. Entropy: 0.173587.\n",
      "Iteration 14331: Policy loss: -3.823213. Value loss: 120.244980. Entropy: 0.175691.\n",
      "episode: 5944   score: 485.0  epsilon: 1.0    steps: 216  evaluation reward: 342.05\n",
      "episode: 5945   score: 705.0  epsilon: 1.0    steps: 869  evaluation reward: 346.3\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14332: Policy loss: -0.501110. Value loss: 42.676929. Entropy: 0.165469.\n",
      "Iteration 14333: Policy loss: 0.054811. Value loss: 24.170742. Entropy: 0.166584.\n",
      "Iteration 14334: Policy loss: -0.018488. Value loss: 19.312403. Entropy: 0.155930.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14335: Policy loss: 1.432583. Value loss: 157.625977. Entropy: 0.138998.\n",
      "Iteration 14336: Policy loss: 0.391007. Value loss: 57.906582. Entropy: 0.133175.\n",
      "Iteration 14337: Policy loss: 0.761942. Value loss: 33.589657. Entropy: 0.135876.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14338: Policy loss: -1.006160. Value loss: 98.185104. Entropy: 0.084054.\n",
      "Iteration 14339: Policy loss: -1.011262. Value loss: 65.045273. Entropy: 0.089247.\n",
      "Iteration 14340: Policy loss: -0.919513. Value loss: 50.860268. Entropy: 0.090423.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14341: Policy loss: -2.557103. Value loss: 63.493069. Entropy: 0.231215.\n",
      "Iteration 14342: Policy loss: -2.697660. Value loss: 34.917953. Entropy: 0.226612.\n",
      "Iteration 14343: Policy loss: -2.616283. Value loss: 28.180408. Entropy: 0.228649.\n",
      "episode: 5946   score: 290.0  epsilon: 1.0    steps: 580  evaluation reward: 346.45\n",
      "episode: 5947   score: 215.0  epsilon: 1.0    steps: 656  evaluation reward: 343.55\n",
      "episode: 5948   score: 585.0  epsilon: 1.0    steps: 1023  evaluation reward: 344.1\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14344: Policy loss: 6.380875. Value loss: 134.284210. Entropy: 0.336462.\n",
      "Iteration 14345: Policy loss: 5.749202. Value loss: 43.274685. Entropy: 0.346493.\n",
      "Iteration 14346: Policy loss: 6.272061. Value loss: 26.562424. Entropy: 0.317804.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14347: Policy loss: -0.762184. Value loss: 41.504322. Entropy: 0.449945.\n",
      "Iteration 14348: Policy loss: -0.396607. Value loss: 22.675938. Entropy: 0.457899.\n",
      "Iteration 14349: Policy loss: -0.465535. Value loss: 18.804245. Entropy: 0.448715.\n",
      "episode: 5949   score: 570.0  epsilon: 1.0    steps: 126  evaluation reward: 346.6\n",
      "episode: 5950   score: 515.0  epsilon: 1.0    steps: 338  evaluation reward: 348.9\n",
      "now time :  2019-02-25 23:08:05.901368\n",
      "episode: 5951   score: 630.0  epsilon: 1.0    steps: 467  evaluation reward: 352.4\n",
      "episode: 5952   score: 245.0  epsilon: 1.0    steps: 804  evaluation reward: 352.35\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14350: Policy loss: 2.671086. Value loss: 33.279301. Entropy: 0.308383.\n",
      "Iteration 14351: Policy loss: 2.726246. Value loss: 23.637369. Entropy: 0.286548.\n",
      "Iteration 14352: Policy loss: 2.501129. Value loss: 19.345140. Entropy: 0.280722.\n",
      "episode: 5953   score: 335.0  epsilon: 1.0    steps: 255  evaluation reward: 349.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14353: Policy loss: 2.378241. Value loss: 41.983036. Entropy: 0.228134.\n",
      "Iteration 14354: Policy loss: 2.396376. Value loss: 19.233805. Entropy: 0.216522.\n",
      "Iteration 14355: Policy loss: 2.549323. Value loss: 14.999738. Entropy: 0.225978.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14356: Policy loss: 2.735402. Value loss: 36.216782. Entropy: 0.282347.\n",
      "Iteration 14357: Policy loss: 2.674175. Value loss: 25.633646. Entropy: 0.291446.\n",
      "Iteration 14358: Policy loss: 2.656276. Value loss: 19.922417. Entropy: 0.284639.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14359: Policy loss: 1.011027. Value loss: 28.431585. Entropy: 0.260566.\n",
      "Iteration 14360: Policy loss: 1.033900. Value loss: 16.448011. Entropy: 0.267174.\n",
      "Iteration 14361: Policy loss: 0.945165. Value loss: 14.526196. Entropy: 0.258332.\n",
      "episode: 5954   score: 210.0  epsilon: 1.0    steps: 914  evaluation reward: 346.75\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14362: Policy loss: -0.011656. Value loss: 45.687347. Entropy: 0.094424.\n",
      "Iteration 14363: Policy loss: 0.243756. Value loss: 20.657463. Entropy: 0.103144.\n",
      "Iteration 14364: Policy loss: -0.136443. Value loss: 14.261354. Entropy: 0.093213.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14365: Policy loss: 4.466979. Value loss: 73.181801. Entropy: 0.103409.\n",
      "Iteration 14366: Policy loss: 4.852342. Value loss: 32.348907. Entropy: 0.109911.\n",
      "Iteration 14367: Policy loss: 4.078934. Value loss: 23.586622. Entropy: 0.166109.\n",
      "episode: 5955   score: 375.0  epsilon: 1.0    steps: 578  evaluation reward: 344.7\n",
      "episode: 5956   score: 340.0  epsilon: 1.0    steps: 662  evaluation reward: 345.3\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14368: Policy loss: -0.856023. Value loss: 37.943287. Entropy: 0.141983.\n",
      "Iteration 14369: Policy loss: -0.869550. Value loss: 15.605413. Entropy: 0.155607.\n",
      "Iteration 14370: Policy loss: -0.712405. Value loss: 12.448367. Entropy: 0.158618.\n",
      "episode: 5957   score: 260.0  epsilon: 1.0    steps: 192  evaluation reward: 345.05\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14371: Policy loss: -0.987881. Value loss: 187.012253. Entropy: 0.260856.\n",
      "Iteration 14372: Policy loss: -1.365234. Value loss: 115.820549. Entropy: 0.281974.\n",
      "Iteration 14373: Policy loss: -0.413689. Value loss: 81.426651. Entropy: 0.266913.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14374: Policy loss: 0.008195. Value loss: 35.528439. Entropy: 0.346512.\n",
      "Iteration 14375: Policy loss: 0.078810. Value loss: 16.733505. Entropy: 0.334119.\n",
      "Iteration 14376: Policy loss: -0.291578. Value loss: 13.293345. Entropy: 0.332013.\n",
      "episode: 5958   score: 275.0  epsilon: 1.0    steps: 342  evaluation reward: 340.35\n",
      "episode: 5959   score: 290.0  epsilon: 1.0    steps: 496  evaluation reward: 338.7\n",
      "episode: 5960   score: 580.0  epsilon: 1.0    steps: 852  evaluation reward: 341.9\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14377: Policy loss: 4.361153. Value loss: 47.109108. Entropy: 0.321165.\n",
      "Iteration 14378: Policy loss: 4.616328. Value loss: 17.718788. Entropy: 0.301147.\n",
      "Iteration 14379: Policy loss: 4.822728. Value loss: 14.755445. Entropy: 0.303254.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14380: Policy loss: 0.392301. Value loss: 36.731171. Entropy: 0.332253.\n",
      "Iteration 14381: Policy loss: 0.401577. Value loss: 20.363560. Entropy: 0.334018.\n",
      "Iteration 14382: Policy loss: 0.171210. Value loss: 15.173625. Entropy: 0.314963.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14383: Policy loss: 2.479404. Value loss: 50.481949. Entropy: 0.264652.\n",
      "Iteration 14384: Policy loss: 2.383031. Value loss: 21.824060. Entropy: 0.256308.\n",
      "Iteration 14385: Policy loss: 2.230324. Value loss: 17.329197. Entropy: 0.248038.\n",
      "episode: 5961   score: 270.0  epsilon: 1.0    steps: 590  evaluation reward: 341.1\n",
      "episode: 5962   score: 285.0  epsilon: 1.0    steps: 651  evaluation reward: 341.35\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14386: Policy loss: 1.351835. Value loss: 27.017439. Entropy: 0.246164.\n",
      "Iteration 14387: Policy loss: 1.332637. Value loss: 14.367878. Entropy: 0.237678.\n",
      "Iteration 14388: Policy loss: 1.516569. Value loss: 13.753414. Entropy: 0.237285.\n",
      "episode: 5963   score: 240.0  epsilon: 1.0    steps: 63  evaluation reward: 339.15\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14389: Policy loss: 0.256670. Value loss: 109.741882. Entropy: 0.379332.\n",
      "Iteration 14390: Policy loss: 0.200671. Value loss: 55.455696. Entropy: 0.400312.\n",
      "Iteration 14391: Policy loss: 0.427445. Value loss: 37.238235. Entropy: 0.398991.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14392: Policy loss: -1.253376. Value loss: 36.853622. Entropy: 0.306290.\n",
      "Iteration 14393: Policy loss: -1.859949. Value loss: 18.996849. Entropy: 0.279982.\n",
      "Iteration 14394: Policy loss: -1.660403. Value loss: 18.050758. Entropy: 0.292997.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14395: Policy loss: -0.600116. Value loss: 55.912754. Entropy: 0.338306.\n",
      "Iteration 14396: Policy loss: -1.048341. Value loss: 27.039965. Entropy: 0.347055.\n",
      "Iteration 14397: Policy loss: -0.602127. Value loss: 15.560494. Entropy: 0.322858.\n",
      "episode: 5964   score: 300.0  epsilon: 1.0    steps: 783  evaluation reward: 340.05\n",
      "episode: 5965   score: 425.0  epsilon: 1.0    steps: 981  evaluation reward: 341.65\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14398: Policy loss: 2.079948. Value loss: 25.536793. Entropy: 0.432768.\n",
      "Iteration 14399: Policy loss: 2.123370. Value loss: 11.670487. Entropy: 0.439922.\n",
      "Iteration 14400: Policy loss: 2.046572. Value loss: 9.175314. Entropy: 0.466029.\n",
      "episode: 5966   score: 335.0  epsilon: 1.0    steps: 355  evaluation reward: 342.6\n",
      "episode: 5967   score: 210.0  epsilon: 1.0    steps: 606  evaluation reward: 342.5\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14401: Policy loss: 0.844494. Value loss: 35.360275. Entropy: 0.320281.\n",
      "Iteration 14402: Policy loss: 0.755432. Value loss: 24.149305. Entropy: 0.344508.\n",
      "Iteration 14403: Policy loss: 0.936898. Value loss: 15.886828. Entropy: 0.336662.\n",
      "episode: 5968   score: 255.0  epsilon: 1.0    steps: 662  evaluation reward: 342.65\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14404: Policy loss: -0.114703. Value loss: 36.836456. Entropy: 0.347570.\n",
      "Iteration 14405: Policy loss: 0.075167. Value loss: 16.701368. Entropy: 0.341987.\n",
      "Iteration 14406: Policy loss: 0.069125. Value loss: 13.707692. Entropy: 0.339383.\n",
      "episode: 5969   score: 400.0  epsilon: 1.0    steps: 233  evaluation reward: 343.35\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14407: Policy loss: 1.437733. Value loss: 23.574871. Entropy: 0.468045.\n",
      "Iteration 14408: Policy loss: 1.312565. Value loss: 16.815779. Entropy: 0.469172.\n",
      "Iteration 14409: Policy loss: 1.372444. Value loss: 14.274071. Entropy: 0.460110.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14410: Policy loss: 2.644229. Value loss: 20.802891. Entropy: 0.428703.\n",
      "Iteration 14411: Policy loss: 2.708735. Value loss: 11.134554. Entropy: 0.446757.\n",
      "Iteration 14412: Policy loss: 2.501946. Value loss: 9.821411. Entropy: 0.469158.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14413: Policy loss: 1.335811. Value loss: 17.984886. Entropy: 0.390500.\n",
      "Iteration 14414: Policy loss: 1.125271. Value loss: 10.177629. Entropy: 0.378998.\n",
      "Iteration 14415: Policy loss: 1.242596. Value loss: 8.014540. Entropy: 0.393973.\n",
      "episode: 5970   score: 285.0  epsilon: 1.0    steps: 441  evaluation reward: 343.35\n",
      "episode: 5971   score: 240.0  epsilon: 1.0    steps: 782  evaluation reward: 341.85\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14416: Policy loss: 1.417505. Value loss: 14.887192. Entropy: 0.324882.\n",
      "Iteration 14417: Policy loss: 1.491677. Value loss: 8.894625. Entropy: 0.329390.\n",
      "Iteration 14418: Policy loss: 1.543560. Value loss: 7.059552. Entropy: 0.327179.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14419: Policy loss: 0.398723. Value loss: 32.136654. Entropy: 0.293542.\n",
      "Iteration 14420: Policy loss: 0.372660. Value loss: 10.670304. Entropy: 0.308932.\n",
      "Iteration 14421: Policy loss: 0.298873. Value loss: 10.322083. Entropy: 0.274226.\n",
      "episode: 5972   score: 325.0  epsilon: 1.0    steps: 15  evaluation reward: 341.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14422: Policy loss: -0.440938. Value loss: 10.056741. Entropy: 0.291187.\n",
      "Iteration 14423: Policy loss: -0.566495. Value loss: 7.966238. Entropy: 0.297021.\n",
      "Iteration 14424: Policy loss: -0.573848. Value loss: 6.020120. Entropy: 0.296860.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14425: Policy loss: 1.253473. Value loss: 19.230742. Entropy: 0.271853.\n",
      "Iteration 14426: Policy loss: 0.778856. Value loss: 9.768477. Entropy: 0.284350.\n",
      "Iteration 14427: Policy loss: 1.205890. Value loss: 6.777538. Entropy: 0.275501.\n",
      "episode: 5973   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 341.4\n",
      "episode: 5974   score: 235.0  epsilon: 1.0    steps: 1018  evaluation reward: 341.15\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14428: Policy loss: -0.690050. Value loss: 109.487625. Entropy: 0.249711.\n",
      "Iteration 14429: Policy loss: -0.765041. Value loss: 80.714035. Entropy: 0.248049.\n",
      "Iteration 14430: Policy loss: -0.854064. Value loss: 58.377468. Entropy: 0.256303.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14431: Policy loss: 2.277476. Value loss: 24.043907. Entropy: 0.244117.\n",
      "Iteration 14432: Policy loss: 2.220196. Value loss: 7.916920. Entropy: 0.245467.\n",
      "Iteration 14433: Policy loss: 2.215438. Value loss: 5.220724. Entropy: 0.245276.\n",
      "episode: 5975   score: 260.0  epsilon: 1.0    steps: 829  evaluation reward: 339.05\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14434: Policy loss: 2.233068. Value loss: 34.985939. Entropy: 0.178644.\n",
      "Iteration 14435: Policy loss: 2.154853. Value loss: 12.798315. Entropy: 0.179294.\n",
      "Iteration 14436: Policy loss: 2.519848. Value loss: 9.056588. Entropy: 0.192925.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14437: Policy loss: -0.607648. Value loss: 19.570938. Entropy: 0.277169.\n",
      "Iteration 14438: Policy loss: -0.269392. Value loss: 10.266690. Entropy: 0.283859.\n",
      "Iteration 14439: Policy loss: -0.151823. Value loss: 7.457503. Entropy: 0.288011.\n",
      "episode: 5976   score: 225.0  epsilon: 1.0    steps: 51  evaluation reward: 338.7\n",
      "episode: 5977   score: 290.0  epsilon: 1.0    steps: 728  evaluation reward: 339.2\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14440: Policy loss: 0.005487. Value loss: 20.022436. Entropy: 0.371460.\n",
      "Iteration 14441: Policy loss: 0.168397. Value loss: 11.068254. Entropy: 0.362396.\n",
      "Iteration 14442: Policy loss: -0.073673. Value loss: 9.229848. Entropy: 0.367545.\n",
      "episode: 5978   score: 335.0  epsilon: 1.0    steps: 344  evaluation reward: 339.3\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14443: Policy loss: 0.131812. Value loss: 19.350716. Entropy: 0.455281.\n",
      "Iteration 14444: Policy loss: 0.265252. Value loss: 11.286963. Entropy: 0.455059.\n",
      "Iteration 14445: Policy loss: 0.230502. Value loss: 10.808234. Entropy: 0.459557.\n",
      "episode: 5979   score: 415.0  epsilon: 1.0    steps: 587  evaluation reward: 340.9\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14446: Policy loss: 0.274609. Value loss: 24.871124. Entropy: 0.470318.\n",
      "Iteration 14447: Policy loss: 0.281485. Value loss: 14.402681. Entropy: 0.495746.\n",
      "Iteration 14448: Policy loss: 0.201134. Value loss: 12.106862. Entropy: 0.466197.\n",
      "episode: 5980   score: 275.0  epsilon: 1.0    steps: 214  evaluation reward: 341.2\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14449: Policy loss: 0.301530. Value loss: 22.239233. Entropy: 0.396238.\n",
      "Iteration 14450: Policy loss: 0.364104. Value loss: 10.914130. Entropy: 0.389204.\n",
      "Iteration 14451: Policy loss: 0.522449. Value loss: 8.455124. Entropy: 0.403007.\n",
      "episode: 5981   score: 315.0  epsilon: 1.0    steps: 468  evaluation reward: 340.4\n",
      "episode: 5982   score: 290.0  epsilon: 1.0    steps: 828  evaluation reward: 341.05\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14452: Policy loss: 0.684717. Value loss: 24.447731. Entropy: 0.511029.\n",
      "Iteration 14453: Policy loss: 0.733316. Value loss: 14.073101. Entropy: 0.500611.\n",
      "Iteration 14454: Policy loss: 0.646695. Value loss: 11.966249. Entropy: 0.517421.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14455: Policy loss: -1.357744. Value loss: 23.040815. Entropy: 0.495127.\n",
      "Iteration 14456: Policy loss: -1.241972. Value loss: 15.260072. Entropy: 0.483519.\n",
      "Iteration 14457: Policy loss: -1.257740. Value loss: 11.820243. Entropy: 0.471809.\n",
      "episode: 5983   score: 255.0  epsilon: 1.0    steps: 73  evaluation reward: 341.15\n",
      "episode: 5984   score: 225.0  epsilon: 1.0    steps: 665  evaluation reward: 341.0\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14458: Policy loss: -0.419028. Value loss: 27.462715. Entropy: 0.595877.\n",
      "Iteration 14459: Policy loss: -0.424500. Value loss: 16.142099. Entropy: 0.594840.\n",
      "Iteration 14460: Policy loss: -0.642133. Value loss: 12.403979. Entropy: 0.591970.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14461: Policy loss: 1.372126. Value loss: 40.336754. Entropy: 0.617021.\n",
      "Iteration 14462: Policy loss: 1.571421. Value loss: 18.464647. Entropy: 0.619759.\n",
      "Iteration 14463: Policy loss: 1.250391. Value loss: 16.085409. Entropy: 0.613647.\n",
      "episode: 5985   score: 335.0  epsilon: 1.0    steps: 339  evaluation reward: 342.05\n",
      "episode: 5986   score: 310.0  epsilon: 1.0    steps: 949  evaluation reward: 342.25\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14464: Policy loss: 0.533214. Value loss: 35.416710. Entropy: 0.566850.\n",
      "Iteration 14465: Policy loss: 0.721380. Value loss: 17.969589. Entropy: 0.567551.\n",
      "Iteration 14466: Policy loss: 0.430089. Value loss: 16.141476. Entropy: 0.568915.\n",
      "episode: 5987   score: 245.0  epsilon: 1.0    steps: 229  evaluation reward: 341.85\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14467: Policy loss: 1.467146. Value loss: 30.332058. Entropy: 0.517168.\n",
      "Iteration 14468: Policy loss: 1.531748. Value loss: 14.716825. Entropy: 0.483481.\n",
      "Iteration 14469: Policy loss: 1.295019. Value loss: 13.764124. Entropy: 0.534681.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14470: Policy loss: -0.759509. Value loss: 38.764416. Entropy: 0.516434.\n",
      "Iteration 14471: Policy loss: -1.204387. Value loss: 26.677826. Entropy: 0.504786.\n",
      "Iteration 14472: Policy loss: -0.906600. Value loss: 16.743813. Entropy: 0.509933.\n",
      "episode: 5988   score: 385.0  epsilon: 1.0    steps: 612  evaluation reward: 343.6\n",
      "episode: 5989   score: 370.0  epsilon: 1.0    steps: 795  evaluation reward: 345.05\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14473: Policy loss: -0.740496. Value loss: 25.785164. Entropy: 0.512103.\n",
      "Iteration 14474: Policy loss: -0.672357. Value loss: 11.855563. Entropy: 0.488013.\n",
      "Iteration 14475: Policy loss: -0.811793. Value loss: 11.153382. Entropy: 0.476835.\n",
      "episode: 5990   score: 360.0  epsilon: 1.0    steps: 499  evaluation reward: 342.35\n",
      "episode: 5991   score: 260.0  epsilon: 1.0    steps: 670  evaluation reward: 342.35\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14476: Policy loss: -0.474628. Value loss: 23.623878. Entropy: 0.523482.\n",
      "Iteration 14477: Policy loss: -0.507250. Value loss: 14.103464. Entropy: 0.530998.\n",
      "Iteration 14478: Policy loss: -0.535686. Value loss: 12.516208. Entropy: 0.514727.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14479: Policy loss: -0.307614. Value loss: 28.475172. Entropy: 0.504842.\n",
      "Iteration 14480: Policy loss: -0.039679. Value loss: 15.787380. Entropy: 0.528143.\n",
      "Iteration 14481: Policy loss: -0.311142. Value loss: 12.811996. Entropy: 0.514015.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14482: Policy loss: -0.160649. Value loss: 30.428513. Entropy: 0.548875.\n",
      "Iteration 14483: Policy loss: -0.519531. Value loss: 16.796270. Entropy: 0.557548.\n",
      "Iteration 14484: Policy loss: -0.287680. Value loss: 12.088277. Entropy: 0.537809.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14485: Policy loss: 0.244010. Value loss: 17.251347. Entropy: 0.583848.\n",
      "Iteration 14486: Policy loss: 0.426702. Value loss: 7.870821. Entropy: 0.564000.\n",
      "Iteration 14487: Policy loss: 0.286254. Value loss: 5.587209. Entropy: 0.549837.\n",
      "episode: 5992   score: 280.0  epsilon: 1.0    steps: 189  evaluation reward: 342.85\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14488: Policy loss: 0.929270. Value loss: 14.595320. Entropy: 0.518733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14489: Policy loss: 0.768151. Value loss: 8.271681. Entropy: 0.505083.\n",
      "Iteration 14490: Policy loss: 0.793221. Value loss: 6.381874. Entropy: 0.513273.\n",
      "episode: 5993   score: 260.0  epsilon: 1.0    steps: 755  evaluation reward: 342.4\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14491: Policy loss: 0.695440. Value loss: 24.861967. Entropy: 0.532916.\n",
      "Iteration 14492: Policy loss: 0.710054. Value loss: 13.175575. Entropy: 0.528292.\n",
      "Iteration 14493: Policy loss: 0.534054. Value loss: 10.254898. Entropy: 0.524033.\n",
      "episode: 5994   score: 315.0  epsilon: 1.0    steps: 8  evaluation reward: 342.4\n",
      "episode: 5995   score: 190.0  epsilon: 1.0    steps: 944  evaluation reward: 340.3\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14494: Policy loss: -0.139757. Value loss: 32.420620. Entropy: 0.444462.\n",
      "Iteration 14495: Policy loss: 0.024642. Value loss: 16.594629. Entropy: 0.423936.\n",
      "Iteration 14496: Policy loss: -0.189145. Value loss: 12.049762. Entropy: 0.429371.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14497: Policy loss: -3.955884. Value loss: 208.434906. Entropy: 0.453831.\n",
      "Iteration 14498: Policy loss: -4.360232. Value loss: 137.700455. Entropy: 0.473244.\n",
      "Iteration 14499: Policy loss: -4.906979. Value loss: 64.030586. Entropy: 0.439031.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14500: Policy loss: -0.642167. Value loss: 24.965118. Entropy: 0.636373.\n",
      "Iteration 14501: Policy loss: -0.440971. Value loss: 16.638313. Entropy: 0.636302.\n",
      "Iteration 14502: Policy loss: -0.546992. Value loss: 14.851183. Entropy: 0.631055.\n",
      "episode: 5996   score: 390.0  epsilon: 1.0    steps: 392  evaluation reward: 341.9\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14503: Policy loss: 3.390396. Value loss: 26.359179. Entropy: 0.568377.\n",
      "Iteration 14504: Policy loss: 3.413357. Value loss: 12.807107. Entropy: 0.569106.\n",
      "Iteration 14505: Policy loss: 3.263474. Value loss: 9.711339. Entropy: 0.581685.\n",
      "episode: 5997   score: 575.0  epsilon: 1.0    steps: 273  evaluation reward: 345.05\n",
      "episode: 5998   score: 375.0  epsilon: 1.0    steps: 518  evaluation reward: 345.05\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14506: Policy loss: 1.122603. Value loss: 22.310850. Entropy: 0.522676.\n",
      "Iteration 14507: Policy loss: 1.235238. Value loss: 13.928337. Entropy: 0.515703.\n",
      "Iteration 14508: Policy loss: 1.074518. Value loss: 12.047073. Entropy: 0.520281.\n",
      "episode: 5999   score: 335.0  epsilon: 1.0    steps: 783  evaluation reward: 344.7\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14509: Policy loss: 0.261305. Value loss: 108.209290. Entropy: 0.561437.\n",
      "Iteration 14510: Policy loss: -0.402723. Value loss: 78.160454. Entropy: 0.551130.\n",
      "Iteration 14511: Policy loss: 0.276243. Value loss: 58.594074. Entropy: 0.586961.\n",
      "episode: 6000   score: 310.0  epsilon: 1.0    steps: 9  evaluation reward: 344.3\n",
      "now time :  2019-02-25 23:11:06.333179\n",
      "episode: 6001   score: 540.0  epsilon: 1.0    steps: 255  evaluation reward: 345.9\n",
      "episode: 6002   score: 260.0  epsilon: 1.0    steps: 959  evaluation reward: 345.9\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14512: Policy loss: 1.270644. Value loss: 23.111759. Entropy: 0.511013.\n",
      "Iteration 14513: Policy loss: 1.161714. Value loss: 16.295959. Entropy: 0.497485.\n",
      "Iteration 14514: Policy loss: 1.208102. Value loss: 13.711130. Entropy: 0.516668.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14515: Policy loss: -0.926648. Value loss: 22.513550. Entropy: 0.405059.\n",
      "Iteration 14516: Policy loss: -1.215120. Value loss: 15.995792. Entropy: 0.391942.\n",
      "Iteration 14517: Policy loss: -0.971771. Value loss: 11.165547. Entropy: 0.424482.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14518: Policy loss: -1.935205. Value loss: 193.668976. Entropy: 0.477594.\n",
      "Iteration 14519: Policy loss: -1.535741. Value loss: 108.493233. Entropy: 0.463413.\n",
      "Iteration 14520: Policy loss: -2.068159. Value loss: 75.813354. Entropy: 0.484355.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14521: Policy loss: 0.512676. Value loss: 33.796227. Entropy: 0.420709.\n",
      "Iteration 14522: Policy loss: 0.368344. Value loss: 14.739855. Entropy: 0.414905.\n",
      "Iteration 14523: Policy loss: 0.487179. Value loss: 11.002581. Entropy: 0.407228.\n",
      "episode: 6003   score: 290.0  epsilon: 1.0    steps: 579  evaluation reward: 345.2\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14524: Policy loss: -2.541341. Value loss: 234.634979. Entropy: 0.390281.\n",
      "Iteration 14525: Policy loss: -2.327472. Value loss: 72.915215. Entropy: 0.381171.\n",
      "Iteration 14526: Policy loss: -2.589869. Value loss: 139.610092. Entropy: 0.368603.\n",
      "episode: 6004   score: 230.0  epsilon: 1.0    steps: 786  evaluation reward: 344.9\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14527: Policy loss: 0.400690. Value loss: 44.718948. Entropy: 0.287082.\n",
      "Iteration 14528: Policy loss: 0.158552. Value loss: 27.452208. Entropy: 0.311798.\n",
      "Iteration 14529: Policy loss: 0.224044. Value loss: 17.552395. Entropy: 0.289628.\n",
      "episode: 6005   score: 580.0  epsilon: 1.0    steps: 401  evaluation reward: 347.55\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14530: Policy loss: 2.122209. Value loss: 60.573231. Entropy: 0.326164.\n",
      "Iteration 14531: Policy loss: 2.416402. Value loss: 27.597540. Entropy: 0.303134.\n",
      "Iteration 14532: Policy loss: 2.403160. Value loss: 20.799492. Entropy: 0.326084.\n",
      "episode: 6006   score: 290.0  epsilon: 1.0    steps: 282  evaluation reward: 348.05\n",
      "episode: 6007   score: 890.0  epsilon: 1.0    steps: 764  evaluation reward: 354.35\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14533: Policy loss: 2.383905. Value loss: 70.029274. Entropy: 0.298758.\n",
      "Iteration 14534: Policy loss: 2.261020. Value loss: 33.567852. Entropy: 0.306955.\n",
      "Iteration 14535: Policy loss: 1.986523. Value loss: 26.310532. Entropy: 0.306519.\n",
      "episode: 6008   score: 415.0  epsilon: 1.0    steps: 38  evaluation reward: 355.75\n",
      "episode: 6009   score: 565.0  epsilon: 1.0    steps: 922  evaluation reward: 356.3\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14536: Policy loss: 0.388398. Value loss: 22.510687. Entropy: 0.285293.\n",
      "Iteration 14537: Policy loss: 0.404628. Value loss: 15.077885. Entropy: 0.282789.\n",
      "Iteration 14538: Policy loss: 0.429549. Value loss: 13.727552. Entropy: 0.286933.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14539: Policy loss: -0.178661. Value loss: 29.480352. Entropy: 0.360760.\n",
      "Iteration 14540: Policy loss: -0.264898. Value loss: 20.842783. Entropy: 0.369937.\n",
      "Iteration 14541: Policy loss: -0.216630. Value loss: 17.674160. Entropy: 0.364637.\n",
      "episode: 6010   score: 435.0  epsilon: 1.0    steps: 229  evaluation reward: 358.0\n",
      "episode: 6011   score: 260.0  epsilon: 1.0    steps: 631  evaluation reward: 357.85\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14542: Policy loss: 2.059812. Value loss: 31.266504. Entropy: 0.552589.\n",
      "Iteration 14543: Policy loss: 2.168234. Value loss: 21.515337. Entropy: 0.561909.\n",
      "Iteration 14544: Policy loss: 2.087621. Value loss: 15.312911. Entropy: 0.567396.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14545: Policy loss: -1.606111. Value loss: 112.953773. Entropy: 0.270481.\n",
      "Iteration 14546: Policy loss: -1.549786. Value loss: 39.210178. Entropy: 0.296177.\n",
      "Iteration 14547: Policy loss: -1.447895. Value loss: 41.338531. Entropy: 0.303835.\n",
      "episode: 6012   score: 240.0  epsilon: 1.0    steps: 408  evaluation reward: 355.5\n",
      "episode: 6013   score: 210.0  epsilon: 1.0    steps: 924  evaluation reward: 354.1\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14548: Policy loss: 0.261969. Value loss: 21.250116. Entropy: 0.394861.\n",
      "Iteration 14549: Policy loss: 0.211712. Value loss: 16.839851. Entropy: 0.396363.\n",
      "Iteration 14550: Policy loss: 0.252962. Value loss: 13.825564. Entropy: 0.396740.\n",
      "episode: 6014   score: 260.0  epsilon: 1.0    steps: 286  evaluation reward: 354.1\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14551: Policy loss: 0.779110. Value loss: 52.360703. Entropy: 0.474984.\n",
      "Iteration 14552: Policy loss: 0.823556. Value loss: 31.934313. Entropy: 0.474577.\n",
      "Iteration 14553: Policy loss: 0.710721. Value loss: 25.631004. Entropy: 0.483189.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6015   score: 410.0  epsilon: 1.0    steps: 866  evaluation reward: 355.55\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14554: Policy loss: 2.239010. Value loss: 40.872444. Entropy: 0.478517.\n",
      "Iteration 14555: Policy loss: 2.067181. Value loss: 19.772409. Entropy: 0.505128.\n",
      "Iteration 14556: Policy loss: 2.269552. Value loss: 15.151053. Entropy: 0.509628.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14557: Policy loss: 1.233135. Value loss: 21.726089. Entropy: 0.374108.\n",
      "Iteration 14558: Policy loss: 1.042361. Value loss: 13.741803. Entropy: 0.335761.\n",
      "Iteration 14559: Policy loss: 1.145771. Value loss: 9.604845. Entropy: 0.349700.\n",
      "episode: 6016   score: 350.0  epsilon: 1.0    steps: 25  evaluation reward: 349.7\n",
      "episode: 6017   score: 260.0  epsilon: 1.0    steps: 234  evaluation reward: 349.35\n",
      "episode: 6018   score: 530.0  epsilon: 1.0    steps: 727  evaluation reward: 352.05\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14560: Policy loss: 0.291640. Value loss: 33.043274. Entropy: 0.396984.\n",
      "Iteration 14561: Policy loss: 0.215902. Value loss: 21.868944. Entropy: 0.393066.\n",
      "Iteration 14562: Policy loss: 0.269657. Value loss: 17.807425. Entropy: 0.394128.\n",
      "episode: 6019   score: 210.0  epsilon: 1.0    steps: 956  evaluation reward: 351.3\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14563: Policy loss: 1.907391. Value loss: 36.955021. Entropy: 0.362342.\n",
      "Iteration 14564: Policy loss: 2.309050. Value loss: 19.201603. Entropy: 0.363043.\n",
      "Iteration 14565: Policy loss: 2.104494. Value loss: 16.899517. Entropy: 0.354306.\n",
      "episode: 6020   score: 375.0  epsilon: 1.0    steps: 532  evaluation reward: 350.95\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14566: Policy loss: 0.626371. Value loss: 35.739323. Entropy: 0.498779.\n",
      "Iteration 14567: Policy loss: 0.429584. Value loss: 20.385986. Entropy: 0.499388.\n",
      "Iteration 14568: Policy loss: 0.367323. Value loss: 13.715083. Entropy: 0.510452.\n",
      "episode: 6021   score: 180.0  epsilon: 1.0    steps: 121  evaluation reward: 350.3\n",
      "episode: 6022   score: 350.0  epsilon: 1.0    steps: 383  evaluation reward: 351.2\n",
      "episode: 6023   score: 220.0  epsilon: 1.0    steps: 480  evaluation reward: 350.7\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14569: Policy loss: 3.942742. Value loss: 27.264952. Entropy: 0.434866.\n",
      "Iteration 14570: Policy loss: 3.611199. Value loss: 17.051435. Entropy: 0.424993.\n",
      "Iteration 14571: Policy loss: 3.528456. Value loss: 14.895983. Entropy: 0.431743.\n",
      "episode: 6024   score: 135.0  epsilon: 1.0    steps: 225  evaluation reward: 349.25\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14572: Policy loss: 0.694044. Value loss: 24.104889. Entropy: 0.329657.\n",
      "Iteration 14573: Policy loss: 0.707481. Value loss: 14.482216. Entropy: 0.338631.\n",
      "Iteration 14574: Policy loss: 0.864823. Value loss: 12.994129. Entropy: 0.326153.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14575: Policy loss: -0.648581. Value loss: 20.300173. Entropy: 0.327739.\n",
      "Iteration 14576: Policy loss: -0.731276. Value loss: 12.233541. Entropy: 0.320638.\n",
      "Iteration 14577: Policy loss: -0.730439. Value loss: 8.860167. Entropy: 0.319475.\n",
      "episode: 6025   score: 290.0  epsilon: 1.0    steps: 745  evaluation reward: 348.25\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14578: Policy loss: 1.298440. Value loss: 21.291656. Entropy: 0.374427.\n",
      "Iteration 14579: Policy loss: 1.428922. Value loss: 10.302476. Entropy: 0.393496.\n",
      "Iteration 14580: Policy loss: 1.328748. Value loss: 9.208312. Entropy: 0.383882.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14581: Policy loss: -0.186173. Value loss: 22.061569. Entropy: 0.252554.\n",
      "Iteration 14582: Policy loss: -0.296957. Value loss: 10.695108. Entropy: 0.250948.\n",
      "Iteration 14583: Policy loss: 0.002584. Value loss: 8.589589. Entropy: 0.268079.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14584: Policy loss: 1.098870. Value loss: 18.911106. Entropy: 0.396801.\n",
      "Iteration 14585: Policy loss: 1.156322. Value loss: 12.131351. Entropy: 0.401332.\n",
      "Iteration 14586: Policy loss: 1.194172. Value loss: 7.561124. Entropy: 0.427760.\n",
      "episode: 6026   score: 260.0  epsilon: 1.0    steps: 371  evaluation reward: 348.4\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14587: Policy loss: 1.769220. Value loss: 21.617554. Entropy: 0.213522.\n",
      "Iteration 14588: Policy loss: 1.250413. Value loss: 16.225998. Entropy: 0.200523.\n",
      "Iteration 14589: Policy loss: 1.488425. Value loss: 10.485209. Entropy: 0.221358.\n",
      "episode: 6027   score: 155.0  epsilon: 1.0    steps: 674  evaluation reward: 345.75\n",
      "episode: 6028   score: 325.0  epsilon: 1.0    steps: 1017  evaluation reward: 344.05\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14590: Policy loss: 0.607685. Value loss: 21.721233. Entropy: 0.235006.\n",
      "Iteration 14591: Policy loss: 0.491139. Value loss: 14.417281. Entropy: 0.242775.\n",
      "Iteration 14592: Policy loss: 0.425079. Value loss: 11.203457. Entropy: 0.232816.\n",
      "episode: 6029   score: 265.0  epsilon: 1.0    steps: 136  evaluation reward: 342.85\n",
      "episode: 6030   score: 405.0  epsilon: 1.0    steps: 445  evaluation reward: 342.0\n",
      "episode: 6031   score: 280.0  epsilon: 1.0    steps: 849  evaluation reward: 342.2\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14593: Policy loss: 0.039384. Value loss: 26.068361. Entropy: 0.303052.\n",
      "Iteration 14594: Policy loss: 0.004898. Value loss: 16.288414. Entropy: 0.291039.\n",
      "Iteration 14595: Policy loss: -0.095929. Value loss: 13.469306. Entropy: 0.292220.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14596: Policy loss: 0.180677. Value loss: 24.789862. Entropy: 0.311052.\n",
      "Iteration 14597: Policy loss: 0.284491. Value loss: 15.625543. Entropy: 0.311292.\n",
      "Iteration 14598: Policy loss: 0.119436. Value loss: 12.581433. Entropy: 0.315044.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14599: Policy loss: -3.736160. Value loss: 92.461052. Entropy: 0.568034.\n",
      "Iteration 14600: Policy loss: -2.549611. Value loss: 27.597473. Entropy: 0.577141.\n",
      "Iteration 14601: Policy loss: -3.327340. Value loss: 33.219379. Entropy: 0.558844.\n",
      "episode: 6032   score: 440.0  epsilon: 1.0    steps: 47  evaluation reward: 342.7\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14602: Policy loss: 0.667178. Value loss: 17.844681. Entropy: 0.622707.\n",
      "Iteration 14603: Policy loss: 0.378950. Value loss: 10.478308. Entropy: 0.650010.\n",
      "Iteration 14604: Policy loss: 0.583143. Value loss: 9.265666. Entropy: 0.635033.\n",
      "episode: 6033   score: 300.0  epsilon: 1.0    steps: 612  evaluation reward: 339.1\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14605: Policy loss: 1.146635. Value loss: 22.967770. Entropy: 0.570605.\n",
      "Iteration 14606: Policy loss: 1.287484. Value loss: 13.856030. Entropy: 0.564952.\n",
      "Iteration 14607: Policy loss: 1.131394. Value loss: 11.081156. Entropy: 0.561266.\n",
      "episode: 6034   score: 210.0  epsilon: 1.0    steps: 149  evaluation reward: 338.4\n",
      "episode: 6035   score: 465.0  epsilon: 1.0    steps: 339  evaluation reward: 340.8\n",
      "episode: 6036   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 337.35\n",
      "episode: 6037   score: 265.0  epsilon: 1.0    steps: 722  evaluation reward: 336.4\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14608: Policy loss: 0.971263. Value loss: 25.106619. Entropy: 0.497938.\n",
      "Iteration 14609: Policy loss: 1.012350. Value loss: 17.614834. Entropy: 0.510072.\n",
      "Iteration 14610: Policy loss: 0.975837. Value loss: 16.182827. Entropy: 0.494788.\n",
      "episode: 6038   score: 270.0  epsilon: 1.0    steps: 822  evaluation reward: 337.0\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14611: Policy loss: -0.433264. Value loss: 29.986237. Entropy: 0.485212.\n",
      "Iteration 14612: Policy loss: -0.634150. Value loss: 19.625061. Entropy: 0.493905.\n",
      "Iteration 14613: Policy loss: -0.397020. Value loss: 15.891882. Entropy: 0.488982.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14614: Policy loss: -0.575519. Value loss: 30.750347. Entropy: 0.478244.\n",
      "Iteration 14615: Policy loss: -0.461106. Value loss: 19.938524. Entropy: 0.473947.\n",
      "Iteration 14616: Policy loss: -0.454428. Value loss: 13.911042. Entropy: 0.464616.\n",
      "episode: 6039   score: 360.0  epsilon: 1.0    steps: 946  evaluation reward: 335.85\n",
      "Training network. lr: 0.000138. clip: 0.055155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14617: Policy loss: -1.229941. Value loss: 17.011293. Entropy: 0.621372.\n",
      "Iteration 14618: Policy loss: -1.093515. Value loss: 9.820701. Entropy: 0.597662.\n",
      "Iteration 14619: Policy loss: -1.050782. Value loss: 10.252944. Entropy: 0.631392.\n",
      "episode: 6040   score: 270.0  epsilon: 1.0    steps: 119  evaluation reward: 334.35\n",
      "episode: 6041   score: 180.0  epsilon: 1.0    steps: 316  evaluation reward: 333.4\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14620: Policy loss: 1.357802. Value loss: 31.169031. Entropy: 0.469401.\n",
      "Iteration 14621: Policy loss: 1.508877. Value loss: 15.016833. Entropy: 0.457253.\n",
      "Iteration 14622: Policy loss: 1.845128. Value loss: 12.100787. Entropy: 0.476692.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14623: Policy loss: -0.571924. Value loss: 27.014927. Entropy: 0.400001.\n",
      "Iteration 14624: Policy loss: -0.772574. Value loss: 13.471923. Entropy: 0.372471.\n",
      "Iteration 14625: Policy loss: -0.625712. Value loss: 11.097292. Entropy: 0.383674.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14626: Policy loss: 0.441643. Value loss: 22.541275. Entropy: 0.448615.\n",
      "Iteration 14627: Policy loss: 0.376770. Value loss: 11.566729. Entropy: 0.461583.\n",
      "Iteration 14628: Policy loss: 0.585361. Value loss: 8.646799. Entropy: 0.467006.\n",
      "episode: 6042   score: 260.0  epsilon: 1.0    steps: 204  evaluation reward: 332.55\n",
      "episode: 6043   score: 225.0  epsilon: 1.0    steps: 777  evaluation reward: 330.2\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14629: Policy loss: 2.557315. Value loss: 26.144035. Entropy: 0.495413.\n",
      "Iteration 14630: Policy loss: 2.944548. Value loss: 13.451946. Entropy: 0.487622.\n",
      "Iteration 14631: Policy loss: 2.948379. Value loss: 11.018397. Entropy: 0.482167.\n",
      "episode: 6044   score: 155.0  epsilon: 1.0    steps: 1002  evaluation reward: 326.9\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14632: Policy loss: 1.049910. Value loss: 28.967972. Entropy: 0.419773.\n",
      "Iteration 14633: Policy loss: 1.094503. Value loss: 18.340921. Entropy: 0.404202.\n",
      "Iteration 14634: Policy loss: 0.699787. Value loss: 14.503778. Entropy: 0.420276.\n",
      "episode: 6045   score: 345.0  epsilon: 1.0    steps: 389  evaluation reward: 323.3\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14635: Policy loss: 1.075977. Value loss: 22.110907. Entropy: 0.463545.\n",
      "Iteration 14636: Policy loss: 0.872794. Value loss: 13.082767. Entropy: 0.486275.\n",
      "Iteration 14637: Policy loss: 1.129300. Value loss: 10.692253. Entropy: 0.483098.\n",
      "episode: 6046   score: 155.0  epsilon: 1.0    steps: 48  evaluation reward: 321.95\n",
      "episode: 6047   score: 260.0  epsilon: 1.0    steps: 329  evaluation reward: 322.4\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14638: Policy loss: -0.418844. Value loss: 29.990379. Entropy: 0.584645.\n",
      "Iteration 14639: Policy loss: -0.352410. Value loss: 20.422375. Entropy: 0.570768.\n",
      "Iteration 14640: Policy loss: -0.252847. Value loss: 15.587445. Entropy: 0.566665.\n",
      "episode: 6048   score: 410.0  epsilon: 1.0    steps: 642  evaluation reward: 320.65\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14641: Policy loss: 0.522106. Value loss: 19.165359. Entropy: 0.510755.\n",
      "Iteration 14642: Policy loss: 0.518999. Value loss: 11.989818. Entropy: 0.519526.\n",
      "Iteration 14643: Policy loss: 0.495010. Value loss: 9.650637. Entropy: 0.506245.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14644: Policy loss: -0.431198. Value loss: 25.478497. Entropy: 0.510141.\n",
      "Iteration 14645: Policy loss: -0.354299. Value loss: 16.381712. Entropy: 0.523645.\n",
      "Iteration 14646: Policy loss: -0.300820. Value loss: 12.446265. Entropy: 0.531479.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14647: Policy loss: 0.481488. Value loss: 13.805493. Entropy: 0.473291.\n",
      "Iteration 14648: Policy loss: 0.681054. Value loss: 6.874759. Entropy: 0.478224.\n",
      "Iteration 14649: Policy loss: 0.655097. Value loss: 6.492282. Entropy: 0.474783.\n",
      "episode: 6049   score: 220.0  epsilon: 1.0    steps: 192  evaluation reward: 317.15\n",
      "episode: 6050   score: 495.0  epsilon: 1.0    steps: 581  evaluation reward: 316.95\n",
      "now time :  2019-02-25 23:13:41.556486\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 6051   score: 275.0  epsilon: 1.0    steps: 794  evaluation reward: 313.4\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14650: Policy loss: -2.754395. Value loss: 229.655304. Entropy: 0.297488.\n",
      "Iteration 14651: Policy loss: -2.001891. Value loss: 113.124771. Entropy: 0.285768.\n",
      "Iteration 14652: Policy loss: -1.902879. Value loss: 106.416283. Entropy: 0.314791.\n",
      "episode: 6052   score: 220.0  epsilon: 1.0    steps: 915  evaluation reward: 313.15\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14653: Policy loss: 0.538899. Value loss: 36.412708. Entropy: 0.382337.\n",
      "Iteration 14654: Policy loss: 0.396688. Value loss: 26.152397. Entropy: 0.382981.\n",
      "Iteration 14655: Policy loss: 0.422794. Value loss: 20.248308. Entropy: 0.389733.\n",
      "episode: 6053   score: 190.0  epsilon: 1.0    steps: 665  evaluation reward: 311.7\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14656: Policy loss: 0.949882. Value loss: 23.913784. Entropy: 0.398801.\n",
      "Iteration 14657: Policy loss: 0.960770. Value loss: 18.113129. Entropy: 0.405184.\n",
      "Iteration 14658: Policy loss: 1.009604. Value loss: 12.967566. Entropy: 0.394896.\n",
      "episode: 6054   score: 245.0  epsilon: 1.0    steps: 121  evaluation reward: 312.05\n",
      "episode: 6055   score: 190.0  epsilon: 1.0    steps: 280  evaluation reward: 310.2\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14659: Policy loss: -3.360320. Value loss: 273.735992. Entropy: 0.407615.\n",
      "Iteration 14660: Policy loss: -2.889047. Value loss: 100.011238. Entropy: 0.441748.\n",
      "Iteration 14661: Policy loss: -4.180826. Value loss: 136.790894. Entropy: 0.419064.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14662: Policy loss: -0.713173. Value loss: 33.005096. Entropy: 0.287503.\n",
      "Iteration 14663: Policy loss: -0.870421. Value loss: 17.923609. Entropy: 0.292615.\n",
      "Iteration 14664: Policy loss: -0.744177. Value loss: 15.027573. Entropy: 0.293269.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14665: Policy loss: -3.065138. Value loss: 34.464146. Entropy: 0.377817.\n",
      "Iteration 14666: Policy loss: -3.134693. Value loss: 19.890528. Entropy: 0.368244.\n",
      "Iteration 14667: Policy loss: -2.904216. Value loss: 17.198586. Entropy: 0.378042.\n",
      "episode: 6056   score: 485.0  epsilon: 1.0    steps: 169  evaluation reward: 311.65\n",
      "episode: 6057   score: 505.0  epsilon: 1.0    steps: 422  evaluation reward: 314.1\n",
      "episode: 6058   score: 265.0  epsilon: 1.0    steps: 780  evaluation reward: 314.0\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14668: Policy loss: 0.629291. Value loss: 34.579559. Entropy: 0.458183.\n",
      "Iteration 14669: Policy loss: 0.706210. Value loss: 16.590010. Entropy: 0.455519.\n",
      "Iteration 14670: Policy loss: 0.392667. Value loss: 12.735888. Entropy: 0.459279.\n",
      "episode: 6059   score: 270.0  epsilon: 1.0    steps: 519  evaluation reward: 313.8\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14671: Policy loss: -0.440335. Value loss: 200.599380. Entropy: 0.279345.\n",
      "Iteration 14672: Policy loss: -0.007732. Value loss: 100.423279. Entropy: 0.276360.\n",
      "Iteration 14673: Policy loss: -0.349957. Value loss: 78.812202. Entropy: 0.282818.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14674: Policy loss: 0.143570. Value loss: 27.505388. Entropy: 0.381013.\n",
      "Iteration 14675: Policy loss: 0.190927. Value loss: 17.431356. Entropy: 0.368862.\n",
      "Iteration 14676: Policy loss: 0.068747. Value loss: 15.933251. Entropy: 0.372703.\n",
      "episode: 6060   score: 260.0  epsilon: 1.0    steps: 334  evaluation reward: 310.6\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14677: Policy loss: -1.726534. Value loss: 164.207214. Entropy: 0.282844.\n",
      "Iteration 14678: Policy loss: -3.269780. Value loss: 158.456207. Entropy: 0.291858.\n",
      "Iteration 14679: Policy loss: -2.475800. Value loss: 96.321503. Entropy: 0.262088.\n",
      "episode: 6061   score: 230.0  epsilon: 1.0    steps: 52  evaluation reward: 310.2\n",
      "episode: 6062   score: 340.0  epsilon: 1.0    steps: 909  evaluation reward: 310.75\n",
      "Training network. lr: 0.000137. clip: 0.054998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14680: Policy loss: -0.700819. Value loss: 307.868988. Entropy: 0.398361.\n",
      "Iteration 14681: Policy loss: -0.774871. Value loss: 186.943634. Entropy: 0.386894.\n",
      "Iteration 14682: Policy loss: -1.225349. Value loss: 216.985321. Entropy: 0.392542.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14683: Policy loss: -2.950744. Value loss: 198.861298. Entropy: 0.335366.\n",
      "Iteration 14684: Policy loss: -2.918508. Value loss: 94.591606. Entropy: 0.326396.\n",
      "Iteration 14685: Policy loss: -3.061625. Value loss: 79.262207. Entropy: 0.330240.\n",
      "episode: 6063   score: 485.0  epsilon: 1.0    steps: 168  evaluation reward: 313.2\n",
      "episode: 6064   score: 260.0  epsilon: 1.0    steps: 418  evaluation reward: 312.8\n",
      "episode: 6065   score: 260.0  epsilon: 1.0    steps: 820  evaluation reward: 311.15\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14686: Policy loss: -0.286669. Value loss: 37.859467. Entropy: 0.411856.\n",
      "Iteration 14687: Policy loss: -0.314150. Value loss: 22.968311. Entropy: 0.410491.\n",
      "Iteration 14688: Policy loss: -0.286727. Value loss: 19.155128. Entropy: 0.384519.\n",
      "episode: 6066   score: 460.0  epsilon: 1.0    steps: 554  evaluation reward: 312.4\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14689: Policy loss: 3.640102. Value loss: 68.268585. Entropy: 0.360285.\n",
      "Iteration 14690: Policy loss: 3.970366. Value loss: 27.284761. Entropy: 0.359563.\n",
      "Iteration 14691: Policy loss: 3.533347. Value loss: 20.561222. Entropy: 0.357910.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14692: Policy loss: 2.147221. Value loss: 47.937565. Entropy: 0.432851.\n",
      "Iteration 14693: Policy loss: 1.543073. Value loss: 27.424173. Entropy: 0.433125.\n",
      "Iteration 14694: Policy loss: 1.977710. Value loss: 18.482635. Entropy: 0.422616.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14695: Policy loss: 0.862291. Value loss: 138.197433. Entropy: 0.412061.\n",
      "Iteration 14696: Policy loss: -0.229670. Value loss: 182.410889. Entropy: 0.424412.\n",
      "Iteration 14697: Policy loss: 0.053552. Value loss: 102.167488. Entropy: 0.439957.\n",
      "episode: 6067   score: 755.0  epsilon: 1.0    steps: 738  evaluation reward: 317.85\n",
      "episode: 6068   score: 425.0  epsilon: 1.0    steps: 990  evaluation reward: 319.55\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14698: Policy loss: 0.322993. Value loss: 41.314983. Entropy: 0.529340.\n",
      "Iteration 14699: Policy loss: 0.572328. Value loss: 20.591488. Entropy: 0.532756.\n",
      "Iteration 14700: Policy loss: 0.492429. Value loss: 15.951097. Entropy: 0.512384.\n",
      "episode: 6069   score: 330.0  epsilon: 1.0    steps: 329  evaluation reward: 318.85\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14701: Policy loss: 1.931583. Value loss: 71.044579. Entropy: 0.465365.\n",
      "Iteration 14702: Policy loss: 1.648779. Value loss: 32.565449. Entropy: 0.438956.\n",
      "Iteration 14703: Policy loss: 2.072379. Value loss: 21.216368. Entropy: 0.454709.\n",
      "episode: 6070   score: 210.0  epsilon: 1.0    steps: 539  evaluation reward: 318.1\n",
      "episode: 6071   score: 235.0  epsilon: 1.0    steps: 885  evaluation reward: 318.05\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14704: Policy loss: 1.178262. Value loss: 62.173992. Entropy: 0.464051.\n",
      "Iteration 14705: Policy loss: 1.606744. Value loss: 27.370033. Entropy: 0.460196.\n",
      "Iteration 14706: Policy loss: 1.731705. Value loss: 23.250538. Entropy: 0.460852.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14707: Policy loss: -2.127324. Value loss: 215.338470. Entropy: 0.517293.\n",
      "Iteration 14708: Policy loss: -1.528381. Value loss: 67.383171. Entropy: 0.557636.\n",
      "Iteration 14709: Policy loss: -2.308928. Value loss: 69.222748. Entropy: 0.534702.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14710: Policy loss: -2.769942. Value loss: 310.787567. Entropy: 0.574131.\n",
      "Iteration 14711: Policy loss: -1.722803. Value loss: 53.288506. Entropy: 0.574855.\n",
      "Iteration 14712: Policy loss: -1.814029. Value loss: 32.784760. Entropy: 0.596496.\n",
      "episode: 6072   score: 465.0  epsilon: 1.0    steps: 17  evaluation reward: 319.45\n",
      "episode: 6073   score: 470.0  epsilon: 1.0    steps: 406  evaluation reward: 322.05\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14713: Policy loss: 1.010425. Value loss: 46.090622. Entropy: 0.366036.\n",
      "Iteration 14714: Policy loss: 1.216578. Value loss: 30.775364. Entropy: 0.362441.\n",
      "Iteration 14715: Policy loss: 1.263386. Value loss: 21.740963. Entropy: 0.367912.\n",
      "episode: 6074   score: 670.0  epsilon: 1.0    steps: 177  evaluation reward: 326.4\n",
      "episode: 6075   score: 210.0  epsilon: 1.0    steps: 326  evaluation reward: 325.9\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14716: Policy loss: -2.012356. Value loss: 146.912781. Entropy: 0.521434.\n",
      "Iteration 14717: Policy loss: -1.951665. Value loss: 87.556389. Entropy: 0.527732.\n",
      "Iteration 14718: Policy loss: -1.591053. Value loss: 55.247429. Entropy: 0.524242.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14719: Policy loss: -2.862598. Value loss: 101.767113. Entropy: 0.577066.\n",
      "Iteration 14720: Policy loss: -3.268957. Value loss: 50.572071. Entropy: 0.581496.\n",
      "Iteration 14721: Policy loss: -2.483100. Value loss: 34.180305. Entropy: 0.593866.\n",
      "episode: 6076   score: 490.0  epsilon: 1.0    steps: 578  evaluation reward: 328.55\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14722: Policy loss: 2.704525. Value loss: 155.531570. Entropy: 0.330777.\n",
      "Iteration 14723: Policy loss: 4.564538. Value loss: 105.547890. Entropy: 0.314581.\n",
      "Iteration 14724: Policy loss: 3.537841. Value loss: 75.683746. Entropy: 0.280893.\n",
      "episode: 6077   score: 625.0  epsilon: 1.0    steps: 757  evaluation reward: 331.9\n",
      "episode: 6078   score: 285.0  epsilon: 1.0    steps: 776  evaluation reward: 331.4\n",
      "episode: 6079   score: 590.0  epsilon: 1.0    steps: 901  evaluation reward: 333.15\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14725: Policy loss: 5.774903. Value loss: 89.289627. Entropy: 0.385700.\n",
      "Iteration 14726: Policy loss: 5.285444. Value loss: 33.662533. Entropy: 0.365434.\n",
      "Iteration 14727: Policy loss: 5.733536. Value loss: 23.684465. Entropy: 0.386474.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14728: Policy loss: 2.061661. Value loss: 168.775024. Entropy: 0.408261.\n",
      "Iteration 14729: Policy loss: 2.173247. Value loss: 74.200981. Entropy: 0.445308.\n",
      "Iteration 14730: Policy loss: 2.253561. Value loss: 60.194450. Entropy: 0.448428.\n",
      "episode: 6080   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 332.5\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14731: Policy loss: -1.089383. Value loss: 138.694061. Entropy: 0.454257.\n",
      "Iteration 14732: Policy loss: -1.116861. Value loss: 95.707832. Entropy: 0.442001.\n",
      "Iteration 14733: Policy loss: -1.361937. Value loss: 73.811333. Entropy: 0.471566.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14734: Policy loss: 0.210494. Value loss: 35.935959. Entropy: 0.380658.\n",
      "Iteration 14735: Policy loss: 0.368783. Value loss: 17.969709. Entropy: 0.363638.\n",
      "Iteration 14736: Policy loss: 0.257768. Value loss: 14.025276. Entropy: 0.336612.\n",
      "episode: 6081   score: 425.0  epsilon: 1.0    steps: 136  evaluation reward: 333.6\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14737: Policy loss: -1.762529. Value loss: 277.607086. Entropy: 0.418852.\n",
      "Iteration 14738: Policy loss: -1.011624. Value loss: 161.433395. Entropy: 0.425470.\n",
      "Iteration 14739: Policy loss: -1.610069. Value loss: 141.655334. Entropy: 0.423069.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14740: Policy loss: -0.619787. Value loss: 187.733719. Entropy: 0.345872.\n",
      "Iteration 14741: Policy loss: -0.508948. Value loss: 80.838791. Entropy: 0.354612.\n",
      "Iteration 14742: Policy loss: 0.050879. Value loss: 65.025925. Entropy: 0.371027.\n",
      "episode: 6082   score: 360.0  epsilon: 1.0    steps: 308  evaluation reward: 334.3\n",
      "episode: 6083   score: 765.0  epsilon: 1.0    steps: 474  evaluation reward: 339.4\n",
      "episode: 6084   score: 215.0  epsilon: 1.0    steps: 555  evaluation reward: 339.3\n",
      "episode: 6085   score: 260.0  epsilon: 1.0    steps: 744  evaluation reward: 338.55\n",
      "episode: 6086   score: 240.0  epsilon: 1.0    steps: 781  evaluation reward: 337.85\n",
      "Training network. lr: 0.000137. clip: 0.054851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14743: Policy loss: 1.793031. Value loss: 33.695065. Entropy: 0.301057.\n",
      "Iteration 14744: Policy loss: 1.837685. Value loss: 25.822229. Entropy: 0.325959.\n",
      "Iteration 14745: Policy loss: 1.988296. Value loss: 18.814024. Entropy: 0.296347.\n",
      "episode: 6087   score: 530.0  epsilon: 1.0    steps: 928  evaluation reward: 340.7\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14746: Policy loss: 0.806759. Value loss: 122.191475. Entropy: 0.250293.\n",
      "Iteration 14747: Policy loss: 0.699934. Value loss: 35.925266. Entropy: 0.245431.\n",
      "Iteration 14748: Policy loss: 1.210875. Value loss: 27.601877. Entropy: 0.249197.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14749: Policy loss: 1.453062. Value loss: 38.167858. Entropy: 0.465938.\n",
      "Iteration 14750: Policy loss: 1.584901. Value loss: 23.392206. Entropy: 0.451366.\n",
      "Iteration 14751: Policy loss: 1.644988. Value loss: 17.251474. Entropy: 0.465463.\n",
      "episode: 6088   score: 210.0  epsilon: 1.0    steps: 42  evaluation reward: 338.95\n",
      "episode: 6089   score: 260.0  epsilon: 1.0    steps: 245  evaluation reward: 337.85\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14752: Policy loss: 1.604224. Value loss: 128.123520. Entropy: 0.362678.\n",
      "Iteration 14753: Policy loss: 1.517933. Value loss: 58.566376. Entropy: 0.404520.\n",
      "Iteration 14754: Policy loss: 1.137545. Value loss: 47.504177. Entropy: 0.383351.\n",
      "episode: 6090   score: 150.0  epsilon: 1.0    steps: 737  evaluation reward: 335.75\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14755: Policy loss: 5.724016. Value loss: 81.922852. Entropy: 0.333008.\n",
      "Iteration 14756: Policy loss: 4.970428. Value loss: 29.625021. Entropy: 0.341427.\n",
      "Iteration 14757: Policy loss: 5.825984. Value loss: 22.131487. Entropy: 0.337422.\n",
      "episode: 6091   score: 210.0  epsilon: 1.0    steps: 469  evaluation reward: 335.25\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14758: Policy loss: 0.740698. Value loss: 18.071138. Entropy: 0.485061.\n",
      "Iteration 14759: Policy loss: 0.796723. Value loss: 11.500332. Entropy: 0.512038.\n",
      "Iteration 14760: Policy loss: 0.775723. Value loss: 9.649518. Entropy: 0.509313.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14761: Policy loss: 1.293181. Value loss: 27.008215. Entropy: 0.522972.\n",
      "Iteration 14762: Policy loss: 1.368497. Value loss: 16.150492. Entropy: 0.510190.\n",
      "Iteration 14763: Policy loss: 1.145272. Value loss: 13.305698. Entropy: 0.517033.\n",
      "episode: 6092   score: 260.0  epsilon: 1.0    steps: 375  evaluation reward: 335.05\n",
      "episode: 6093   score: 260.0  epsilon: 1.0    steps: 841  evaluation reward: 335.05\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14764: Policy loss: 2.027417. Value loss: 51.649483. Entropy: 0.412765.\n",
      "Iteration 14765: Policy loss: 1.846754. Value loss: 21.215462. Entropy: 0.404493.\n",
      "Iteration 14766: Policy loss: 2.416440. Value loss: 14.511721. Entropy: 0.395202.\n",
      "episode: 6094   score: 180.0  epsilon: 1.0    steps: 147  evaluation reward: 333.7\n",
      "episode: 6095   score: 545.0  epsilon: 1.0    steps: 525  evaluation reward: 337.25\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14767: Policy loss: 0.482775. Value loss: 22.205418. Entropy: 0.332351.\n",
      "Iteration 14768: Policy loss: 0.070509. Value loss: 14.677696. Entropy: 0.320503.\n",
      "Iteration 14769: Policy loss: 0.288067. Value loss: 10.298716. Entropy: 0.320026.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14770: Policy loss: 0.685103. Value loss: 25.446808. Entropy: 0.323964.\n",
      "Iteration 14771: Policy loss: 0.824877. Value loss: 16.226141. Entropy: 0.323000.\n",
      "Iteration 14772: Policy loss: 0.821238. Value loss: 11.984558. Entropy: 0.313397.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14773: Policy loss: -2.430708. Value loss: 205.466003. Entropy: 0.321348.\n",
      "Iteration 14774: Policy loss: -2.924494. Value loss: 203.960327. Entropy: 0.305775.\n",
      "Iteration 14775: Policy loss: -2.489239. Value loss: 176.456589. Entropy: 0.309886.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14776: Policy loss: -0.309120. Value loss: 112.679420. Entropy: 0.359980.\n",
      "Iteration 14777: Policy loss: -0.224716. Value loss: 79.331680. Entropy: 0.365836.\n",
      "Iteration 14778: Policy loss: -0.570237. Value loss: 50.260666. Entropy: 0.379550.\n",
      "episode: 6096   score: 365.0  epsilon: 1.0    steps: 905  evaluation reward: 337.0\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14779: Policy loss: 3.130338. Value loss: 49.441898. Entropy: 0.332947.\n",
      "Iteration 14780: Policy loss: 2.931257. Value loss: 21.580494. Entropy: 0.352440.\n",
      "Iteration 14781: Policy loss: 2.898980. Value loss: 17.261095. Entropy: 0.348996.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14782: Policy loss: 0.142401. Value loss: 52.249321. Entropy: 0.553227.\n",
      "Iteration 14783: Policy loss: 0.110269. Value loss: 27.076427. Entropy: 0.574724.\n",
      "Iteration 14784: Policy loss: 0.333928. Value loss: 19.517010. Entropy: 0.565283.\n",
      "episode: 6097   score: 405.0  epsilon: 1.0    steps: 83  evaluation reward: 335.3\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14785: Policy loss: 1.459219. Value loss: 63.456413. Entropy: 0.497552.\n",
      "Iteration 14786: Policy loss: 1.440798. Value loss: 27.842453. Entropy: 0.505889.\n",
      "Iteration 14787: Policy loss: 1.185518. Value loss: 21.510836. Entropy: 0.485623.\n",
      "episode: 6098   score: 310.0  epsilon: 1.0    steps: 702  evaluation reward: 334.65\n",
      "episode: 6099   score: 460.0  epsilon: 1.0    steps: 800  evaluation reward: 335.9\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14788: Policy loss: 5.001418. Value loss: 46.709991. Entropy: 0.449134.\n",
      "Iteration 14789: Policy loss: 4.918535. Value loss: 20.574385. Entropy: 0.434016.\n",
      "Iteration 14790: Policy loss: 4.734307. Value loss: 15.246203. Entropy: 0.448633.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14791: Policy loss: 2.454170. Value loss: 32.711048. Entropy: 0.440036.\n",
      "Iteration 14792: Policy loss: 2.488821. Value loss: 23.395035. Entropy: 0.445708.\n",
      "Iteration 14793: Policy loss: 2.420719. Value loss: 18.255066. Entropy: 0.460247.\n",
      "episode: 6100   score: 345.0  epsilon: 1.0    steps: 398  evaluation reward: 336.25\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14794: Policy loss: 0.657864. Value loss: 33.827625. Entropy: 0.508736.\n",
      "Iteration 14795: Policy loss: 0.499904. Value loss: 21.193460. Entropy: 0.524257.\n",
      "Iteration 14796: Policy loss: 0.510157. Value loss: 18.216877. Entropy: 0.487462.\n",
      "now time :  2019-02-25 23:16:24.639349\n",
      "episode: 6101   score: 365.0  epsilon: 1.0    steps: 304  evaluation reward: 334.5\n",
      "episode: 6102   score: 460.0  epsilon: 1.0    steps: 540  evaluation reward: 336.5\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14797: Policy loss: 0.514055. Value loss: 27.603903. Entropy: 0.458455.\n",
      "Iteration 14798: Policy loss: 0.535106. Value loss: 16.152498. Entropy: 0.455574.\n",
      "Iteration 14799: Policy loss: 0.461136. Value loss: 14.584025. Entropy: 0.440103.\n",
      "episode: 6103   score: 745.0  epsilon: 1.0    steps: 134  evaluation reward: 341.05\n",
      "episode: 6104   score: 135.0  epsilon: 1.0    steps: 691  evaluation reward: 340.1\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14800: Policy loss: 2.068307. Value loss: 46.837475. Entropy: 0.454713.\n",
      "Iteration 14801: Policy loss: 2.107267. Value loss: 27.630791. Entropy: 0.455063.\n",
      "Iteration 14802: Policy loss: 1.945636. Value loss: 24.972689. Entropy: 0.448232.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14803: Policy loss: 0.150824. Value loss: 20.230997. Entropy: 0.385086.\n",
      "Iteration 14804: Policy loss: 0.116602. Value loss: 11.062130. Entropy: 0.417438.\n",
      "Iteration 14805: Policy loss: 0.017742. Value loss: 8.765586. Entropy: 0.412702.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14806: Policy loss: 1.333850. Value loss: 22.408386. Entropy: 0.439469.\n",
      "Iteration 14807: Policy loss: 1.284286. Value loss: 12.113313. Entropy: 0.426943.\n",
      "Iteration 14808: Policy loss: 1.245359. Value loss: 9.543812. Entropy: 0.401694.\n",
      "episode: 6105   score: 290.0  epsilon: 1.0    steps: 22  evaluation reward: 337.2\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14809: Policy loss: -0.140843. Value loss: 25.162397. Entropy: 0.473294.\n",
      "Iteration 14810: Policy loss: 0.122340. Value loss: 13.910748. Entropy: 0.499206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14811: Policy loss: -0.039896. Value loss: 11.887508. Entropy: 0.486212.\n",
      "episode: 6106   score: 390.0  epsilon: 1.0    steps: 906  evaluation reward: 338.2\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14812: Policy loss: 1.052391. Value loss: 33.891953. Entropy: 0.531203.\n",
      "Iteration 14813: Policy loss: 0.874507. Value loss: 22.118767. Entropy: 0.508257.\n",
      "Iteration 14814: Policy loss: 1.013022. Value loss: 18.318853. Entropy: 0.528210.\n",
      "episode: 6107   score: 340.0  epsilon: 1.0    steps: 798  evaluation reward: 332.7\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14815: Policy loss: 0.559067. Value loss: 24.093740. Entropy: 0.517433.\n",
      "Iteration 14816: Policy loss: 0.584302. Value loss: 11.923223. Entropy: 0.499693.\n",
      "Iteration 14817: Policy loss: 0.638273. Value loss: 11.704606. Entropy: 0.499488.\n",
      "episode: 6108   score: 250.0  epsilon: 1.0    steps: 268  evaluation reward: 331.05\n",
      "episode: 6109   score: 260.0  epsilon: 1.0    steps: 627  evaluation reward: 328.0\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14818: Policy loss: 1.294344. Value loss: 24.669331. Entropy: 0.459434.\n",
      "Iteration 14819: Policy loss: 1.264065. Value loss: 14.961730. Entropy: 0.468873.\n",
      "Iteration 14820: Policy loss: 1.548748. Value loss: 11.607139. Entropy: 0.479403.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14821: Policy loss: 2.215048. Value loss: 25.683531. Entropy: 0.461560.\n",
      "Iteration 14822: Policy loss: 2.085186. Value loss: 16.249846. Entropy: 0.463300.\n",
      "Iteration 14823: Policy loss: 2.271974. Value loss: 13.338221. Entropy: 0.461145.\n",
      "episode: 6110   score: 275.0  epsilon: 1.0    steps: 696  evaluation reward: 326.4\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14824: Policy loss: 0.824808. Value loss: 43.341179. Entropy: 0.604312.\n",
      "Iteration 14825: Policy loss: 0.525608. Value loss: 21.624542. Entropy: 0.600871.\n",
      "Iteration 14826: Policy loss: 0.949696. Value loss: 18.363377. Entropy: 0.614438.\n",
      "episode: 6111   score: 285.0  epsilon: 1.0    steps: 65  evaluation reward: 326.65\n",
      "episode: 6112   score: 465.0  epsilon: 1.0    steps: 510  evaluation reward: 328.9\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14827: Policy loss: -1.945476. Value loss: 85.099205. Entropy: 0.585471.\n",
      "Iteration 14828: Policy loss: -3.795138. Value loss: 74.812904. Entropy: 0.596240.\n",
      "Iteration 14829: Policy loss: -3.332932. Value loss: 60.334759. Entropy: 0.575895.\n",
      "episode: 6113   score: 395.0  epsilon: 1.0    steps: 141  evaluation reward: 330.75\n",
      "episode: 6114   score: 150.0  epsilon: 1.0    steps: 810  evaluation reward: 329.65\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14830: Policy loss: 1.122715. Value loss: 19.579586. Entropy: 0.458900.\n",
      "Iteration 14831: Policy loss: 0.952313. Value loss: 11.400341. Entropy: 0.482290.\n",
      "Iteration 14832: Policy loss: 0.973761. Value loss: 9.476735. Entropy: 0.484391.\n",
      "episode: 6115   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 327.65\n",
      "episode: 6116   score: 275.0  epsilon: 1.0    steps: 949  evaluation reward: 326.9\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14833: Policy loss: 2.183764. Value loss: 27.646637. Entropy: 0.511418.\n",
      "Iteration 14834: Policy loss: 2.312302. Value loss: 17.978710. Entropy: 0.511855.\n",
      "Iteration 14835: Policy loss: 2.279051. Value loss: 15.219757. Entropy: 0.509563.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14836: Policy loss: -2.059226. Value loss: 236.867523. Entropy: 0.430925.\n",
      "Iteration 14837: Policy loss: -2.592760. Value loss: 207.342545. Entropy: 0.417505.\n",
      "Iteration 14838: Policy loss: -2.198504. Value loss: 152.533295. Entropy: 0.422299.\n",
      "episode: 6117   score: 100.0  epsilon: 1.0    steps: 137  evaluation reward: 325.3\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14839: Policy loss: -1.180901. Value loss: 188.405350. Entropy: 0.425972.\n",
      "Iteration 14840: Policy loss: 0.249493. Value loss: 102.701004. Entropy: 0.404724.\n",
      "Iteration 14841: Policy loss: -0.306011. Value loss: 90.284981. Entropy: 0.414479.\n",
      "episode: 6118   score: 155.0  epsilon: 1.0    steps: 436  evaluation reward: 321.55\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14842: Policy loss: 0.948790. Value loss: 39.009090. Entropy: 0.478711.\n",
      "Iteration 14843: Policy loss: 1.093389. Value loss: 19.619293. Entropy: 0.486956.\n",
      "Iteration 14844: Policy loss: 0.718562. Value loss: 17.501217. Entropy: 0.508715.\n",
      "episode: 6119   score: 545.0  epsilon: 1.0    steps: 626  evaluation reward: 324.9\n",
      "episode: 6120   score: 275.0  epsilon: 1.0    steps: 715  evaluation reward: 323.9\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14845: Policy loss: 0.641001. Value loss: 56.590614. Entropy: 0.406976.\n",
      "Iteration 14846: Policy loss: 0.761872. Value loss: 25.965370. Entropy: 0.417527.\n",
      "Iteration 14847: Policy loss: 0.637657. Value loss: 22.086014. Entropy: 0.421117.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14848: Policy loss: 1.808346. Value loss: 36.590908. Entropy: 0.547815.\n",
      "Iteration 14849: Policy loss: 1.818842. Value loss: 16.680458. Entropy: 0.565874.\n",
      "Iteration 14850: Policy loss: 1.647528. Value loss: 12.831297. Entropy: 0.549123.\n",
      "episode: 6121   score: 535.0  epsilon: 1.0    steps: 126  evaluation reward: 327.45\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14851: Policy loss: -3.225776. Value loss: 225.176132. Entropy: 0.369412.\n",
      "Iteration 14852: Policy loss: -3.381076. Value loss: 121.065308. Entropy: 0.344939.\n",
      "Iteration 14853: Policy loss: -3.337322. Value loss: 66.061630. Entropy: 0.349554.\n",
      "episode: 6122   score: 260.0  epsilon: 1.0    steps: 362  evaluation reward: 326.55\n",
      "episode: 6123   score: 490.0  epsilon: 1.0    steps: 851  evaluation reward: 329.25\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14854: Policy loss: -2.617512. Value loss: 339.674072. Entropy: 0.351453.\n",
      "Iteration 14855: Policy loss: -3.363086. Value loss: 207.017395. Entropy: 0.357225.\n",
      "Iteration 14856: Policy loss: -2.699910. Value loss: 163.074234. Entropy: 0.330030.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14857: Policy loss: 3.900611. Value loss: 121.974655. Entropy: 0.343874.\n",
      "Iteration 14858: Policy loss: 3.089982. Value loss: 53.290619. Entropy: 0.381130.\n",
      "Iteration 14859: Policy loss: 3.079173. Value loss: 37.391048. Entropy: 0.359854.\n",
      "episode: 6124   score: 480.0  epsilon: 1.0    steps: 936  evaluation reward: 332.7\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14860: Policy loss: 3.312497. Value loss: 42.281273. Entropy: 0.381522.\n",
      "Iteration 14861: Policy loss: 3.757077. Value loss: 23.115791. Entropy: 0.398383.\n",
      "Iteration 14862: Policy loss: 3.626133. Value loss: 19.932762. Entropy: 0.387316.\n",
      "episode: 6125   score: 155.0  epsilon: 1.0    steps: 359  evaluation reward: 331.35\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14863: Policy loss: 3.352232. Value loss: 52.372261. Entropy: 0.256718.\n",
      "Iteration 14864: Policy loss: 3.566699. Value loss: 26.481596. Entropy: 0.273071.\n",
      "Iteration 14865: Policy loss: 3.361807. Value loss: 19.167757. Entropy: 0.279781.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14866: Policy loss: 1.310917. Value loss: 54.852261. Entropy: 0.556805.\n",
      "Iteration 14867: Policy loss: 0.977069. Value loss: 25.591213. Entropy: 0.532896.\n",
      "Iteration 14868: Policy loss: 1.052119. Value loss: 18.586435. Entropy: 0.550647.\n",
      "episode: 6126   score: 210.0  epsilon: 1.0    steps: 55  evaluation reward: 330.85\n",
      "episode: 6127   score: 435.0  epsilon: 1.0    steps: 199  evaluation reward: 333.65\n",
      "episode: 6128   score: 580.0  epsilon: 1.0    steps: 441  evaluation reward: 336.2\n",
      "episode: 6129   score: 210.0  epsilon: 1.0    steps: 892  evaluation reward: 335.65\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14869: Policy loss: 1.179909. Value loss: 39.941490. Entropy: 0.468219.\n",
      "Iteration 14870: Policy loss: 1.143505. Value loss: 33.056881. Entropy: 0.474054.\n",
      "Iteration 14871: Policy loss: 1.031088. Value loss: 24.900988. Entropy: 0.451155.\n",
      "episode: 6130   score: 535.0  epsilon: 1.0    steps: 656  evaluation reward: 336.95\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14872: Policy loss: 3.484900. Value loss: 32.347702. Entropy: 0.393981.\n",
      "Iteration 14873: Policy loss: 3.260287. Value loss: 19.124138. Entropy: 0.400265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14874: Policy loss: 3.194839. Value loss: 16.832150. Entropy: 0.410456.\n",
      "episode: 6131   score: 155.0  epsilon: 1.0    steps: 351  evaluation reward: 335.7\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14875: Policy loss: 3.732513. Value loss: 40.798210. Entropy: 0.344825.\n",
      "Iteration 14876: Policy loss: 3.626330. Value loss: 22.931454. Entropy: 0.377045.\n",
      "Iteration 14877: Policy loss: 3.825872. Value loss: 18.130384. Entropy: 0.362835.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14878: Policy loss: -1.331388. Value loss: 119.604500. Entropy: 0.369541.\n",
      "Iteration 14879: Policy loss: -1.320592. Value loss: 72.093086. Entropy: 0.390536.\n",
      "Iteration 14880: Policy loss: -1.454566. Value loss: 68.647919. Entropy: 0.368175.\n",
      "episode: 6132   score: 180.0  epsilon: 1.0    steps: 397  evaluation reward: 333.1\n",
      "episode: 6133   score: 260.0  epsilon: 1.0    steps: 1000  evaluation reward: 332.7\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14881: Policy loss: 2.666399. Value loss: 26.920282. Entropy: 0.361192.\n",
      "Iteration 14882: Policy loss: 2.558927. Value loss: 18.532377. Entropy: 0.356599.\n",
      "Iteration 14883: Policy loss: 2.615327. Value loss: 16.995594. Entropy: 0.361241.\n",
      "episode: 6134   score: 135.0  epsilon: 1.0    steps: 133  evaluation reward: 331.95\n",
      "episode: 6135   score: 365.0  epsilon: 1.0    steps: 515  evaluation reward: 330.95\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14884: Policy loss: -3.040696. Value loss: 157.548615. Entropy: 0.403567.\n",
      "Iteration 14885: Policy loss: -3.211924. Value loss: 87.885544. Entropy: 0.401201.\n",
      "Iteration 14886: Policy loss: -3.007015. Value loss: 63.795097. Entropy: 0.392983.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14887: Policy loss: -1.304673. Value loss: 35.700844. Entropy: 0.256951.\n",
      "Iteration 14888: Policy loss: -1.885298. Value loss: 25.649836. Entropy: 0.245692.\n",
      "Iteration 14889: Policy loss: -1.483979. Value loss: 20.670843. Entropy: 0.242177.\n",
      "episode: 6136   score: 320.0  epsilon: 1.0    steps: 32  evaluation reward: 332.05\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14890: Policy loss: 2.379769. Value loss: 250.390335. Entropy: 0.308542.\n",
      "Iteration 14891: Policy loss: 2.286749. Value loss: 110.444214. Entropy: 0.291611.\n",
      "Iteration 14892: Policy loss: 2.332688. Value loss: 71.789009. Entropy: 0.277836.\n",
      "episode: 6137   score: 210.0  epsilon: 1.0    steps: 386  evaluation reward: 331.5\n",
      "episode: 6138   score: 155.0  epsilon: 1.0    steps: 945  evaluation reward: 330.35\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14893: Policy loss: 4.267385. Value loss: 45.905785. Entropy: 0.302434.\n",
      "Iteration 14894: Policy loss: 4.357031. Value loss: 24.545776. Entropy: 0.280116.\n",
      "Iteration 14895: Policy loss: 4.429310. Value loss: 18.826349. Entropy: 0.272929.\n",
      "episode: 6139   score: 210.0  epsilon: 1.0    steps: 282  evaluation reward: 328.85\n",
      "episode: 6140   score: 490.0  epsilon: 1.0    steps: 858  evaluation reward: 331.05\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14896: Policy loss: 0.668829. Value loss: 34.816994. Entropy: 0.317385.\n",
      "Iteration 14897: Policy loss: 0.664781. Value loss: 25.490969. Entropy: 0.320162.\n",
      "Iteration 14898: Policy loss: 0.768970. Value loss: 17.315145. Entropy: 0.327720.\n",
      "episode: 6141   score: 535.0  epsilon: 1.0    steps: 646  evaluation reward: 334.6\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14899: Policy loss: -1.104392. Value loss: 20.375629. Entropy: 0.290949.\n",
      "Iteration 14900: Policy loss: -1.134967. Value loss: 12.285525. Entropy: 0.274490.\n",
      "Iteration 14901: Policy loss: -1.033965. Value loss: 11.246493. Entropy: 0.274202.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14902: Policy loss: 3.477668. Value loss: 30.452009. Entropy: 0.258475.\n",
      "Iteration 14903: Policy loss: 3.438699. Value loss: 19.201225. Entropy: 0.262452.\n",
      "Iteration 14904: Policy loss: 3.140983. Value loss: 13.653157. Entropy: 0.366797.\n",
      "episode: 6142   score: 180.0  epsilon: 1.0    steps: 78  evaluation reward: 333.8\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14905: Policy loss: -0.528913. Value loss: 83.085907. Entropy: 0.390041.\n",
      "Iteration 14906: Policy loss: -0.688986. Value loss: 44.580547. Entropy: 0.405895.\n",
      "Iteration 14907: Policy loss: -0.870474. Value loss: 27.573624. Entropy: 0.409496.\n",
      "episode: 6143   score: 180.0  epsilon: 1.0    steps: 282  evaluation reward: 333.35\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14908: Policy loss: 3.595380. Value loss: 39.554665. Entropy: 0.418551.\n",
      "Iteration 14909: Policy loss: 3.450803. Value loss: 18.279678. Entropy: 0.422519.\n",
      "Iteration 14910: Policy loss: 3.408491. Value loss: 14.817439. Entropy: 0.421194.\n",
      "episode: 6144   score: 210.0  epsilon: 1.0    steps: 418  evaluation reward: 333.9\n",
      "episode: 6145   score: 210.0  epsilon: 1.0    steps: 769  evaluation reward: 332.55\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14911: Policy loss: 1.695147. Value loss: 36.998890. Entropy: 0.354100.\n",
      "Iteration 14912: Policy loss: 1.425870. Value loss: 17.639088. Entropy: 0.350284.\n",
      "Iteration 14913: Policy loss: 1.944477. Value loss: 12.221951. Entropy: 0.364358.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14914: Policy loss: -1.023751. Value loss: 28.419018. Entropy: 0.394648.\n",
      "Iteration 14915: Policy loss: -1.125787. Value loss: 19.491625. Entropy: 0.391573.\n",
      "Iteration 14916: Policy loss: -0.962907. Value loss: 15.132579. Entropy: 0.395692.\n",
      "episode: 6146   score: 425.0  epsilon: 1.0    steps: 554  evaluation reward: 335.25\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14917: Policy loss: 0.261043. Value loss: 57.184490. Entropy: 0.458985.\n",
      "Iteration 14918: Policy loss: 0.119864. Value loss: 33.287373. Entropy: 0.472386.\n",
      "Iteration 14919: Policy loss: 0.112403. Value loss: 23.396744. Entropy: 0.464792.\n",
      "episode: 6147   score: 365.0  epsilon: 1.0    steps: 737  evaluation reward: 336.3\n",
      "episode: 6148   score: 330.0  epsilon: 1.0    steps: 980  evaluation reward: 335.5\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14920: Policy loss: 0.853570. Value loss: 41.574146. Entropy: 0.440639.\n",
      "Iteration 14921: Policy loss: 0.850380. Value loss: 22.518257. Entropy: 0.441913.\n",
      "Iteration 14922: Policy loss: 0.857299. Value loss: 18.029612. Entropy: 0.455964.\n",
      "episode: 6149   score: 265.0  epsilon: 1.0    steps: 81  evaluation reward: 335.95\n",
      "episode: 6150   score: 705.0  epsilon: 1.0    steps: 134  evaluation reward: 338.05\n",
      "now time :  2019-02-25 23:18:45.964118\n",
      "episode: 6151   score: 180.0  epsilon: 1.0    steps: 396  evaluation reward: 337.1\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14923: Policy loss: -0.810936. Value loss: 29.451452. Entropy: 0.336184.\n",
      "Iteration 14924: Policy loss: -0.918309. Value loss: 19.866455. Entropy: 0.326711.\n",
      "Iteration 14925: Policy loss: -0.941816. Value loss: 18.044062. Entropy: 0.320411.\n",
      "episode: 6152   score: 290.0  epsilon: 1.0    steps: 292  evaluation reward: 337.8\n",
      "episode: 6153   score: 180.0  epsilon: 1.0    steps: 780  evaluation reward: 337.7\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14926: Policy loss: 0.704627. Value loss: 23.480814. Entropy: 0.236218.\n",
      "Iteration 14927: Policy loss: 0.723221. Value loss: 16.194273. Entropy: 0.248177.\n",
      "Iteration 14928: Policy loss: 0.570634. Value loss: 15.216940. Entropy: 0.235531.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14929: Policy loss: -1.510797. Value loss: 37.231911. Entropy: 0.424267.\n",
      "Iteration 14930: Policy loss: -1.321159. Value loss: 22.011578. Entropy: 0.435505.\n",
      "Iteration 14931: Policy loss: -1.284340. Value loss: 19.587271. Entropy: 0.436274.\n",
      "episode: 6154   score: 155.0  epsilon: 1.0    steps: 239  evaluation reward: 336.8\n",
      "episode: 6155   score: 210.0  epsilon: 1.0    steps: 1001  evaluation reward: 337.0\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14932: Policy loss: 2.508684. Value loss: 59.784168. Entropy: 0.507341.\n",
      "Iteration 14933: Policy loss: 2.316947. Value loss: 22.576153. Entropy: 0.503689.\n",
      "Iteration 14934: Policy loss: 2.321302. Value loss: 22.863384. Entropy: 0.516549.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14935: Policy loss: 2.817197. Value loss: 47.440109. Entropy: 0.231990.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14936: Policy loss: 3.367829. Value loss: 16.832731. Entropy: 0.194340.\n",
      "Iteration 14937: Policy loss: 3.494467. Value loss: 12.397567. Entropy: 0.229900.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14938: Policy loss: 0.631321. Value loss: 37.304279. Entropy: 0.340723.\n",
      "Iteration 14939: Policy loss: 0.650581. Value loss: 25.264151. Entropy: 0.337536.\n",
      "Iteration 14940: Policy loss: 0.656619. Value loss: 19.638512. Entropy: 0.345058.\n",
      "episode: 6156   score: 240.0  epsilon: 1.0    steps: 648  evaluation reward: 334.55\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14941: Policy loss: -3.043473. Value loss: 206.427551. Entropy: 0.480414.\n",
      "Iteration 14942: Policy loss: -2.999434. Value loss: 107.659447. Entropy: 0.443811.\n",
      "Iteration 14943: Policy loss: -3.171620. Value loss: 63.596661. Entropy: 0.430654.\n",
      "episode: 6157   score: 260.0  epsilon: 1.0    steps: 347  evaluation reward: 332.1\n",
      "episode: 6158   score: 410.0  epsilon: 1.0    steps: 618  evaluation reward: 333.55\n",
      "episode: 6159   score: 260.0  epsilon: 1.0    steps: 823  evaluation reward: 333.45\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14944: Policy loss: 2.203772. Value loss: 42.260361. Entropy: 0.319117.\n",
      "Iteration 14945: Policy loss: 1.963579. Value loss: 21.244467. Entropy: 0.313908.\n",
      "Iteration 14946: Policy loss: 2.291315. Value loss: 12.640499. Entropy: 0.315819.\n",
      "episode: 6160   score: 320.0  epsilon: 1.0    steps: 36  evaluation reward: 334.05\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14947: Policy loss: 1.194616. Value loss: 28.255676. Entropy: 0.497181.\n",
      "Iteration 14948: Policy loss: 1.249451. Value loss: 16.338833. Entropy: 0.521373.\n",
      "Iteration 14949: Policy loss: 1.316234. Value loss: 13.668190. Entropy: 0.527073.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14950: Policy loss: 1.786659. Value loss: 27.441246. Entropy: 0.371797.\n",
      "Iteration 14951: Policy loss: 1.826270. Value loss: 18.910439. Entropy: 0.352252.\n",
      "Iteration 14952: Policy loss: 1.726251. Value loss: 13.938339. Entropy: 0.362180.\n",
      "episode: 6161   score: 260.0  epsilon: 1.0    steps: 133  evaluation reward: 334.35\n",
      "episode: 6162   score: 675.0  epsilon: 1.0    steps: 442  evaluation reward: 337.7\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14953: Policy loss: 0.968930. Value loss: 33.769295. Entropy: 0.526831.\n",
      "Iteration 14954: Policy loss: 1.002292. Value loss: 21.820328. Entropy: 0.506980.\n",
      "Iteration 14955: Policy loss: 1.031221. Value loss: 20.127806. Entropy: 0.526954.\n",
      "episode: 6163   score: 155.0  epsilon: 1.0    steps: 561  evaluation reward: 334.4\n",
      "episode: 6164   score: 405.0  epsilon: 1.0    steps: 952  evaluation reward: 335.85\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14956: Policy loss: 2.688669. Value loss: 33.877819. Entropy: 0.286700.\n",
      "Iteration 14957: Policy loss: 2.519717. Value loss: 20.016523. Entropy: 0.275431.\n",
      "Iteration 14958: Policy loss: 2.834347. Value loss: 17.627337. Entropy: 0.269599.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14959: Policy loss: 2.744285. Value loss: 21.438858. Entropy: 0.349428.\n",
      "Iteration 14960: Policy loss: 2.764472. Value loss: 10.223845. Entropy: 0.328154.\n",
      "Iteration 14961: Policy loss: 2.580582. Value loss: 10.712913. Entropy: 0.337653.\n",
      "episode: 6165   score: 270.0  epsilon: 1.0    steps: 723  evaluation reward: 335.95\n",
      "episode: 6166   score: 210.0  epsilon: 1.0    steps: 870  evaluation reward: 333.45\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14962: Policy loss: 2.016864. Value loss: 21.701616. Entropy: 0.318172.\n",
      "Iteration 14963: Policy loss: 2.197600. Value loss: 15.829340. Entropy: 0.326051.\n",
      "Iteration 14964: Policy loss: 2.438045. Value loss: 13.511065. Entropy: 0.321433.\n",
      "episode: 6167   score: 180.0  epsilon: 1.0    steps: 459  evaluation reward: 327.7\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14965: Policy loss: 0.404261. Value loss: 24.453516. Entropy: 0.369978.\n",
      "Iteration 14966: Policy loss: 0.348836. Value loss: 16.404997. Entropy: 0.378864.\n",
      "Iteration 14967: Policy loss: 0.536362. Value loss: 14.491853. Entropy: 0.372403.\n",
      "episode: 6168   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 325.55\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14968: Policy loss: -0.796630. Value loss: 204.554276. Entropy: 0.429481.\n",
      "Iteration 14969: Policy loss: -0.055471. Value loss: 95.281509. Entropy: 0.407924.\n",
      "Iteration 14970: Policy loss: -0.533229. Value loss: 77.597878. Entropy: 0.398469.\n",
      "episode: 6169   score: 330.0  epsilon: 1.0    steps: 25  evaluation reward: 325.55\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14971: Policy loss: 0.283591. Value loss: 27.501657. Entropy: 0.406479.\n",
      "Iteration 14972: Policy loss: 0.176109. Value loss: 19.942369. Entropy: 0.381456.\n",
      "Iteration 14973: Policy loss: 0.415904. Value loss: 16.112144. Entropy: 0.417273.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14974: Policy loss: -1.488847. Value loss: 29.500484. Entropy: 0.297909.\n",
      "Iteration 14975: Policy loss: -1.660189. Value loss: 18.415037. Entropy: 0.292928.\n",
      "Iteration 14976: Policy loss: -1.471877. Value loss: 12.967587. Entropy: 0.300001.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14977: Policy loss: 0.654264. Value loss: 21.462200. Entropy: 0.514055.\n",
      "Iteration 14978: Policy loss: 0.736714. Value loss: 10.786916. Entropy: 0.495792.\n",
      "Iteration 14979: Policy loss: 0.623481. Value loss: 8.637089. Entropy: 0.504444.\n",
      "episode: 6170   score: 210.0  epsilon: 1.0    steps: 476  evaluation reward: 325.55\n",
      "episode: 6171   score: 265.0  epsilon: 1.0    steps: 723  evaluation reward: 325.85\n",
      "episode: 6172   score: 240.0  epsilon: 1.0    steps: 895  evaluation reward: 323.6\n",
      "episode: 6173   score: 550.0  epsilon: 1.0    steps: 897  evaluation reward: 324.4\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14980: Policy loss: -1.854622. Value loss: 234.381668. Entropy: 0.264310.\n",
      "Iteration 14981: Policy loss: -2.398703. Value loss: 92.045425. Entropy: 0.254190.\n",
      "Iteration 14982: Policy loss: -2.273000. Value loss: 98.447510. Entropy: 0.246324.\n",
      "episode: 6174   score: 420.0  epsilon: 1.0    steps: 354  evaluation reward: 321.9\n",
      "episode: 6175   score: 550.0  epsilon: 1.0    steps: 608  evaluation reward: 325.3\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14983: Policy loss: 2.529676. Value loss: 36.541126. Entropy: 0.162759.\n",
      "Iteration 14984: Policy loss: 2.558090. Value loss: 18.931522. Entropy: 0.166696.\n",
      "Iteration 14985: Policy loss: 2.815191. Value loss: 16.325882. Entropy: 0.157457.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14986: Policy loss: -1.590805. Value loss: 42.637829. Entropy: 0.249923.\n",
      "Iteration 14987: Policy loss: -1.884291. Value loss: 20.788481. Entropy: 0.249839.\n",
      "Iteration 14988: Policy loss: -1.898107. Value loss: 16.329836. Entropy: 0.246000.\n",
      "episode: 6176   score: 265.0  epsilon: 1.0    steps: 60  evaluation reward: 323.05\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14989: Policy loss: 1.670439. Value loss: 29.237415. Entropy: 0.467142.\n",
      "Iteration 14990: Policy loss: 1.505689. Value loss: 17.329233. Entropy: 0.472942.\n",
      "Iteration 14991: Policy loss: 1.495441. Value loss: 14.018269. Entropy: 0.490477.\n",
      "episode: 6177   score: 260.0  epsilon: 1.0    steps: 177  evaluation reward: 319.4\n",
      "episode: 6178   score: 210.0  epsilon: 1.0    steps: 754  evaluation reward: 318.65\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14992: Policy loss: 2.321100. Value loss: 24.996937. Entropy: 0.424568.\n",
      "Iteration 14993: Policy loss: 2.184083. Value loss: 15.187573. Entropy: 0.429206.\n",
      "Iteration 14994: Policy loss: 2.038511. Value loss: 16.425432. Entropy: 0.448172.\n",
      "episode: 6179   score: 180.0  epsilon: 1.0    steps: 301  evaluation reward: 314.55\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14995: Policy loss: 1.051125. Value loss: 18.383335. Entropy: 0.402664.\n",
      "Iteration 14996: Policy loss: 1.163212. Value loss: 9.949225. Entropy: 0.394966.\n",
      "Iteration 14997: Policy loss: 1.184602. Value loss: 11.938685. Entropy: 0.391979.\n",
      "episode: 6180   score: 225.0  epsilon: 1.0    steps: 904  evaluation reward: 314.7\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14998: Policy loss: 0.563222. Value loss: 18.084229. Entropy: 0.241880.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14999: Policy loss: 0.481873. Value loss: 13.198456. Entropy: 0.212045.\n",
      "Iteration 15000: Policy loss: 0.601482. Value loss: 12.178910. Entropy: 0.223300.\n",
      "episode: 6181   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 312.55\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15001: Policy loss: 0.764728. Value loss: 33.129414. Entropy: 0.387307.\n",
      "Iteration 15002: Policy loss: 0.554387. Value loss: 16.169422. Entropy: 0.393994.\n",
      "Iteration 15003: Policy loss: 0.746967. Value loss: 14.900433. Entropy: 0.380377.\n",
      "episode: 6182   score: 215.0  epsilon: 1.0    steps: 557  evaluation reward: 311.1\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15004: Policy loss: 0.818181. Value loss: 22.679392. Entropy: 0.350129.\n",
      "Iteration 15005: Policy loss: 1.025180. Value loss: 14.395096. Entropy: 0.350536.\n",
      "Iteration 15006: Policy loss: 0.452619. Value loss: 11.875287. Entropy: 0.339399.\n",
      "episode: 6183   score: 285.0  epsilon: 1.0    steps: 67  evaluation reward: 306.3\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15007: Policy loss: -0.232961. Value loss: 35.119236. Entropy: 0.387970.\n",
      "Iteration 15008: Policy loss: -0.401181. Value loss: 15.468052. Entropy: 0.400565.\n",
      "Iteration 15009: Policy loss: -0.113817. Value loss: 12.826059. Entropy: 0.391408.\n",
      "episode: 6184   score: 270.0  epsilon: 1.0    steps: 189  evaluation reward: 306.85\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15010: Policy loss: 1.028979. Value loss: 28.809967. Entropy: 0.463672.\n",
      "Iteration 15011: Policy loss: 1.026770. Value loss: 12.845924. Entropy: 0.470611.\n",
      "Iteration 15012: Policy loss: 0.767747. Value loss: 11.271777. Entropy: 0.456229.\n",
      "episode: 6185   score: 265.0  epsilon: 1.0    steps: 643  evaluation reward: 306.9\n",
      "episode: 6186   score: 420.0  epsilon: 1.0    steps: 828  evaluation reward: 308.7\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15013: Policy loss: 0.160320. Value loss: 19.457235. Entropy: 0.327422.\n",
      "Iteration 15014: Policy loss: 0.136622. Value loss: 13.380813. Entropy: 0.321272.\n",
      "Iteration 15015: Policy loss: 0.185856. Value loss: 10.668727. Entropy: 0.311536.\n",
      "episode: 6187   score: 210.0  epsilon: 1.0    steps: 493  evaluation reward: 305.5\n",
      "episode: 6188   score: 285.0  epsilon: 1.0    steps: 977  evaluation reward: 306.25\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15016: Policy loss: 2.538201. Value loss: 41.774303. Entropy: 0.408426.\n",
      "Iteration 15017: Policy loss: 2.364427. Value loss: 18.915415. Entropy: 0.424470.\n",
      "Iteration 15018: Policy loss: 2.444239. Value loss: 15.228468. Entropy: 0.418398.\n",
      "episode: 6189   score: 210.0  epsilon: 1.0    steps: 588  evaluation reward: 305.75\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15019: Policy loss: -0.229155. Value loss: 16.373997. Entropy: 0.374551.\n",
      "Iteration 15020: Policy loss: -0.276439. Value loss: 10.345883. Entropy: 0.360702.\n",
      "Iteration 15021: Policy loss: -0.361190. Value loss: 9.882186. Entropy: 0.346847.\n",
      "episode: 6190   score: 155.0  epsilon: 1.0    steps: 205  evaluation reward: 305.8\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15022: Policy loss: 1.869488. Value loss: 19.198395. Entropy: 0.377711.\n",
      "Iteration 15023: Policy loss: 2.036892. Value loss: 14.316983. Entropy: 0.387369.\n",
      "Iteration 15024: Policy loss: 1.732114. Value loss: 10.171390. Entropy: 0.402035.\n",
      "episode: 6191   score: 225.0  epsilon: 1.0    steps: 96  evaluation reward: 305.95\n",
      "episode: 6192   score: 410.0  epsilon: 1.0    steps: 268  evaluation reward: 307.45\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15025: Policy loss: 0.059665. Value loss: 30.692179. Entropy: 0.563594.\n",
      "Iteration 15026: Policy loss: 0.423273. Value loss: 14.296857. Entropy: 0.585147.\n",
      "Iteration 15027: Policy loss: 0.120930. Value loss: 11.171551. Entropy: 0.547244.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15028: Policy loss: -2.122859. Value loss: 148.148895. Entropy: 0.381041.\n",
      "Iteration 15029: Policy loss: -1.636461. Value loss: 76.970085. Entropy: 0.385465.\n",
      "Iteration 15030: Policy loss: -1.986750. Value loss: 51.406948. Entropy: 0.378799.\n",
      "episode: 6193   score: 270.0  epsilon: 1.0    steps: 863  evaluation reward: 307.55\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15031: Policy loss: 0.972794. Value loss: 47.746838. Entropy: 0.488257.\n",
      "Iteration 15032: Policy loss: 0.851687. Value loss: 26.119860. Entropy: 0.466475.\n",
      "Iteration 15033: Policy loss: 0.901930. Value loss: 19.781317. Entropy: 0.496960.\n",
      "episode: 6194   score: 135.0  epsilon: 1.0    steps: 373  evaluation reward: 307.1\n",
      "episode: 6195   score: 280.0  epsilon: 1.0    steps: 675  evaluation reward: 304.45\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15034: Policy loss: 2.424268. Value loss: 36.750160. Entropy: 0.526648.\n",
      "Iteration 15035: Policy loss: 2.600982. Value loss: 19.401358. Entropy: 0.516553.\n",
      "Iteration 15036: Policy loss: 2.174866. Value loss: 16.681705. Entropy: 0.568411.\n",
      "episode: 6196   score: 240.0  epsilon: 1.0    steps: 406  evaluation reward: 303.2\n",
      "episode: 6197   score: 410.0  epsilon: 1.0    steps: 930  evaluation reward: 303.25\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15037: Policy loss: -0.099262. Value loss: 51.901512. Entropy: 0.486395.\n",
      "Iteration 15038: Policy loss: -0.194178. Value loss: 20.862267. Entropy: 0.512357.\n",
      "Iteration 15039: Policy loss: -0.165983. Value loss: 17.510479. Entropy: 0.525362.\n",
      "episode: 6198   score: 210.0  epsilon: 1.0    steps: 516  evaluation reward: 302.25\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15040: Policy loss: -0.158836. Value loss: 31.649569. Entropy: 0.417295.\n",
      "Iteration 15041: Policy loss: -0.202980. Value loss: 19.973974. Entropy: 0.415167.\n",
      "Iteration 15042: Policy loss: -0.003340. Value loss: 18.361876. Entropy: 0.420333.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15043: Policy loss: 0.221827. Value loss: 30.209352. Entropy: 0.439398.\n",
      "Iteration 15044: Policy loss: 0.436516. Value loss: 16.975084. Entropy: 0.442323.\n",
      "Iteration 15045: Policy loss: 0.355999. Value loss: 13.567048. Entropy: 0.442404.\n",
      "episode: 6199   score: 210.0  epsilon: 1.0    steps: 30  evaluation reward: 299.75\n",
      "episode: 6200   score: 210.0  epsilon: 1.0    steps: 860  evaluation reward: 298.4\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15046: Policy loss: 1.410612. Value loss: 31.177971. Entropy: 0.487372.\n",
      "Iteration 15047: Policy loss: 1.286050. Value loss: 15.201303. Entropy: 0.477197.\n",
      "Iteration 15048: Policy loss: 1.071658. Value loss: 11.911545. Entropy: 0.463392.\n",
      "now time :  2019-02-25 23:21:06.581834\n",
      "Training went nowhere, starting again at best model\n",
      "episode: 6201   score: 365.0  epsilon: 1.0    steps: 240  evaluation reward: 298.4\n",
      "episode: 6202   score: 210.0  epsilon: 1.0    steps: 407  evaluation reward: 295.9\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15049: Policy loss: 0.646305. Value loss: 55.147541. Entropy: 0.463248.\n",
      "Iteration 15050: Policy loss: 0.390806. Value loss: 26.049881. Entropy: 0.474605.\n",
      "Iteration 15051: Policy loss: 0.662390. Value loss: 21.600986. Entropy: 0.474689.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15052: Policy loss: 0.623738. Value loss: 33.671871. Entropy: 0.382003.\n",
      "Iteration 15053: Policy loss: 0.542402. Value loss: 23.209148. Entropy: 0.365788.\n",
      "Iteration 15054: Policy loss: 0.741737. Value loss: 18.523790. Entropy: 0.381338.\n",
      "episode: 6203   score: 285.0  epsilon: 1.0    steps: 639  evaluation reward: 291.3\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15055: Policy loss: -1.021760. Value loss: 32.479031. Entropy: 0.255356.\n",
      "Iteration 15056: Policy loss: -1.599742. Value loss: 20.719332. Entropy: 0.261571.\n",
      "Iteration 15057: Policy loss: -1.059026. Value loss: 14.175705. Entropy: 0.259775.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15058: Policy loss: 1.024670. Value loss: 32.945774. Entropy: 0.223318.\n",
      "Iteration 15059: Policy loss: 1.446948. Value loss: 17.240042. Entropy: 0.227609.\n",
      "Iteration 15060: Policy loss: 0.961146. Value loss: 15.150309. Entropy: 0.243745.\n",
      "episode: 6204   score: 225.0  epsilon: 1.0    steps: 115  evaluation reward: 292.2\n",
      "episode: 6205   score: 445.0  epsilon: 1.0    steps: 656  evaluation reward: 293.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15061: Policy loss: 0.913162. Value loss: 40.959023. Entropy: 0.406971.\n",
      "Iteration 15062: Policy loss: 0.630818. Value loss: 22.996719. Entropy: 0.432329.\n",
      "Iteration 15063: Policy loss: 0.807782. Value loss: 19.405457. Entropy: 0.422063.\n",
      "episode: 6206   score: 240.0  epsilon: 1.0    steps: 477  evaluation reward: 292.25\n",
      "episode: 6207   score: 260.0  epsilon: 1.0    steps: 847  evaluation reward: 291.45\n",
      "episode: 6208   score: 420.0  epsilon: 1.0    steps: 966  evaluation reward: 293.15\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15064: Policy loss: 1.020694. Value loss: 27.772072. Entropy: 0.409107.\n",
      "Iteration 15065: Policy loss: 1.049446. Value loss: 16.633759. Entropy: 0.382153.\n",
      "Iteration 15066: Policy loss: 0.912709. Value loss: 14.768144. Entropy: 0.393872.\n",
      "episode: 6209   score: 585.0  epsilon: 1.0    steps: 354  evaluation reward: 296.4\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15067: Policy loss: -0.955720. Value loss: 46.202999. Entropy: 0.481386.\n",
      "Iteration 15068: Policy loss: -0.597760. Value loss: 32.437603. Entropy: 0.460549.\n",
      "Iteration 15069: Policy loss: -1.013635. Value loss: 30.944216. Entropy: 0.471649.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15070: Policy loss: 2.158087. Value loss: 28.330542. Entropy: 0.498615.\n",
      "Iteration 15071: Policy loss: 1.656377. Value loss: 22.210291. Entropy: 0.468328.\n",
      "Iteration 15072: Policy loss: 1.951546. Value loss: 14.573705. Entropy: 0.496911.\n",
      "episode: 6210   score: 210.0  epsilon: 1.0    steps: 766  evaluation reward: 295.75\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15073: Policy loss: 0.233839. Value loss: 26.653460. Entropy: 0.294292.\n",
      "Iteration 15074: Policy loss: 0.518182. Value loss: 16.262852. Entropy: 0.262466.\n",
      "Iteration 15075: Policy loss: 0.054357. Value loss: 15.556719. Entropy: 0.289813.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15076: Policy loss: 0.810205. Value loss: 19.869734. Entropy: 0.273473.\n",
      "Iteration 15077: Policy loss: 0.906894. Value loss: 13.375192. Entropy: 0.283748.\n",
      "Iteration 15078: Policy loss: 0.776604. Value loss: 9.945758. Entropy: 0.270487.\n",
      "episode: 6211   score: 225.0  epsilon: 1.0    steps: 95  evaluation reward: 295.15\n",
      "episode: 6212   score: 210.0  epsilon: 1.0    steps: 979  evaluation reward: 292.6\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15079: Policy loss: -3.744567. Value loss: 188.044922. Entropy: 0.101919.\n",
      "Iteration 15080: Policy loss: -3.905682. Value loss: 71.428322. Entropy: 0.091349.\n",
      "Iteration 15081: Policy loss: -3.866273. Value loss: 60.041103. Entropy: 0.113719.\n",
      "episode: 6213   score: 305.0  epsilon: 1.0    steps: 617  evaluation reward: 291.7\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15082: Policy loss: 3.238129. Value loss: 34.882332. Entropy: 0.328794.\n",
      "Iteration 15083: Policy loss: 3.151841. Value loss: 19.074865. Entropy: 0.361365.\n",
      "Iteration 15084: Policy loss: 3.027445. Value loss: 15.191547. Entropy: 0.369471.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15085: Policy loss: 0.613983. Value loss: 26.881325. Entropy: 0.470085.\n",
      "Iteration 15086: Policy loss: 0.445640. Value loss: 16.416090. Entropy: 0.474938.\n",
      "Iteration 15087: Policy loss: 0.244557. Value loss: 14.797917. Entropy: 0.475946.\n",
      "episode: 6214   score: 775.0  epsilon: 1.0    steps: 183  evaluation reward: 297.95\n",
      "episode: 6215   score: 345.0  epsilon: 1.0    steps: 341  evaluation reward: 299.3\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15088: Policy loss: 1.057118. Value loss: 24.143984. Entropy: 0.393982.\n",
      "Iteration 15089: Policy loss: 0.955975. Value loss: 15.802315. Entropy: 0.396528.\n",
      "Iteration 15090: Policy loss: 1.062857. Value loss: 13.704601. Entropy: 0.410367.\n",
      "episode: 6216   score: 320.0  epsilon: 1.0    steps: 486  evaluation reward: 299.75\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15091: Policy loss: 1.169197. Value loss: 28.350729. Entropy: 0.434781.\n",
      "Iteration 15092: Policy loss: 1.254009. Value loss: 17.763283. Entropy: 0.425160.\n",
      "Iteration 15093: Policy loss: 1.267782. Value loss: 13.993814. Entropy: 0.435368.\n",
      "episode: 6217   score: 320.0  epsilon: 1.0    steps: 866  evaluation reward: 301.95\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15094: Policy loss: 0.136592. Value loss: 47.676003. Entropy: 0.518804.\n",
      "Iteration 15095: Policy loss: 0.120870. Value loss: 28.358215. Entropy: 0.489113.\n",
      "Iteration 15096: Policy loss: -0.026086. Value loss: 23.374571. Entropy: 0.529854.\n",
      "episode: 6218   score: 240.0  epsilon: 1.0    steps: 964  evaluation reward: 302.8\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15097: Policy loss: -0.337218. Value loss: 27.756466. Entropy: 0.352039.\n",
      "Iteration 15098: Policy loss: -0.143312. Value loss: 21.254992. Entropy: 0.362040.\n",
      "Iteration 15099: Policy loss: -0.397659. Value loss: 15.715265. Entropy: 0.348797.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15100: Policy loss: 0.506765. Value loss: 40.498150. Entropy: 0.270192.\n",
      "Iteration 15101: Policy loss: 0.307317. Value loss: 23.292328. Entropy: 0.266232.\n",
      "Iteration 15102: Policy loss: 0.439632. Value loss: 17.668602. Entropy: 0.288292.\n",
      "episode: 6219   score: 540.0  epsilon: 1.0    steps: 104  evaluation reward: 302.75\n",
      "episode: 6220   score: 220.0  epsilon: 1.0    steps: 218  evaluation reward: 302.2\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15103: Policy loss: -2.887048. Value loss: 206.626022. Entropy: 0.278592.\n",
      "Iteration 15104: Policy loss: -3.227776. Value loss: 77.513451. Entropy: 0.280756.\n",
      "Iteration 15105: Policy loss: -2.893262. Value loss: 48.848248. Entropy: 0.302028.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15106: Policy loss: -0.514899. Value loss: 22.369755. Entropy: 0.445188.\n",
      "Iteration 15107: Policy loss: -0.473490. Value loss: 14.659536. Entropy: 0.450645.\n",
      "Iteration 15108: Policy loss: -0.334237. Value loss: 13.954515. Entropy: 0.441513.\n",
      "episode: 6221   score: 260.0  epsilon: 1.0    steps: 491  evaluation reward: 299.45\n",
      "episode: 6222   score: 445.0  epsilon: 1.0    steps: 594  evaluation reward: 301.3\n",
      "episode: 6223   score: 470.0  epsilon: 1.0    steps: 768  evaluation reward: 301.1\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15109: Policy loss: 0.385654. Value loss: 34.401630. Entropy: 0.538614.\n",
      "Iteration 15110: Policy loss: 0.393025. Value loss: 22.624580. Entropy: 0.541918.\n",
      "Iteration 15111: Policy loss: 0.273246. Value loss: 23.827780. Entropy: 0.536186.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15112: Policy loss: 0.320105. Value loss: 23.218513. Entropy: 0.497347.\n",
      "Iteration 15113: Policy loss: 0.636267. Value loss: 13.986642. Entropy: 0.500220.\n",
      "Iteration 15114: Policy loss: 0.507480. Value loss: 12.049116. Entropy: 0.486650.\n",
      "episode: 6224   score: 390.0  epsilon: 1.0    steps: 873  evaluation reward: 300.2\n",
      "episode: 6225   score: 285.0  epsilon: 1.0    steps: 983  evaluation reward: 301.5\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15115: Policy loss: 0.159432. Value loss: 35.402042. Entropy: 0.486012.\n",
      "Iteration 15116: Policy loss: 0.524128. Value loss: 21.319754. Entropy: 0.474250.\n",
      "Iteration 15117: Policy loss: 0.125466. Value loss: 16.760813. Entropy: 0.475297.\n",
      "episode: 6226   score: 420.0  epsilon: 1.0    steps: 342  evaluation reward: 303.6\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15118: Policy loss: 1.219768. Value loss: 34.697910. Entropy: 0.400422.\n",
      "Iteration 15119: Policy loss: 1.019537. Value loss: 22.018322. Entropy: 0.427221.\n",
      "Iteration 15120: Policy loss: 1.159723. Value loss: 17.355562. Entropy: 0.397768.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15121: Policy loss: 1.397509. Value loss: 25.173168. Entropy: 0.412783.\n",
      "Iteration 15122: Policy loss: 1.404185. Value loss: 15.356667. Entropy: 0.417136.\n",
      "Iteration 15123: Policy loss: 1.427952. Value loss: 13.263410. Entropy: 0.406857.\n",
      "episode: 6227   score: 245.0  epsilon: 1.0    steps: 23  evaluation reward: 301.7\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15124: Policy loss: -0.858359. Value loss: 24.355762. Entropy: 0.452974.\n",
      "Iteration 15125: Policy loss: -1.065559. Value loss: 12.780541. Entropy: 0.449210.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15126: Policy loss: -1.359000. Value loss: 9.809820. Entropy: 0.451981.\n",
      "episode: 6228   score: 250.0  epsilon: 1.0    steps: 256  evaluation reward: 298.4\n",
      "episode: 6229   score: 260.0  epsilon: 1.0    steps: 464  evaluation reward: 298.9\n",
      "episode: 6230   score: 260.0  epsilon: 1.0    steps: 613  evaluation reward: 296.15\n",
      "episode: 6231   score: 210.0  epsilon: 1.0    steps: 684  evaluation reward: 296.7\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15127: Policy loss: 3.427373. Value loss: 27.316761. Entropy: 0.421318.\n",
      "Iteration 15128: Policy loss: 3.004091. Value loss: 13.481393. Entropy: 0.397344.\n",
      "Iteration 15129: Policy loss: 3.394981. Value loss: 11.996657. Entropy: 0.404032.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15130: Policy loss: -0.401854. Value loss: 20.575052. Entropy: 0.477096.\n",
      "Iteration 15131: Policy loss: -0.176020. Value loss: 11.365851. Entropy: 0.488136.\n",
      "Iteration 15132: Policy loss: -0.329001. Value loss: 11.621424. Entropy: 0.475653.\n",
      "episode: 6232   score: 210.0  epsilon: 1.0    steps: 372  evaluation reward: 297.0\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15133: Policy loss: 3.826513. Value loss: 36.053959. Entropy: 0.460732.\n",
      "Iteration 15134: Policy loss: 3.728162. Value loss: 21.157125. Entropy: 0.421802.\n",
      "Iteration 15135: Policy loss: 3.646644. Value loss: 16.880770. Entropy: 0.448501.\n",
      "episode: 6233   score: 210.0  epsilon: 1.0    steps: 845  evaluation reward: 296.5\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15136: Policy loss: -0.242243. Value loss: 15.768943. Entropy: 0.499246.\n",
      "Iteration 15137: Policy loss: -0.430230. Value loss: 11.043848. Entropy: 0.502818.\n",
      "Iteration 15138: Policy loss: -0.127139. Value loss: 7.987965. Entropy: 0.491750.\n",
      "episode: 6234   score: 345.0  epsilon: 1.0    steps: 949  evaluation reward: 298.6\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15139: Policy loss: 1.326185. Value loss: 20.168810. Entropy: 0.510614.\n",
      "Iteration 15140: Policy loss: 1.222916. Value loss: 11.879489. Entropy: 0.514778.\n",
      "Iteration 15141: Policy loss: 1.235719. Value loss: 11.034111. Entropy: 0.544462.\n",
      "episode: 6235   score: 210.0  epsilon: 1.0    steps: 486  evaluation reward: 297.05\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15142: Policy loss: -1.267807. Value loss: 26.541655. Entropy: 0.518230.\n",
      "Iteration 15143: Policy loss: -1.372414. Value loss: 15.079370. Entropy: 0.514906.\n",
      "Iteration 15144: Policy loss: -1.240306. Value loss: 12.124392. Entropy: 0.494047.\n",
      "episode: 6236   score: 260.0  epsilon: 1.0    steps: 677  evaluation reward: 296.45\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15145: Policy loss: -2.319248. Value loss: 28.104624. Entropy: 0.511556.\n",
      "Iteration 15146: Policy loss: -2.715278. Value loss: 16.791996. Entropy: 0.509854.\n",
      "Iteration 15147: Policy loss: -2.579370. Value loss: 13.339314. Entropy: 0.515908.\n",
      "episode: 6237   score: 375.0  epsilon: 1.0    steps: 70  evaluation reward: 298.1\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15148: Policy loss: 0.374028. Value loss: 184.429947. Entropy: 0.643865.\n",
      "Iteration 15149: Policy loss: 0.344144. Value loss: 64.504539. Entropy: 0.605617.\n",
      "Iteration 15150: Policy loss: 0.605094. Value loss: 46.665325. Entropy: 0.593679.\n",
      "episode: 6238   score: 395.0  epsilon: 1.0    steps: 199  evaluation reward: 300.5\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15151: Policy loss: -1.679824. Value loss: 35.386612. Entropy: 0.549893.\n",
      "Iteration 15152: Policy loss: -1.700492. Value loss: 21.319296. Entropy: 0.552263.\n",
      "Iteration 15153: Policy loss: -1.711443. Value loss: 16.833607. Entropy: 0.540585.\n",
      "episode: 6239   score: 335.0  epsilon: 1.0    steps: 602  evaluation reward: 301.75\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15154: Policy loss: 1.901268. Value loss: 22.737276. Entropy: 0.446371.\n",
      "Iteration 15155: Policy loss: 1.891597. Value loss: 10.973454. Entropy: 0.443084.\n",
      "Iteration 15156: Policy loss: 2.084386. Value loss: 7.740852. Entropy: 0.438142.\n",
      "episode: 6240   score: 210.0  epsilon: 1.0    steps: 475  evaluation reward: 298.95\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15157: Policy loss: 0.521457. Value loss: 28.904392. Entropy: 0.615774.\n",
      "Iteration 15158: Policy loss: 0.630815. Value loss: 16.273016. Entropy: 0.611074.\n",
      "Iteration 15159: Policy loss: 0.410920. Value loss: 13.406329. Entropy: 0.612552.\n",
      "episode: 6241   score: 540.0  epsilon: 1.0    steps: 841  evaluation reward: 299.0\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15160: Policy loss: 0.938235. Value loss: 35.617645. Entropy: 0.586717.\n",
      "Iteration 15161: Policy loss: 0.998718. Value loss: 22.614878. Entropy: 0.555011.\n",
      "Iteration 15162: Policy loss: 0.621400. Value loss: 17.950172. Entropy: 0.551738.\n",
      "episode: 6242   score: 435.0  epsilon: 1.0    steps: 342  evaluation reward: 301.55\n",
      "episode: 6243   score: 250.0  epsilon: 1.0    steps: 713  evaluation reward: 302.25\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15163: Policy loss: 0.848285. Value loss: 33.937584. Entropy: 0.588885.\n",
      "Iteration 15164: Policy loss: 0.981238. Value loss: 16.584209. Entropy: 0.612715.\n",
      "Iteration 15165: Policy loss: 1.100570. Value loss: 11.667437. Entropy: 0.590971.\n",
      "episode: 6244   score: 285.0  epsilon: 1.0    steps: 928  evaluation reward: 303.0\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15166: Policy loss: -3.203802. Value loss: 172.151825. Entropy: 0.495385.\n",
      "Iteration 15167: Policy loss: -2.747107. Value loss: 70.267075. Entropy: 0.486040.\n",
      "Iteration 15168: Policy loss: -3.171228. Value loss: 68.211899. Entropy: 0.483349.\n",
      "episode: 6245   score: 280.0  epsilon: 1.0    steps: 39  evaluation reward: 303.7\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15169: Policy loss: 0.223067. Value loss: 25.574007. Entropy: 0.364248.\n",
      "Iteration 15170: Policy loss: 0.512263. Value loss: 15.167949. Entropy: 0.348510.\n",
      "Iteration 15171: Policy loss: 0.128057. Value loss: 11.403210. Entropy: 0.359595.\n",
      "episode: 6246   score: 270.0  epsilon: 1.0    steps: 240  evaluation reward: 302.15\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15172: Policy loss: 2.981559. Value loss: 90.148659. Entropy: 0.294126.\n",
      "Iteration 15173: Policy loss: 2.781798. Value loss: 36.174530. Entropy: 0.283457.\n",
      "Iteration 15174: Policy loss: 2.773081. Value loss: 24.449783. Entropy: 0.286172.\n",
      "episode: 6247   score: 440.0  epsilon: 1.0    steps: 431  evaluation reward: 302.9\n",
      "episode: 6248   score: 220.0  epsilon: 1.0    steps: 577  evaluation reward: 301.8\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15175: Policy loss: -0.031043. Value loss: 24.163855. Entropy: 0.272750.\n",
      "Iteration 15176: Policy loss: -0.101937. Value loss: 14.649121. Entropy: 0.284829.\n",
      "Iteration 15177: Policy loss: 0.065611. Value loss: 11.639095. Entropy: 0.274221.\n",
      "episode: 6249   score: 240.0  epsilon: 1.0    steps: 830  evaluation reward: 301.55\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15178: Policy loss: -0.775217. Value loss: 48.480980. Entropy: 0.487387.\n",
      "Iteration 15179: Policy loss: -0.559024. Value loss: 30.954182. Entropy: 0.481712.\n",
      "Iteration 15180: Policy loss: -0.683686. Value loss: 23.436668. Entropy: 0.476048.\n",
      "episode: 6250   score: 290.0  epsilon: 1.0    steps: 327  evaluation reward: 297.4\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15181: Policy loss: -0.027834. Value loss: 37.321129. Entropy: 0.506507.\n",
      "Iteration 15182: Policy loss: 0.134130. Value loss: 24.351900. Entropy: 0.499232.\n",
      "Iteration 15183: Policy loss: 0.048247. Value loss: 20.962059. Entropy: 0.499420.\n",
      "now time :  2019-02-25 23:23:38.813290\n",
      "episode: 6251   score: 295.0  epsilon: 1.0    steps: 654  evaluation reward: 298.55\n",
      "episode: 6252   score: 360.0  epsilon: 1.0    steps: 965  evaluation reward: 299.25\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15184: Policy loss: 0.008080. Value loss: 56.902073. Entropy: 0.436649.\n",
      "Iteration 15185: Policy loss: -0.193763. Value loss: 31.627321. Entropy: 0.427302.\n",
      "Iteration 15186: Policy loss: -0.251996. Value loss: 22.578678. Entropy: 0.416790.\n",
      "episode: 6253   score: 285.0  epsilon: 1.0    steps: 29  evaluation reward: 300.3\n",
      "Training network. lr: 0.000134. clip: 0.053468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15187: Policy loss: 0.530889. Value loss: 37.311756. Entropy: 0.535032.\n",
      "Iteration 15188: Policy loss: 0.398243. Value loss: 22.982355. Entropy: 0.554684.\n",
      "Iteration 15189: Policy loss: 0.425622. Value loss: 17.369209. Entropy: 0.522993.\n",
      "episode: 6254   score: 210.0  epsilon: 1.0    steps: 388  evaluation reward: 300.85\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15190: Policy loss: -0.593709. Value loss: 23.762758. Entropy: 0.448690.\n",
      "Iteration 15191: Policy loss: -0.946374. Value loss: 13.661889. Entropy: 0.467052.\n",
      "Iteration 15192: Policy loss: -1.036475. Value loss: 13.373862. Entropy: 0.455904.\n",
      "episode: 6255   score: 220.0  epsilon: 1.0    steps: 881  evaluation reward: 300.95\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15193: Policy loss: 1.384543. Value loss: 22.057804. Entropy: 0.387665.\n",
      "Iteration 15194: Policy loss: 1.279642. Value loss: 12.941618. Entropy: 0.388184.\n",
      "Iteration 15195: Policy loss: 1.357941. Value loss: 11.541144. Entropy: 0.399695.\n",
      "episode: 6256   score: 210.0  epsilon: 1.0    steps: 301  evaluation reward: 300.65\n",
      "episode: 6257   score: 260.0  epsilon: 1.0    steps: 524  evaluation reward: 300.65\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15196: Policy loss: -1.869877. Value loss: 37.065189. Entropy: 0.455419.\n",
      "Iteration 15197: Policy loss: -2.046340. Value loss: 18.803928. Entropy: 0.487478.\n",
      "Iteration 15198: Policy loss: -1.927608. Value loss: 14.495810. Entropy: 0.494845.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15199: Policy loss: -3.957029. Value loss: 208.095276. Entropy: 0.538400.\n",
      "Iteration 15200: Policy loss: -4.188854. Value loss: 100.556252. Entropy: 0.516328.\n",
      "Iteration 15201: Policy loss: -3.655544. Value loss: 49.042324. Entropy: 0.509282.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15202: Policy loss: -2.201827. Value loss: 63.753895. Entropy: 0.338689.\n",
      "Iteration 15203: Policy loss: -2.321674. Value loss: 37.362553. Entropy: 0.323377.\n",
      "Iteration 15204: Policy loss: -2.195794. Value loss: 22.578508. Entropy: 0.322003.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15205: Policy loss: 3.166914. Value loss: 50.028694. Entropy: 0.507532.\n",
      "Iteration 15206: Policy loss: 3.233099. Value loss: 23.419952. Entropy: 0.464538.\n",
      "Iteration 15207: Policy loss: 3.390158. Value loss: 14.735331. Entropy: 0.489790.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15208: Policy loss: 1.924960. Value loss: 44.082680. Entropy: 0.362755.\n",
      "Iteration 15209: Policy loss: 1.361788. Value loss: 25.296635. Entropy: 0.367846.\n",
      "Iteration 15210: Policy loss: 1.576275. Value loss: 17.379194. Entropy: 0.369283.\n",
      "episode: 6258   score: 385.0  epsilon: 1.0    steps: 122  evaluation reward: 300.4\n",
      "episode: 6259   score: 555.0  epsilon: 1.0    steps: 165  evaluation reward: 303.35\n",
      "episode: 6260   score: 485.0  epsilon: 1.0    steps: 1006  evaluation reward: 305.0\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15211: Policy loss: 4.584350. Value loss: 43.820644. Entropy: 0.520340.\n",
      "Iteration 15212: Policy loss: 4.552039. Value loss: 20.147486. Entropy: 0.529043.\n",
      "Iteration 15213: Policy loss: 4.550345. Value loss: 15.669183. Entropy: 0.538493.\n",
      "episode: 6261   score: 260.0  epsilon: 1.0    steps: 542  evaluation reward: 305.0\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15214: Policy loss: 2.817394. Value loss: 38.464924. Entropy: 0.548626.\n",
      "Iteration 15215: Policy loss: 2.795146. Value loss: 22.467003. Entropy: 0.549180.\n",
      "Iteration 15216: Policy loss: 2.464473. Value loss: 17.677525. Entropy: 0.536954.\n",
      "episode: 6262   score: 575.0  epsilon: 1.0    steps: 471  evaluation reward: 304.0\n",
      "episode: 6263   score: 360.0  epsilon: 1.0    steps: 860  evaluation reward: 306.05\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15217: Policy loss: 1.408187. Value loss: 61.459927. Entropy: 0.442322.\n",
      "Iteration 15218: Policy loss: 1.226736. Value loss: 29.826256. Entropy: 0.419635.\n",
      "Iteration 15219: Policy loss: 1.455225. Value loss: 28.075935. Entropy: 0.421863.\n",
      "episode: 6264   score: 475.0  epsilon: 1.0    steps: 647  evaluation reward: 306.75\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15220: Policy loss: -0.345766. Value loss: 34.970112. Entropy: 0.475448.\n",
      "Iteration 15221: Policy loss: -0.222122. Value loss: 21.993011. Entropy: 0.444248.\n",
      "Iteration 15222: Policy loss: -0.232645. Value loss: 20.605656. Entropy: 0.441062.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15223: Policy loss: 2.765222. Value loss: 43.429062. Entropy: 0.422381.\n",
      "Iteration 15224: Policy loss: 2.626505. Value loss: 20.193129. Entropy: 0.426619.\n",
      "Iteration 15225: Policy loss: 2.808776. Value loss: 19.581514. Entropy: 0.430547.\n",
      "episode: 6265   score: 430.0  epsilon: 1.0    steps: 334  evaluation reward: 308.35\n",
      "episode: 6266   score: 155.0  epsilon: 1.0    steps: 998  evaluation reward: 307.8\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15226: Policy loss: 1.382104. Value loss: 34.605007. Entropy: 0.328470.\n",
      "Iteration 15227: Policy loss: 1.400932. Value loss: 20.647753. Entropy: 0.329456.\n",
      "Iteration 15228: Policy loss: 1.461933. Value loss: 16.150734. Entropy: 0.329242.\n",
      "episode: 6267   score: 260.0  epsilon: 1.0    steps: 45  evaluation reward: 308.6\n",
      "episode: 6268   score: 260.0  epsilon: 1.0    steps: 167  evaluation reward: 309.1\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15229: Policy loss: -0.621611. Value loss: 27.393360. Entropy: 0.529867.\n",
      "Iteration 15230: Policy loss: -0.620073. Value loss: 17.793322. Entropy: 0.503393.\n",
      "Iteration 15231: Policy loss: -0.811663. Value loss: 15.712556. Entropy: 0.507264.\n",
      "episode: 6269   score: 210.0  epsilon: 1.0    steps: 851  evaluation reward: 307.9\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15232: Policy loss: -0.056247. Value loss: 42.263405. Entropy: 0.471607.\n",
      "Iteration 15233: Policy loss: 0.322170. Value loss: 19.244349. Entropy: 0.486865.\n",
      "Iteration 15234: Policy loss: 0.015157. Value loss: 16.565065. Entropy: 0.491224.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15235: Policy loss: -2.267884. Value loss: 143.187225. Entropy: 0.425275.\n",
      "Iteration 15236: Policy loss: -2.305264. Value loss: 82.500328. Entropy: 0.415762.\n",
      "Iteration 15237: Policy loss: -2.342856. Value loss: 51.280102. Entropy: 0.417171.\n",
      "episode: 6270   score: 360.0  epsilon: 1.0    steps: 429  evaluation reward: 309.4\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15238: Policy loss: 0.880206. Value loss: 38.509270. Entropy: 0.296299.\n",
      "Iteration 15239: Policy loss: 0.863816. Value loss: 19.229679. Entropy: 0.300437.\n",
      "Iteration 15240: Policy loss: 1.046163. Value loss: 15.697963. Entropy: 0.296198.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15241: Policy loss: -0.249773. Value loss: 35.016434. Entropy: 0.496785.\n",
      "Iteration 15242: Policy loss: -0.119648. Value loss: 17.890738. Entropy: 0.496235.\n",
      "Iteration 15243: Policy loss: -0.222708. Value loss: 13.459632. Entropy: 0.513696.\n",
      "episode: 6271   score: 285.0  epsilon: 1.0    steps: 269  evaluation reward: 309.6\n",
      "episode: 6272   score: 330.0  epsilon: 1.0    steps: 689  evaluation reward: 310.5\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15244: Policy loss: 1.149476. Value loss: 52.831635. Entropy: 0.285321.\n",
      "Iteration 15245: Policy loss: 0.572524. Value loss: 28.477806. Entropy: 0.279209.\n",
      "Iteration 15246: Policy loss: 1.241252. Value loss: 21.826689. Entropy: 0.270065.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15247: Policy loss: 2.328015. Value loss: 37.130726. Entropy: 0.385387.\n",
      "Iteration 15248: Policy loss: 2.296809. Value loss: 23.212570. Entropy: 0.386028.\n",
      "Iteration 15249: Policy loss: 2.511524. Value loss: 18.330822. Entropy: 0.390749.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15250: Policy loss: 2.317391. Value loss: 36.991035. Entropy: 0.377859.\n",
      "Iteration 15251: Policy loss: 2.288292. Value loss: 24.084846. Entropy: 0.389371.\n",
      "Iteration 15252: Policy loss: 2.300810. Value loss: 19.230942. Entropy: 0.383860.\n",
      "episode: 6273   score: 715.0  epsilon: 1.0    steps: 625  evaluation reward: 312.15\n",
      "episode: 6274   score: 270.0  epsilon: 1.0    steps: 793  evaluation reward: 310.65\n",
      "episode: 6275   score: 330.0  epsilon: 1.0    steps: 967  evaluation reward: 308.45\n",
      "Training network. lr: 0.000133. clip: 0.053155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15253: Policy loss: 1.229157. Value loss: 36.098679. Entropy: 0.550969.\n",
      "Iteration 15254: Policy loss: 1.247334. Value loss: 22.304287. Entropy: 0.543376.\n",
      "Iteration 15255: Policy loss: 1.207406. Value loss: 16.578377. Entropy: 0.534481.\n",
      "episode: 6276   score: 475.0  epsilon: 1.0    steps: 48  evaluation reward: 310.55\n",
      "episode: 6277   score: 180.0  epsilon: 1.0    steps: 282  evaluation reward: 309.75\n",
      "episode: 6278   score: 240.0  epsilon: 1.0    steps: 425  evaluation reward: 310.05\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15256: Policy loss: -0.177517. Value loss: 21.327196. Entropy: 0.401317.\n",
      "Iteration 15257: Policy loss: -0.199387. Value loss: 18.309290. Entropy: 0.385056.\n",
      "Iteration 15258: Policy loss: -0.229306. Value loss: 13.208226. Entropy: 0.423549.\n",
      "episode: 6279   score: 210.0  epsilon: 1.0    steps: 666  evaluation reward: 310.35\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15259: Policy loss: -0.379808. Value loss: 47.974743. Entropy: 0.359375.\n",
      "Iteration 15260: Policy loss: -0.463245. Value loss: 34.214611. Entropy: 0.367165.\n",
      "Iteration 15261: Policy loss: -0.413176. Value loss: 29.991302. Entropy: 0.347241.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15262: Policy loss: 1.431852. Value loss: 31.019461. Entropy: 0.306837.\n",
      "Iteration 15263: Policy loss: 1.431520. Value loss: 19.816845. Entropy: 0.293964.\n",
      "Iteration 15264: Policy loss: 1.282355. Value loss: 14.450905. Entropy: 0.303464.\n",
      "episode: 6280   score: 485.0  epsilon: 1.0    steps: 226  evaluation reward: 312.95\n",
      "episode: 6281   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 312.95\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15265: Policy loss: -3.420217. Value loss: 213.246201. Entropy: 0.320569.\n",
      "Iteration 15266: Policy loss: -4.748476. Value loss: 119.935524. Entropy: 0.291396.\n",
      "Iteration 15267: Policy loss: -4.003782. Value loss: 84.973686. Entropy: 0.302714.\n",
      "episode: 6282   score: 210.0  epsilon: 1.0    steps: 991  evaluation reward: 312.9\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15268: Policy loss: -0.345919. Value loss: 37.149590. Entropy: 0.333407.\n",
      "Iteration 15269: Policy loss: -0.522153. Value loss: 25.798210. Entropy: 0.340306.\n",
      "Iteration 15270: Policy loss: -0.385652. Value loss: 19.064583. Entropy: 0.337848.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15271: Policy loss: 0.027381. Value loss: 29.172432. Entropy: 0.433554.\n",
      "Iteration 15272: Policy loss: 0.412274. Value loss: 19.276382. Entropy: 0.438735.\n",
      "Iteration 15273: Policy loss: 0.170245. Value loss: 15.153031. Entropy: 0.440911.\n",
      "episode: 6283   score: 230.0  epsilon: 1.0    steps: 49  evaluation reward: 312.35\n",
      "episode: 6284   score: 380.0  epsilon: 1.0    steps: 628  evaluation reward: 313.45\n",
      "episode: 6285   score: 210.0  epsilon: 1.0    steps: 647  evaluation reward: 312.9\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15274: Policy loss: 0.009269. Value loss: 46.044041. Entropy: 0.221817.\n",
      "Iteration 15275: Policy loss: 0.003648. Value loss: 22.455616. Entropy: 0.223988.\n",
      "Iteration 15276: Policy loss: -0.050174. Value loss: 17.220955. Entropy: 0.223674.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15277: Policy loss: -0.271968. Value loss: 28.333378. Entropy: 0.323593.\n",
      "Iteration 15278: Policy loss: -0.456820. Value loss: 16.773449. Entropy: 0.317478.\n",
      "Iteration 15279: Policy loss: -0.388390. Value loss: 14.424570. Entropy: 0.302026.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15280: Policy loss: -0.605478. Value loss: 42.262394. Entropy: 0.311966.\n",
      "Iteration 15281: Policy loss: -1.026912. Value loss: 24.991011. Entropy: 0.323283.\n",
      "Iteration 15282: Policy loss: -0.639194. Value loss: 20.607054. Entropy: 0.297512.\n",
      "episode: 6286   score: 260.0  epsilon: 1.0    steps: 845  evaluation reward: 311.3\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15283: Policy loss: 2.258321. Value loss: 41.397461. Entropy: 0.351736.\n",
      "Iteration 15284: Policy loss: 2.215259. Value loss: 22.392824. Entropy: 0.336040.\n",
      "Iteration 15285: Policy loss: 2.380959. Value loss: 17.338568. Entropy: 0.320961.\n",
      "episode: 6287   score: 295.0  epsilon: 1.0    steps: 424  evaluation reward: 312.15\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15286: Policy loss: 1.295715. Value loss: 27.700012. Entropy: 0.426249.\n",
      "Iteration 15287: Policy loss: 1.295022. Value loss: 20.320974. Entropy: 0.440157.\n",
      "Iteration 15288: Policy loss: 1.324379. Value loss: 16.373106. Entropy: 0.435498.\n",
      "episode: 6288   score: 775.0  epsilon: 1.0    steps: 357  evaluation reward: 317.05\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15289: Policy loss: 1.165313. Value loss: 50.351540. Entropy: 0.472848.\n",
      "Iteration 15290: Policy loss: 1.530746. Value loss: 23.826132. Entropy: 0.461449.\n",
      "Iteration 15291: Policy loss: 1.223444. Value loss: 17.382919. Entropy: 0.481269.\n",
      "episode: 6289   score: 215.0  epsilon: 1.0    steps: 523  evaluation reward: 317.1\n",
      "episode: 6290   score: 285.0  epsilon: 1.0    steps: 642  evaluation reward: 318.4\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15292: Policy loss: 1.816470. Value loss: 32.228268. Entropy: 0.454204.\n",
      "Iteration 15293: Policy loss: 1.216143. Value loss: 19.827568. Entropy: 0.472963.\n",
      "Iteration 15294: Policy loss: 1.641911. Value loss: 13.790459. Entropy: 0.467534.\n",
      "episode: 6291   score: 300.0  epsilon: 1.0    steps: 58  evaluation reward: 319.15\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15295: Policy loss: 1.483759. Value loss: 18.798960. Entropy: 0.325864.\n",
      "Iteration 15296: Policy loss: 1.382812. Value loss: 12.394055. Entropy: 0.340087.\n",
      "Iteration 15297: Policy loss: 1.473136. Value loss: 11.317733. Entropy: 0.345594.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15298: Policy loss: -0.508186. Value loss: 22.878366. Entropy: 0.282200.\n",
      "Iteration 15299: Policy loss: -0.403289. Value loss: 12.954013. Entropy: 0.269644.\n",
      "Iteration 15300: Policy loss: -0.493100. Value loss: 10.011603. Entropy: 0.256460.\n",
      "episode: 6292   score: 475.0  epsilon: 1.0    steps: 143  evaluation reward: 319.8\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15301: Policy loss: -0.847133. Value loss: 22.832815. Entropy: 0.284814.\n",
      "Iteration 15302: Policy loss: -0.912303. Value loss: 13.411637. Entropy: 0.280288.\n",
      "Iteration 15303: Policy loss: -0.771734. Value loss: 11.934638. Entropy: 0.288175.\n",
      "episode: 6293   score: 285.0  epsilon: 1.0    steps: 406  evaluation reward: 319.95\n",
      "episode: 6294   score: 460.0  epsilon: 1.0    steps: 1022  evaluation reward: 323.2\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15304: Policy loss: 0.706517. Value loss: 23.493761. Entropy: 0.465116.\n",
      "Iteration 15305: Policy loss: 0.681628. Value loss: 13.855129. Entropy: 0.481799.\n",
      "Iteration 15306: Policy loss: 0.828204. Value loss: 12.894786. Entropy: 0.467733.\n",
      "episode: 6295   score: 280.0  epsilon: 1.0    steps: 344  evaluation reward: 323.2\n",
      "episode: 6296   score: 285.0  epsilon: 1.0    steps: 634  evaluation reward: 323.65\n",
      "episode: 6297   score: 210.0  epsilon: 1.0    steps: 673  evaluation reward: 321.65\n",
      "episode: 6298   score: 260.0  epsilon: 1.0    steps: 796  evaluation reward: 322.15\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15307: Policy loss: -0.282270. Value loss: 23.885874. Entropy: 0.366653.\n",
      "Iteration 15308: Policy loss: -0.270492. Value loss: 14.587843. Entropy: 0.363760.\n",
      "Iteration 15309: Policy loss: -0.285912. Value loss: 14.359751. Entropy: 0.368530.\n",
      "episode: 6299   score: 185.0  epsilon: 1.0    steps: 75  evaluation reward: 321.9\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15310: Policy loss: -1.585408. Value loss: 31.792885. Entropy: 0.444060.\n",
      "Iteration 15311: Policy loss: -2.350472. Value loss: 21.737856. Entropy: 0.433180.\n",
      "Iteration 15312: Policy loss: -1.561347. Value loss: 22.923931. Entropy: 0.450835.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15313: Policy loss: -3.906033. Value loss: 281.453003. Entropy: 0.313107.\n",
      "Iteration 15314: Policy loss: -4.175035. Value loss: 172.764404. Entropy: 0.308810.\n",
      "Iteration 15315: Policy loss: -3.749369. Value loss: 95.938362. Entropy: 0.288421.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15316: Policy loss: 0.531619. Value loss: 36.574646. Entropy: 0.246437.\n",
      "Iteration 15317: Policy loss: 0.601416. Value loss: 20.919786. Entropy: 0.250453.\n",
      "Iteration 15318: Policy loss: 0.838251. Value loss: 19.872307. Entropy: 0.258465.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15319: Policy loss: 1.472726. Value loss: 39.756203. Entropy: 0.468328.\n",
      "Iteration 15320: Policy loss: 1.337312. Value loss: 22.448505. Entropy: 0.484312.\n",
      "Iteration 15321: Policy loss: 1.337524. Value loss: 17.362906. Entropy: 0.483633.\n",
      "episode: 6300   score: 270.0  epsilon: 1.0    steps: 429  evaluation reward: 322.5\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15322: Policy loss: -0.427304. Value loss: 40.990711. Entropy: 0.525369.\n",
      "Iteration 15323: Policy loss: -0.490727. Value loss: 24.340193. Entropy: 0.577584.\n",
      "Iteration 15324: Policy loss: -0.596509. Value loss: 21.369080. Entropy: 0.537916.\n",
      "now time :  2019-02-25 23:26:14.860272\n",
      "episode: 6301   score: 415.0  epsilon: 1.0    steps: 173  evaluation reward: 323.0\n",
      "episode: 6302   score: 320.0  epsilon: 1.0    steps: 652  evaluation reward: 324.1\n",
      "episode: 6303   score: 350.0  epsilon: 1.0    steps: 854  evaluation reward: 324.75\n",
      "episode: 6304   score: 565.0  epsilon: 1.0    steps: 1023  evaluation reward: 328.15\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15325: Policy loss: 0.777320. Value loss: 27.287315. Entropy: 0.458761.\n",
      "Iteration 15326: Policy loss: 1.097253. Value loss: 14.548282. Entropy: 0.480296.\n",
      "Iteration 15327: Policy loss: 1.015455. Value loss: 13.186390. Entropy: 0.492054.\n",
      "episode: 6305   score: 260.0  epsilon: 1.0    steps: 74  evaluation reward: 326.3\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15328: Policy loss: 0.740761. Value loss: 25.376295. Entropy: 0.503420.\n",
      "Iteration 15329: Policy loss: 0.831584. Value loss: 18.036804. Entropy: 0.495783.\n",
      "Iteration 15330: Policy loss: 0.744439. Value loss: 14.913578. Entropy: 0.479985.\n",
      "episode: 6306   score: 390.0  epsilon: 1.0    steps: 622  evaluation reward: 327.8\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15331: Policy loss: -0.320797. Value loss: 33.982098. Entropy: 0.396861.\n",
      "Iteration 15332: Policy loss: -0.309193. Value loss: 22.798538. Entropy: 0.393512.\n",
      "Iteration 15333: Policy loss: -0.167204. Value loss: 16.873810. Entropy: 0.395128.\n",
      "episode: 6307   score: 345.0  epsilon: 1.0    steps: 303  evaluation reward: 328.65\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15334: Policy loss: -0.194270. Value loss: 27.728895. Entropy: 0.305668.\n",
      "Iteration 15335: Policy loss: 0.028522. Value loss: 16.592182. Entropy: 0.311118.\n",
      "Iteration 15336: Policy loss: -0.214975. Value loss: 12.464809. Entropy: 0.292323.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15337: Policy loss: -2.328198. Value loss: 132.158737. Entropy: 0.528004.\n",
      "Iteration 15338: Policy loss: -2.383213. Value loss: 65.907143. Entropy: 0.545389.\n",
      "Iteration 15339: Policy loss: -2.136794. Value loss: 41.199352. Entropy: 0.510144.\n",
      "episode: 6308   score: 295.0  epsilon: 1.0    steps: 486  evaluation reward: 327.4\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15340: Policy loss: -0.388279. Value loss: 49.922737. Entropy: 0.555508.\n",
      "Iteration 15341: Policy loss: -0.326038. Value loss: 26.943623. Entropy: 0.556296.\n",
      "Iteration 15342: Policy loss: 0.054490. Value loss: 19.570547. Entropy: 0.570721.\n"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (e < EPISODES):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                    \n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n",
    "\n",
    "'''\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        curr_state = history[3,:,:]\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \"\"\"\n",
    "        if terminal_state:\n",
    "            r -= 20\n",
    "        \"\"\"\n",
    "        # Store the transition in memory \n",
    "        \n",
    "        agent.memory.push(deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            _, frame_next_val = agent.get_action(np.float32(history[1:, :, :]) / 255.)\n",
    "            agent.train_policy_net(frame, frame_next_val)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "            torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 700 and len(evaluation_reward) > 40:\n",
    "                torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                sys.exit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
