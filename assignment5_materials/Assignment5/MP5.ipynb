{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 340.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 973     evaluation reward: 252.5\n",
      "episode: 1   score: 285.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1099     evaluation reward: 250.65\n",
      "episode: 2   score: 175.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 799     evaluation reward: 249.8\n",
      "episode: 3   score: 235.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1049     evaluation reward: 251.1\n",
      "episode: 4   score: 380.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 834     evaluation reward: 253.2\n",
      "episode: 5   score: 110.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 671     evaluation reward: 248.85\n",
      "episode: 6   score: 230.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 877     evaluation reward: 248.7\n",
      "episode: 7   score: 245.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 770     evaluation reward: 250.05\n",
      "episode: 8   score: 305.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1070     evaluation reward: 250.95\n",
      "episode: 9   score: 150.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 658     evaluation reward: 249.4\n",
      "episode: 10   score: 335.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 975     evaluation reward: 248.95\n",
      "episode: 11   score: 170.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 697     evaluation reward: 248.65\n",
      "episode: 12   score: 125.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 598     evaluation reward: 249.35\n",
      "episode: 13   score: 370.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1028     evaluation reward: 249.65\n",
      "episode: 14   score: 140.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 555     evaluation reward: 247.9\n",
      "episode: 15   score: 185.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 843     evaluation reward: 248.65\n",
      "episode: 16   score: 110.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 656     evaluation reward: 246.65\n",
      "episode: 17   score: 155.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 491     evaluation reward: 245.15\n",
      "episode: 18   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 637     evaluation reward: 242.7\n",
      "episode: 19   score: 470.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1081     evaluation reward: 245.45\n",
      "episode: 20   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 640     evaluation reward: 244.8\n",
      "episode: 21   score: 90.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 406     evaluation reward: 243.1\n",
      "episode: 22   score: 270.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1209     evaluation reward: 242.95\n",
      "episode: 23   score: 165.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 872     evaluation reward: 242.5\n",
      "episode: 24   score: 75.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 425     evaluation reward: 240.95\n",
      "episode: 25   score: 140.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 668     evaluation reward: 237.1\n",
      "episode: 26   score: 300.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1086     evaluation reward: 237.15\n",
      "episode: 27   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 752     evaluation reward: 235.3\n",
      "episode: 28   score: 230.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 901     evaluation reward: 237.0\n",
      "episode: 29   score: 330.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1256     evaluation reward: 239.05\n",
      "episode: 30   score: 235.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 837     evaluation reward: 237.95\n",
      "episode: 31   score: 155.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 546     evaluation reward: 237.7\n",
      "episode: 32   score: 130.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 659     evaluation reward: 236.9\n",
      "episode: 33   score: 105.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 466     evaluation reward: 236.45\n",
      "episode: 34   score: 150.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 728     evaluation reward: 236.1\n",
      "episode: 35   score: 385.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1134     evaluation reward: 238.25\n",
      "episode: 36   score: 210.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 609     evaluation reward: 236.3\n",
      "episode: 37   score: 225.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 889     evaluation reward: 237.4\n",
      "episode: 38   score: 405.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1296     evaluation reward: 240.65\n",
      "now time :  2018-12-30 10:53:39.336776\n",
      "episode: 39   score: 375.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 977     evaluation reward: 241.15\n",
      "episode: 40   score: 415.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1258     evaluation reward: 243.85\n",
      "episode: 41   score: 305.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 707     evaluation reward: 244.1\n",
      "episode: 42   score: 260.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 886     evaluation reward: 244.85\n",
      "episode: 43   score: 410.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 857     evaluation reward: 247.85\n",
      "episode: 44   score: 275.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1098     evaluation reward: 249.55\n",
      "episode: 45   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 522     evaluation reward: 248.3\n",
      "episode: 46   score: 110.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 463     evaluation reward: 248.4\n",
      "episode: 47   score: 180.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 651     evaluation reward: 246.55\n",
      "episode: 48   score: 210.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 622     evaluation reward: 244.25\n",
      "episode: 49   score: 535.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 962     evaluation reward: 247.85\n",
      "episode: 50   score: 100.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 498     evaluation reward: 246.3\n",
      "episode: 51   score: 280.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 966     evaluation reward: 247.4\n",
      "episode: 52   score: 105.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 654     evaluation reward: 244.55\n",
      "episode: 53   score: 415.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 869     evaluation reward: 242.9\n",
      "episode: 54   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 777     evaluation reward: 242.8\n",
      "episode: 55   score: 200.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 878     evaluation reward: 243.45\n",
      "episode: 56   score: 425.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1066     evaluation reward: 245.85\n",
      "episode: 57   score: 350.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 954     evaluation reward: 248.05\n",
      "episode: 58   score: 340.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 975     evaluation reward: 248.55\n",
      "episode: 59   score: 120.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 841     evaluation reward: 247.55\n",
      "episode: 60   score: 395.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1489     evaluation reward: 246.1\n",
      "episode: 61   score: 150.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 644     evaluation reward: 245.55\n",
      "episode: 62   score: 200.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 818     evaluation reward: 243.9\n",
      "episode: 63   score: 95.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 633     evaluation reward: 240.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 130.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 660     evaluation reward: 240.9\n",
      "episode: 65   score: 255.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 821     evaluation reward: 241.05\n",
      "episode: 66   score: 265.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1009     evaluation reward: 242.6\n",
      "episode: 67   score: 155.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 600     evaluation reward: 243.05\n",
      "episode: 68   score: 620.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1354     evaluation reward: 243.75\n",
      "episode: 69   score: 480.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1201     evaluation reward: 243.35\n",
      "episode: 70   score: 255.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 901     evaluation reward: 244.7\n",
      "episode: 71   score: 60.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 475     evaluation reward: 240.4\n",
      "episode: 72   score: 370.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1238     evaluation reward: 240.2\n",
      "episode: 73   score: 80.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 358     evaluation reward: 238.85\n",
      "episode: 74   score: 315.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 941     evaluation reward: 240.7\n",
      "episode: 75   score: 610.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1197     evaluation reward: 244.6\n",
      "episode: 76   score: 140.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 662     evaluation reward: 244.05\n",
      "episode: 77   score: 545.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1464     evaluation reward: 247.1\n",
      "episode: 78   score: 220.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 790     evaluation reward: 240.85\n",
      "episode: 79   score: 355.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1246     evaluation reward: 243.3\n",
      "episode: 80   score: 405.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 851     evaluation reward: 242.65\n",
      "episode: 81   score: 270.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 999     evaluation reward: 242.3\n",
      "episode: 82   score: 95.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 496     evaluation reward: 241.9\n",
      "episode: 83   score: 330.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1188     evaluation reward: 244.0\n",
      "episode: 84   score: 180.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 515     evaluation reward: 244.6\n",
      "episode: 85   score: 210.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 630     evaluation reward: 244.4\n",
      "episode: 86   score: 175.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 644     evaluation reward: 245.05\n",
      "episode: 87   score: 65.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 406     evaluation reward: 243.55\n",
      "episode: 88   score: 140.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 490     evaluation reward: 244.45\n",
      "episode: 89   score: 145.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 636     evaluation reward: 244.35\n",
      "episode: 90   score: 225.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 818     evaluation reward: 245.5\n",
      "episode: 91   score: 605.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1204     evaluation reward: 249.55\n",
      "episode: 92   score: 235.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 860     evaluation reward: 250.75\n",
      "episode: 93   score: 700.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1739     evaluation reward: 257.0\n",
      "episode: 94   score: 90.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 483     evaluation reward: 252.9\n",
      "episode: 95   score: 105.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 627     evaluation reward: 252.45\n",
      "episode: 96   score: 535.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1175     evaluation reward: 254.0\n",
      "episode: 97   score: 80.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 582     evaluation reward: 252.15\n",
      "now time :  2018-12-30 11:26:40.931299\n",
      "episode: 98   score: 170.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 742     evaluation reward: 251.25\n",
      "episode: 99   score: 215.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 806     evaluation reward: 247.95\n",
      "episode: 100   score: 425.0   memory length: 1000000   epsilon: 0.049999050000437695    steps: 1122     evaluation reward: 248.8\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame and frame % train_freq == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 700:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
