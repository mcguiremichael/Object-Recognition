{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/.local/lib/python3.5/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 120.0   memory length: 623   epsilon: 1.0    steps: 623     evaluation reward: 120.0\n",
      "episode: 1   score: 80.0   memory length: 1197   epsilon: 1.0    steps: 574     evaluation reward: 100.0\n",
      "episode: 2   score: 110.0   memory length: 1884   epsilon: 1.0    steps: 687     evaluation reward: 103.33333333333333\n",
      "episode: 3   score: 335.0   memory length: 2655   epsilon: 1.0    steps: 771     evaluation reward: 161.25\n",
      "episode: 4   score: 365.0   memory length: 3619   epsilon: 1.0    steps: 964     evaluation reward: 202.0\n",
      "episode: 5   score: 105.0   memory length: 4320   epsilon: 1.0    steps: 701     evaluation reward: 185.83333333333334\n",
      "episode: 6   score: 380.0   memory length: 5504   epsilon: 1.0    steps: 1184     evaluation reward: 213.57142857142858\n",
      "episode: 7   score: 180.0   memory length: 6140   epsilon: 1.0    steps: 636     evaluation reward: 209.375\n",
      "episode: 8   score: 110.0   memory length: 6750   epsilon: 1.0    steps: 610     evaluation reward: 198.33333333333334\n",
      "episode: 9   score: 80.0   memory length: 7278   epsilon: 1.0    steps: 528     evaluation reward: 186.5\n",
      "episode: 10   score: 110.0   memory length: 7826   epsilon: 1.0    steps: 548     evaluation reward: 179.54545454545453\n",
      "episode: 11   score: 105.0   memory length: 8315   epsilon: 1.0    steps: 489     evaluation reward: 173.33333333333334\n",
      "episode: 12   score: 80.0   memory length: 8870   epsilon: 1.0    steps: 555     evaluation reward: 166.15384615384616\n",
      "episode: 13   score: 120.0   memory length: 9527   epsilon: 1.0    steps: 657     evaluation reward: 162.85714285714286\n",
      "episode: 14   score: 180.0   memory length: 10345   epsilon: 1.0    steps: 818     evaluation reward: 164.0\n",
      "episode: 15   score: 30.0   memory length: 10781   epsilon: 1.0    steps: 436     evaluation reward: 155.625\n",
      "episode: 16   score: 155.0   memory length: 11447   epsilon: 1.0    steps: 666     evaluation reward: 155.58823529411765\n",
      "episode: 17   score: 65.0   memory length: 12095   epsilon: 1.0    steps: 648     evaluation reward: 150.55555555555554\n",
      "episode: 18   score: 135.0   memory length: 12734   epsilon: 1.0    steps: 639     evaluation reward: 149.73684210526315\n",
      "episode: 19   score: 175.0   memory length: 13633   epsilon: 1.0    steps: 899     evaluation reward: 151.0\n",
      "episode: 20   score: 180.0   memory length: 14406   epsilon: 1.0    steps: 773     evaluation reward: 152.38095238095238\n",
      "episode: 21   score: 630.0   memory length: 15380   epsilon: 1.0    steps: 974     evaluation reward: 174.0909090909091\n",
      "episode: 22   score: 195.0   memory length: 16310   epsilon: 1.0    steps: 930     evaluation reward: 175.0\n",
      "episode: 23   score: 330.0   memory length: 17013   epsilon: 1.0    steps: 703     evaluation reward: 181.45833333333334\n",
      "episode: 24   score: 125.0   memory length: 17626   epsilon: 1.0    steps: 613     evaluation reward: 179.2\n",
      "episode: 25   score: 65.0   memory length: 18230   epsilon: 1.0    steps: 604     evaluation reward: 174.80769230769232\n",
      "episode: 26   score: 205.0   memory length: 19074   epsilon: 1.0    steps: 844     evaluation reward: 175.92592592592592\n",
      "episode: 27   score: 100.0   memory length: 19726   epsilon: 1.0    steps: 652     evaluation reward: 173.21428571428572\n",
      "episode: 28   score: 155.0   memory length: 20519   epsilon: 1.0    steps: 793     evaluation reward: 172.58620689655172\n",
      "episode: 29   score: 105.0   memory length: 21062   epsilon: 1.0    steps: 543     evaluation reward: 170.33333333333334\n",
      "episode: 30   score: 35.0   memory length: 21542   epsilon: 1.0    steps: 480     evaluation reward: 165.96774193548387\n",
      "episode: 31   score: 160.0   memory length: 22620   epsilon: 1.0    steps: 1078     evaluation reward: 165.78125\n",
      "episode: 32   score: 240.0   memory length: 23661   epsilon: 1.0    steps: 1041     evaluation reward: 168.03030303030303\n",
      "episode: 33   score: 125.0   memory length: 24220   epsilon: 1.0    steps: 559     evaluation reward: 166.76470588235293\n",
      "episode: 34   score: 60.0   memory length: 24861   epsilon: 1.0    steps: 641     evaluation reward: 163.71428571428572\n",
      "episode: 35   score: 90.0   memory length: 25468   epsilon: 1.0    steps: 607     evaluation reward: 161.66666666666666\n",
      "episode: 36   score: 80.0   memory length: 25959   epsilon: 1.0    steps: 491     evaluation reward: 159.45945945945945\n",
      "episode: 37   score: 110.0   memory length: 26652   epsilon: 1.0    steps: 693     evaluation reward: 158.1578947368421\n",
      "episode: 38   score: 210.0   memory length: 27577   epsilon: 1.0    steps: 925     evaluation reward: 159.48717948717947\n",
      "episode: 39   score: 255.0   memory length: 28472   epsilon: 1.0    steps: 895     evaluation reward: 161.875\n",
      "episode: 40   score: 110.0   memory length: 29132   epsilon: 1.0    steps: 660     evaluation reward: 160.609756097561\n",
      "episode: 41   score: 135.0   memory length: 29953   epsilon: 1.0    steps: 821     evaluation reward: 160.0\n",
      "episode: 42   score: 285.0   memory length: 30840   epsilon: 1.0    steps: 887     evaluation reward: 162.90697674418604\n",
      "episode: 43   score: 105.0   memory length: 31338   epsilon: 1.0    steps: 498     evaluation reward: 161.5909090909091\n",
      "episode: 44   score: 120.0   memory length: 31977   epsilon: 1.0    steps: 639     evaluation reward: 160.66666666666666\n",
      "episode: 45   score: 105.0   memory length: 32489   epsilon: 1.0    steps: 512     evaluation reward: 159.45652173913044\n",
      "episode: 46   score: 135.0   memory length: 33138   epsilon: 1.0    steps: 649     evaluation reward: 158.93617021276594\n",
      "episode: 47   score: 230.0   memory length: 33971   epsilon: 1.0    steps: 833     evaluation reward: 160.41666666666666\n",
      "episode: 48   score: 180.0   memory length: 34741   epsilon: 1.0    steps: 770     evaluation reward: 160.81632653061226\n",
      "episode: 49   score: 80.0   memory length: 35496   epsilon: 1.0    steps: 755     evaluation reward: 159.2\n",
      "episode: 50   score: 50.0   memory length: 35911   epsilon: 1.0    steps: 415     evaluation reward: 157.05882352941177\n",
      "episode: 51   score: 65.0   memory length: 36356   epsilon: 1.0    steps: 445     evaluation reward: 155.28846153846155\n",
      "episode: 52   score: 120.0   memory length: 37088   epsilon: 1.0    steps: 732     evaluation reward: 154.62264150943398\n",
      "episode: 53   score: 130.0   memory length: 37512   epsilon: 1.0    steps: 424     evaluation reward: 154.16666666666666\n",
      "episode: 54   score: 30.0   memory length: 37921   epsilon: 1.0    steps: 409     evaluation reward: 151.9090909090909\n",
      "episode: 55   score: 75.0   memory length: 38385   epsilon: 1.0    steps: 464     evaluation reward: 150.53571428571428\n",
      "episode: 56   score: 110.0   memory length: 39002   epsilon: 1.0    steps: 617     evaluation reward: 149.82456140350877\n",
      "episode: 57   score: 315.0   memory length: 40028   epsilon: 1.0    steps: 1026     evaluation reward: 152.67241379310346\n",
      "episode: 58   score: 105.0   memory length: 40542   epsilon: 1.0    steps: 514     evaluation reward: 151.864406779661\n",
      "episode: 59   score: 135.0   memory length: 41318   epsilon: 1.0    steps: 776     evaluation reward: 151.58333333333334\n",
      "episode: 60   score: 30.0   memory length: 41715   epsilon: 1.0    steps: 397     evaluation reward: 149.59016393442624\n",
      "episode: 61   score: 35.0   memory length: 42119   epsilon: 1.0    steps: 404     evaluation reward: 147.74193548387098\n",
      "episode: 62   score: 225.0   memory length: 43375   epsilon: 1.0    steps: 1256     evaluation reward: 148.96825396825398\n",
      "episode: 63   score: 280.0   memory length: 44487   epsilon: 1.0    steps: 1112     evaluation reward: 151.015625\n",
      "episode: 64   score: 105.0   memory length: 45058   epsilon: 1.0    steps: 571     evaluation reward: 150.30769230769232\n",
      "episode: 65   score: 50.0   memory length: 45449   epsilon: 1.0    steps: 391     evaluation reward: 148.78787878787878\n",
      "episode: 66   score: 155.0   memory length: 46254   epsilon: 1.0    steps: 805     evaluation reward: 148.88059701492537\n",
      "episode: 67   score: 35.0   memory length: 46852   epsilon: 1.0    steps: 598     evaluation reward: 147.2058823529412\n",
      "episode: 68   score: 90.0   memory length: 47643   epsilon: 1.0    steps: 791     evaluation reward: 146.3768115942029\n",
      "episode: 69   score: 75.0   memory length: 48284   epsilon: 1.0    steps: 641     evaluation reward: 145.35714285714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 70   score: 65.0   memory length: 48680   epsilon: 1.0    steps: 396     evaluation reward: 144.22535211267606\n",
      "episode: 71   score: 100.0   memory length: 49265   epsilon: 1.0    steps: 585     evaluation reward: 143.61111111111111\n",
      "episode: 72   score: 90.0   memory length: 49786   epsilon: 1.0    steps: 521     evaluation reward: 142.87671232876713\n",
      "now time :  2020-02-29 12:48:41.237882\n",
      "episode: 73   score: 285.0   memory length: 50968   epsilon: 0.9990794500000004    steps: 1182     evaluation reward: 144.7972972972973\n",
      "episode: 74   score: 210.0   memory length: 51736   epsilon: 0.9983498500000008    steps: 768     evaluation reward: 145.66666666666666\n",
      "episode: 75   score: 125.0   memory length: 52382   epsilon: 0.997736150000001    steps: 646     evaluation reward: 145.39473684210526\n",
      "episode: 76   score: 55.0   memory length: 52990   epsilon: 0.9971585500000013    steps: 608     evaluation reward: 144.2207792207792\n",
      "episode: 77   score: 120.0   memory length: 53655   epsilon: 0.9965268000000016    steps: 665     evaluation reward: 143.9102564102564\n",
      "episode: 78   score: 215.0   memory length: 54521   epsilon: 0.995704100000002    steps: 866     evaluation reward: 144.81012658227849\n",
      "episode: 79   score: 240.0   memory length: 55461   epsilon: 0.9948111000000024    steps: 940     evaluation reward: 146.0\n",
      "episode: 80   score: 355.0   memory length: 56753   epsilon: 0.993583700000003    steps: 1292     evaluation reward: 148.58024691358025\n",
      "episode: 81   score: 125.0   memory length: 57289   epsilon: 0.9930745000000032    steps: 536     evaluation reward: 148.29268292682926\n",
      "episode: 82   score: 180.0   memory length: 57893   epsilon: 0.9925007000000035    steps: 604     evaluation reward: 148.67469879518072\n",
      "episode: 83   score: 180.0   memory length: 58704   epsilon: 0.9917302500000038    steps: 811     evaluation reward: 149.04761904761904\n",
      "episode: 84   score: 155.0   memory length: 59501   epsilon: 0.9909731000000042    steps: 797     evaluation reward: 149.11764705882354\n",
      "episode: 85   score: 155.0   memory length: 60335   epsilon: 0.9901808000000045    steps: 834     evaluation reward: 149.1860465116279\n",
      "episode: 86   score: 50.0   memory length: 60820   epsilon: 0.9897200500000047    steps: 485     evaluation reward: 148.04597701149424\n",
      "episode: 87   score: 55.0   memory length: 61227   epsilon: 0.9893334000000049    steps: 407     evaluation reward: 146.98863636363637\n",
      "episode: 88   score: 80.0   memory length: 61687   epsilon: 0.9888964000000051    steps: 460     evaluation reward: 146.23595505617976\n",
      "episode: 89   score: 210.0   memory length: 62372   epsilon: 0.9882456500000054    steps: 685     evaluation reward: 146.94444444444446\n",
      "episode: 90   score: 65.0   memory length: 63013   epsilon: 0.9876367000000057    steps: 641     evaluation reward: 146.04395604395606\n",
      "episode: 91   score: 35.0   memory length: 63512   epsilon: 0.9871626500000059    steps: 499     evaluation reward: 144.83695652173913\n",
      "episode: 92   score: 135.0   memory length: 64196   epsilon: 0.9865128500000062    steps: 684     evaluation reward: 144.7311827956989\n",
      "episode: 93   score: 180.0   memory length: 64844   epsilon: 0.9858972500000065    steps: 648     evaluation reward: 145.10638297872342\n",
      "episode: 94   score: 180.0   memory length: 65612   epsilon: 0.9851676500000068    steps: 768     evaluation reward: 145.47368421052633\n",
      "episode: 95   score: 210.0   memory length: 66282   epsilon: 0.9845311500000071    steps: 670     evaluation reward: 146.14583333333334\n",
      "episode: 96   score: 55.0   memory length: 66911   epsilon: 0.9839336000000074    steps: 629     evaluation reward: 145.20618556701032\n",
      "episode: 97   score: 155.0   memory length: 67506   epsilon: 0.9833683500000077    steps: 595     evaluation reward: 145.30612244897958\n",
      "episode: 98   score: 105.0   memory length: 68248   epsilon: 0.982663450000008    steps: 742     evaluation reward: 144.8989898989899\n",
      "episode: 99   score: 260.0   memory length: 69130   epsilon: 0.9818255500000084    steps: 882     evaluation reward: 146.05\n",
      "episode: 100   score: 60.0   memory length: 69526   epsilon: 0.9814493500000085    steps: 396     evaluation reward: 145.45\n",
      "episode: 101   score: 105.0   memory length: 70195   epsilon: 0.9808138000000088    steps: 669     evaluation reward: 145.7\n",
      "episode: 102   score: 135.0   memory length: 70839   epsilon: 0.9802020000000091    steps: 644     evaluation reward: 145.95\n",
      "episode: 103   score: 260.0   memory length: 71970   epsilon: 0.9791275500000096    steps: 1131     evaluation reward: 145.2\n",
      "episode: 104   score: 20.0   memory length: 72465   epsilon: 0.9786573000000098    steps: 495     evaluation reward: 141.75\n",
      "episode: 105   score: 65.0   memory length: 72984   epsilon: 0.9781642500000101    steps: 519     evaluation reward: 141.35\n",
      "episode: 106   score: 35.0   memory length: 73375   epsilon: 0.9777928000000102    steps: 391     evaluation reward: 137.9\n",
      "episode: 107   score: 135.0   memory length: 74030   epsilon: 0.9771705500000105    steps: 655     evaluation reward: 137.45\n",
      "episode: 108   score: 105.0   memory length: 74683   epsilon: 0.9765502000000108    steps: 653     evaluation reward: 137.4\n",
      "episode: 109   score: 60.0   memory length: 75245   epsilon: 0.976016300000011    steps: 562     evaluation reward: 137.2\n",
      "episode: 110   score: 150.0   memory length: 76188   epsilon: 0.9751204500000115    steps: 943     evaluation reward: 137.6\n",
      "episode: 111   score: 305.0   memory length: 77333   epsilon: 0.974032700000012    steps: 1145     evaluation reward: 139.6\n",
      "episode: 112   score: 210.0   memory length: 78052   epsilon: 0.9733496500000123    steps: 719     evaluation reward: 140.9\n",
      "episode: 113   score: 120.0   memory length: 78688   epsilon: 0.9727454500000126    steps: 636     evaluation reward: 140.9\n",
      "episode: 114   score: 75.0   memory length: 79109   epsilon: 0.9723455000000127    steps: 421     evaluation reward: 139.85\n",
      "episode: 115   score: 180.0   memory length: 79686   epsilon: 0.971797350000013    steps: 577     evaluation reward: 141.35\n",
      "episode: 116   score: 595.0   memory length: 80877   epsilon: 0.9706659000000135    steps: 1191     evaluation reward: 145.75\n",
      "episode: 117   score: 135.0   memory length: 81493   epsilon: 0.9700807000000138    steps: 616     evaluation reward: 146.45\n",
      "episode: 118   score: 105.0   memory length: 82026   epsilon: 0.969574350000014    steps: 533     evaluation reward: 146.15\n",
      "episode: 119   score: 10.0   memory length: 82422   epsilon: 0.9691981500000142    steps: 396     evaluation reward: 144.5\n",
      "episode: 120   score: 180.0   memory length: 83062   epsilon: 0.9685901500000145    steps: 640     evaluation reward: 144.5\n",
      "episode: 121   score: 200.0   memory length: 83842   epsilon: 0.9678491500000148    steps: 780     evaluation reward: 140.2\n",
      "episode: 122   score: 165.0   memory length: 84566   epsilon: 0.9671613500000151    steps: 724     evaluation reward: 139.9\n",
      "episode: 123   score: 130.0   memory length: 85198   epsilon: 0.9665609500000154    steps: 632     evaluation reward: 137.9\n",
      "episode: 124   score: 120.0   memory length: 85844   epsilon: 0.9659472500000157    steps: 646     evaluation reward: 137.85\n",
      "episode: 125   score: 125.0   memory length: 86637   epsilon: 0.965193900000016    steps: 793     evaluation reward: 138.45\n",
      "episode: 126   score: 105.0   memory length: 87287   epsilon: 0.9645764000000163    steps: 650     evaluation reward: 137.45\n",
      "episode: 127   score: 155.0   memory length: 88074   epsilon: 0.9638287500000167    steps: 787     evaluation reward: 138.0\n",
      "episode: 128   score: 110.0   memory length: 88725   epsilon: 0.963210300000017    steps: 651     evaluation reward: 137.55\n",
      "episode: 129   score: 170.0   memory length: 89543   epsilon: 0.9624332000000173    steps: 818     evaluation reward: 138.2\n",
      "episode: 130   score: 125.0   memory length: 90061   epsilon: 0.9619411000000175    steps: 518     evaluation reward: 139.1\n",
      "episode: 131   score: 570.0   memory length: 91177   epsilon: 0.960880900000018    steps: 1116     evaluation reward: 143.2\n",
      "episode: 132   score: 225.0   memory length: 92363   epsilon: 0.9597542000000185    steps: 1186     evaluation reward: 143.05\n",
      "episode: 133   score: 350.0   memory length: 93421   epsilon: 0.958749100000019    steps: 1058     evaluation reward: 145.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 134   score: 155.0   memory length: 94248   epsilon: 0.9579634500000194    steps: 827     evaluation reward: 146.25\n",
      "episode: 135   score: 70.0   memory length: 94858   epsilon: 0.9573839500000196    steps: 610     evaluation reward: 146.05\n",
      "episode: 136   score: 125.0   memory length: 95577   epsilon: 0.95670090000002    steps: 719     evaluation reward: 146.5\n",
      "episode: 137   score: 50.0   memory length: 95962   epsilon: 0.9563351500000201    steps: 385     evaluation reward: 145.9\n",
      "episode: 138   score: 140.0   memory length: 96511   epsilon: 0.9558136000000204    steps: 549     evaluation reward: 145.2\n",
      "episode: 139   score: 135.0   memory length: 97214   epsilon: 0.9551457500000207    steps: 703     evaluation reward: 144.0\n",
      "episode: 140   score: 180.0   memory length: 97998   epsilon: 0.954400950000021    steps: 784     evaluation reward: 144.7\n",
      "episode: 141   score: 20.0   memory length: 98447   epsilon: 0.9539744000000212    steps: 449     evaluation reward: 143.55\n",
      "episode: 142   score: 180.0   memory length: 99099   epsilon: 0.9533550000000215    steps: 652     evaluation reward: 142.5\n",
      "now time :  2020-02-29 12:59:03.505432\n",
      "episode: 143   score: 195.0   memory length: 100038   epsilon: 0.9524629500000219    steps: 939     evaluation reward: 143.4\n",
      "episode: 144   score: 30.0   memory length: 100583   epsilon: 0.9519452000000221    steps: 545     evaluation reward: 142.5\n",
      "episode: 145   score: 230.0   memory length: 101491   epsilon: 0.9510826000000225    steps: 908     evaluation reward: 143.75\n",
      "episode: 146   score: 155.0   memory length: 102116   epsilon: 0.9504888500000228    steps: 625     evaluation reward: 143.95\n",
      "episode: 147   score: 410.0   memory length: 102900   epsilon: 0.9497440500000232    steps: 784     evaluation reward: 145.75\n",
      "episode: 148   score: 155.0   memory length: 103579   epsilon: 0.9490990000000235    steps: 679     evaluation reward: 145.5\n",
      "episode: 149   score: 210.0   memory length: 104390   epsilon: 0.9483285500000238    steps: 811     evaluation reward: 146.8\n",
      "episode: 150   score: 100.0   memory length: 104918   epsilon: 0.947826950000024    steps: 528     evaluation reward: 147.3\n",
      "episode: 151   score: 200.0   memory length: 105836   epsilon: 0.9469548500000244    steps: 918     evaluation reward: 148.65\n",
      "episode: 152   score: 410.0   memory length: 106649   epsilon: 0.9461825000000248    steps: 813     evaluation reward: 151.55\n",
      "episode: 153   score: 120.0   memory length: 107305   epsilon: 0.9455593000000251    steps: 656     evaluation reward: 151.45\n",
      "episode: 154   score: 135.0   memory length: 108009   epsilon: 0.9448905000000254    steps: 704     evaluation reward: 152.5\n",
      "episode: 155   score: 45.0   memory length: 108543   epsilon: 0.9443832000000256    steps: 534     evaluation reward: 152.2\n",
      "episode: 156   score: 50.0   memory length: 109174   epsilon: 0.9437837500000259    steps: 631     evaluation reward: 151.6\n",
      "episode: 157   score: 255.0   memory length: 110243   epsilon: 0.9427682000000264    steps: 1069     evaluation reward: 151.0\n",
      "episode: 158   score: 135.0   memory length: 111033   epsilon: 0.9420177000000267    steps: 790     evaluation reward: 151.3\n",
      "episode: 159   score: 155.0   memory length: 111867   epsilon: 0.9412254000000271    steps: 834     evaluation reward: 151.5\n",
      "episode: 160   score: 215.0   memory length: 112687   epsilon: 0.9404464000000274    steps: 820     evaluation reward: 153.35\n",
      "episode: 161   score: 150.0   memory length: 113371   epsilon: 0.9397966000000277    steps: 684     evaluation reward: 154.5\n",
      "episode: 162   score: 210.0   memory length: 114158   epsilon: 0.9390489500000281    steps: 787     evaluation reward: 154.35\n",
      "episode: 163   score: 155.0   memory length: 114855   epsilon: 0.9383868000000284    steps: 697     evaluation reward: 153.1\n",
      "episode: 164   score: 185.0   memory length: 115517   epsilon: 0.9377579000000287    steps: 662     evaluation reward: 153.9\n",
      "episode: 165   score: 340.0   memory length: 116844   epsilon: 0.9364972500000293    steps: 1327     evaluation reward: 156.8\n",
      "episode: 166   score: 90.0   memory length: 117453   epsilon: 0.9359187000000295    steps: 609     evaluation reward: 156.15\n",
      "episode: 167   score: 155.0   memory length: 118049   epsilon: 0.9353525000000298    steps: 596     evaluation reward: 157.35\n",
      "episode: 168   score: 155.0   memory length: 118540   epsilon: 0.93488605000003    steps: 491     evaluation reward: 158.0\n",
      "episode: 169   score: 215.0   memory length: 119313   epsilon: 0.9341517000000303    steps: 773     evaluation reward: 159.4\n",
      "episode: 170   score: 135.0   memory length: 119851   epsilon: 0.9336406000000306    steps: 538     evaluation reward: 160.1\n",
      "episode: 171   score: 50.0   memory length: 120418   epsilon: 0.9331019500000308    steps: 567     evaluation reward: 159.6\n",
      "episode: 172   score: 155.0   memory length: 121097   epsilon: 0.9324569000000311    steps: 679     evaluation reward: 160.25\n",
      "episode: 173   score: 385.0   memory length: 122046   epsilon: 0.9315553500000315    steps: 949     evaluation reward: 161.25\n",
      "episode: 174   score: 155.0   memory length: 122538   epsilon: 0.9310879500000317    steps: 492     evaluation reward: 160.7\n",
      "episode: 175   score: 130.0   memory length: 123121   epsilon: 0.930534100000032    steps: 583     evaluation reward: 160.75\n",
      "episode: 176   score: 125.0   memory length: 123687   epsilon: 0.9299964000000323    steps: 566     evaluation reward: 161.45\n",
      "episode: 177   score: 210.0   memory length: 124471   epsilon: 0.9292516000000326    steps: 784     evaluation reward: 162.35\n",
      "episode: 178   score: 50.0   memory length: 124857   epsilon: 0.9288849000000328    steps: 386     evaluation reward: 160.7\n",
      "episode: 179   score: 45.0   memory length: 125258   epsilon: 0.9285039500000329    steps: 401     evaluation reward: 158.75\n",
      "episode: 180   score: 160.0   memory length: 126040   epsilon: 0.9277610500000333    steps: 782     evaluation reward: 156.8\n",
      "episode: 181   score: 120.0   memory length: 126899   epsilon: 0.9269450000000337    steps: 859     evaluation reward: 156.75\n",
      "episode: 182   score: 100.0   memory length: 127283   epsilon: 0.9265802000000338    steps: 384     evaluation reward: 155.95\n",
      "episode: 183   score: 110.0   memory length: 128026   epsilon: 0.9258743500000342    steps: 743     evaluation reward: 155.25\n",
      "episode: 184   score: 80.0   memory length: 128364   epsilon: 0.9255532500000343    steps: 338     evaluation reward: 154.5\n",
      "episode: 185   score: 120.0   memory length: 129102   epsilon: 0.9248521500000346    steps: 738     evaluation reward: 154.15\n",
      "episode: 186   score: 575.0   memory length: 130326   epsilon: 0.9236893500000352    steps: 1224     evaluation reward: 159.4\n",
      "episode: 187   score: 120.0   memory length: 130911   epsilon: 0.9231336000000354    steps: 585     evaluation reward: 160.05\n",
      "episode: 188   score: 75.0   memory length: 131441   epsilon: 0.9226301000000356    steps: 530     evaluation reward: 160.0\n",
      "episode: 189   score: 195.0   memory length: 132200   epsilon: 0.921909050000036    steps: 759     evaluation reward: 159.85\n",
      "episode: 190   score: 110.0   memory length: 132879   epsilon: 0.9212640000000363    steps: 679     evaluation reward: 160.3\n",
      "episode: 191   score: 230.0   memory length: 133651   epsilon: 0.9205306000000366    steps: 772     evaluation reward: 162.25\n",
      "episode: 192   score: 120.0   memory length: 134354   epsilon: 0.9198627500000369    steps: 703     evaluation reward: 162.1\n",
      "episode: 193   score: 115.0   memory length: 134917   epsilon: 0.9193279000000372    steps: 563     evaluation reward: 161.45\n",
      "episode: 194   score: 45.0   memory length: 135299   epsilon: 0.9189650000000373    steps: 382     evaluation reward: 160.1\n",
      "episode: 195   score: 120.0   memory length: 135991   epsilon: 0.9183076000000376    steps: 692     evaluation reward: 159.2\n",
      "episode: 196   score: 365.0   memory length: 136978   epsilon: 0.9173699500000381    steps: 987     evaluation reward: 162.3\n",
      "episode: 197   score: 110.0   memory length: 137601   epsilon: 0.9167781000000383    steps: 623     evaluation reward: 161.85\n",
      "episode: 198   score: 110.0   memory length: 138263   epsilon: 0.9161492000000386    steps: 662     evaluation reward: 161.9\n",
      "episode: 199   score: 210.0   memory length: 138900   epsilon: 0.9155440500000389    steps: 637     evaluation reward: 161.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200   score: 65.0   memory length: 139396   epsilon: 0.9150728500000391    steps: 496     evaluation reward: 161.45\n",
      "episode: 201   score: 135.0   memory length: 140184   epsilon: 0.9143242500000395    steps: 788     evaluation reward: 161.75\n",
      "episode: 202   score: 115.0   memory length: 140693   epsilon: 0.9138407000000397    steps: 509     evaluation reward: 161.55\n",
      "episode: 203   score: 45.0   memory length: 141101   epsilon: 0.9134531000000399    steps: 408     evaluation reward: 159.4\n",
      "episode: 204   score: 430.0   memory length: 142051   epsilon: 0.9125506000000403    steps: 950     evaluation reward: 163.5\n",
      "episode: 205   score: 15.0   memory length: 142548   epsilon: 0.9120784500000405    steps: 497     evaluation reward: 163.0\n",
      "episode: 206   score: 120.0   memory length: 143159   epsilon: 0.9114980000000408    steps: 611     evaluation reward: 163.85\n",
      "episode: 207   score: 50.0   memory length: 143571   epsilon: 0.911106600000041    steps: 412     evaluation reward: 163.0\n",
      "episode: 208   score: 45.0   memory length: 143954   epsilon: 0.9107427500000411    steps: 383     evaluation reward: 162.4\n",
      "episode: 209   score: 155.0   memory length: 144591   epsilon: 0.9101376000000414    steps: 637     evaluation reward: 163.35\n",
      "episode: 210   score: 55.0   memory length: 145340   epsilon: 0.9094260500000417    steps: 749     evaluation reward: 162.4\n",
      "episode: 211   score: 105.0   memory length: 145733   epsilon: 0.9090527000000419    steps: 393     evaluation reward: 160.4\n",
      "episode: 212   score: 75.0   memory length: 146129   epsilon: 0.9086765000000421    steps: 396     evaluation reward: 159.05\n",
      "episode: 213   score: 35.0   memory length: 146537   epsilon: 0.9082889000000423    steps: 408     evaluation reward: 158.2\n",
      "episode: 214   score: 30.0   memory length: 146927   epsilon: 0.9079184000000424    steps: 390     evaluation reward: 157.75\n",
      "episode: 215   score: 255.0   memory length: 148002   epsilon: 0.9068971500000429    steps: 1075     evaluation reward: 158.5\n",
      "episode: 216   score: 215.0   memory length: 148817   epsilon: 0.9061229000000433    steps: 815     evaluation reward: 154.7\n",
      "episode: 217   score: 135.0   memory length: 149425   epsilon: 0.9055453000000435    steps: 608     evaluation reward: 154.7\n",
      "now time :  2020-02-29 13:09:45.194334\n",
      "episode: 218   score: 240.0   memory length: 150190   epsilon: 0.9048185500000439    steps: 765     evaluation reward: 156.05\n",
      "episode: 219   score: 515.0   memory length: 151528   epsilon: 0.9035474500000444    steps: 1338     evaluation reward: 161.1\n",
      "episode: 220   score: 395.0   memory length: 152458   epsilon: 0.9026639500000448    steps: 930     evaluation reward: 163.25\n",
      "episode: 221   score: 35.0   memory length: 153038   epsilon: 0.9021129500000451    steps: 580     evaluation reward: 161.6\n",
      "episode: 222   score: 100.0   memory length: 153419   epsilon: 0.9017510000000453    steps: 381     evaluation reward: 160.95\n",
      "episode: 223   score: 75.0   memory length: 154011   epsilon: 0.9011886000000455    steps: 592     evaluation reward: 160.4\n",
      "episode: 224   score: 520.0   memory length: 155355   epsilon: 0.8999118000000461    steps: 1344     evaluation reward: 164.4\n",
      "episode: 225   score: 110.0   memory length: 156106   epsilon: 0.8991983500000464    steps: 751     evaluation reward: 164.25\n",
      "episode: 226   score: 125.0   memory length: 156747   epsilon: 0.8985894000000467    steps: 641     evaluation reward: 164.45\n",
      "episode: 227   score: 140.0   memory length: 157558   epsilon: 0.8978189500000471    steps: 811     evaluation reward: 164.3\n",
      "episode: 228   score: 155.0   memory length: 158359   epsilon: 0.8970580000000474    steps: 801     evaluation reward: 164.75\n",
      "episode: 229   score: 230.0   memory length: 159470   epsilon: 0.8960025500000479    steps: 1111     evaluation reward: 165.35\n",
      "episode: 230   score: 185.0   memory length: 160334   epsilon: 0.8951817500000483    steps: 864     evaluation reward: 165.95\n",
      "episode: 231   score: 10.0   memory length: 160895   epsilon: 0.8946488000000485    steps: 561     evaluation reward: 160.35\n",
      "episode: 232   score: 390.0   memory length: 162282   epsilon: 0.8933311500000491    steps: 1387     evaluation reward: 162.0\n",
      "episode: 233   score: 75.0   memory length: 162951   epsilon: 0.8926956000000494    steps: 669     evaluation reward: 159.25\n",
      "episode: 234   score: 180.0   memory length: 163805   epsilon: 0.8918843000000498    steps: 854     evaluation reward: 159.5\n",
      "episode: 235   score: 120.0   memory length: 164590   epsilon: 0.8911385500000502    steps: 785     evaluation reward: 160.0\n",
      "episode: 236   score: 120.0   memory length: 165315   epsilon: 0.8904498000000505    steps: 725     evaluation reward: 159.95\n",
      "episode: 237   score: 105.0   memory length: 165825   epsilon: 0.8899653000000507    steps: 510     evaluation reward: 160.5\n",
      "episode: 238   score: 50.0   memory length: 166214   epsilon: 0.8895957500000509    steps: 389     evaluation reward: 159.6\n",
      "episode: 239   score: 510.0   memory length: 167423   epsilon: 0.8884472000000514    steps: 1209     evaluation reward: 163.35\n",
      "episode: 240   score: 75.0   memory length: 167834   epsilon: 0.8880567500000516    steps: 411     evaluation reward: 162.3\n",
      "episode: 241   score: 460.0   memory length: 168834   epsilon: 0.887106750000052    steps: 1000     evaluation reward: 166.7\n",
      "episode: 242   score: 80.0   memory length: 169481   epsilon: 0.8864921000000523    steps: 647     evaluation reward: 165.7\n",
      "episode: 243   score: 105.0   memory length: 170043   epsilon: 0.8859582000000525    steps: 562     evaluation reward: 164.8\n",
      "episode: 244   score: 110.0   memory length: 170664   epsilon: 0.8853682500000528    steps: 621     evaluation reward: 165.6\n",
      "episode: 245   score: 520.0   memory length: 171904   epsilon: 0.8841902500000534    steps: 1240     evaluation reward: 168.5\n",
      "episode: 246   score: 135.0   memory length: 172660   epsilon: 0.8834720500000537    steps: 756     evaluation reward: 168.3\n",
      "episode: 247   score: 165.0   memory length: 173455   epsilon: 0.882716800000054    steps: 795     evaluation reward: 165.85\n",
      "episode: 248   score: 170.0   memory length: 174243   epsilon: 0.8819682000000544    steps: 788     evaluation reward: 166.0\n",
      "episode: 249   score: 135.0   memory length: 174900   epsilon: 0.8813440500000547    steps: 657     evaluation reward: 165.25\n",
      "episode: 250   score: 120.0   memory length: 175706   epsilon: 0.880578350000055    steps: 806     evaluation reward: 165.45\n",
      "episode: 251   score: 165.0   memory length: 176311   epsilon: 0.8800036000000553    steps: 605     evaluation reward: 165.1\n",
      "episode: 252   score: 110.0   memory length: 176859   epsilon: 0.8794830000000555    steps: 548     evaluation reward: 162.1\n",
      "episode: 253   score: 80.0   memory length: 177396   epsilon: 0.8789728500000558    steps: 537     evaluation reward: 161.7\n",
      "episode: 254   score: 330.0   memory length: 178681   epsilon: 0.8777521000000563    steps: 1285     evaluation reward: 163.65\n",
      "episode: 255   score: 110.0   memory length: 179297   epsilon: 0.8771669000000566    steps: 616     evaluation reward: 164.3\n",
      "episode: 256   score: 55.0   memory length: 179689   epsilon: 0.8767945000000568    steps: 392     evaluation reward: 164.35\n",
      "episode: 257   score: 55.0   memory length: 180306   epsilon: 0.876208350000057    steps: 617     evaluation reward: 162.35\n",
      "episode: 258   score: 195.0   memory length: 181299   epsilon: 0.8752650000000575    steps: 993     evaluation reward: 162.95\n",
      "episode: 259   score: 120.0   memory length: 182024   epsilon: 0.8745762500000578    steps: 725     evaluation reward: 162.6\n",
      "episode: 260   score: 105.0   memory length: 182762   epsilon: 0.8738751500000581    steps: 738     evaluation reward: 161.5\n",
      "episode: 261   score: 195.0   memory length: 183420   epsilon: 0.8732500500000584    steps: 658     evaluation reward: 161.95\n",
      "episode: 262   score: 180.0   memory length: 184230   epsilon: 0.8724805500000588    steps: 810     evaluation reward: 161.65\n",
      "episode: 263   score: 180.0   memory length: 185022   epsilon: 0.8717281500000591    steps: 792     evaluation reward: 161.9\n",
      "episode: 264   score: 80.0   memory length: 185472   epsilon: 0.8713006500000593    steps: 450     evaluation reward: 160.85\n",
      "episode: 265   score: 265.0   memory length: 186435   epsilon: 0.8703858000000597    steps: 963     evaluation reward: 160.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 266   score: 280.0   memory length: 187376   epsilon: 0.8694918500000601    steps: 941     evaluation reward: 162.0\n",
      "episode: 267   score: 105.0   memory length: 187991   epsilon: 0.8689076000000604    steps: 615     evaluation reward: 161.5\n",
      "episode: 268   score: 75.0   memory length: 188430   epsilon: 0.8684905500000606    steps: 439     evaluation reward: 160.7\n",
      "episode: 269   score: 300.0   memory length: 189412   epsilon: 0.867557650000061    steps: 982     evaluation reward: 161.55\n",
      "episode: 270   score: 55.0   memory length: 189992   epsilon: 0.8670066500000613    steps: 580     evaluation reward: 160.75\n",
      "episode: 271   score: 115.0   memory length: 190771   epsilon: 0.8662666000000616    steps: 779     evaluation reward: 161.4\n",
      "episode: 272   score: 185.0   memory length: 191583   epsilon: 0.865495200000062    steps: 812     evaluation reward: 161.7\n",
      "episode: 273   score: 180.0   memory length: 192276   epsilon: 0.8648368500000623    steps: 693     evaluation reward: 159.65\n",
      "episode: 274   score: 80.0   memory length: 192850   epsilon: 0.8642915500000625    steps: 574     evaluation reward: 158.9\n",
      "episode: 275   score: 135.0   memory length: 193665   epsilon: 0.8635173000000629    steps: 815     evaluation reward: 158.95\n",
      "episode: 276   score: 120.0   memory length: 194417   epsilon: 0.8628029000000632    steps: 752     evaluation reward: 158.9\n",
      "episode: 277   score: 325.0   memory length: 195577   epsilon: 0.8617009000000637    steps: 1160     evaluation reward: 160.05\n",
      "episode: 278   score: 30.0   memory length: 195987   epsilon: 0.8613114000000639    steps: 410     evaluation reward: 159.85\n",
      "episode: 279   score: 180.0   memory length: 196805   epsilon: 0.8605343000000643    steps: 818     evaluation reward: 161.2\n",
      "episode: 280   score: 210.0   memory length: 197620   epsilon: 0.8597600500000646    steps: 815     evaluation reward: 161.7\n",
      "episode: 281   score: 320.0   memory length: 198678   epsilon: 0.8587549500000651    steps: 1058     evaluation reward: 163.7\n",
      "episode: 282   score: 115.0   memory length: 199416   epsilon: 0.8580538500000654    steps: 738     evaluation reward: 163.85\n",
      "episode: 283   score: 50.0   memory length: 199841   epsilon: 0.8576501000000656    steps: 425     evaluation reward: 163.25\n",
      "now time :  2020-02-29 13:20:50.938842\n",
      "episode: 284   score: 105.0   memory length: 200447   epsilon: 0.8570744000000659    steps: 606     evaluation reward: 163.5\n",
      "episode: 285   score: 230.0   memory length: 201540   epsilon: 0.8560360500000663    steps: 1093     evaluation reward: 164.6\n",
      "episode: 286   score: 75.0   memory length: 201943   epsilon: 0.8556532000000665    steps: 403     evaluation reward: 159.6\n",
      "episode: 287   score: 210.0   memory length: 203073   epsilon: 0.854579700000067    steps: 1130     evaluation reward: 160.5\n",
      "episode: 288   score: 230.0   memory length: 203991   epsilon: 0.8537076000000674    steps: 918     evaluation reward: 162.05\n",
      "episode: 289   score: 120.0   memory length: 204648   epsilon: 0.8530834500000677    steps: 657     evaluation reward: 161.3\n",
      "episode: 290   score: 105.0   memory length: 205289   epsilon: 0.852474500000068    steps: 641     evaluation reward: 161.25\n",
      "episode: 291   score: 140.0   memory length: 206069   epsilon: 0.8517335000000683    steps: 780     evaluation reward: 160.35\n",
      "episode: 292   score: 110.0   memory length: 206658   epsilon: 0.8511739500000686    steps: 589     evaluation reward: 160.25\n",
      "episode: 293   score: 65.0   memory length: 207256   epsilon: 0.8506058500000688    steps: 598     evaluation reward: 159.75\n",
      "episode: 294   score: 185.0   memory length: 208051   epsilon: 0.8498506000000692    steps: 795     evaluation reward: 161.15\n",
      "episode: 295   score: 155.0   memory length: 208861   epsilon: 0.8490811000000695    steps: 810     evaluation reward: 161.5\n",
      "episode: 296   score: 180.0   memory length: 209667   epsilon: 0.8483154000000699    steps: 806     evaluation reward: 159.65\n",
      "episode: 297   score: 135.0   memory length: 210448   epsilon: 0.8475734500000702    steps: 781     evaluation reward: 159.9\n",
      "episode: 298   score: 120.0   memory length: 211091   epsilon: 0.8469626000000705    steps: 643     evaluation reward: 160.0\n",
      "episode: 299   score: 120.0   memory length: 212130   epsilon: 0.845975550000071    steps: 1039     evaluation reward: 159.1\n",
      "episode: 300   score: 30.0   memory length: 212615   epsilon: 0.8455148000000712    steps: 485     evaluation reward: 158.75\n",
      "episode: 301   score: 65.0   memory length: 213023   epsilon: 0.8451272000000714    steps: 408     evaluation reward: 158.05\n",
      "episode: 302   score: 155.0   memory length: 213796   epsilon: 0.8443928500000717    steps: 773     evaluation reward: 158.45\n",
      "episode: 303   score: 185.0   memory length: 214592   epsilon: 0.843636650000072    steps: 796     evaluation reward: 159.85\n",
      "episode: 304   score: 80.0   memory length: 214980   epsilon: 0.8432680500000722    steps: 388     evaluation reward: 156.35\n",
      "episode: 305   score: 105.0   memory length: 215740   epsilon: 0.8425460500000725    steps: 760     evaluation reward: 157.25\n",
      "episode: 306   score: 135.0   memory length: 216437   epsilon: 0.8418839000000728    steps: 697     evaluation reward: 157.4\n",
      "episode: 307   score: 85.0   memory length: 216947   epsilon: 0.8413994000000731    steps: 510     evaluation reward: 157.75\n",
      "episode: 308   score: 110.0   memory length: 217586   epsilon: 0.8407923500000734    steps: 639     evaluation reward: 158.4\n",
      "episode: 309   score: 110.0   memory length: 218170   epsilon: 0.8402375500000736    steps: 584     evaluation reward: 157.95\n",
      "episode: 310   score: 160.0   memory length: 218918   epsilon: 0.8395269500000739    steps: 748     evaluation reward: 159.0\n",
      "episode: 311   score: 225.0   memory length: 219864   epsilon: 0.8386282500000743    steps: 946     evaluation reward: 160.2\n",
      "episode: 312   score: 195.0   memory length: 220832   epsilon: 0.8377086500000748    steps: 968     evaluation reward: 161.4\n",
      "episode: 313   score: 110.0   memory length: 221456   epsilon: 0.837115850000075    steps: 624     evaluation reward: 162.15\n",
      "episode: 314   score: 180.0   memory length: 222292   epsilon: 0.8363216500000754    steps: 836     evaluation reward: 163.65\n",
      "episode: 315   score: 110.0   memory length: 222944   epsilon: 0.8357022500000757    steps: 652     evaluation reward: 162.2\n",
      "episode: 316   score: 105.0   memory length: 223505   epsilon: 0.8351693000000759    steps: 561     evaluation reward: 161.1\n",
      "episode: 317   score: 180.0   memory length: 224386   epsilon: 0.8343323500000763    steps: 881     evaluation reward: 161.55\n",
      "episode: 318   score: 130.0   memory length: 225291   epsilon: 0.8334726000000767    steps: 905     evaluation reward: 160.45\n",
      "episode: 319   score: 30.0   memory length: 225871   epsilon: 0.832921600000077    steps: 580     evaluation reward: 155.6\n",
      "episode: 320   score: 15.0   memory length: 226254   epsilon: 0.8325577500000771    steps: 383     evaluation reward: 151.8\n",
      "episode: 321   score: 150.0   memory length: 226873   epsilon: 0.8319697000000774    steps: 619     evaluation reward: 152.95\n",
      "episode: 322   score: 160.0   memory length: 227654   epsilon: 0.8312277500000778    steps: 781     evaluation reward: 153.55\n",
      "episode: 323   score: 155.0   memory length: 228439   epsilon: 0.8304820000000781    steps: 785     evaluation reward: 154.35\n",
      "episode: 324   score: 225.0   memory length: 229887   epsilon: 0.8291064000000787    steps: 1448     evaluation reward: 151.4\n",
      "episode: 325   score: 210.0   memory length: 230660   epsilon: 0.8283720500000791    steps: 773     evaluation reward: 152.4\n",
      "episode: 326   score: 135.0   memory length: 231392   epsilon: 0.8276766500000794    steps: 732     evaluation reward: 152.5\n",
      "episode: 327   score: 75.0   memory length: 231813   epsilon: 0.8272767000000796    steps: 421     evaluation reward: 151.85\n",
      "episode: 328   score: 385.0   memory length: 232723   epsilon: 0.82641220000008    steps: 910     evaluation reward: 154.15\n",
      "episode: 329   score: 75.0   memory length: 233232   epsilon: 0.8259286500000802    steps: 509     evaluation reward: 152.6\n",
      "episode: 330   score: 180.0   memory length: 233980   epsilon: 0.8252180500000805    steps: 748     evaluation reward: 152.55\n",
      "episode: 331   score: 240.0   memory length: 234914   epsilon: 0.8243307500000809    steps: 934     evaluation reward: 154.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 332   score: 385.0   memory length: 235971   epsilon: 0.8233266000000814    steps: 1057     evaluation reward: 154.8\n",
      "episode: 333   score: 75.0   memory length: 236395   epsilon: 0.8229238000000816    steps: 424     evaluation reward: 154.8\n",
      "episode: 334   score: 110.0   memory length: 237063   epsilon: 0.8222892000000819    steps: 668     evaluation reward: 154.1\n",
      "episode: 335   score: 240.0   memory length: 237865   epsilon: 0.8215273000000822    steps: 802     evaluation reward: 155.3\n",
      "episode: 336   score: 135.0   memory length: 238638   epsilon: 0.8207929500000826    steps: 773     evaluation reward: 155.45\n",
      "episode: 337   score: 110.0   memory length: 239277   epsilon: 0.8201859000000828    steps: 639     evaluation reward: 155.5\n",
      "episode: 338   score: 35.0   memory length: 239824   epsilon: 0.8196662500000831    steps: 547     evaluation reward: 155.35\n",
      "episode: 339   score: 225.0   memory length: 240693   epsilon: 0.8188407000000835    steps: 869     evaluation reward: 152.5\n",
      "episode: 340   score: 330.0   memory length: 241996   epsilon: 0.817602850000084    steps: 1303     evaluation reward: 155.05\n",
      "episode: 341   score: 80.0   memory length: 242704   epsilon: 0.8169302500000843    steps: 708     evaluation reward: 151.25\n",
      "episode: 342   score: 65.0   memory length: 243274   epsilon: 0.8163887500000846    steps: 570     evaluation reward: 151.1\n",
      "episode: 343   score: 110.0   memory length: 243735   epsilon: 0.8159508000000848    steps: 461     evaluation reward: 151.15\n",
      "episode: 344   score: 35.0   memory length: 244183   epsilon: 0.815525200000085    steps: 448     evaluation reward: 150.4\n",
      "episode: 345   score: 170.0   memory length: 245019   epsilon: 0.8147310000000854    steps: 836     evaluation reward: 146.9\n",
      "episode: 346   score: 210.0   memory length: 245835   epsilon: 0.8139558000000857    steps: 816     evaluation reward: 147.65\n",
      "episode: 347   score: 105.0   memory length: 246216   epsilon: 0.8135938500000859    steps: 381     evaluation reward: 147.05\n",
      "episode: 348   score: 115.0   memory length: 246602   epsilon: 0.813227150000086    steps: 386     evaluation reward: 146.5\n",
      "episode: 349   score: 360.0   memory length: 247400   epsilon: 0.8124690500000864    steps: 798     evaluation reward: 148.75\n",
      "episode: 350   score: 185.0   memory length: 248182   epsilon: 0.8117261500000867    steps: 782     evaluation reward: 149.4\n",
      "episode: 351   score: 105.0   memory length: 248824   epsilon: 0.811116250000087    steps: 642     evaluation reward: 148.8\n",
      "episode: 352   score: 50.0   memory length: 249253   epsilon: 0.8107087000000872    steps: 429     evaluation reward: 148.2\n",
      "now time :  2020-02-29 13:32:40.127553\n",
      "episode: 353   score: 120.0   memory length: 250023   epsilon: 0.8099772000000875    steps: 770     evaluation reward: 148.6\n",
      "episode: 354   score: 75.0   memory length: 250437   epsilon: 0.8095839000000877    steps: 414     evaluation reward: 146.05\n",
      "episode: 355   score: 105.0   memory length: 250986   epsilon: 0.809062350000088    steps: 549     evaluation reward: 146.0\n",
      "episode: 356   score: 135.0   memory length: 251753   epsilon: 0.8083337000000883    steps: 767     evaluation reward: 146.8\n",
      "episode: 357   score: 110.0   memory length: 252483   epsilon: 0.8076402000000886    steps: 730     evaluation reward: 147.35\n",
      "episode: 358   score: 135.0   memory length: 253130   epsilon: 0.8070255500000889    steps: 647     evaluation reward: 146.75\n",
      "episode: 359   score: 135.0   memory length: 253902   epsilon: 0.8062921500000892    steps: 772     evaluation reward: 146.9\n",
      "episode: 360   score: 185.0   memory length: 254868   epsilon: 0.8053744500000897    steps: 966     evaluation reward: 147.7\n",
      "episode: 361   score: 180.0   memory length: 255673   epsilon: 0.80460970000009    steps: 805     evaluation reward: 147.55\n",
      "episode: 362   score: 370.0   memory length: 256940   epsilon: 0.8034060500000906    steps: 1267     evaluation reward: 149.45\n",
      "episode: 363   score: 35.0   memory length: 257342   epsilon: 0.8030241500000908    steps: 402     evaluation reward: 148.0\n",
      "episode: 364   score: 50.0   memory length: 257800   epsilon: 0.802589050000091    steps: 458     evaluation reward: 147.7\n",
      "episode: 365   score: 75.0   memory length: 258197   epsilon: 0.8022119000000911    steps: 397     evaluation reward: 145.8\n",
      "episode: 366   score: 105.0   memory length: 258902   epsilon: 0.8015421500000914    steps: 705     evaluation reward: 144.05\n",
      "episode: 367   score: 180.0   memory length: 259706   epsilon: 0.8007783500000918    steps: 804     evaluation reward: 144.8\n",
      "episode: 368   score: 110.0   memory length: 260465   epsilon: 0.8000573000000921    steps: 759     evaluation reward: 145.15\n",
      "episode: 369   score: 135.0   memory length: 261225   epsilon: 0.7993353000000925    steps: 760     evaluation reward: 143.5\n",
      "episode: 370   score: 180.0   memory length: 262007   epsilon: 0.7985924000000928    steps: 782     evaluation reward: 144.75\n",
      "episode: 371   score: 110.0   memory length: 262660   epsilon: 0.7979720500000931    steps: 653     evaluation reward: 144.7\n",
      "episode: 372   score: 180.0   memory length: 263519   epsilon: 0.7971560000000935    steps: 859     evaluation reward: 144.65\n",
      "episode: 373   score: 155.0   memory length: 264424   epsilon: 0.7962962500000939    steps: 905     evaluation reward: 144.4\n",
      "episode: 374   score: 545.0   memory length: 265478   epsilon: 0.7952949500000943    steps: 1054     evaluation reward: 149.05\n",
      "episode: 375   score: 165.0   memory length: 266134   epsilon: 0.7946717500000946    steps: 656     evaluation reward: 149.35\n",
      "episode: 376   score: 180.0   memory length: 266896   epsilon: 0.7939478500000949    steps: 762     evaluation reward: 149.95\n",
      "episode: 377   score: 200.0   memory length: 267820   epsilon: 0.7930700500000953    steps: 924     evaluation reward: 148.7\n",
      "episode: 378   score: 580.0   memory length: 269174   epsilon: 0.7917837500000959    steps: 1354     evaluation reward: 154.2\n",
      "episode: 379   score: 455.0   memory length: 270167   epsilon: 0.7908404000000964    steps: 993     evaluation reward: 156.95\n",
      "episode: 380   score: 105.0   memory length: 270803   epsilon: 0.7902362000000966    steps: 636     evaluation reward: 155.9\n",
      "episode: 381   score: 110.0   memory length: 271434   epsilon: 0.7896367500000969    steps: 631     evaluation reward: 153.8\n",
      "episode: 382   score: 155.0   memory length: 272152   epsilon: 0.7889546500000972    steps: 718     evaluation reward: 154.2\n",
      "episode: 383   score: 120.0   memory length: 272544   epsilon: 0.7885822500000974    steps: 392     evaluation reward: 154.9\n",
      "episode: 384   score: 140.0   memory length: 273278   epsilon: 0.7878849500000977    steps: 734     evaluation reward: 155.25\n",
      "episode: 385   score: 80.0   memory length: 273695   epsilon: 0.7874888000000979    steps: 417     evaluation reward: 153.75\n",
      "episode: 386   score: 210.0   memory length: 274594   epsilon: 0.7866347500000983    steps: 899     evaluation reward: 155.1\n",
      "episode: 387   score: 210.0   memory length: 275453   epsilon: 0.7858187000000987    steps: 859     evaluation reward: 155.1\n",
      "episode: 388   score: 140.0   memory length: 276292   epsilon: 0.785021650000099    steps: 839     evaluation reward: 154.2\n",
      "episode: 389   score: 145.0   memory length: 276911   epsilon: 0.7844336000000993    steps: 619     evaluation reward: 154.45\n",
      "episode: 390   score: 35.0   memory length: 277300   epsilon: 0.7840640500000995    steps: 389     evaluation reward: 153.75\n",
      "episode: 391   score: 50.0   memory length: 277678   epsilon: 0.7837049500000997    steps: 378     evaluation reward: 152.85\n",
      "episode: 392   score: 535.0   memory length: 278727   epsilon: 0.7827084000001001    steps: 1049     evaluation reward: 157.1\n",
      "episode: 393   score: 335.0   memory length: 279433   epsilon: 0.7820377000001004    steps: 706     evaluation reward: 159.8\n",
      "episode: 394   score: 105.0   memory length: 279811   epsilon: 0.7816786000001006    steps: 378     evaluation reward: 159.0\n",
      "episode: 395   score: 180.0   memory length: 280590   epsilon: 0.7809385500001009    steps: 779     evaluation reward: 159.25\n",
      "episode: 396   score: 135.0   memory length: 281247   epsilon: 0.7803144000001012    steps: 657     evaluation reward: 158.8\n",
      "episode: 397   score: 180.0   memory length: 282017   epsilon: 0.7795829000001016    steps: 770     evaluation reward: 159.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 398   score: 380.0   memory length: 282879   epsilon: 0.7787640000001019    steps: 862     evaluation reward: 161.85\n",
      "episode: 399   score: 210.0   memory length: 283679   epsilon: 0.7780040000001023    steps: 800     evaluation reward: 162.75\n",
      "episode: 400   score: 110.0   memory length: 284376   epsilon: 0.7773418500001026    steps: 697     evaluation reward: 163.55\n",
      "episode: 401   score: 375.0   memory length: 285170   epsilon: 0.7765875500001029    steps: 794     evaluation reward: 166.65\n",
      "episode: 402   score: 120.0   memory length: 285808   epsilon: 0.7759814500001032    steps: 638     evaluation reward: 166.3\n",
      "episode: 403   score: 110.0   memory length: 286497   epsilon: 0.7753269000001035    steps: 689     evaluation reward: 165.55\n",
      "episode: 404   score: 125.0   memory length: 287276   epsilon: 0.7745868500001039    steps: 779     evaluation reward: 166.0\n",
      "episode: 405   score: 155.0   memory length: 288127   epsilon: 0.7737784000001042    steps: 851     evaluation reward: 166.5\n",
      "episode: 406   score: 50.0   memory length: 288538   epsilon: 0.7733879500001044    steps: 411     evaluation reward: 165.65\n",
      "episode: 407   score: 110.0   memory length: 289187   epsilon: 0.7727714000001047    steps: 649     evaluation reward: 165.9\n",
      "episode: 408   score: 180.0   memory length: 289981   epsilon: 0.772017100000105    steps: 794     evaluation reward: 166.6\n",
      "episode: 409   score: 110.0   memory length: 290740   epsilon: 0.7712960500001054    steps: 759     evaluation reward: 166.6\n",
      "episode: 410   score: 180.0   memory length: 291536   epsilon: 0.7705398500001057    steps: 796     evaluation reward: 166.8\n",
      "episode: 411   score: 395.0   memory length: 292558   epsilon: 0.7695689500001062    steps: 1022     evaluation reward: 168.5\n",
      "episode: 412   score: 200.0   memory length: 293567   epsilon: 0.7686104000001066    steps: 1009     evaluation reward: 168.55\n",
      "episode: 413   score: 35.0   memory length: 293977   epsilon: 0.7682209000001068    steps: 410     evaluation reward: 167.8\n",
      "episode: 414   score: 210.0   memory length: 294802   epsilon: 0.7674371500001071    steps: 825     evaluation reward: 168.1\n",
      "episode: 415   score: 55.0   memory length: 295264   epsilon: 0.7669982500001074    steps: 462     evaluation reward: 167.55\n",
      "episode: 416   score: 110.0   memory length: 295905   epsilon: 0.7663893000001076    steps: 641     evaluation reward: 167.6\n",
      "episode: 417   score: 210.0   memory length: 296692   epsilon: 0.765641650000108    steps: 787     evaluation reward: 167.9\n",
      "episode: 418   score: 180.0   memory length: 297568   epsilon: 0.7648094500001084    steps: 876     evaluation reward: 168.4\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame and frame % train_freq == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 700:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
