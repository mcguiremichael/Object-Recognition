{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 135   epsilon: 1.0    steps: 135     evaluation reward: 0.0\n",
      "episode: 1   score: 2.0   memory length: 336   epsilon: 1.0    steps: 201     evaluation reward: 1.0\n",
      "episode: 2   score: 3.0   memory length: 590   epsilon: 1.0    steps: 254     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 1.0   memory length: 770   epsilon: 1.0    steps: 180     evaluation reward: 1.5\n",
      "episode: 4   score: 0.0   memory length: 909   epsilon: 1.0    steps: 139     evaluation reward: 1.2\n",
      "episode: 5   score: 1.0   memory length: 1082   epsilon: 1.0    steps: 173     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 0.0   memory length: 1222   epsilon: 1.0    steps: 140     evaluation reward: 1.0\n",
      "episode: 7   score: 1.0   memory length: 1378   epsilon: 1.0    steps: 156     evaluation reward: 1.0\n",
      "episode: 8   score: 0.0   memory length: 1507   epsilon: 1.0    steps: 129     evaluation reward: 0.8888888888888888\n",
      "episode: 9   score: 1.0   memory length: 1683   epsilon: 1.0    steps: 176     evaluation reward: 0.9\n",
      "episode: 10   score: 3.0   memory length: 1913   epsilon: 1.0    steps: 230     evaluation reward: 1.0909090909090908\n",
      "episode: 11   score: 0.0   memory length: 2039   epsilon: 1.0    steps: 126     evaluation reward: 1.0\n",
      "episode: 12   score: 3.0   memory length: 2289   epsilon: 1.0    steps: 250     evaluation reward: 1.1538461538461537\n",
      "episode: 13   score: 2.0   memory length: 2492   epsilon: 1.0    steps: 203     evaluation reward: 1.2142857142857142\n",
      "episode: 14   score: 0.0   memory length: 2640   epsilon: 1.0    steps: 148     evaluation reward: 1.1333333333333333\n",
      "episode: 15   score: 2.0   memory length: 2851   epsilon: 1.0    steps: 211     evaluation reward: 1.1875\n",
      "episode: 16   score: 0.0   memory length: 2986   epsilon: 1.0    steps: 135     evaluation reward: 1.1176470588235294\n",
      "episode: 17   score: 1.0   memory length: 3164   epsilon: 1.0    steps: 178     evaluation reward: 1.1111111111111112\n",
      "episode: 18   score: 2.0   memory length: 3373   epsilon: 1.0    steps: 209     evaluation reward: 1.1578947368421053\n",
      "episode: 19   score: 0.0   memory length: 3499   epsilon: 1.0    steps: 126     evaluation reward: 1.1\n",
      "episode: 20   score: 3.0   memory length: 3754   epsilon: 1.0    steps: 255     evaluation reward: 1.1904761904761905\n",
      "episode: 21   score: 0.0   memory length: 3888   epsilon: 1.0    steps: 134     evaluation reward: 1.1363636363636365\n",
      "episode: 22   score: 1.0   memory length: 4065   epsilon: 1.0    steps: 177     evaluation reward: 1.1304347826086956\n",
      "episode: 23   score: 2.0   memory length: 4261   epsilon: 1.0    steps: 196     evaluation reward: 1.1666666666666667\n",
      "episode: 24   score: 3.0   memory length: 4512   epsilon: 1.0    steps: 251     evaluation reward: 1.24\n",
      "episode: 25   score: 2.0   memory length: 4719   epsilon: 1.0    steps: 207     evaluation reward: 1.2692307692307692\n",
      "episode: 26   score: 0.0   memory length: 4850   epsilon: 1.0    steps: 131     evaluation reward: 1.2222222222222223\n",
      "episode: 27   score: 1.0   memory length: 5023   epsilon: 1.0    steps: 173     evaluation reward: 1.2142857142857142\n",
      "episode: 28   score: 0.0   memory length: 5156   epsilon: 1.0    steps: 133     evaluation reward: 1.1724137931034482\n",
      "episode: 29   score: 1.0   memory length: 5328   epsilon: 1.0    steps: 172     evaluation reward: 1.1666666666666667\n",
      "episode: 30   score: 2.0   memory length: 5511   epsilon: 1.0    steps: 183     evaluation reward: 1.1935483870967742\n",
      "episode: 31   score: 0.0   memory length: 5648   epsilon: 1.0    steps: 137     evaluation reward: 1.15625\n",
      "episode: 32   score: 2.0   memory length: 5859   epsilon: 1.0    steps: 211     evaluation reward: 1.1818181818181819\n",
      "episode: 33   score: 2.0   memory length: 6081   epsilon: 1.0    steps: 222     evaluation reward: 1.2058823529411764\n",
      "episode: 34   score: 0.0   memory length: 6212   epsilon: 1.0    steps: 131     evaluation reward: 1.1714285714285715\n",
      "episode: 35   score: 3.0   memory length: 6469   epsilon: 1.0    steps: 257     evaluation reward: 1.2222222222222223\n",
      "episode: 36   score: 0.0   memory length: 6604   epsilon: 1.0    steps: 135     evaluation reward: 1.1891891891891893\n",
      "episode: 37   score: 0.0   memory length: 6727   epsilon: 1.0    steps: 123     evaluation reward: 1.1578947368421053\n",
      "episode: 38   score: 1.0   memory length: 6893   epsilon: 1.0    steps: 166     evaluation reward: 1.1538461538461537\n",
      "episode: 39   score: 3.0   memory length: 7129   epsilon: 1.0    steps: 236     evaluation reward: 1.2\n",
      "episode: 40   score: 0.0   memory length: 7261   epsilon: 1.0    steps: 132     evaluation reward: 1.170731707317073\n",
      "episode: 41   score: 0.0   memory length: 7390   epsilon: 1.0    steps: 129     evaluation reward: 1.1428571428571428\n",
      "episode: 42   score: 2.0   memory length: 7591   epsilon: 1.0    steps: 201     evaluation reward: 1.1627906976744187\n",
      "episode: 43   score: 0.0   memory length: 7720   epsilon: 1.0    steps: 129     evaluation reward: 1.1363636363636365\n",
      "episode: 44   score: 1.0   memory length: 7887   epsilon: 1.0    steps: 167     evaluation reward: 1.1333333333333333\n",
      "episode: 45   score: 2.0   memory length: 8101   epsilon: 1.0    steps: 214     evaluation reward: 1.1521739130434783\n",
      "episode: 46   score: 2.0   memory length: 8307   epsilon: 1.0    steps: 206     evaluation reward: 1.1702127659574468\n",
      "episode: 47   score: 0.0   memory length: 8450   epsilon: 1.0    steps: 143     evaluation reward: 1.1458333333333333\n",
      "episode: 48   score: 4.0   memory length: 8713   epsilon: 1.0    steps: 263     evaluation reward: 1.2040816326530612\n",
      "episode: 49   score: 4.0   memory length: 8991   epsilon: 1.0    steps: 278     evaluation reward: 1.26\n",
      "episode: 50   score: 1.0   memory length: 9164   epsilon: 1.0    steps: 173     evaluation reward: 1.2549019607843137\n",
      "episode: 51   score: 1.0   memory length: 9339   epsilon: 1.0    steps: 175     evaluation reward: 1.25\n",
      "episode: 52   score: 0.0   memory length: 9471   epsilon: 1.0    steps: 132     evaluation reward: 1.2264150943396226\n",
      "episode: 53   score: 2.0   memory length: 9693   epsilon: 1.0    steps: 222     evaluation reward: 1.2407407407407407\n",
      "episode: 54   score: 3.0   memory length: 9938   epsilon: 1.0    steps: 245     evaluation reward: 1.2727272727272727\n",
      "episode: 55   score: 1.0   memory length: 10123   epsilon: 1.0    steps: 185     evaluation reward: 1.2678571428571428\n",
      "episode: 56   score: 2.0   memory length: 10324   epsilon: 1.0    steps: 201     evaluation reward: 1.280701754385965\n",
      "episode: 57   score: 0.0   memory length: 10457   epsilon: 1.0    steps: 133     evaluation reward: 1.2586206896551724\n",
      "episode: 58   score: 1.0   memory length: 10644   epsilon: 1.0    steps: 187     evaluation reward: 1.2542372881355932\n",
      "episode: 59   score: 1.0   memory length: 10803   epsilon: 1.0    steps: 159     evaluation reward: 1.25\n",
      "episode: 60   score: 1.0   memory length: 10983   epsilon: 1.0    steps: 180     evaluation reward: 1.2459016393442623\n",
      "episode: 61   score: 1.0   memory length: 11138   epsilon: 1.0    steps: 155     evaluation reward: 1.2419354838709677\n",
      "episode: 62   score: 0.0   memory length: 11270   epsilon: 1.0    steps: 132     evaluation reward: 1.2222222222222223\n",
      "episode: 63   score: 8.0   memory length: 11620   epsilon: 1.0    steps: 350     evaluation reward: 1.328125\n",
      "episode: 64   score: 0.0   memory length: 11751   epsilon: 1.0    steps: 131     evaluation reward: 1.3076923076923077\n",
      "episode: 65   score: 1.0   memory length: 11954   epsilon: 1.0    steps: 203     evaluation reward: 1.303030303030303\n",
      "episode: 66   score: 2.0   memory length: 12145   epsilon: 1.0    steps: 191     evaluation reward: 1.3134328358208955\n",
      "episode: 67   score: 1.0   memory length: 12326   epsilon: 1.0    steps: 181     evaluation reward: 1.3088235294117647\n",
      "episode: 68   score: 2.0   memory length: 12529   epsilon: 1.0    steps: 203     evaluation reward: 1.318840579710145\n",
      "episode: 69   score: 5.0   memory length: 12890   epsilon: 1.0    steps: 361     evaluation reward: 1.3714285714285714\n",
      "episode: 70   score: 2.0   memory length: 13083   epsilon: 1.0    steps: 193     evaluation reward: 1.380281690140845\n",
      "episode: 71   score: 0.0   memory length: 13211   epsilon: 1.0    steps: 128     evaluation reward: 1.3611111111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 72   score: 0.0   memory length: 13346   epsilon: 1.0    steps: 135     evaluation reward: 1.3424657534246576\n",
      "episode: 73   score: 2.0   memory length: 13554   epsilon: 1.0    steps: 208     evaluation reward: 1.3513513513513513\n",
      "episode: 74   score: 1.0   memory length: 13727   epsilon: 1.0    steps: 173     evaluation reward: 1.3466666666666667\n",
      "episode: 75   score: 1.0   memory length: 13880   epsilon: 1.0    steps: 153     evaluation reward: 1.3421052631578947\n",
      "episode: 76   score: 0.0   memory length: 14005   epsilon: 1.0    steps: 125     evaluation reward: 1.3246753246753247\n",
      "episode: 77   score: 2.0   memory length: 14212   epsilon: 1.0    steps: 207     evaluation reward: 1.3333333333333333\n",
      "episode: 78   score: 1.0   memory length: 14382   epsilon: 1.0    steps: 170     evaluation reward: 1.3291139240506329\n",
      "episode: 79   score: 0.0   memory length: 14511   epsilon: 1.0    steps: 129     evaluation reward: 1.3125\n",
      "episode: 80   score: 2.0   memory length: 14714   epsilon: 1.0    steps: 203     evaluation reward: 1.3209876543209877\n",
      "episode: 81   score: 3.0   memory length: 14962   epsilon: 1.0    steps: 248     evaluation reward: 1.3414634146341464\n",
      "episode: 82   score: 1.0   memory length: 15139   epsilon: 1.0    steps: 177     evaluation reward: 1.3373493975903614\n",
      "episode: 83   score: 1.0   memory length: 15315   epsilon: 1.0    steps: 176     evaluation reward: 1.3333333333333333\n",
      "episode: 84   score: 2.0   memory length: 15550   epsilon: 1.0    steps: 235     evaluation reward: 1.3411764705882352\n",
      "episode: 85   score: 0.0   memory length: 15680   epsilon: 1.0    steps: 130     evaluation reward: 1.3255813953488371\n",
      "episode: 86   score: 0.0   memory length: 15805   epsilon: 1.0    steps: 125     evaluation reward: 1.3103448275862069\n",
      "episode: 87   score: 1.0   memory length: 15978   epsilon: 1.0    steps: 173     evaluation reward: 1.3068181818181819\n",
      "episode: 88   score: 0.0   memory length: 16107   epsilon: 1.0    steps: 129     evaluation reward: 1.2921348314606742\n",
      "episode: 89   score: 0.0   memory length: 16236   epsilon: 1.0    steps: 129     evaluation reward: 1.2777777777777777\n",
      "episode: 90   score: 2.0   memory length: 16436   epsilon: 1.0    steps: 200     evaluation reward: 1.2857142857142858\n",
      "episode: 91   score: 0.0   memory length: 16572   epsilon: 1.0    steps: 136     evaluation reward: 1.2717391304347827\n",
      "episode: 92   score: 1.0   memory length: 16731   epsilon: 1.0    steps: 159     evaluation reward: 1.2688172043010753\n",
      "episode: 93   score: 2.0   memory length: 16961   epsilon: 1.0    steps: 230     evaluation reward: 1.2765957446808511\n",
      "episode: 94   score: 2.0   memory length: 17162   epsilon: 1.0    steps: 201     evaluation reward: 1.2842105263157895\n",
      "episode: 95   score: 1.0   memory length: 17337   epsilon: 1.0    steps: 175     evaluation reward: 1.28125\n",
      "episode: 96   score: 3.0   memory length: 17567   epsilon: 1.0    steps: 230     evaluation reward: 1.2989690721649485\n",
      "episode: 97   score: 2.0   memory length: 17768   epsilon: 1.0    steps: 201     evaluation reward: 1.3061224489795917\n",
      "episode: 98   score: 4.0   memory length: 18046   epsilon: 1.0    steps: 278     evaluation reward: 1.3333333333333333\n",
      "episode: 99   score: 1.0   memory length: 18200   epsilon: 1.0    steps: 154     evaluation reward: 1.33\n",
      "episode: 100   score: 1.0   memory length: 18379   epsilon: 1.0    steps: 179     evaluation reward: 1.34\n",
      "episode: 101   score: 0.0   memory length: 18505   epsilon: 1.0    steps: 126     evaluation reward: 1.32\n",
      "episode: 102   score: 0.0   memory length: 18639   epsilon: 1.0    steps: 134     evaluation reward: 1.29\n",
      "episode: 103   score: 5.0   memory length: 18989   epsilon: 1.0    steps: 350     evaluation reward: 1.33\n",
      "episode: 104   score: 0.0   memory length: 19123   epsilon: 1.0    steps: 134     evaluation reward: 1.33\n",
      "episode: 105   score: 2.0   memory length: 19316   epsilon: 1.0    steps: 193     evaluation reward: 1.34\n",
      "episode: 106   score: 0.0   memory length: 19446   epsilon: 1.0    steps: 130     evaluation reward: 1.34\n",
      "episode: 107   score: 0.0   memory length: 19575   epsilon: 1.0    steps: 129     evaluation reward: 1.33\n",
      "episode: 108   score: 2.0   memory length: 19778   epsilon: 1.0    steps: 203     evaluation reward: 1.35\n",
      "episode: 109   score: 1.0   memory length: 19954   epsilon: 1.0    steps: 176     evaluation reward: 1.35\n",
      "episode: 110   score: 3.0   memory length: 20195   epsilon: 1.0    steps: 241     evaluation reward: 1.35\n",
      "episode: 111   score: 0.0   memory length: 20318   epsilon: 1.0    steps: 123     evaluation reward: 1.35\n",
      "episode: 112   score: 3.0   memory length: 20561   epsilon: 1.0    steps: 243     evaluation reward: 1.35\n",
      "episode: 113   score: 2.0   memory length: 20754   epsilon: 1.0    steps: 193     evaluation reward: 1.35\n",
      "episode: 114   score: 1.0   memory length: 20935   epsilon: 1.0    steps: 181     evaluation reward: 1.36\n",
      "episode: 115   score: 0.0   memory length: 21071   epsilon: 1.0    steps: 136     evaluation reward: 1.34\n",
      "episode: 116   score: 2.0   memory length: 21278   epsilon: 1.0    steps: 207     evaluation reward: 1.36\n",
      "episode: 117   score: 2.0   memory length: 21478   epsilon: 1.0    steps: 200     evaluation reward: 1.37\n",
      "episode: 118   score: 1.0   memory length: 21656   epsilon: 1.0    steps: 178     evaluation reward: 1.36\n",
      "episode: 119   score: 2.0   memory length: 21861   epsilon: 1.0    steps: 205     evaluation reward: 1.38\n",
      "episode: 120   score: 0.0   memory length: 21991   epsilon: 1.0    steps: 130     evaluation reward: 1.35\n",
      "episode: 121   score: 2.0   memory length: 22232   epsilon: 1.0    steps: 241     evaluation reward: 1.37\n",
      "episode: 122   score: 0.0   memory length: 22362   epsilon: 1.0    steps: 130     evaluation reward: 1.36\n",
      "episode: 123   score: 0.0   memory length: 22493   epsilon: 1.0    steps: 131     evaluation reward: 1.34\n",
      "episode: 124   score: 3.0   memory length: 22728   epsilon: 1.0    steps: 235     evaluation reward: 1.34\n",
      "episode: 125   score: 0.0   memory length: 22857   epsilon: 1.0    steps: 129     evaluation reward: 1.32\n",
      "episode: 126   score: 0.0   memory length: 22992   epsilon: 1.0    steps: 135     evaluation reward: 1.32\n",
      "episode: 127   score: 0.0   memory length: 23123   epsilon: 1.0    steps: 131     evaluation reward: 1.31\n",
      "episode: 128   score: 2.0   memory length: 23310   epsilon: 1.0    steps: 187     evaluation reward: 1.33\n",
      "episode: 129   score: 0.0   memory length: 23446   epsilon: 1.0    steps: 136     evaluation reward: 1.32\n",
      "episode: 130   score: 1.0   memory length: 23608   epsilon: 1.0    steps: 162     evaluation reward: 1.31\n",
      "episode: 131   score: 0.0   memory length: 23744   epsilon: 1.0    steps: 136     evaluation reward: 1.31\n",
      "episode: 132   score: 1.0   memory length: 23918   epsilon: 1.0    steps: 174     evaluation reward: 1.3\n",
      "episode: 133   score: 1.0   memory length: 24109   epsilon: 1.0    steps: 191     evaluation reward: 1.29\n",
      "episode: 134   score: 0.0   memory length: 24243   epsilon: 1.0    steps: 134     evaluation reward: 1.29\n",
      "episode: 135   score: 1.0   memory length: 24426   epsilon: 1.0    steps: 183     evaluation reward: 1.27\n",
      "episode: 136   score: 0.0   memory length: 24568   epsilon: 1.0    steps: 142     evaluation reward: 1.27\n",
      "episode: 137   score: 2.0   memory length: 24781   epsilon: 1.0    steps: 213     evaluation reward: 1.29\n",
      "episode: 138   score: 0.0   memory length: 24914   epsilon: 1.0    steps: 133     evaluation reward: 1.28\n",
      "episode: 139   score: 1.0   memory length: 25091   epsilon: 1.0    steps: 177     evaluation reward: 1.26\n",
      "episode: 140   score: 0.0   memory length: 25222   epsilon: 1.0    steps: 131     evaluation reward: 1.26\n",
      "episode: 141   score: 2.0   memory length: 25426   epsilon: 1.0    steps: 204     evaluation reward: 1.28\n",
      "episode: 142   score: 1.0   memory length: 25610   epsilon: 1.0    steps: 184     evaluation reward: 1.27\n",
      "episode: 143   score: 0.0   memory length: 25737   epsilon: 1.0    steps: 127     evaluation reward: 1.27\n",
      "episode: 144   score: 1.0   memory length: 25901   epsilon: 1.0    steps: 164     evaluation reward: 1.27\n",
      "episode: 145   score: 0.0   memory length: 26033   epsilon: 1.0    steps: 132     evaluation reward: 1.25\n",
      "episode: 146   score: 0.0   memory length: 26159   epsilon: 1.0    steps: 126     evaluation reward: 1.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 147   score: 1.0   memory length: 26315   epsilon: 1.0    steps: 156     evaluation reward: 1.24\n",
      "episode: 148   score: 2.0   memory length: 26518   epsilon: 1.0    steps: 203     evaluation reward: 1.22\n",
      "episode: 149   score: 1.0   memory length: 26695   epsilon: 1.0    steps: 177     evaluation reward: 1.19\n",
      "episode: 150   score: 0.0   memory length: 26825   epsilon: 1.0    steps: 130     evaluation reward: 1.18\n",
      "episode: 151   score: 1.0   memory length: 26985   epsilon: 1.0    steps: 160     evaluation reward: 1.18\n",
      "episode: 152   score: 1.0   memory length: 27139   epsilon: 1.0    steps: 154     evaluation reward: 1.19\n",
      "episode: 153   score: 1.0   memory length: 27322   epsilon: 1.0    steps: 183     evaluation reward: 1.18\n",
      "episode: 154   score: 1.0   memory length: 27499   epsilon: 1.0    steps: 177     evaluation reward: 1.16\n",
      "episode: 155   score: 2.0   memory length: 27686   epsilon: 1.0    steps: 187     evaluation reward: 1.17\n",
      "episode: 156   score: 1.0   memory length: 27861   epsilon: 1.0    steps: 175     evaluation reward: 1.16\n",
      "episode: 157   score: 0.0   memory length: 27991   epsilon: 1.0    steps: 130     evaluation reward: 1.16\n",
      "episode: 158   score: 1.0   memory length: 28154   epsilon: 1.0    steps: 163     evaluation reward: 1.16\n",
      "episode: 159   score: 3.0   memory length: 28382   epsilon: 1.0    steps: 228     evaluation reward: 1.18\n",
      "episode: 160   score: 1.0   memory length: 28557   epsilon: 1.0    steps: 175     evaluation reward: 1.18\n",
      "episode: 161   score: 0.0   memory length: 28684   epsilon: 1.0    steps: 127     evaluation reward: 1.17\n",
      "episode: 162   score: 1.0   memory length: 28846   epsilon: 1.0    steps: 162     evaluation reward: 1.18\n",
      "episode: 163   score: 3.0   memory length: 29101   epsilon: 1.0    steps: 255     evaluation reward: 1.13\n",
      "episode: 164   score: 0.0   memory length: 29237   epsilon: 1.0    steps: 136     evaluation reward: 1.13\n",
      "episode: 165   score: 0.0   memory length: 29365   epsilon: 1.0    steps: 128     evaluation reward: 1.12\n",
      "episode: 166   score: 3.0   memory length: 29594   epsilon: 1.0    steps: 229     evaluation reward: 1.13\n",
      "episode: 167   score: 0.0   memory length: 29726   epsilon: 1.0    steps: 132     evaluation reward: 1.12\n",
      "episode: 168   score: 1.0   memory length: 29911   epsilon: 1.0    steps: 185     evaluation reward: 1.11\n",
      "episode: 169   score: 0.0   memory length: 30037   epsilon: 1.0    steps: 126     evaluation reward: 1.06\n",
      "episode: 170   score: 0.0   memory length: 30160   epsilon: 1.0    steps: 123     evaluation reward: 1.04\n",
      "episode: 171   score: 0.0   memory length: 30296   epsilon: 1.0    steps: 136     evaluation reward: 1.04\n",
      "episode: 172   score: 0.0   memory length: 30429   epsilon: 1.0    steps: 133     evaluation reward: 1.04\n",
      "episode: 173   score: 1.0   memory length: 30599   epsilon: 1.0    steps: 170     evaluation reward: 1.03\n",
      "episode: 174   score: 1.0   memory length: 30777   epsilon: 1.0    steps: 178     evaluation reward: 1.03\n",
      "episode: 175   score: 0.0   memory length: 30915   epsilon: 1.0    steps: 138     evaluation reward: 1.02\n",
      "episode: 176   score: 4.0   memory length: 31203   epsilon: 1.0    steps: 288     evaluation reward: 1.06\n",
      "episode: 177   score: 0.0   memory length: 31327   epsilon: 1.0    steps: 124     evaluation reward: 1.04\n",
      "episode: 178   score: 0.0   memory length: 31465   epsilon: 1.0    steps: 138     evaluation reward: 1.03\n",
      "episode: 179   score: 3.0   memory length: 31693   epsilon: 1.0    steps: 228     evaluation reward: 1.06\n",
      "episode: 180   score: 1.0   memory length: 31874   epsilon: 1.0    steps: 181     evaluation reward: 1.05\n",
      "episode: 181   score: 1.0   memory length: 32034   epsilon: 1.0    steps: 160     evaluation reward: 1.03\n",
      "episode: 182   score: 0.0   memory length: 32169   epsilon: 1.0    steps: 135     evaluation reward: 1.02\n",
      "episode: 183   score: 0.0   memory length: 32297   epsilon: 1.0    steps: 128     evaluation reward: 1.01\n",
      "episode: 184   score: 1.0   memory length: 32465   epsilon: 1.0    steps: 168     evaluation reward: 1.0\n",
      "episode: 185   score: 0.0   memory length: 32596   epsilon: 1.0    steps: 131     evaluation reward: 1.0\n",
      "episode: 186   score: 5.0   memory length: 32907   epsilon: 1.0    steps: 311     evaluation reward: 1.05\n",
      "episode: 187   score: 3.0   memory length: 33143   epsilon: 1.0    steps: 236     evaluation reward: 1.07\n",
      "episode: 188   score: 0.0   memory length: 33268   epsilon: 1.0    steps: 125     evaluation reward: 1.07\n",
      "episode: 189   score: 0.0   memory length: 33403   epsilon: 1.0    steps: 135     evaluation reward: 1.07\n",
      "episode: 190   score: 0.0   memory length: 33538   epsilon: 1.0    steps: 135     evaluation reward: 1.05\n",
      "episode: 191   score: 0.0   memory length: 33668   epsilon: 1.0    steps: 130     evaluation reward: 1.05\n",
      "episode: 192   score: 0.0   memory length: 33795   epsilon: 1.0    steps: 127     evaluation reward: 1.04\n",
      "episode: 193   score: 2.0   memory length: 34025   epsilon: 1.0    steps: 230     evaluation reward: 1.04\n",
      "episode: 194   score: 1.0   memory length: 34205   epsilon: 1.0    steps: 180     evaluation reward: 1.03\n",
      "episode: 195   score: 0.0   memory length: 34330   epsilon: 1.0    steps: 125     evaluation reward: 1.02\n",
      "episode: 196   score: 0.0   memory length: 34462   epsilon: 1.0    steps: 132     evaluation reward: 0.99\n",
      "episode: 197   score: 1.0   memory length: 34624   epsilon: 1.0    steps: 162     evaluation reward: 0.98\n",
      "episode: 198   score: 1.0   memory length: 34780   epsilon: 1.0    steps: 156     evaluation reward: 0.95\n",
      "episode: 199   score: 0.0   memory length: 34907   epsilon: 1.0    steps: 127     evaluation reward: 0.94\n",
      "episode: 200   score: 0.0   memory length: 35038   epsilon: 1.0    steps: 131     evaluation reward: 0.93\n",
      "episode: 201   score: 1.0   memory length: 35207   epsilon: 1.0    steps: 169     evaluation reward: 0.94\n",
      "episode: 202   score: 0.0   memory length: 35355   epsilon: 1.0    steps: 148     evaluation reward: 0.94\n",
      "episode: 203   score: 2.0   memory length: 35556   epsilon: 1.0    steps: 201     evaluation reward: 0.91\n",
      "episode: 204   score: 0.0   memory length: 35694   epsilon: 1.0    steps: 138     evaluation reward: 0.91\n",
      "episode: 205   score: 1.0   memory length: 35870   epsilon: 1.0    steps: 176     evaluation reward: 0.9\n",
      "episode: 206   score: 0.0   memory length: 36002   epsilon: 1.0    steps: 132     evaluation reward: 0.9\n",
      "episode: 207   score: 0.0   memory length: 36128   epsilon: 1.0    steps: 126     evaluation reward: 0.9\n",
      "episode: 208   score: 1.0   memory length: 36304   epsilon: 1.0    steps: 176     evaluation reward: 0.89\n",
      "episode: 209   score: 4.0   memory length: 36611   epsilon: 1.0    steps: 307     evaluation reward: 0.92\n",
      "episode: 210   score: 0.0   memory length: 36736   epsilon: 1.0    steps: 125     evaluation reward: 0.89\n",
      "episode: 211   score: 3.0   memory length: 36993   epsilon: 1.0    steps: 257     evaluation reward: 0.92\n",
      "episode: 212   score: 2.0   memory length: 37195   epsilon: 1.0    steps: 202     evaluation reward: 0.91\n",
      "episode: 213   score: 0.0   memory length: 37330   epsilon: 1.0    steps: 135     evaluation reward: 0.89\n",
      "episode: 214   score: 2.0   memory length: 37539   epsilon: 1.0    steps: 209     evaluation reward: 0.9\n",
      "episode: 215   score: 2.0   memory length: 37721   epsilon: 1.0    steps: 182     evaluation reward: 0.92\n",
      "episode: 216   score: 0.0   memory length: 37844   epsilon: 1.0    steps: 123     evaluation reward: 0.9\n",
      "episode: 217   score: 0.0   memory length: 37970   epsilon: 1.0    steps: 126     evaluation reward: 0.88\n",
      "episode: 218   score: 0.0   memory length: 38098   epsilon: 1.0    steps: 128     evaluation reward: 0.87\n",
      "episode: 219   score: 3.0   memory length: 38351   epsilon: 1.0    steps: 253     evaluation reward: 0.88\n",
      "episode: 220   score: 1.0   memory length: 38510   epsilon: 1.0    steps: 159     evaluation reward: 0.89\n",
      "episode: 221   score: 0.0   memory length: 38633   epsilon: 1.0    steps: 123     evaluation reward: 0.87\n",
      "episode: 222   score: 1.0   memory length: 38803   epsilon: 1.0    steps: 170     evaluation reward: 0.88\n",
      "episode: 223   score: 0.0   memory length: 38926   epsilon: 1.0    steps: 123     evaluation reward: 0.88\n",
      "episode: 224   score: 0.0   memory length: 39065   epsilon: 1.0    steps: 139     evaluation reward: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 225   score: 2.0   memory length: 39250   epsilon: 1.0    steps: 185     evaluation reward: 0.87\n",
      "episode: 226   score: 0.0   memory length: 39376   epsilon: 1.0    steps: 126     evaluation reward: 0.87\n",
      "episode: 227   score: 1.0   memory length: 39558   epsilon: 1.0    steps: 182     evaluation reward: 0.88\n",
      "episode: 228   score: 0.0   memory length: 39680   epsilon: 1.0    steps: 122     evaluation reward: 0.86\n",
      "episode: 229   score: 0.0   memory length: 39822   epsilon: 1.0    steps: 142     evaluation reward: 0.86\n",
      "episode: 230   score: 1.0   memory length: 39995   epsilon: 1.0    steps: 173     evaluation reward: 0.86\n",
      "episode: 231   score: 1.0   memory length: 40152   epsilon: 1.0    steps: 157     evaluation reward: 0.87\n",
      "episode: 232   score: 3.0   memory length: 40382   epsilon: 1.0    steps: 230     evaluation reward: 0.89\n",
      "episode: 233   score: 2.0   memory length: 40570   epsilon: 1.0    steps: 188     evaluation reward: 0.9\n",
      "episode: 234   score: 3.0   memory length: 40817   epsilon: 1.0    steps: 247     evaluation reward: 0.93\n",
      "episode: 235   score: 1.0   memory length: 40989   epsilon: 1.0    steps: 172     evaluation reward: 0.93\n",
      "episode: 236   score: 0.0   memory length: 41116   epsilon: 1.0    steps: 127     evaluation reward: 0.93\n",
      "episode: 237   score: 1.0   memory length: 41282   epsilon: 1.0    steps: 166     evaluation reward: 0.92\n",
      "episode: 238   score: 1.0   memory length: 41443   epsilon: 1.0    steps: 161     evaluation reward: 0.93\n",
      "episode: 239   score: 2.0   memory length: 41641   epsilon: 1.0    steps: 198     evaluation reward: 0.94\n",
      "episode: 240   score: 2.0   memory length: 41850   epsilon: 1.0    steps: 209     evaluation reward: 0.96\n",
      "episode: 241   score: 2.0   memory length: 42092   epsilon: 1.0    steps: 242     evaluation reward: 0.96\n",
      "episode: 242   score: 0.0   memory length: 42223   epsilon: 1.0    steps: 131     evaluation reward: 0.95\n",
      "episode: 243   score: 2.0   memory length: 42432   epsilon: 1.0    steps: 209     evaluation reward: 0.97\n",
      "episode: 244   score: 0.0   memory length: 42565   epsilon: 1.0    steps: 133     evaluation reward: 0.96\n",
      "episode: 245   score: 1.0   memory length: 42751   epsilon: 1.0    steps: 186     evaluation reward: 0.97\n",
      "episode: 246   score: 0.0   memory length: 42888   epsilon: 1.0    steps: 137     evaluation reward: 0.97\n",
      "episode: 247   score: 1.0   memory length: 43057   epsilon: 1.0    steps: 169     evaluation reward: 0.97\n",
      "episode: 248   score: 0.0   memory length: 43197   epsilon: 1.0    steps: 140     evaluation reward: 0.95\n",
      "episode: 249   score: 1.0   memory length: 43353   epsilon: 1.0    steps: 156     evaluation reward: 0.95\n",
      "episode: 250   score: 3.0   memory length: 43624   epsilon: 1.0    steps: 271     evaluation reward: 0.98\n",
      "episode: 251   score: 2.0   memory length: 43822   epsilon: 1.0    steps: 198     evaluation reward: 0.99\n",
      "episode: 252   score: 1.0   memory length: 44007   epsilon: 1.0    steps: 185     evaluation reward: 0.99\n",
      "episode: 253   score: 0.0   memory length: 44135   epsilon: 1.0    steps: 128     evaluation reward: 0.98\n",
      "episode: 254   score: 0.0   memory length: 44263   epsilon: 1.0    steps: 128     evaluation reward: 0.97\n",
      "episode: 255   score: 2.0   memory length: 44452   epsilon: 1.0    steps: 189     evaluation reward: 0.97\n",
      "episode: 256   score: 0.0   memory length: 44581   epsilon: 1.0    steps: 129     evaluation reward: 0.96\n",
      "episode: 257   score: 5.0   memory length: 44918   epsilon: 1.0    steps: 337     evaluation reward: 1.01\n",
      "episode: 258   score: 2.0   memory length: 45102   epsilon: 1.0    steps: 184     evaluation reward: 1.02\n",
      "episode: 259   score: 1.0   memory length: 45284   epsilon: 1.0    steps: 182     evaluation reward: 1.0\n",
      "episode: 260   score: 0.0   memory length: 45415   epsilon: 1.0    steps: 131     evaluation reward: 0.99\n",
      "episode: 261   score: 1.0   memory length: 45588   epsilon: 1.0    steps: 173     evaluation reward: 1.0\n",
      "episode: 262   score: 0.0   memory length: 45720   epsilon: 1.0    steps: 132     evaluation reward: 0.99\n",
      "episode: 263   score: 0.0   memory length: 45857   epsilon: 1.0    steps: 137     evaluation reward: 0.96\n",
      "episode: 264   score: 1.0   memory length: 46029   epsilon: 1.0    steps: 172     evaluation reward: 0.97\n",
      "episode: 265   score: 2.0   memory length: 46227   epsilon: 1.0    steps: 198     evaluation reward: 0.99\n",
      "episode: 266   score: 2.0   memory length: 46431   epsilon: 1.0    steps: 204     evaluation reward: 0.98\n",
      "episode: 267   score: 1.0   memory length: 46592   epsilon: 1.0    steps: 161     evaluation reward: 0.99\n",
      "episode: 268   score: 0.0   memory length: 46720   epsilon: 1.0    steps: 128     evaluation reward: 0.98\n",
      "episode: 269   score: 1.0   memory length: 46880   epsilon: 1.0    steps: 160     evaluation reward: 0.99\n",
      "episode: 270   score: 1.0   memory length: 47050   epsilon: 1.0    steps: 170     evaluation reward: 1.0\n",
      "episode: 271   score: 2.0   memory length: 47236   epsilon: 1.0    steps: 186     evaluation reward: 1.02\n",
      "episode: 272   score: 1.0   memory length: 47412   epsilon: 1.0    steps: 176     evaluation reward: 1.03\n",
      "episode: 273   score: 1.0   memory length: 47565   epsilon: 1.0    steps: 153     evaluation reward: 1.03\n",
      "episode: 274   score: 0.0   memory length: 47689   epsilon: 1.0    steps: 124     evaluation reward: 1.02\n",
      "episode: 275   score: 0.0   memory length: 47825   epsilon: 1.0    steps: 136     evaluation reward: 1.02\n",
      "episode: 276   score: 0.0   memory length: 47966   epsilon: 1.0    steps: 141     evaluation reward: 0.98\n",
      "episode: 277   score: 1.0   memory length: 48128   epsilon: 1.0    steps: 162     evaluation reward: 0.99\n",
      "episode: 278   score: 1.0   memory length: 48300   epsilon: 1.0    steps: 172     evaluation reward: 1.0\n",
      "episode: 279   score: 1.0   memory length: 48486   epsilon: 1.0    steps: 186     evaluation reward: 0.98\n",
      "episode: 280   score: 3.0   memory length: 48720   epsilon: 1.0    steps: 234     evaluation reward: 1.0\n",
      "episode: 281   score: 0.0   memory length: 48847   epsilon: 1.0    steps: 127     evaluation reward: 0.99\n",
      "episode: 282   score: 2.0   memory length: 49049   epsilon: 1.0    steps: 202     evaluation reward: 1.01\n",
      "episode: 283   score: 2.0   memory length: 49276   epsilon: 1.0    steps: 227     evaluation reward: 1.03\n",
      "episode: 284   score: 0.0   memory length: 49414   epsilon: 1.0    steps: 138     evaluation reward: 1.02\n",
      "episode: 285   score: 0.0   memory length: 49544   epsilon: 1.0    steps: 130     evaluation reward: 1.02\n",
      "episode: 286   score: 0.0   memory length: 49670   epsilon: 1.0    steps: 126     evaluation reward: 0.97\n",
      "episode: 287   score: 1.0   memory length: 49852   epsilon: 1.0    steps: 182     evaluation reward: 0.95\n",
      "now time :  2018-12-12 10:23:26.194933\n",
      "episode: 288   score: 1.0   memory length: 50007   epsilon: 1.0    steps: 155     evaluation reward: 0.96\n",
      "episode: 289   score: 1.0   memory length: 50168   epsilon: 1.0    steps: 161     evaluation reward: 0.97\n",
      "episode: 290   score: 2.0   memory length: 50388   epsilon: 1.0    steps: 220     evaluation reward: 0.99\n",
      "episode: 291   score: 1.0   memory length: 50571   epsilon: 1.0    steps: 183     evaluation reward: 1.0\n",
      "episode: 292   score: 0.0   memory length: 50720   epsilon: 1.0    steps: 149     evaluation reward: 1.0\n",
      "episode: 293   score: 1.0   memory length: 50897   epsilon: 1.0    steps: 177     evaluation reward: 0.99\n",
      "episode: 294   score: 1.0   memory length: 51063   epsilon: 1.0    steps: 166     evaluation reward: 0.99\n",
      "episode: 295   score: 1.0   memory length: 51226   epsilon: 1.0    steps: 163     evaluation reward: 1.0\n",
      "episode: 296   score: 1.0   memory length: 51413   epsilon: 1.0    steps: 187     evaluation reward: 1.01\n",
      "episode: 297   score: 1.0   memory length: 51597   epsilon: 1.0    steps: 184     evaluation reward: 1.01\n",
      "episode: 298   score: 0.0   memory length: 51728   epsilon: 1.0    steps: 131     evaluation reward: 1.0\n",
      "episode: 299   score: 3.0   memory length: 51992   epsilon: 1.0    steps: 264     evaluation reward: 1.03\n",
      "episode: 300   score: 0.0   memory length: 52150   epsilon: 1.0    steps: 158     evaluation reward: 1.03\n",
      "episode: 301   score: 1.0   memory length: 52306   epsilon: 1.0    steps: 156     evaluation reward: 1.03\n",
      "episode: 302   score: 3.0   memory length: 52564   epsilon: 1.0    steps: 258     evaluation reward: 1.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 303   score: 4.0   memory length: 52873   epsilon: 1.0    steps: 309     evaluation reward: 1.08\n",
      "episode: 304   score: 3.0   memory length: 53127   epsilon: 1.0    steps: 254     evaluation reward: 1.11\n",
      "episode: 305   score: 2.0   memory length: 53329   epsilon: 1.0    steps: 202     evaluation reward: 1.12\n",
      "episode: 306   score: 0.0   memory length: 53468   epsilon: 1.0    steps: 139     evaluation reward: 1.12\n",
      "episode: 307   score: 4.0   memory length: 53762   epsilon: 1.0    steps: 294     evaluation reward: 1.16\n",
      "episode: 308   score: 0.0   memory length: 53900   epsilon: 1.0    steps: 138     evaluation reward: 1.15\n",
      "episode: 309   score: 1.0   memory length: 54069   epsilon: 1.0    steps: 169     evaluation reward: 1.12\n",
      "episode: 310   score: 1.0   memory length: 54256   epsilon: 1.0    steps: 187     evaluation reward: 1.13\n",
      "episode: 311   score: 1.0   memory length: 54434   epsilon: 1.0    steps: 178     evaluation reward: 1.11\n",
      "episode: 312   score: 1.0   memory length: 54592   epsilon: 1.0    steps: 158     evaluation reward: 1.1\n",
      "episode: 313   score: 1.0   memory length: 54771   epsilon: 1.0    steps: 179     evaluation reward: 1.11\n",
      "episode: 314   score: 2.0   memory length: 55006   epsilon: 1.0    steps: 235     evaluation reward: 1.11\n",
      "episode: 315   score: 0.0   memory length: 55138   epsilon: 1.0    steps: 132     evaluation reward: 1.09\n",
      "episode: 316   score: 3.0   memory length: 55406   epsilon: 1.0    steps: 268     evaluation reward: 1.12\n",
      "episode: 317   score: 0.0   memory length: 55532   epsilon: 1.0    steps: 126     evaluation reward: 1.12\n",
      "episode: 318   score: 2.0   memory length: 55735   epsilon: 1.0    steps: 203     evaluation reward: 1.14\n",
      "episode: 319   score: 1.0   memory length: 55907   epsilon: 1.0    steps: 172     evaluation reward: 1.12\n",
      "episode: 320   score: 1.0   memory length: 56066   epsilon: 1.0    steps: 159     evaluation reward: 1.12\n",
      "episode: 321   score: 3.0   memory length: 56304   epsilon: 1.0    steps: 238     evaluation reward: 1.15\n",
      "episode: 322   score: 2.0   memory length: 56511   epsilon: 1.0    steps: 207     evaluation reward: 1.16\n",
      "episode: 323   score: 1.0   memory length: 56691   epsilon: 1.0    steps: 180     evaluation reward: 1.17\n",
      "episode: 324   score: 4.0   memory length: 57018   epsilon: 1.0    steps: 327     evaluation reward: 1.21\n",
      "episode: 325   score: 2.0   memory length: 57220   epsilon: 1.0    steps: 202     evaluation reward: 1.21\n",
      "episode: 326   score: 1.0   memory length: 57394   epsilon: 1.0    steps: 174     evaluation reward: 1.22\n",
      "episode: 327   score: 2.0   memory length: 57606   epsilon: 1.0    steps: 212     evaluation reward: 1.23\n",
      "episode: 328   score: 0.0   memory length: 57736   epsilon: 1.0    steps: 130     evaluation reward: 1.23\n",
      "episode: 329   score: 2.0   memory length: 57950   epsilon: 1.0    steps: 214     evaluation reward: 1.25\n",
      "episode: 330   score: 4.0   memory length: 58219   epsilon: 1.0    steps: 269     evaluation reward: 1.28\n",
      "episode: 331   score: 2.0   memory length: 58423   epsilon: 1.0    steps: 204     evaluation reward: 1.29\n",
      "episode: 332   score: 3.0   memory length: 58688   epsilon: 1.0    steps: 265     evaluation reward: 1.29\n",
      "episode: 333   score: 2.0   memory length: 58905   epsilon: 1.0    steps: 217     evaluation reward: 1.29\n",
      "episode: 334   score: 3.0   memory length: 59167   epsilon: 1.0    steps: 262     evaluation reward: 1.29\n",
      "episode: 335   score: 1.0   memory length: 59322   epsilon: 1.0    steps: 155     evaluation reward: 1.29\n",
      "episode: 336   score: 2.0   memory length: 59543   epsilon: 1.0    steps: 221     evaluation reward: 1.31\n",
      "episode: 337   score: 2.0   memory length: 59762   epsilon: 1.0    steps: 219     evaluation reward: 1.32\n",
      "episode: 338   score: 1.0   memory length: 59938   epsilon: 1.0    steps: 176     evaluation reward: 1.32\n",
      "episode: 339   score: 0.0   memory length: 60070   epsilon: 1.0    steps: 132     evaluation reward: 1.3\n",
      "episode: 340   score: 4.0   memory length: 60339   epsilon: 1.0    steps: 269     evaluation reward: 1.32\n",
      "episode: 341   score: 0.0   memory length: 60466   epsilon: 1.0    steps: 127     evaluation reward: 1.3\n",
      "episode: 342   score: 0.0   memory length: 60605   epsilon: 1.0    steps: 139     evaluation reward: 1.3\n",
      "episode: 343   score: 2.0   memory length: 60828   epsilon: 1.0    steps: 223     evaluation reward: 1.3\n",
      "episode: 344   score: 2.0   memory length: 61040   epsilon: 1.0    steps: 212     evaluation reward: 1.32\n",
      "episode: 345   score: 1.0   memory length: 61207   epsilon: 1.0    steps: 167     evaluation reward: 1.32\n",
      "episode: 346   score: 2.0   memory length: 61409   epsilon: 1.0    steps: 202     evaluation reward: 1.34\n",
      "episode: 347   score: 0.0   memory length: 61532   epsilon: 1.0    steps: 123     evaluation reward: 1.33\n",
      "episode: 348   score: 0.0   memory length: 61666   epsilon: 1.0    steps: 134     evaluation reward: 1.33\n",
      "episode: 349   score: 1.0   memory length: 61838   epsilon: 1.0    steps: 172     evaluation reward: 1.33\n",
      "episode: 350   score: 0.0   memory length: 61969   epsilon: 1.0    steps: 131     evaluation reward: 1.3\n",
      "episode: 351   score: 1.0   memory length: 62145   epsilon: 1.0    steps: 176     evaluation reward: 1.29\n",
      "episode: 352   score: 2.0   memory length: 62355   epsilon: 1.0    steps: 210     evaluation reward: 1.3\n",
      "episode: 353   score: 1.0   memory length: 62529   epsilon: 1.0    steps: 174     evaluation reward: 1.31\n",
      "episode: 354   score: 0.0   memory length: 62654   epsilon: 1.0    steps: 125     evaluation reward: 1.31\n",
      "episode: 355   score: 1.0   memory length: 62830   epsilon: 1.0    steps: 176     evaluation reward: 1.3\n",
      "episode: 356   score: 2.0   memory length: 63032   epsilon: 1.0    steps: 202     evaluation reward: 1.32\n",
      "episode: 357   score: 2.0   memory length: 63234   epsilon: 1.0    steps: 202     evaluation reward: 1.29\n",
      "episode: 358   score: 1.0   memory length: 63393   epsilon: 1.0    steps: 159     evaluation reward: 1.28\n",
      "episode: 359   score: 1.0   memory length: 63568   epsilon: 1.0    steps: 175     evaluation reward: 1.28\n",
      "episode: 360   score: 1.0   memory length: 63725   epsilon: 1.0    steps: 157     evaluation reward: 1.29\n",
      "episode: 361   score: 0.0   memory length: 63863   epsilon: 1.0    steps: 138     evaluation reward: 1.28\n",
      "episode: 362   score: 3.0   memory length: 64107   epsilon: 1.0    steps: 244     evaluation reward: 1.31\n",
      "episode: 363   score: 0.0   memory length: 64238   epsilon: 1.0    steps: 131     evaluation reward: 1.31\n",
      "episode: 364   score: 3.0   memory length: 64466   epsilon: 1.0    steps: 228     evaluation reward: 1.33\n",
      "episode: 365   score: 0.0   memory length: 64604   epsilon: 1.0    steps: 138     evaluation reward: 1.31\n",
      "episode: 366   score: 2.0   memory length: 64806   epsilon: 1.0    steps: 202     evaluation reward: 1.31\n",
      "episode: 367   score: 3.0   memory length: 65059   epsilon: 1.0    steps: 253     evaluation reward: 1.33\n",
      "episode: 368   score: 4.0   memory length: 65369   epsilon: 1.0    steps: 310     evaluation reward: 1.37\n",
      "episode: 369   score: 2.0   memory length: 65553   epsilon: 1.0    steps: 184     evaluation reward: 1.38\n",
      "episode: 370   score: 3.0   memory length: 65792   epsilon: 1.0    steps: 239     evaluation reward: 1.4\n",
      "episode: 371   score: 2.0   memory length: 66001   epsilon: 1.0    steps: 209     evaluation reward: 1.4\n",
      "episode: 372   score: 1.0   memory length: 66164   epsilon: 1.0    steps: 163     evaluation reward: 1.4\n",
      "episode: 373   score: 0.0   memory length: 66291   epsilon: 1.0    steps: 127     evaluation reward: 1.39\n",
      "episode: 374   score: 1.0   memory length: 66449   epsilon: 1.0    steps: 158     evaluation reward: 1.4\n",
      "episode: 375   score: 0.0   memory length: 66583   epsilon: 1.0    steps: 134     evaluation reward: 1.4\n",
      "episode: 376   score: 2.0   memory length: 66782   epsilon: 1.0    steps: 199     evaluation reward: 1.42\n",
      "episode: 377   score: 1.0   memory length: 66957   epsilon: 1.0    steps: 175     evaluation reward: 1.42\n",
      "episode: 378   score: 1.0   memory length: 67117   epsilon: 1.0    steps: 160     evaluation reward: 1.42\n",
      "episode: 379   score: 2.0   memory length: 67324   epsilon: 1.0    steps: 207     evaluation reward: 1.43\n",
      "episode: 380   score: 1.0   memory length: 67478   epsilon: 1.0    steps: 154     evaluation reward: 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 381   score: 1.0   memory length: 67648   epsilon: 1.0    steps: 170     evaluation reward: 1.42\n",
      "episode: 382   score: 2.0   memory length: 67857   epsilon: 1.0    steps: 209     evaluation reward: 1.42\n",
      "episode: 383   score: 1.0   memory length: 68027   epsilon: 1.0    steps: 170     evaluation reward: 1.41\n",
      "episode: 384   score: 1.0   memory length: 68179   epsilon: 1.0    steps: 152     evaluation reward: 1.42\n",
      "episode: 385   score: 3.0   memory length: 68434   epsilon: 1.0    steps: 255     evaluation reward: 1.45\n",
      "episode: 386   score: 0.0   memory length: 68567   epsilon: 1.0    steps: 133     evaluation reward: 1.45\n",
      "episode: 387   score: 0.0   memory length: 68700   epsilon: 1.0    steps: 133     evaluation reward: 1.44\n",
      "episode: 388   score: 3.0   memory length: 68933   epsilon: 1.0    steps: 233     evaluation reward: 1.46\n",
      "episode: 389   score: 1.0   memory length: 69107   epsilon: 1.0    steps: 174     evaluation reward: 1.46\n",
      "episode: 390   score: 3.0   memory length: 69355   epsilon: 1.0    steps: 248     evaluation reward: 1.47\n",
      "episode: 391   score: 0.0   memory length: 69485   epsilon: 1.0    steps: 130     evaluation reward: 1.46\n",
      "episode: 392   score: 0.0   memory length: 69616   epsilon: 1.0    steps: 131     evaluation reward: 1.46\n",
      "episode: 393   score: 1.0   memory length: 69776   epsilon: 1.0    steps: 160     evaluation reward: 1.46\n",
      "episode: 394   score: 2.0   memory length: 69974   epsilon: 1.0    steps: 198     evaluation reward: 1.47\n",
      "episode: 395   score: 2.0   memory length: 70194   epsilon: 1.0    steps: 220     evaluation reward: 1.48\n",
      "episode: 396   score: 5.0   memory length: 70510   epsilon: 1.0    steps: 316     evaluation reward: 1.52\n",
      "episode: 397   score: 0.0   memory length: 70640   epsilon: 1.0    steps: 130     evaluation reward: 1.51\n",
      "episode: 398   score: 0.0   memory length: 70781   epsilon: 1.0    steps: 141     evaluation reward: 1.51\n",
      "episode: 399   score: 3.0   memory length: 71018   epsilon: 1.0    steps: 237     evaluation reward: 1.51\n",
      "episode: 400   score: 3.0   memory length: 71241   epsilon: 1.0    steps: 223     evaluation reward: 1.54\n",
      "episode: 401   score: 2.0   memory length: 71445   epsilon: 1.0    steps: 204     evaluation reward: 1.55\n",
      "episode: 402   score: 0.0   memory length: 71580   epsilon: 1.0    steps: 135     evaluation reward: 1.52\n",
      "episode: 403   score: 1.0   memory length: 71755   epsilon: 1.0    steps: 175     evaluation reward: 1.49\n",
      "episode: 404   score: 0.0   memory length: 71879   epsilon: 1.0    steps: 124     evaluation reward: 1.46\n",
      "episode: 405   score: 1.0   memory length: 72056   epsilon: 1.0    steps: 177     evaluation reward: 1.45\n",
      "episode: 406   score: 3.0   memory length: 72331   epsilon: 1.0    steps: 275     evaluation reward: 1.48\n",
      "episode: 407   score: 1.0   memory length: 72485   epsilon: 1.0    steps: 154     evaluation reward: 1.45\n",
      "episode: 408   score: 0.0   memory length: 72619   epsilon: 1.0    steps: 134     evaluation reward: 1.45\n",
      "episode: 409   score: 0.0   memory length: 72752   epsilon: 1.0    steps: 133     evaluation reward: 1.44\n",
      "episode: 410   score: 3.0   memory length: 73015   epsilon: 1.0    steps: 263     evaluation reward: 1.46\n",
      "episode: 411   score: 3.0   memory length: 73262   epsilon: 1.0    steps: 247     evaluation reward: 1.48\n",
      "episode: 412   score: 2.0   memory length: 73455   epsilon: 1.0    steps: 193     evaluation reward: 1.49\n",
      "episode: 413   score: 0.0   memory length: 73581   epsilon: 1.0    steps: 126     evaluation reward: 1.48\n",
      "episode: 414   score: 0.0   memory length: 73714   epsilon: 1.0    steps: 133     evaluation reward: 1.46\n",
      "episode: 415   score: 3.0   memory length: 73948   epsilon: 1.0    steps: 234     evaluation reward: 1.49\n",
      "episode: 416   score: 0.0   memory length: 74088   epsilon: 1.0    steps: 140     evaluation reward: 1.46\n",
      "episode: 417   score: 2.0   memory length: 74296   epsilon: 1.0    steps: 208     evaluation reward: 1.48\n",
      "episode: 418   score: 1.0   memory length: 74453   epsilon: 1.0    steps: 157     evaluation reward: 1.47\n",
      "episode: 419   score: 1.0   memory length: 74626   epsilon: 1.0    steps: 173     evaluation reward: 1.47\n",
      "episode: 420   score: 0.0   memory length: 74755   epsilon: 1.0    steps: 129     evaluation reward: 1.46\n",
      "episode: 421   score: 4.0   memory length: 75060   epsilon: 1.0    steps: 305     evaluation reward: 1.47\n",
      "episode: 422   score: 3.0   memory length: 75288   epsilon: 1.0    steps: 228     evaluation reward: 1.48\n",
      "episode: 423   score: 1.0   memory length: 75458   epsilon: 1.0    steps: 170     evaluation reward: 1.48\n",
      "episode: 424   score: 1.0   memory length: 75628   epsilon: 1.0    steps: 170     evaluation reward: 1.45\n",
      "episode: 425   score: 1.0   memory length: 75781   epsilon: 1.0    steps: 153     evaluation reward: 1.44\n",
      "episode: 426   score: 0.0   memory length: 75909   epsilon: 1.0    steps: 128     evaluation reward: 1.43\n",
      "episode: 427   score: 0.0   memory length: 76040   epsilon: 1.0    steps: 131     evaluation reward: 1.41\n",
      "episode: 428   score: 1.0   memory length: 76198   epsilon: 1.0    steps: 158     evaluation reward: 1.42\n",
      "episode: 429   score: 0.0   memory length: 76325   epsilon: 1.0    steps: 127     evaluation reward: 1.4\n",
      "episode: 430   score: 0.0   memory length: 76454   epsilon: 1.0    steps: 129     evaluation reward: 1.36\n",
      "episode: 431   score: 0.0   memory length: 76584   epsilon: 1.0    steps: 130     evaluation reward: 1.34\n",
      "episode: 432   score: 2.0   memory length: 76788   epsilon: 1.0    steps: 204     evaluation reward: 1.33\n",
      "episode: 433   score: 2.0   memory length: 77021   epsilon: 1.0    steps: 233     evaluation reward: 1.33\n",
      "episode: 434   score: 2.0   memory length: 77227   epsilon: 1.0    steps: 206     evaluation reward: 1.32\n",
      "episode: 435   score: 0.0   memory length: 77360   epsilon: 1.0    steps: 133     evaluation reward: 1.31\n",
      "episode: 436   score: 0.0   memory length: 77496   epsilon: 1.0    steps: 136     evaluation reward: 1.29\n",
      "episode: 437   score: 2.0   memory length: 77685   epsilon: 1.0    steps: 189     evaluation reward: 1.29\n",
      "episode: 438   score: 1.0   memory length: 77860   epsilon: 1.0    steps: 175     evaluation reward: 1.29\n",
      "episode: 439   score: 0.0   memory length: 77994   epsilon: 1.0    steps: 134     evaluation reward: 1.29\n",
      "episode: 440   score: 2.0   memory length: 78199   epsilon: 1.0    steps: 205     evaluation reward: 1.27\n",
      "episode: 441   score: 0.0   memory length: 78341   epsilon: 1.0    steps: 142     evaluation reward: 1.27\n",
      "episode: 442   score: 0.0   memory length: 78472   epsilon: 1.0    steps: 131     evaluation reward: 1.27\n",
      "episode: 443   score: 3.0   memory length: 78707   epsilon: 1.0    steps: 235     evaluation reward: 1.28\n",
      "episode: 444   score: 4.0   memory length: 79032   epsilon: 1.0    steps: 325     evaluation reward: 1.3\n",
      "episode: 445   score: 2.0   memory length: 79240   epsilon: 1.0    steps: 208     evaluation reward: 1.31\n",
      "episode: 446   score: 3.0   memory length: 79468   epsilon: 1.0    steps: 228     evaluation reward: 1.32\n",
      "episode: 447   score: 0.0   memory length: 79611   epsilon: 1.0    steps: 143     evaluation reward: 1.32\n",
      "episode: 448   score: 1.0   memory length: 79794   epsilon: 1.0    steps: 183     evaluation reward: 1.33\n",
      "episode: 449   score: 0.0   memory length: 79924   epsilon: 1.0    steps: 130     evaluation reward: 1.32\n",
      "episode: 450   score: 1.0   memory length: 80085   epsilon: 1.0    steps: 161     evaluation reward: 1.33\n",
      "episode: 451   score: 1.0   memory length: 80239   epsilon: 1.0    steps: 154     evaluation reward: 1.33\n",
      "episode: 452   score: 3.0   memory length: 80521   epsilon: 1.0    steps: 282     evaluation reward: 1.34\n",
      "episode: 453   score: 2.0   memory length: 80726   epsilon: 1.0    steps: 205     evaluation reward: 1.35\n",
      "episode: 454   score: 0.0   memory length: 80854   epsilon: 1.0    steps: 128     evaluation reward: 1.35\n",
      "episode: 455   score: 1.0   memory length: 81040   epsilon: 1.0    steps: 186     evaluation reward: 1.35\n",
      "episode: 456   score: 0.0   memory length: 81168   epsilon: 1.0    steps: 128     evaluation reward: 1.33\n",
      "episode: 457   score: 3.0   memory length: 81418   epsilon: 1.0    steps: 250     evaluation reward: 1.34\n",
      "episode: 458   score: 0.0   memory length: 81548   epsilon: 1.0    steps: 130     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 459   score: 0.0   memory length: 81676   epsilon: 1.0    steps: 128     evaluation reward: 1.32\n",
      "episode: 460   score: 0.0   memory length: 81804   epsilon: 1.0    steps: 128     evaluation reward: 1.31\n",
      "episode: 461   score: 0.0   memory length: 81946   epsilon: 1.0    steps: 142     evaluation reward: 1.31\n",
      "episode: 462   score: 1.0   memory length: 82125   epsilon: 1.0    steps: 179     evaluation reward: 1.29\n",
      "episode: 463   score: 4.0   memory length: 82419   epsilon: 1.0    steps: 294     evaluation reward: 1.33\n",
      "episode: 464   score: 0.0   memory length: 82553   epsilon: 1.0    steps: 134     evaluation reward: 1.3\n",
      "episode: 465   score: 0.0   memory length: 82694   epsilon: 1.0    steps: 141     evaluation reward: 1.3\n",
      "episode: 466   score: 1.0   memory length: 82846   epsilon: 1.0    steps: 152     evaluation reward: 1.29\n",
      "episode: 467   score: 1.0   memory length: 83022   epsilon: 1.0    steps: 176     evaluation reward: 1.27\n",
      "episode: 468   score: 4.0   memory length: 83329   epsilon: 1.0    steps: 307     evaluation reward: 1.27\n",
      "episode: 469   score: 1.0   memory length: 83487   epsilon: 1.0    steps: 158     evaluation reward: 1.26\n",
      "episode: 470   score: 2.0   memory length: 83687   epsilon: 1.0    steps: 200     evaluation reward: 1.25\n",
      "episode: 471   score: 0.0   memory length: 83821   epsilon: 1.0    steps: 134     evaluation reward: 1.23\n",
      "episode: 472   score: 1.0   memory length: 83979   epsilon: 1.0    steps: 158     evaluation reward: 1.23\n",
      "episode: 473   score: 3.0   memory length: 84204   epsilon: 1.0    steps: 225     evaluation reward: 1.26\n",
      "episode: 474   score: 0.0   memory length: 84358   epsilon: 1.0    steps: 154     evaluation reward: 1.25\n",
      "episode: 475   score: 2.0   memory length: 84567   epsilon: 1.0    steps: 209     evaluation reward: 1.27\n",
      "episode: 476   score: 3.0   memory length: 84820   epsilon: 1.0    steps: 253     evaluation reward: 1.28\n",
      "episode: 477   score: 0.0   memory length: 84944   epsilon: 1.0    steps: 124     evaluation reward: 1.27\n",
      "episode: 478   score: 1.0   memory length: 85112   epsilon: 1.0    steps: 168     evaluation reward: 1.27\n",
      "episode: 479   score: 2.0   memory length: 85316   epsilon: 1.0    steps: 204     evaluation reward: 1.27\n",
      "episode: 480   score: 1.0   memory length: 85491   epsilon: 1.0    steps: 175     evaluation reward: 1.27\n",
      "episode: 481   score: 2.0   memory length: 85716   epsilon: 1.0    steps: 225     evaluation reward: 1.28\n",
      "episode: 482   score: 1.0   memory length: 85899   epsilon: 1.0    steps: 183     evaluation reward: 1.27\n",
      "episode: 483   score: 2.0   memory length: 86104   epsilon: 1.0    steps: 205     evaluation reward: 1.28\n",
      "episode: 484   score: 0.0   memory length: 86233   epsilon: 1.0    steps: 129     evaluation reward: 1.27\n",
      "episode: 485   score: 1.0   memory length: 86408   epsilon: 1.0    steps: 175     evaluation reward: 1.25\n",
      "episode: 486   score: 2.0   memory length: 86631   epsilon: 1.0    steps: 223     evaluation reward: 1.27\n",
      "episode: 487   score: 2.0   memory length: 86832   epsilon: 1.0    steps: 201     evaluation reward: 1.29\n",
      "episode: 488   score: 1.0   memory length: 87006   epsilon: 1.0    steps: 174     evaluation reward: 1.27\n",
      "episode: 489   score: 2.0   memory length: 87218   epsilon: 1.0    steps: 212     evaluation reward: 1.28\n",
      "episode: 490   score: 1.0   memory length: 87398   epsilon: 1.0    steps: 180     evaluation reward: 1.26\n",
      "episode: 491   score: 2.0   memory length: 87599   epsilon: 1.0    steps: 201     evaluation reward: 1.28\n",
      "episode: 492   score: 0.0   memory length: 87731   epsilon: 1.0    steps: 132     evaluation reward: 1.28\n",
      "episode: 493   score: 3.0   memory length: 87985   epsilon: 1.0    steps: 254     evaluation reward: 1.3\n",
      "episode: 494   score: 0.0   memory length: 88110   epsilon: 1.0    steps: 125     evaluation reward: 1.28\n",
      "episode: 495   score: 1.0   memory length: 88267   epsilon: 1.0    steps: 157     evaluation reward: 1.27\n",
      "episode: 496   score: 1.0   memory length: 88446   epsilon: 1.0    steps: 179     evaluation reward: 1.23\n",
      "episode: 497   score: 1.0   memory length: 88625   epsilon: 1.0    steps: 179     evaluation reward: 1.24\n",
      "episode: 498   score: 2.0   memory length: 88856   epsilon: 1.0    steps: 231     evaluation reward: 1.26\n",
      "episode: 499   score: 1.0   memory length: 89020   epsilon: 1.0    steps: 164     evaluation reward: 1.24\n",
      "episode: 500   score: 3.0   memory length: 89276   epsilon: 1.0    steps: 256     evaluation reward: 1.24\n",
      "episode: 501   score: 0.0   memory length: 89405   epsilon: 1.0    steps: 129     evaluation reward: 1.22\n",
      "episode: 502   score: 1.0   memory length: 89596   epsilon: 1.0    steps: 191     evaluation reward: 1.23\n",
      "episode: 503   score: 2.0   memory length: 89799   epsilon: 1.0    steps: 203     evaluation reward: 1.24\n",
      "episode: 504   score: 1.0   memory length: 89952   epsilon: 1.0    steps: 153     evaluation reward: 1.25\n",
      "episode: 505   score: 1.0   memory length: 90133   epsilon: 1.0    steps: 181     evaluation reward: 1.25\n",
      "episode: 506   score: 0.0   memory length: 90261   epsilon: 1.0    steps: 128     evaluation reward: 1.22\n",
      "episode: 507   score: 3.0   memory length: 90512   epsilon: 1.0    steps: 251     evaluation reward: 1.24\n",
      "episode: 508   score: 1.0   memory length: 90686   epsilon: 1.0    steps: 174     evaluation reward: 1.25\n",
      "episode: 509   score: 3.0   memory length: 90922   epsilon: 1.0    steps: 236     evaluation reward: 1.28\n",
      "episode: 510   score: 0.0   memory length: 91053   epsilon: 1.0    steps: 131     evaluation reward: 1.25\n",
      "episode: 511   score: 2.0   memory length: 91273   epsilon: 1.0    steps: 220     evaluation reward: 1.24\n",
      "episode: 512   score: 0.0   memory length: 91404   epsilon: 1.0    steps: 131     evaluation reward: 1.22\n",
      "episode: 513   score: 0.0   memory length: 91531   epsilon: 1.0    steps: 127     evaluation reward: 1.22\n",
      "episode: 514   score: 0.0   memory length: 91665   epsilon: 1.0    steps: 134     evaluation reward: 1.22\n",
      "episode: 515   score: 1.0   memory length: 91856   epsilon: 1.0    steps: 191     evaluation reward: 1.2\n",
      "episode: 516   score: 1.0   memory length: 92032   epsilon: 1.0    steps: 176     evaluation reward: 1.21\n",
      "episode: 517   score: 1.0   memory length: 92195   epsilon: 1.0    steps: 163     evaluation reward: 1.2\n",
      "episode: 518   score: 0.0   memory length: 92322   epsilon: 1.0    steps: 127     evaluation reward: 1.19\n",
      "episode: 519   score: 3.0   memory length: 92583   epsilon: 1.0    steps: 261     evaluation reward: 1.21\n",
      "episode: 520   score: 0.0   memory length: 92727   epsilon: 1.0    steps: 144     evaluation reward: 1.21\n",
      "episode: 521   score: 0.0   memory length: 92860   epsilon: 1.0    steps: 133     evaluation reward: 1.17\n",
      "episode: 522   score: 0.0   memory length: 92987   epsilon: 1.0    steps: 127     evaluation reward: 1.14\n",
      "episode: 523   score: 3.0   memory length: 93228   epsilon: 1.0    steps: 241     evaluation reward: 1.16\n",
      "episode: 524   score: 3.0   memory length: 93480   epsilon: 1.0    steps: 252     evaluation reward: 1.18\n",
      "episode: 525   score: 1.0   memory length: 93641   epsilon: 1.0    steps: 161     evaluation reward: 1.18\n",
      "episode: 526   score: 4.0   memory length: 93894   epsilon: 1.0    steps: 253     evaluation reward: 1.22\n",
      "episode: 527   score: 3.0   memory length: 94128   epsilon: 1.0    steps: 234     evaluation reward: 1.25\n",
      "episode: 528   score: 1.0   memory length: 94289   epsilon: 1.0    steps: 161     evaluation reward: 1.25\n",
      "episode: 529   score: 1.0   memory length: 94441   epsilon: 1.0    steps: 152     evaluation reward: 1.26\n",
      "episode: 530   score: 0.0   memory length: 94569   epsilon: 1.0    steps: 128     evaluation reward: 1.26\n",
      "episode: 531   score: 1.0   memory length: 94745   epsilon: 1.0    steps: 176     evaluation reward: 1.27\n",
      "episode: 532   score: 2.0   memory length: 94934   epsilon: 1.0    steps: 189     evaluation reward: 1.27\n",
      "episode: 533   score: 0.0   memory length: 95060   epsilon: 1.0    steps: 126     evaluation reward: 1.25\n",
      "episode: 534   score: 2.0   memory length: 95260   epsilon: 1.0    steps: 200     evaluation reward: 1.25\n",
      "episode: 535   score: 1.0   memory length: 95443   epsilon: 1.0    steps: 183     evaluation reward: 1.26\n",
      "episode: 536   score: 0.0   memory length: 95572   epsilon: 1.0    steps: 129     evaluation reward: 1.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 537   score: 1.0   memory length: 95743   epsilon: 1.0    steps: 171     evaluation reward: 1.25\n",
      "episode: 538   score: 1.0   memory length: 95907   epsilon: 1.0    steps: 164     evaluation reward: 1.25\n",
      "episode: 539   score: 3.0   memory length: 96136   epsilon: 1.0    steps: 229     evaluation reward: 1.28\n",
      "episode: 540   score: 0.0   memory length: 96276   epsilon: 1.0    steps: 140     evaluation reward: 1.26\n",
      "episode: 541   score: 1.0   memory length: 96442   epsilon: 1.0    steps: 166     evaluation reward: 1.27\n",
      "episode: 542   score: 2.0   memory length: 96649   epsilon: 1.0    steps: 207     evaluation reward: 1.29\n",
      "episode: 543   score: 3.0   memory length: 96886   epsilon: 1.0    steps: 237     evaluation reward: 1.29\n",
      "episode: 544   score: 2.0   memory length: 97109   epsilon: 1.0    steps: 223     evaluation reward: 1.27\n",
      "episode: 545   score: 0.0   memory length: 97244   epsilon: 1.0    steps: 135     evaluation reward: 1.25\n",
      "episode: 546   score: 2.0   memory length: 97448   epsilon: 1.0    steps: 204     evaluation reward: 1.24\n",
      "episode: 547   score: 0.0   memory length: 97586   epsilon: 1.0    steps: 138     evaluation reward: 1.24\n",
      "episode: 548   score: 0.0   memory length: 97727   epsilon: 1.0    steps: 141     evaluation reward: 1.23\n",
      "episode: 549   score: 1.0   memory length: 97884   epsilon: 1.0    steps: 157     evaluation reward: 1.24\n",
      "episode: 550   score: 3.0   memory length: 98131   epsilon: 1.0    steps: 247     evaluation reward: 1.26\n",
      "episode: 551   score: 1.0   memory length: 98310   epsilon: 1.0    steps: 179     evaluation reward: 1.26\n",
      "episode: 552   score: 0.0   memory length: 98443   epsilon: 1.0    steps: 133     evaluation reward: 1.23\n",
      "episode: 553   score: 0.0   memory length: 98579   epsilon: 1.0    steps: 136     evaluation reward: 1.21\n",
      "episode: 554   score: 0.0   memory length: 98704   epsilon: 1.0    steps: 125     evaluation reward: 1.21\n",
      "episode: 555   score: 3.0   memory length: 98946   epsilon: 1.0    steps: 242     evaluation reward: 1.23\n",
      "episode: 556   score: 1.0   memory length: 99124   epsilon: 1.0    steps: 178     evaluation reward: 1.24\n",
      "episode: 557   score: 1.0   memory length: 99300   epsilon: 1.0    steps: 176     evaluation reward: 1.22\n",
      "episode: 558   score: 0.0   memory length: 99428   epsilon: 1.0    steps: 128     evaluation reward: 1.22\n",
      "episode: 559   score: 0.0   memory length: 99552   epsilon: 1.0    steps: 124     evaluation reward: 1.22\n",
      "episode: 560   score: 0.0   memory length: 99684   epsilon: 1.0    steps: 132     evaluation reward: 1.22\n",
      "episode: 561   score: 0.0   memory length: 99813   epsilon: 1.0    steps: 129     evaluation reward: 1.22\n",
      "now time :  2018-12-12 10:26:34.384139\n",
      "episode: 562   score: 4.0   memory length: 100094   epsilon: 0.999905950000002    steps: 281     evaluation reward: 1.25\n",
      "episode: 563   score: 1.0   memory length: 100271   epsilon: 0.9997307200000058    steps: 177     evaluation reward: 1.22\n",
      "episode: 564   score: 1.0   memory length: 100439   epsilon: 0.9995644000000095    steps: 168     evaluation reward: 1.23\n",
      "episode: 565   score: 2.0   memory length: 100640   epsilon: 0.9993654100000138    steps: 201     evaluation reward: 1.25\n",
      "episode: 566   score: 0.0   memory length: 100765   epsilon: 0.9992416600000165    steps: 125     evaluation reward: 1.24\n",
      "episode: 567   score: 4.0   memory length: 101028   epsilon: 0.9989812900000221    steps: 263     evaluation reward: 1.27\n",
      "episode: 568   score: 0.0   memory length: 101163   epsilon: 0.998847640000025    steps: 135     evaluation reward: 1.23\n",
      "episode: 569   score: 0.0   memory length: 101297   epsilon: 0.9987149800000279    steps: 134     evaluation reward: 1.22\n",
      "episode: 570   score: 1.0   memory length: 101473   epsilon: 0.9985407400000317    steps: 176     evaluation reward: 1.21\n",
      "episode: 571   score: 0.0   memory length: 101606   epsilon: 0.9984090700000345    steps: 133     evaluation reward: 1.21\n",
      "episode: 572   score: 1.0   memory length: 101782   epsilon: 0.9982348300000383    steps: 176     evaluation reward: 1.21\n",
      "episode: 573   score: 3.0   memory length: 102036   epsilon: 0.9979833700000438    steps: 254     evaluation reward: 1.21\n",
      "episode: 574   score: 0.0   memory length: 102173   epsilon: 0.9978477400000467    steps: 137     evaluation reward: 1.21\n",
      "episode: 575   score: 1.0   memory length: 102368   epsilon: 0.9976546900000509    steps: 195     evaluation reward: 1.2\n",
      "episode: 576   score: 2.0   memory length: 102576   epsilon: 0.9974487700000554    steps: 208     evaluation reward: 1.19\n",
      "episode: 577   score: 1.0   memory length: 102750   epsilon: 0.9972765100000591    steps: 174     evaluation reward: 1.2\n",
      "episode: 578   score: 1.0   memory length: 102923   epsilon: 0.9971052400000628    steps: 173     evaluation reward: 1.2\n",
      "episode: 579   score: 1.0   memory length: 103090   epsilon: 0.9969399100000664    steps: 167     evaluation reward: 1.19\n",
      "episode: 580   score: 2.0   memory length: 103280   epsilon: 0.9967518100000705    steps: 190     evaluation reward: 1.2\n",
      "episode: 581   score: 1.0   memory length: 103437   epsilon: 0.9965963800000739    steps: 157     evaluation reward: 1.19\n",
      "episode: 582   score: 1.0   memory length: 103612   epsilon: 0.9964231300000777    steps: 175     evaluation reward: 1.19\n",
      "episode: 583   score: 0.0   memory length: 103741   epsilon: 0.9962954200000804    steps: 129     evaluation reward: 1.17\n",
      "episode: 584   score: 0.0   memory length: 103878   epsilon: 0.9961597900000834    steps: 137     evaluation reward: 1.17\n",
      "episode: 585   score: 2.0   memory length: 104059   epsilon: 0.9959806000000873    steps: 181     evaluation reward: 1.18\n",
      "episode: 586   score: 1.0   memory length: 104227   epsilon: 0.9958142800000909    steps: 168     evaluation reward: 1.17\n",
      "episode: 587   score: 2.0   memory length: 104426   epsilon: 0.9956172700000951    steps: 199     evaluation reward: 1.17\n",
      "episode: 588   score: 0.0   memory length: 104557   epsilon: 0.995487580000098    steps: 131     evaluation reward: 1.16\n",
      "episode: 589   score: 0.0   memory length: 104683   epsilon: 0.9953628400001007    steps: 126     evaluation reward: 1.14\n",
      "episode: 590   score: 0.0   memory length: 104829   epsilon: 0.9952183000001038    steps: 146     evaluation reward: 1.13\n",
      "episode: 591   score: 1.0   memory length: 104989   epsilon: 0.9950599000001072    steps: 160     evaluation reward: 1.12\n",
      "episode: 592   score: 0.0   memory length: 105119   epsilon: 0.99493120000011    steps: 130     evaluation reward: 1.12\n",
      "episode: 593   score: 0.0   memory length: 105259   epsilon: 0.994792600000113    steps: 140     evaluation reward: 1.09\n",
      "episode: 594   score: 2.0   memory length: 105470   epsilon: 0.9945837100001176    steps: 211     evaluation reward: 1.11\n",
      "episode: 595   score: 1.0   memory length: 105628   epsilon: 0.994427290000121    steps: 158     evaluation reward: 1.11\n",
      "episode: 596   score: 0.0   memory length: 105767   epsilon: 0.994289680000124    steps: 139     evaluation reward: 1.1\n",
      "episode: 597   score: 0.0   memory length: 105897   epsilon: 0.9941609800001268    steps: 130     evaluation reward: 1.09\n",
      "episode: 598   score: 1.0   memory length: 106072   epsilon: 0.9939877300001305    steps: 175     evaluation reward: 1.08\n",
      "episode: 599   score: 1.0   memory length: 106250   epsilon: 0.9938115100001343    steps: 178     evaluation reward: 1.08\n",
      "episode: 600   score: 3.0   memory length: 106495   epsilon: 0.9935689600001396    steps: 245     evaluation reward: 1.08\n",
      "episode: 601   score: 1.0   memory length: 106650   epsilon: 0.9934155100001429    steps: 155     evaluation reward: 1.09\n",
      "episode: 602   score: 2.0   memory length: 106857   epsilon: 0.9932105800001474    steps: 207     evaluation reward: 1.1\n",
      "episode: 603   score: 3.0   memory length: 107087   epsilon: 0.9929828800001523    steps: 230     evaluation reward: 1.11\n",
      "episode: 604   score: 2.0   memory length: 107290   epsilon: 0.9927819100001567    steps: 203     evaluation reward: 1.12\n",
      "episode: 605   score: 3.0   memory length: 107575   epsilon: 0.9924997600001628    steps: 285     evaluation reward: 1.14\n",
      "episode: 606   score: 1.0   memory length: 107730   epsilon: 0.9923463100001662    steps: 155     evaluation reward: 1.15\n",
      "episode: 607   score: 0.0   memory length: 107859   epsilon: 0.9922186000001689    steps: 129     evaluation reward: 1.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 608   score: 0.0   memory length: 107986   epsilon: 0.9920928700001717    steps: 127     evaluation reward: 1.11\n",
      "episode: 609   score: 0.0   memory length: 108116   epsilon: 0.9919641700001744    steps: 130     evaluation reward: 1.08\n",
      "episode: 610   score: 1.0   memory length: 108285   epsilon: 0.9917968600001781    steps: 169     evaluation reward: 1.09\n",
      "episode: 611   score: 0.0   memory length: 108415   epsilon: 0.9916681600001809    steps: 130     evaluation reward: 1.07\n",
      "episode: 612   score: 0.0   memory length: 108547   epsilon: 0.9915374800001837    steps: 132     evaluation reward: 1.07\n",
      "episode: 613   score: 0.0   memory length: 108680   epsilon: 0.9914058100001866    steps: 133     evaluation reward: 1.07\n",
      "episode: 614   score: 1.0   memory length: 108841   epsilon: 0.99124642000019    steps: 161     evaluation reward: 1.08\n",
      "episode: 615   score: 2.0   memory length: 109055   epsilon: 0.9910345600001946    steps: 214     evaluation reward: 1.09\n",
      "episode: 616   score: 3.0   memory length: 109289   epsilon: 0.9908029000001997    steps: 234     evaluation reward: 1.11\n",
      "episode: 617   score: 1.0   memory length: 109465   epsilon: 0.9906286600002034    steps: 176     evaluation reward: 1.11\n",
      "episode: 618   score: 1.0   memory length: 109638   epsilon: 0.9904573900002072    steps: 173     evaluation reward: 1.12\n",
      "episode: 619   score: 0.0   memory length: 109764   epsilon: 0.9903326500002099    steps: 126     evaluation reward: 1.09\n",
      "episode: 620   score: 0.0   memory length: 109892   epsilon: 0.9902059300002126    steps: 128     evaluation reward: 1.09\n",
      "episode: 621   score: 2.0   memory length: 110095   epsilon: 0.990004960000217    steps: 203     evaluation reward: 1.11\n",
      "episode: 622   score: 5.0   memory length: 110405   epsilon: 0.9896980600002236    steps: 310     evaluation reward: 1.16\n",
      "episode: 623   score: 1.0   memory length: 110576   epsilon: 0.9895287700002273    steps: 171     evaluation reward: 1.14\n",
      "episode: 624   score: 1.0   memory length: 110752   epsilon: 0.9893545300002311    steps: 176     evaluation reward: 1.12\n",
      "episode: 625   score: 0.0   memory length: 110878   epsilon: 0.9892297900002338    steps: 126     evaluation reward: 1.11\n",
      "episode: 626   score: 0.0   memory length: 111009   epsilon: 0.9891001000002366    steps: 131     evaluation reward: 1.07\n",
      "episode: 627   score: 5.0   memory length: 111310   epsilon: 0.9888021100002431    steps: 301     evaluation reward: 1.09\n",
      "episode: 628   score: 2.0   memory length: 111517   epsilon: 0.9885971800002475    steps: 207     evaluation reward: 1.1\n",
      "episode: 629   score: 2.0   memory length: 111744   epsilon: 0.9883724500002524    steps: 227     evaluation reward: 1.11\n",
      "episode: 630   score: 3.0   memory length: 112017   epsilon: 0.9881021800002583    steps: 273     evaluation reward: 1.14\n",
      "episode: 631   score: 1.0   memory length: 112188   epsilon: 0.987932890000262    steps: 171     evaluation reward: 1.14\n",
      "episode: 632   score: 1.0   memory length: 112363   epsilon: 0.9877596400002657    steps: 175     evaluation reward: 1.13\n",
      "episode: 633   score: 0.0   memory length: 112503   epsilon: 0.9876210400002687    steps: 140     evaluation reward: 1.13\n",
      "episode: 634   score: 0.0   memory length: 112632   epsilon: 0.9874933300002715    steps: 129     evaluation reward: 1.11\n",
      "episode: 635   score: 1.0   memory length: 112805   epsilon: 0.9873220600002752    steps: 173     evaluation reward: 1.11\n",
      "episode: 636   score: 2.0   memory length: 112990   epsilon: 0.9871389100002792    steps: 185     evaluation reward: 1.13\n",
      "episode: 637   score: 1.0   memory length: 113167   epsilon: 0.986963680000283    steps: 177     evaluation reward: 1.13\n",
      "episode: 638   score: 0.0   memory length: 113296   epsilon: 0.9868359700002858    steps: 129     evaluation reward: 1.12\n",
      "episode: 639   score: 0.0   memory length: 113422   epsilon: 0.9867112300002885    steps: 126     evaluation reward: 1.09\n",
      "episode: 640   score: 0.0   memory length: 113554   epsilon: 0.9865805500002913    steps: 132     evaluation reward: 1.09\n",
      "episode: 641   score: 0.0   memory length: 113691   epsilon: 0.9864449200002943    steps: 137     evaluation reward: 1.08\n",
      "episode: 642   score: 2.0   memory length: 113899   epsilon: 0.9862390000002987    steps: 208     evaluation reward: 1.08\n",
      "episode: 643   score: 1.0   memory length: 114069   epsilon: 0.9860707000003024    steps: 170     evaluation reward: 1.06\n",
      "episode: 644   score: 0.0   memory length: 114209   epsilon: 0.9859321000003054    steps: 140     evaluation reward: 1.04\n",
      "episode: 645   score: 0.0   memory length: 114341   epsilon: 0.9858014200003082    steps: 132     evaluation reward: 1.04\n",
      "episode: 646   score: 2.0   memory length: 114536   epsilon: 0.9856083700003124    steps: 195     evaluation reward: 1.04\n",
      "episode: 647   score: 1.0   memory length: 114713   epsilon: 0.9854331400003162    steps: 177     evaluation reward: 1.05\n",
      "episode: 648   score: 1.0   memory length: 114872   epsilon: 0.9852757300003196    steps: 159     evaluation reward: 1.06\n",
      "episode: 649   score: 2.0   memory length: 115102   epsilon: 0.9850480300003246    steps: 230     evaluation reward: 1.07\n",
      "episode: 650   score: 2.0   memory length: 115342   epsilon: 0.9848104300003298    steps: 240     evaluation reward: 1.06\n",
      "episode: 651   score: 3.0   memory length: 115602   epsilon: 0.9845530300003353    steps: 260     evaluation reward: 1.08\n",
      "episode: 652   score: 2.0   memory length: 115831   epsilon: 0.9843263200003403    steps: 229     evaluation reward: 1.1\n",
      "episode: 653   score: 3.0   memory length: 116064   epsilon: 0.9840956500003453    steps: 233     evaluation reward: 1.13\n",
      "episode: 654   score: 0.0   memory length: 116209   epsilon: 0.9839521000003484    steps: 145     evaluation reward: 1.13\n",
      "episode: 655   score: 0.0   memory length: 116336   epsilon: 0.9838263700003511    steps: 127     evaluation reward: 1.1\n",
      "episode: 656   score: 0.0   memory length: 116470   epsilon: 0.983693710000354    steps: 134     evaluation reward: 1.09\n",
      "episode: 657   score: 1.0   memory length: 116641   epsilon: 0.9835244200003577    steps: 171     evaluation reward: 1.09\n",
      "episode: 658   score: 3.0   memory length: 116916   epsilon: 0.9832521700003636    steps: 275     evaluation reward: 1.12\n",
      "episode: 659   score: 1.0   memory length: 117095   epsilon: 0.9830749600003674    steps: 179     evaluation reward: 1.13\n",
      "episode: 660   score: 1.0   memory length: 117254   epsilon: 0.9829175500003708    steps: 159     evaluation reward: 1.14\n",
      "episode: 661   score: 1.0   memory length: 117437   epsilon: 0.9827363800003748    steps: 183     evaluation reward: 1.15\n",
      "episode: 662   score: 1.0   memory length: 117622   epsilon: 0.9825532300003788    steps: 185     evaluation reward: 1.12\n",
      "episode: 663   score: 5.0   memory length: 117944   epsilon: 0.9822344500003857    steps: 322     evaluation reward: 1.16\n",
      "episode: 664   score: 0.0   memory length: 118068   epsilon: 0.9821116900003883    steps: 124     evaluation reward: 1.15\n",
      "episode: 665   score: 2.0   memory length: 118275   epsilon: 0.9819067600003928    steps: 207     evaluation reward: 1.15\n",
      "episode: 666   score: 2.0   memory length: 118508   epsilon: 0.9816760900003978    steps: 233     evaluation reward: 1.17\n",
      "episode: 667   score: 2.0   memory length: 118734   epsilon: 0.9814523500004027    steps: 226     evaluation reward: 1.15\n",
      "episode: 668   score: 2.0   memory length: 118939   epsilon: 0.9812494000004071    steps: 205     evaluation reward: 1.17\n",
      "episode: 669   score: 2.0   memory length: 119140   epsilon: 0.9810504100004114    steps: 201     evaluation reward: 1.19\n",
      "episode: 670   score: 0.0   memory length: 119267   epsilon: 0.9809246800004141    steps: 127     evaluation reward: 1.18\n",
      "episode: 671   score: 3.0   memory length: 119516   epsilon: 0.9806781700004195    steps: 249     evaluation reward: 1.21\n",
      "episode: 672   score: 1.0   memory length: 119672   epsilon: 0.9805237300004228    steps: 156     evaluation reward: 1.21\n",
      "episode: 673   score: 0.0   memory length: 119804   epsilon: 0.9803930500004256    steps: 132     evaluation reward: 1.18\n",
      "episode: 674   score: 4.0   memory length: 120067   epsilon: 0.9801326800004313    steps: 263     evaluation reward: 1.22\n",
      "episode: 675   score: 3.0   memory length: 120299   epsilon: 0.9799030000004363    steps: 232     evaluation reward: 1.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 676   score: 1.0   memory length: 120454   epsilon: 0.9797495500004396    steps: 155     evaluation reward: 1.23\n",
      "episode: 677   score: 2.0   memory length: 120641   epsilon: 0.9795644200004436    steps: 187     evaluation reward: 1.24\n",
      "episode: 678   score: 0.0   memory length: 120779   epsilon: 0.9794278000004466    steps: 138     evaluation reward: 1.23\n",
      "episode: 679   score: 2.0   memory length: 120993   epsilon: 0.9792159400004512    steps: 214     evaluation reward: 1.24\n",
      "episode: 680   score: 4.0   memory length: 121270   epsilon: 0.9789417100004572    steps: 277     evaluation reward: 1.26\n",
      "episode: 681   score: 2.0   memory length: 121458   epsilon: 0.9787555900004612    steps: 188     evaluation reward: 1.27\n",
      "episode: 682   score: 0.0   memory length: 121590   epsilon: 0.978624910000464    steps: 132     evaluation reward: 1.26\n",
      "episode: 683   score: 0.0   memory length: 121724   epsilon: 0.9784922500004669    steps: 134     evaluation reward: 1.26\n",
      "episode: 684   score: 2.0   memory length: 121933   epsilon: 0.9782853400004714    steps: 209     evaluation reward: 1.28\n",
      "episode: 685   score: 0.0   memory length: 122077   epsilon: 0.9781427800004745    steps: 144     evaluation reward: 1.26\n",
      "episode: 686   score: 3.0   memory length: 122326   epsilon: 0.9778962700004799    steps: 249     evaluation reward: 1.28\n",
      "episode: 687   score: 0.0   memory length: 122463   epsilon: 0.9777606400004828    steps: 137     evaluation reward: 1.26\n",
      "episode: 688   score: 0.0   memory length: 122599   epsilon: 0.9776260000004857    steps: 136     evaluation reward: 1.26\n",
      "episode: 689   score: 0.0   memory length: 122731   epsilon: 0.9774953200004886    steps: 132     evaluation reward: 1.26\n",
      "episode: 690   score: 0.0   memory length: 122865   epsilon: 0.9773626600004914    steps: 134     evaluation reward: 1.26\n",
      "episode: 691   score: 0.0   memory length: 123002   epsilon: 0.9772270300004944    steps: 137     evaluation reward: 1.25\n",
      "episode: 692   score: 2.0   memory length: 123229   epsilon: 0.9770023000004993    steps: 227     evaluation reward: 1.27\n",
      "episode: 693   score: 0.0   memory length: 123360   epsilon: 0.9768726100005021    steps: 131     evaluation reward: 1.27\n",
      "episode: 694   score: 0.0   memory length: 123487   epsilon: 0.9767468800005048    steps: 127     evaluation reward: 1.25\n",
      "episode: 695   score: 1.0   memory length: 123674   epsilon: 0.9765617500005088    steps: 187     evaluation reward: 1.25\n",
      "episode: 696   score: 1.0   memory length: 123849   epsilon: 0.9763885000005126    steps: 175     evaluation reward: 1.26\n",
      "episode: 697   score: 1.0   memory length: 124007   epsilon: 0.976232080000516    steps: 158     evaluation reward: 1.27\n",
      "episode: 698   score: 2.0   memory length: 124242   epsilon: 0.975999430000521    steps: 235     evaluation reward: 1.28\n",
      "episode: 699   score: 2.0   memory length: 124443   epsilon: 0.9758004400005253    steps: 201     evaluation reward: 1.29\n",
      "episode: 700   score: 1.0   memory length: 124616   epsilon: 0.9756291700005291    steps: 173     evaluation reward: 1.27\n",
      "episode: 701   score: 0.0   memory length: 124757   epsilon: 0.9754895800005321    steps: 141     evaluation reward: 1.26\n",
      "episode: 702   score: 2.0   memory length: 124959   epsilon: 0.9752896000005364    steps: 202     evaluation reward: 1.26\n",
      "episode: 703   score: 0.0   memory length: 125081   epsilon: 0.9751688200005391    steps: 122     evaluation reward: 1.23\n",
      "episode: 704   score: 0.0   memory length: 125207   epsilon: 0.9750440800005418    steps: 126     evaluation reward: 1.21\n",
      "episode: 705   score: 2.0   memory length: 125407   epsilon: 0.9748460800005461    steps: 200     evaluation reward: 1.2\n",
      "episode: 706   score: 0.0   memory length: 125541   epsilon: 0.974713420000549    steps: 134     evaluation reward: 1.19\n",
      "episode: 707   score: 2.0   memory length: 125746   epsilon: 0.9745104700005534    steps: 205     evaluation reward: 1.21\n",
      "episode: 708   score: 1.0   memory length: 125900   epsilon: 0.9743580100005567    steps: 154     evaluation reward: 1.22\n",
      "episode: 709   score: 2.0   memory length: 126104   epsilon: 0.974156050000561    steps: 204     evaluation reward: 1.24\n",
      "episode: 710   score: 1.0   memory length: 126255   epsilon: 0.9740065600005643    steps: 151     evaluation reward: 1.24\n",
      "episode: 711   score: 0.0   memory length: 126382   epsilon: 0.973880830000567    steps: 127     evaluation reward: 1.24\n",
      "episode: 712   score: 1.0   memory length: 126553   epsilon: 0.9737115400005707    steps: 171     evaluation reward: 1.25\n",
      "episode: 713   score: 1.0   memory length: 126708   epsilon: 0.973558090000574    steps: 155     evaluation reward: 1.26\n",
      "episode: 714   score: 0.0   memory length: 126848   epsilon: 0.973419490000577    steps: 140     evaluation reward: 1.25\n",
      "episode: 715   score: 0.0   memory length: 126989   epsilon: 0.9732799000005801    steps: 141     evaluation reward: 1.23\n",
      "episode: 716   score: 4.0   memory length: 127286   epsilon: 0.9729858700005865    steps: 297     evaluation reward: 1.24\n",
      "episode: 717   score: 0.0   memory length: 127427   epsilon: 0.9728462800005895    steps: 141     evaluation reward: 1.23\n",
      "episode: 718   score: 0.0   memory length: 127564   epsilon: 0.9727106500005924    steps: 137     evaluation reward: 1.22\n",
      "episode: 719   score: 1.0   memory length: 127740   epsilon: 0.9725364100005962    steps: 176     evaluation reward: 1.23\n",
      "episode: 720   score: 0.0   memory length: 127866   epsilon: 0.9724116700005989    steps: 126     evaluation reward: 1.23\n",
      "episode: 721   score: 1.0   memory length: 128027   epsilon: 0.9722522800006024    steps: 161     evaluation reward: 1.22\n",
      "episode: 722   score: 0.0   memory length: 128155   epsilon: 0.9721255600006051    steps: 128     evaluation reward: 1.17\n",
      "episode: 723   score: 2.0   memory length: 128358   epsilon: 0.9719245900006095    steps: 203     evaluation reward: 1.18\n",
      "episode: 724   score: 3.0   memory length: 128609   epsilon: 0.9716761000006149    steps: 251     evaluation reward: 1.2\n",
      "episode: 725   score: 2.0   memory length: 128810   epsilon: 0.9714771100006192    steps: 201     evaluation reward: 1.22\n",
      "episode: 726   score: 2.0   memory length: 129010   epsilon: 0.9712791100006235    steps: 200     evaluation reward: 1.24\n",
      "episode: 727   score: 1.0   memory length: 129169   epsilon: 0.9711217000006269    steps: 159     evaluation reward: 1.2\n",
      "episode: 728   score: 0.0   memory length: 129297   epsilon: 0.9709949800006297    steps: 128     evaluation reward: 1.18\n",
      "episode: 729   score: 2.0   memory length: 129523   epsilon: 0.9707712400006345    steps: 226     evaluation reward: 1.18\n",
      "episode: 730   score: 2.0   memory length: 129723   epsilon: 0.9705732400006388    steps: 200     evaluation reward: 1.17\n",
      "episode: 731   score: 0.0   memory length: 129869   epsilon: 0.970428700000642    steps: 146     evaluation reward: 1.16\n",
      "episode: 732   score: 1.0   memory length: 130048   epsilon: 0.9702514900006458    steps: 179     evaluation reward: 1.16\n",
      "episode: 733   score: 2.0   memory length: 130259   epsilon: 0.9700426000006503    steps: 211     evaluation reward: 1.18\n",
      "episode: 734   score: 2.0   memory length: 130449   epsilon: 0.9698545000006544    steps: 190     evaluation reward: 1.2\n",
      "episode: 735   score: 0.0   memory length: 130589   epsilon: 0.9697159000006574    steps: 140     evaluation reward: 1.19\n",
      "episode: 736   score: 2.0   memory length: 130793   epsilon: 0.9695139400006618    steps: 204     evaluation reward: 1.19\n",
      "episode: 737   score: 0.0   memory length: 130926   epsilon: 0.9693822700006647    steps: 133     evaluation reward: 1.18\n",
      "episode: 738   score: 3.0   memory length: 131167   epsilon: 0.9691436800006699    steps: 241     evaluation reward: 1.21\n",
      "episode: 739   score: 2.0   memory length: 131393   epsilon: 0.9689199400006747    steps: 226     evaluation reward: 1.23\n",
      "episode: 740   score: 2.0   memory length: 131596   epsilon: 0.9687189700006791    steps: 203     evaluation reward: 1.25\n",
      "episode: 741   score: 0.0   memory length: 131723   epsilon: 0.9685932400006818    steps: 127     evaluation reward: 1.25\n",
      "episode: 742   score: 1.0   memory length: 131902   epsilon: 0.9684160300006857    steps: 179     evaluation reward: 1.24\n",
      "episode: 743   score: 1.0   memory length: 132077   epsilon: 0.9682427800006894    steps: 175     evaluation reward: 1.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 744   score: 1.0   memory length: 132232   epsilon: 0.9680893300006927    steps: 155     evaluation reward: 1.25\n",
      "episode: 745   score: 0.0   memory length: 132374   epsilon: 0.9679487500006958    steps: 142     evaluation reward: 1.25\n",
      "episode: 746   score: 3.0   memory length: 132617   epsilon: 0.967708180000701    steps: 243     evaluation reward: 1.26\n",
      "episode: 747   score: 0.0   memory length: 132751   epsilon: 0.9675755200007039    steps: 134     evaluation reward: 1.25\n",
      "episode: 748   score: 0.0   memory length: 132878   epsilon: 0.9674497900007066    steps: 127     evaluation reward: 1.24\n",
      "episode: 749   score: 3.0   memory length: 133155   epsilon: 0.9671755600007126    steps: 277     evaluation reward: 1.25\n",
      "episode: 750   score: 1.0   memory length: 133331   epsilon: 0.9670013200007164    steps: 176     evaluation reward: 1.24\n",
      "episode: 751   score: 5.0   memory length: 133646   epsilon: 0.9666894700007231    steps: 315     evaluation reward: 1.26\n",
      "episode: 752   score: 1.0   memory length: 133826   epsilon: 0.966511270000727    steps: 180     evaluation reward: 1.25\n",
      "episode: 753   score: 1.0   memory length: 133999   epsilon: 0.9663400000007307    steps: 173     evaluation reward: 1.23\n",
      "episode: 754   score: 1.0   memory length: 134170   epsilon: 0.9661707100007344    steps: 171     evaluation reward: 1.24\n",
      "episode: 755   score: 1.0   memory length: 134335   epsilon: 0.966007360000738    steps: 165     evaluation reward: 1.25\n",
      "episode: 756   score: 0.0   memory length: 134473   epsilon: 0.9658707400007409    steps: 138     evaluation reward: 1.25\n",
      "episode: 757   score: 1.0   memory length: 134643   epsilon: 0.9657024400007446    steps: 170     evaluation reward: 1.25\n",
      "episode: 758   score: 3.0   memory length: 134891   epsilon: 0.9654569200007499    steps: 248     evaluation reward: 1.25\n",
      "episode: 759   score: 1.0   memory length: 135045   epsilon: 0.9653044600007532    steps: 154     evaluation reward: 1.25\n",
      "episode: 760   score: 2.0   memory length: 135241   epsilon: 0.9651104200007574    steps: 196     evaluation reward: 1.26\n",
      "episode: 761   score: 0.0   memory length: 135368   epsilon: 0.9649846900007601    steps: 127     evaluation reward: 1.25\n",
      "episode: 762   score: 1.0   memory length: 135548   epsilon: 0.964806490000764    steps: 180     evaluation reward: 1.25\n",
      "episode: 763   score: 3.0   memory length: 135786   epsilon: 0.9645708700007691    steps: 238     evaluation reward: 1.23\n",
      "episode: 764   score: 1.0   memory length: 135941   epsilon: 0.9644174200007725    steps: 155     evaluation reward: 1.24\n",
      "episode: 765   score: 2.0   memory length: 136142   epsilon: 0.9642184300007768    steps: 201     evaluation reward: 1.24\n",
      "episode: 766   score: 2.0   memory length: 136363   epsilon: 0.9639996400007815    steps: 221     evaluation reward: 1.24\n",
      "episode: 767   score: 1.0   memory length: 136537   epsilon: 0.9638273800007853    steps: 174     evaluation reward: 1.23\n",
      "episode: 768   score: 1.0   memory length: 136697   epsilon: 0.9636689800007887    steps: 160     evaluation reward: 1.22\n",
      "episode: 769   score: 2.0   memory length: 136902   epsilon: 0.9634660300007931    steps: 205     evaluation reward: 1.22\n",
      "episode: 770   score: 2.0   memory length: 137110   epsilon: 0.9632601100007976    steps: 208     evaluation reward: 1.24\n",
      "episode: 771   score: 1.0   memory length: 137284   epsilon: 0.9630878500008013    steps: 174     evaluation reward: 1.22\n",
      "episode: 772   score: 1.0   memory length: 137445   epsilon: 0.9629284600008048    steps: 161     evaluation reward: 1.22\n",
      "episode: 773   score: 0.0   memory length: 137578   epsilon: 0.9627967900008076    steps: 133     evaluation reward: 1.22\n",
      "episode: 774   score: 2.0   memory length: 137786   epsilon: 0.9625908700008121    steps: 208     evaluation reward: 1.2\n",
      "episode: 775   score: 2.0   memory length: 137997   epsilon: 0.9623819800008167    steps: 211     evaluation reward: 1.19\n",
      "episode: 776   score: 0.0   memory length: 138127   epsilon: 0.9622532800008194    steps: 130     evaluation reward: 1.18\n",
      "episode: 777   score: 3.0   memory length: 138391   epsilon: 0.9619919200008251    steps: 264     evaluation reward: 1.19\n",
      "episode: 778   score: 2.0   memory length: 138593   epsilon: 0.9617919400008295    steps: 202     evaluation reward: 1.21\n",
      "episode: 779   score: 1.0   memory length: 138753   epsilon: 0.9616335400008329    steps: 160     evaluation reward: 1.2\n",
      "episode: 780   score: 1.0   memory length: 138920   epsilon: 0.9614682100008365    steps: 167     evaluation reward: 1.17\n",
      "episode: 781   score: 0.0   memory length: 139051   epsilon: 0.9613385200008393    steps: 131     evaluation reward: 1.15\n",
      "episode: 782   score: 0.0   memory length: 139179   epsilon: 0.961211800000842    steps: 128     evaluation reward: 1.15\n",
      "episode: 783   score: 0.0   memory length: 139306   epsilon: 0.9610860700008448    steps: 127     evaluation reward: 1.15\n",
      "episode: 784   score: 3.0   memory length: 139548   epsilon: 0.96084649000085    steps: 242     evaluation reward: 1.16\n",
      "episode: 785   score: 0.0   memory length: 139672   epsilon: 0.9607237300008526    steps: 124     evaluation reward: 1.16\n",
      "episode: 786   score: 0.0   memory length: 139795   epsilon: 0.9606019600008553    steps: 123     evaluation reward: 1.13\n",
      "episode: 787   score: 1.0   memory length: 139955   epsilon: 0.9604435600008587    steps: 160     evaluation reward: 1.14\n",
      "episode: 788   score: 1.0   memory length: 140126   epsilon: 0.9602742700008624    steps: 171     evaluation reward: 1.15\n",
      "episode: 789   score: 1.0   memory length: 140308   epsilon: 0.9600940900008663    steps: 182     evaluation reward: 1.16\n",
      "episode: 790   score: 1.0   memory length: 140463   epsilon: 0.9599406400008696    steps: 155     evaluation reward: 1.17\n",
      "episode: 791   score: 2.0   memory length: 140682   epsilon: 0.9597238300008744    steps: 219     evaluation reward: 1.19\n",
      "episode: 792   score: 1.0   memory length: 140844   epsilon: 0.9595634500008778    steps: 162     evaluation reward: 1.18\n",
      "episode: 793   score: 1.0   memory length: 141033   epsilon: 0.9593763400008819    steps: 189     evaluation reward: 1.19\n",
      "episode: 794   score: 3.0   memory length: 141285   epsilon: 0.9591268600008873    steps: 252     evaluation reward: 1.22\n",
      "episode: 795   score: 3.0   memory length: 141533   epsilon: 0.9588813400008926    steps: 248     evaluation reward: 1.24\n",
      "episode: 796   score: 0.0   memory length: 141669   epsilon: 0.9587467000008956    steps: 136     evaluation reward: 1.23\n",
      "episode: 797   score: 2.0   memory length: 141873   epsilon: 0.9585447400009    steps: 204     evaluation reward: 1.24\n",
      "episode: 798   score: 3.0   memory length: 142092   epsilon: 0.9583279300009047    steps: 219     evaluation reward: 1.25\n",
      "episode: 799   score: 0.0   memory length: 142218   epsilon: 0.9582031900009074    steps: 126     evaluation reward: 1.23\n",
      "episode: 800   score: 2.0   memory length: 142406   epsilon: 0.9580170700009114    steps: 188     evaluation reward: 1.24\n",
      "episode: 801   score: 2.0   memory length: 142617   epsilon: 0.9578081800009159    steps: 211     evaluation reward: 1.26\n",
      "episode: 802   score: 2.0   memory length: 142818   epsilon: 0.9576091900009203    steps: 201     evaluation reward: 1.26\n",
      "episode: 803   score: 2.0   memory length: 143048   epsilon: 0.9573814900009252    steps: 230     evaluation reward: 1.28\n",
      "episode: 804   score: 2.0   memory length: 143256   epsilon: 0.9571755700009297    steps: 208     evaluation reward: 1.3\n",
      "episode: 805   score: 2.0   memory length: 143442   epsilon: 0.9569914300009337    steps: 186     evaluation reward: 1.3\n",
      "episode: 806   score: 2.0   memory length: 143644   epsilon: 0.956791450000938    steps: 202     evaluation reward: 1.32\n",
      "episode: 807   score: 1.0   memory length: 143806   epsilon: 0.9566310700009415    steps: 162     evaluation reward: 1.31\n",
      "episode: 808   score: 4.0   memory length: 144122   epsilon: 0.9563182300009483    steps: 316     evaluation reward: 1.34\n",
      "episode: 809   score: 2.0   memory length: 144314   epsilon: 0.9561281500009524    steps: 192     evaluation reward: 1.34\n",
      "episode: 810   score: 1.0   memory length: 144485   epsilon: 0.9559588600009561    steps: 171     evaluation reward: 1.34\n",
      "episode: 811   score: 1.0   memory length: 144639   epsilon: 0.9558064000009594    steps: 154     evaluation reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 812   score: 2.0   memory length: 144839   epsilon: 0.9556084000009637    steps: 200     evaluation reward: 1.36\n",
      "episode: 813   score: 3.0   memory length: 145083   epsilon: 0.9553668400009689    steps: 244     evaluation reward: 1.38\n",
      "episode: 814   score: 3.0   memory length: 145320   epsilon: 0.955132210000974    steps: 237     evaluation reward: 1.41\n",
      "episode: 815   score: 0.0   memory length: 145454   epsilon: 0.9549995500009769    steps: 134     evaluation reward: 1.41\n",
      "episode: 816   score: 0.0   memory length: 145583   epsilon: 0.9548718400009797    steps: 129     evaluation reward: 1.37\n",
      "episode: 817   score: 0.0   memory length: 145716   epsilon: 0.9547401700009825    steps: 133     evaluation reward: 1.37\n",
      "episode: 818   score: 2.0   memory length: 145913   epsilon: 0.9545451400009868    steps: 197     evaluation reward: 1.39\n",
      "episode: 819   score: 3.0   memory length: 146167   epsilon: 0.9542936800009922    steps: 254     evaluation reward: 1.41\n",
      "episode: 820   score: 2.0   memory length: 146351   epsilon: 0.9541115200009962    steps: 184     evaluation reward: 1.43\n",
      "episode: 821   score: 1.0   memory length: 146526   epsilon: 0.953938270001    steps: 175     evaluation reward: 1.43\n",
      "episode: 822   score: 1.0   memory length: 146679   epsilon: 0.9537868000010032    steps: 153     evaluation reward: 1.44\n",
      "episode: 823   score: 2.0   memory length: 146889   epsilon: 0.9535789000010078    steps: 210     evaluation reward: 1.44\n",
      "episode: 824   score: 0.0   memory length: 147014   epsilon: 0.9534551500010104    steps: 125     evaluation reward: 1.41\n",
      "episode: 825   score: 1.0   memory length: 147166   epsilon: 0.9533046700010137    steps: 152     evaluation reward: 1.4\n",
      "episode: 826   score: 3.0   memory length: 147439   epsilon: 0.9530344000010196    steps: 273     evaluation reward: 1.41\n",
      "episode: 827   score: 1.0   memory length: 147600   epsilon: 0.952875010001023    steps: 161     evaluation reward: 1.41\n",
      "episode: 828   score: 2.0   memory length: 147807   epsilon: 0.9526700800010275    steps: 207     evaluation reward: 1.43\n",
      "episode: 829   score: 0.0   memory length: 147945   epsilon: 0.9525334600010305    steps: 138     evaluation reward: 1.41\n",
      "episode: 830   score: 0.0   memory length: 148081   epsilon: 0.9523988200010334    steps: 136     evaluation reward: 1.39\n",
      "episode: 831   score: 2.0   memory length: 148291   epsilon: 0.9521909200010379    steps: 210     evaluation reward: 1.41\n",
      "episode: 832   score: 2.0   memory length: 148509   epsilon: 0.9519751000010426    steps: 218     evaluation reward: 1.42\n",
      "episode: 833   score: 2.0   memory length: 148708   epsilon: 0.9517780900010469    steps: 199     evaluation reward: 1.42\n",
      "episode: 834   score: 0.0   memory length: 148853   epsilon: 0.95163454000105    steps: 145     evaluation reward: 1.4\n",
      "episode: 835   score: 0.0   memory length: 149001   epsilon: 0.9514880200010531    steps: 148     evaluation reward: 1.4\n",
      "episode: 836   score: 1.0   memory length: 149183   epsilon: 0.9513078400010571    steps: 182     evaluation reward: 1.39\n",
      "episode: 837   score: 1.0   memory length: 149340   epsilon: 0.9511524100010604    steps: 157     evaluation reward: 1.4\n",
      "episode: 838   score: 3.0   memory length: 149573   epsilon: 0.9509217400010654    steps: 233     evaluation reward: 1.4\n",
      "episode: 839   score: 1.0   memory length: 149728   epsilon: 0.9507682900010688    steps: 155     evaluation reward: 1.39\n",
      "episode: 840   score: 1.0   memory length: 149882   epsilon: 0.9506158300010721    steps: 154     evaluation reward: 1.38\n",
      "now time :  2018-12-12 10:37:47.666880\n",
      "episode: 841   score: 1.0   memory length: 150036   epsilon: 0.9504633700010754    steps: 154     evaluation reward: 1.39\n",
      "episode: 842   score: 1.0   memory length: 150189   epsilon: 0.9503119000010787    steps: 153     evaluation reward: 1.39\n",
      "episode: 843   score: 2.0   memory length: 150382   epsilon: 0.9501208300010828    steps: 193     evaluation reward: 1.4\n",
      "episode: 844   score: 3.0   memory length: 150636   epsilon: 0.9498693700010883    steps: 254     evaluation reward: 1.42\n",
      "episode: 845   score: 4.0   memory length: 150915   epsilon: 0.9495931600010943    steps: 279     evaluation reward: 1.46\n",
      "episode: 846   score: 0.0   memory length: 151044   epsilon: 0.949465450001097    steps: 129     evaluation reward: 1.43\n",
      "episode: 847   score: 2.0   memory length: 151245   epsilon: 0.9492664600011014    steps: 201     evaluation reward: 1.45\n",
      "episode: 848   score: 1.0   memory length: 151424   epsilon: 0.9490892500011052    steps: 179     evaluation reward: 1.46\n",
      "episode: 849   score: 1.0   memory length: 151610   epsilon: 0.9489051100011092    steps: 186     evaluation reward: 1.44\n",
      "episode: 850   score: 1.0   memory length: 151768   epsilon: 0.9487486900011126    steps: 158     evaluation reward: 1.44\n",
      "episode: 851   score: 1.0   memory length: 151932   epsilon: 0.9485863300011161    steps: 164     evaluation reward: 1.4\n",
      "episode: 852   score: 2.0   memory length: 152159   epsilon: 0.948361600001121    steps: 227     evaluation reward: 1.41\n",
      "episode: 853   score: 0.0   memory length: 152300   epsilon: 0.948222010001124    steps: 141     evaluation reward: 1.4\n",
      "episode: 854   score: 1.0   memory length: 152454   epsilon: 0.9480695500011274    steps: 154     evaluation reward: 1.4\n",
      "episode: 855   score: 3.0   memory length: 152676   epsilon: 0.9478497700011321    steps: 222     evaluation reward: 1.42\n",
      "episode: 856   score: 0.0   memory length: 152802   epsilon: 0.9477250300011348    steps: 126     evaluation reward: 1.42\n",
      "episode: 857   score: 4.0   memory length: 153080   epsilon: 0.9474498100011408    steps: 278     evaluation reward: 1.45\n",
      "episode: 858   score: 1.0   memory length: 153249   epsilon: 0.9472825000011444    steps: 169     evaluation reward: 1.43\n",
      "episode: 859   score: 0.0   memory length: 153379   epsilon: 0.9471538000011472    steps: 130     evaluation reward: 1.42\n",
      "episode: 860   score: 2.0   memory length: 153610   epsilon: 0.9469251100011522    steps: 231     evaluation reward: 1.42\n",
      "episode: 861   score: 2.0   memory length: 153802   epsilon: 0.9467350300011563    steps: 192     evaluation reward: 1.44\n",
      "episode: 862   score: 2.0   memory length: 154030   epsilon: 0.9465093100011612    steps: 228     evaluation reward: 1.45\n",
      "episode: 863   score: 1.0   memory length: 154192   epsilon: 0.9463489300011647    steps: 162     evaluation reward: 1.43\n",
      "episode: 864   score: 4.0   memory length: 154490   epsilon: 0.9460539100011711    steps: 298     evaluation reward: 1.46\n",
      "episode: 865   score: 2.0   memory length: 154707   epsilon: 0.9458390800011758    steps: 217     evaluation reward: 1.46\n",
      "episode: 866   score: 3.0   memory length: 154957   epsilon: 0.9455915800011812    steps: 250     evaluation reward: 1.47\n",
      "episode: 867   score: 1.0   memory length: 155133   epsilon: 0.9454173400011849    steps: 176     evaluation reward: 1.47\n",
      "episode: 868   score: 2.0   memory length: 155330   epsilon: 0.9452223100011892    steps: 197     evaluation reward: 1.48\n",
      "episode: 869   score: 0.0   memory length: 155464   epsilon: 0.945089650001192    steps: 134     evaluation reward: 1.46\n",
      "episode: 870   score: 2.0   memory length: 155656   epsilon: 0.9448995700011962    steps: 192     evaluation reward: 1.46\n",
      "episode: 871   score: 0.0   memory length: 155804   epsilon: 0.9447530500011994    steps: 148     evaluation reward: 1.45\n",
      "episode: 872   score: 1.0   memory length: 155983   epsilon: 0.9445758400012032    steps: 179     evaluation reward: 1.45\n",
      "episode: 873   score: 2.0   memory length: 156166   epsilon: 0.9443946700012071    steps: 183     evaluation reward: 1.47\n",
      "episode: 874   score: 2.0   memory length: 156376   epsilon: 0.9441867700012117    steps: 210     evaluation reward: 1.47\n",
      "episode: 875   score: 1.0   memory length: 156550   epsilon: 0.9440145100012154    steps: 174     evaluation reward: 1.46\n",
      "episode: 876   score: 1.0   memory length: 156722   epsilon: 0.9438442300012191    steps: 172     evaluation reward: 1.47\n",
      "episode: 877   score: 0.0   memory length: 156858   epsilon: 0.943709590001222    steps: 136     evaluation reward: 1.44\n",
      "episode: 878   score: 2.0   memory length: 157052   epsilon: 0.9435175300012262    steps: 194     evaluation reward: 1.44\n",
      "episode: 879   score: 3.0   memory length: 157288   epsilon: 0.9432838900012313    steps: 236     evaluation reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 880   score: 1.0   memory length: 157471   epsilon: 0.9431027200012352    steps: 183     evaluation reward: 1.46\n",
      "episode: 881   score: 1.0   memory length: 157627   epsilon: 0.9429482800012385    steps: 156     evaluation reward: 1.47\n",
      "episode: 882   score: 4.0   memory length: 157921   epsilon: 0.9426572200012449    steps: 294     evaluation reward: 1.51\n",
      "episode: 883   score: 2.0   memory length: 158125   epsilon: 0.9424552600012492    steps: 204     evaluation reward: 1.53\n",
      "episode: 884   score: 2.0   memory length: 158348   epsilon: 0.942234490001254    steps: 223     evaluation reward: 1.52\n",
      "episode: 885   score: 1.0   memory length: 158524   epsilon: 0.9420602500012578    steps: 176     evaluation reward: 1.53\n",
      "episode: 886   score: 0.0   memory length: 158652   epsilon: 0.9419335300012606    steps: 128     evaluation reward: 1.53\n",
      "episode: 887   score: 3.0   memory length: 158871   epsilon: 0.9417167200012653    steps: 219     evaluation reward: 1.55\n",
      "episode: 888   score: 2.0   memory length: 159088   epsilon: 0.9415018900012699    steps: 217     evaluation reward: 1.56\n",
      "episode: 889   score: 2.0   memory length: 159274   epsilon: 0.9413177500012739    steps: 186     evaluation reward: 1.57\n",
      "episode: 890   score: 3.0   memory length: 159515   epsilon: 0.9410791600012791    steps: 241     evaluation reward: 1.59\n",
      "episode: 891   score: 1.0   memory length: 159666   epsilon: 0.9409296700012824    steps: 151     evaluation reward: 1.58\n",
      "episode: 892   score: 0.0   memory length: 159797   epsilon: 0.9407999800012852    steps: 131     evaluation reward: 1.57\n",
      "episode: 893   score: 1.0   memory length: 159954   epsilon: 0.9406445500012885    steps: 157     evaluation reward: 1.57\n",
      "episode: 894   score: 1.0   memory length: 160137   epsilon: 0.9404633800012925    steps: 183     evaluation reward: 1.55\n",
      "episode: 895   score: 0.0   memory length: 160266   epsilon: 0.9403356700012953    steps: 129     evaluation reward: 1.52\n",
      "episode: 896   score: 0.0   memory length: 160400   epsilon: 0.9402030100012981    steps: 134     evaluation reward: 1.52\n",
      "episode: 897   score: 3.0   memory length: 160624   epsilon: 0.939981250001303    steps: 224     evaluation reward: 1.53\n",
      "episode: 898   score: 2.0   memory length: 160824   epsilon: 0.9397832500013072    steps: 200     evaluation reward: 1.52\n",
      "episode: 899   score: 2.0   memory length: 161035   epsilon: 0.9395743600013118    steps: 211     evaluation reward: 1.54\n",
      "episode: 900   score: 1.0   memory length: 161209   epsilon: 0.9394021000013155    steps: 174     evaluation reward: 1.53\n",
      "episode: 901   score: 1.0   memory length: 161362   epsilon: 0.9392506300013188    steps: 153     evaluation reward: 1.52\n",
      "episode: 902   score: 0.0   memory length: 161489   epsilon: 0.9391249000013215    steps: 127     evaluation reward: 1.5\n",
      "episode: 903   score: 4.0   memory length: 161747   epsilon: 0.9388694800013271    steps: 258     evaluation reward: 1.52\n",
      "episode: 904   score: 2.0   memory length: 161948   epsilon: 0.9386704900013314    steps: 201     evaluation reward: 1.52\n",
      "episode: 905   score: 0.0   memory length: 162082   epsilon: 0.9385378300013343    steps: 134     evaluation reward: 1.5\n",
      "episode: 906   score: 1.0   memory length: 162251   epsilon: 0.9383705200013379    steps: 169     evaluation reward: 1.49\n",
      "episode: 907   score: 4.0   memory length: 162569   epsilon: 0.9380557000013447    steps: 318     evaluation reward: 1.52\n",
      "episode: 908   score: 2.0   memory length: 162770   epsilon: 0.9378567100013491    steps: 201     evaluation reward: 1.5\n",
      "episode: 909   score: 2.0   memory length: 162986   epsilon: 0.9376428700013537    steps: 216     evaluation reward: 1.5\n",
      "episode: 910   score: 1.0   memory length: 163159   epsilon: 0.9374716000013574    steps: 173     evaluation reward: 1.5\n",
      "episode: 911   score: 0.0   memory length: 163294   epsilon: 0.9373379500013603    steps: 135     evaluation reward: 1.49\n",
      "episode: 912   score: 3.0   memory length: 163513   epsilon: 0.937121140001365    steps: 219     evaluation reward: 1.5\n",
      "episode: 913   score: 2.0   memory length: 163742   epsilon: 0.93689443000137    steps: 229     evaluation reward: 1.49\n",
      "episode: 914   score: 2.0   memory length: 163955   epsilon: 0.9366835600013745    steps: 213     evaluation reward: 1.48\n",
      "episode: 915   score: 5.0   memory length: 164273   epsilon: 0.9363687400013814    steps: 318     evaluation reward: 1.53\n",
      "episode: 916   score: 1.0   memory length: 164445   epsilon: 0.9361984600013851    steps: 172     evaluation reward: 1.54\n",
      "episode: 917   score: 2.0   memory length: 164685   epsilon: 0.9359608600013902    steps: 240     evaluation reward: 1.56\n",
      "episode: 918   score: 0.0   memory length: 164818   epsilon: 0.9358291900013931    steps: 133     evaluation reward: 1.54\n",
      "episode: 919   score: 4.0   memory length: 165135   epsilon: 0.9355153600013999    steps: 317     evaluation reward: 1.55\n",
      "episode: 920   score: 5.0   memory length: 165442   epsilon: 0.9352114300014065    steps: 307     evaluation reward: 1.58\n",
      "episode: 921   score: 1.0   memory length: 165617   epsilon: 0.9350381800014103    steps: 175     evaluation reward: 1.58\n",
      "episode: 922   score: 2.0   memory length: 165817   epsilon: 0.9348401800014146    steps: 200     evaluation reward: 1.59\n",
      "episode: 923   score: 0.0   memory length: 165953   epsilon: 0.9347055400014175    steps: 136     evaluation reward: 1.57\n",
      "episode: 924   score: 0.0   memory length: 166099   epsilon: 0.9345610000014206    steps: 146     evaluation reward: 1.57\n",
      "episode: 925   score: 1.0   memory length: 166258   epsilon: 0.934403590001424    steps: 159     evaluation reward: 1.57\n",
      "episode: 926   score: 2.0   memory length: 166469   epsilon: 0.9341947000014286    steps: 211     evaluation reward: 1.56\n",
      "episode: 927   score: 1.0   memory length: 166652   epsilon: 0.9340135300014325    steps: 183     evaluation reward: 1.56\n",
      "episode: 928   score: 2.0   memory length: 166853   epsilon: 0.9338145400014368    steps: 201     evaluation reward: 1.56\n",
      "episode: 929   score: 2.0   memory length: 167040   epsilon: 0.9336294100014408    steps: 187     evaluation reward: 1.58\n",
      "episode: 930   score: 2.0   memory length: 167237   epsilon: 0.9334343800014451    steps: 197     evaluation reward: 1.6\n",
      "episode: 931   score: 0.0   memory length: 167376   epsilon: 0.9332967700014481    steps: 139     evaluation reward: 1.58\n",
      "episode: 932   score: 2.0   memory length: 167601   epsilon: 0.9330740200014529    steps: 225     evaluation reward: 1.58\n",
      "episode: 933   score: 5.0   memory length: 167959   epsilon: 0.9327196000014606    steps: 358     evaluation reward: 1.61\n",
      "episode: 934   score: 1.0   memory length: 168133   epsilon: 0.9325473400014643    steps: 174     evaluation reward: 1.62\n",
      "episode: 935   score: 0.0   memory length: 168262   epsilon: 0.9324196300014671    steps: 129     evaluation reward: 1.62\n",
      "episode: 936   score: 5.0   memory length: 168562   epsilon: 0.9321226300014736    steps: 300     evaluation reward: 1.66\n",
      "episode: 937   score: 1.0   memory length: 168725   epsilon: 0.931961260001477    steps: 163     evaluation reward: 1.66\n",
      "episode: 938   score: 3.0   memory length: 168960   epsilon: 0.9317286100014821    steps: 235     evaluation reward: 1.66\n",
      "episode: 939   score: 1.0   memory length: 169130   epsilon: 0.9315603100014858    steps: 170     evaluation reward: 1.66\n",
      "episode: 940   score: 2.0   memory length: 169330   epsilon: 0.9313623100014901    steps: 200     evaluation reward: 1.67\n",
      "episode: 941   score: 3.0   memory length: 169563   epsilon: 0.9311316400014951    steps: 233     evaluation reward: 1.69\n",
      "episode: 942   score: 0.0   memory length: 169698   epsilon: 0.930997990001498    steps: 135     evaluation reward: 1.68\n",
      "episode: 943   score: 2.0   memory length: 169892   epsilon: 0.9308059300015021    steps: 194     evaluation reward: 1.68\n",
      "episode: 944   score: 3.0   memory length: 170123   epsilon: 0.9305772400015071    steps: 231     evaluation reward: 1.68\n",
      "episode: 945   score: 1.0   memory length: 170277   epsilon: 0.9304247800015104    steps: 154     evaluation reward: 1.65\n",
      "episode: 946   score: 1.0   memory length: 170439   epsilon: 0.9302644000015139    steps: 162     evaluation reward: 1.66\n",
      "episode: 947   score: 3.0   memory length: 170676   epsilon: 0.930029770001519    steps: 237     evaluation reward: 1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 948   score: 0.0   memory length: 170806   epsilon: 0.9299010700015218    steps: 130     evaluation reward: 1.66\n",
      "episode: 949   score: 2.0   memory length: 171012   epsilon: 0.9296971300015262    steps: 206     evaluation reward: 1.67\n",
      "episode: 950   score: 3.0   memory length: 171269   epsilon: 0.9294427000015317    steps: 257     evaluation reward: 1.69\n",
      "episode: 951   score: 6.0   memory length: 171605   epsilon: 0.929110060001539    steps: 336     evaluation reward: 1.74\n",
      "episode: 952   score: 4.0   memory length: 171887   epsilon: 0.928830880001545    steps: 282     evaluation reward: 1.76\n",
      "episode: 953   score: 1.0   memory length: 172042   epsilon: 0.9286774300015483    steps: 155     evaluation reward: 1.77\n",
      "episode: 954   score: 4.0   memory length: 172341   epsilon: 0.9283814200015548    steps: 299     evaluation reward: 1.8\n",
      "episode: 955   score: 0.0   memory length: 172470   epsilon: 0.9282537100015575    steps: 129     evaluation reward: 1.77\n",
      "episode: 956   score: 3.0   memory length: 172713   epsilon: 0.9280131400015628    steps: 243     evaluation reward: 1.8\n",
      "episode: 957   score: 2.0   memory length: 172939   epsilon: 0.9277894000015676    steps: 226     evaluation reward: 1.78\n",
      "episode: 958   score: 2.0   memory length: 173121   epsilon: 0.9276092200015715    steps: 182     evaluation reward: 1.79\n",
      "episode: 959   score: 1.0   memory length: 173301   epsilon: 0.9274310200015754    steps: 180     evaluation reward: 1.8\n",
      "episode: 960   score: 1.0   memory length: 173477   epsilon: 0.9272567800015792    steps: 176     evaluation reward: 1.79\n",
      "episode: 961   score: 1.0   memory length: 173653   epsilon: 0.927082540001583    steps: 176     evaluation reward: 1.78\n",
      "episode: 962   score: 2.0   memory length: 173857   epsilon: 0.9268805800015874    steps: 204     evaluation reward: 1.78\n",
      "episode: 963   score: 1.0   memory length: 174020   epsilon: 0.9267192100015909    steps: 163     evaluation reward: 1.78\n",
      "episode: 964   score: 0.0   memory length: 174147   epsilon: 0.9265934800015936    steps: 127     evaluation reward: 1.74\n",
      "episode: 965   score: 2.0   memory length: 174339   epsilon: 0.9264034000015977    steps: 192     evaluation reward: 1.74\n",
      "episode: 966   score: 3.0   memory length: 174578   epsilon: 0.9261667900016028    steps: 239     evaluation reward: 1.74\n",
      "episode: 967   score: 0.0   memory length: 174714   epsilon: 0.9260321500016058    steps: 136     evaluation reward: 1.73\n",
      "episode: 968   score: 0.0   memory length: 174848   epsilon: 0.9258994900016086    steps: 134     evaluation reward: 1.71\n",
      "episode: 969   score: 1.0   memory length: 175003   epsilon: 0.925746040001612    steps: 155     evaluation reward: 1.72\n",
      "episode: 970   score: 2.0   memory length: 175197   epsilon: 0.9255539800016162    steps: 194     evaluation reward: 1.72\n",
      "episode: 971   score: 2.0   memory length: 175406   epsilon: 0.9253470700016206    steps: 209     evaluation reward: 1.74\n",
      "episode: 972   score: 1.0   memory length: 175569   epsilon: 0.9251857000016241    steps: 163     evaluation reward: 1.74\n",
      "episode: 973   score: 2.0   memory length: 175797   epsilon: 0.924959980001629    steps: 228     evaluation reward: 1.74\n",
      "episode: 974   score: 1.0   memory length: 175973   epsilon: 0.9247857400016328    steps: 176     evaluation reward: 1.73\n",
      "episode: 975   score: 1.0   memory length: 176131   epsilon: 0.9246293200016362    steps: 158     evaluation reward: 1.73\n",
      "episode: 976   score: 3.0   memory length: 176380   epsilon: 0.9243828100016416    steps: 249     evaluation reward: 1.75\n",
      "episode: 977   score: 3.0   memory length: 176658   epsilon: 0.9241075900016475    steps: 278     evaluation reward: 1.78\n",
      "episode: 978   score: 2.0   memory length: 176864   epsilon: 0.923903650001652    steps: 206     evaluation reward: 1.78\n",
      "episode: 979   score: 3.0   memory length: 177133   epsilon: 0.9236373400016578    steps: 269     evaluation reward: 1.78\n",
      "episode: 980   score: 1.0   memory length: 177296   epsilon: 0.9234759700016613    steps: 163     evaluation reward: 1.78\n",
      "episode: 981   score: 0.0   memory length: 177431   epsilon: 0.9233423200016642    steps: 135     evaluation reward: 1.77\n",
      "episode: 982   score: 1.0   memory length: 177589   epsilon: 0.9231859000016676    steps: 158     evaluation reward: 1.74\n",
      "episode: 983   score: 2.0   memory length: 177791   epsilon: 0.9229859200016719    steps: 202     evaluation reward: 1.74\n",
      "episode: 984   score: 1.0   memory length: 177961   epsilon: 0.9228176200016756    steps: 170     evaluation reward: 1.73\n",
      "episode: 985   score: 2.0   memory length: 178168   epsilon: 0.92261269000168    steps: 207     evaluation reward: 1.74\n",
      "episode: 986   score: 3.0   memory length: 178401   epsilon: 0.922382020001685    steps: 233     evaluation reward: 1.77\n",
      "episode: 987   score: 2.0   memory length: 178618   epsilon: 0.9221671900016897    steps: 217     evaluation reward: 1.76\n",
      "episode: 988   score: 3.0   memory length: 178843   epsilon: 0.9219444400016945    steps: 225     evaluation reward: 1.77\n",
      "episode: 989   score: 1.0   memory length: 179002   epsilon: 0.9217870300016979    steps: 159     evaluation reward: 1.76\n",
      "episode: 990   score: 3.0   memory length: 179260   epsilon: 0.9215316100017035    steps: 258     evaluation reward: 1.76\n",
      "episode: 991   score: 2.0   memory length: 179464   epsilon: 0.9213296500017079    steps: 204     evaluation reward: 1.77\n",
      "episode: 992   score: 2.0   memory length: 179675   epsilon: 0.9211207600017124    steps: 211     evaluation reward: 1.79\n",
      "episode: 993   score: 0.0   memory length: 179800   epsilon: 0.9209970100017151    steps: 125     evaluation reward: 1.78\n",
      "episode: 994   score: 2.0   memory length: 180003   epsilon: 0.9207960400017194    steps: 203     evaluation reward: 1.79\n",
      "episode: 995   score: 0.0   memory length: 180127   epsilon: 0.9206732800017221    steps: 124     evaluation reward: 1.79\n",
      "episode: 996   score: 0.0   memory length: 180251   epsilon: 0.9205505200017248    steps: 124     evaluation reward: 1.79\n",
      "episode: 997   score: 1.0   memory length: 180405   epsilon: 0.9203980600017281    steps: 154     evaluation reward: 1.77\n",
      "episode: 998   score: 1.0   memory length: 180562   epsilon: 0.9202426300017315    steps: 157     evaluation reward: 1.76\n",
      "episode: 999   score: 0.0   memory length: 180700   epsilon: 0.9201060100017344    steps: 138     evaluation reward: 1.74\n",
      "episode: 1000   score: 3.0   memory length: 180945   epsilon: 0.9198634600017397    steps: 245     evaluation reward: 1.76\n",
      "episode: 1001   score: 2.0   memory length: 181171   epsilon: 0.9196397200017445    steps: 226     evaluation reward: 1.77\n",
      "episode: 1002   score: 1.0   memory length: 181349   epsilon: 0.9194635000017484    steps: 178     evaluation reward: 1.78\n",
      "episode: 1003   score: 4.0   memory length: 181648   epsilon: 0.9191674900017548    steps: 299     evaluation reward: 1.78\n",
      "episode: 1004   score: 0.0   memory length: 181776   epsilon: 0.9190407700017575    steps: 128     evaluation reward: 1.76\n",
      "episode: 1005   score: 4.0   memory length: 182063   epsilon: 0.9187566400017637    steps: 287     evaluation reward: 1.8\n",
      "episode: 1006   score: 4.0   memory length: 182334   epsilon: 0.9184883500017695    steps: 271     evaluation reward: 1.83\n",
      "episode: 1007   score: 2.0   memory length: 182544   epsilon: 0.918280450001774    steps: 210     evaluation reward: 1.81\n",
      "episode: 1008   score: 3.0   memory length: 182796   epsilon: 0.9180309700017795    steps: 252     evaluation reward: 1.82\n",
      "episode: 1009   score: 1.0   memory length: 182976   epsilon: 0.9178527700017833    steps: 180     evaluation reward: 1.81\n",
      "episode: 1010   score: 2.0   memory length: 183222   epsilon: 0.9176092300017886    steps: 246     evaluation reward: 1.82\n",
      "episode: 1011   score: 2.0   memory length: 183414   epsilon: 0.9174191500017927    steps: 192     evaluation reward: 1.84\n",
      "episode: 1012   score: 6.0   memory length: 183778   epsilon: 0.9170587900018006    steps: 364     evaluation reward: 1.87\n",
      "episode: 1013   score: 0.0   memory length: 183906   epsilon: 0.9169320700018033    steps: 128     evaluation reward: 1.85\n",
      "episode: 1014   score: 2.0   memory length: 184121   epsilon: 0.9167192200018079    steps: 215     evaluation reward: 1.85\n",
      "episode: 1015   score: 0.0   memory length: 184252   epsilon: 0.9165895300018108    steps: 131     evaluation reward: 1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1016   score: 1.0   memory length: 184413   epsilon: 0.9164301400018142    steps: 161     evaluation reward: 1.8\n",
      "episode: 1017   score: 3.0   memory length: 184667   epsilon: 0.9161786800018197    steps: 254     evaluation reward: 1.81\n",
      "episode: 1018   score: 3.0   memory length: 184925   epsilon: 0.9159232600018252    steps: 258     evaluation reward: 1.84\n",
      "episode: 1019   score: 0.0   memory length: 185056   epsilon: 0.915793570001828    steps: 131     evaluation reward: 1.8\n",
      "episode: 1020   score: 4.0   memory length: 185332   epsilon: 0.915520330001834    steps: 276     evaluation reward: 1.79\n",
      "episode: 1021   score: 1.0   memory length: 185489   epsilon: 0.9153649000018373    steps: 157     evaluation reward: 1.79\n",
      "episode: 1022   score: 2.0   memory length: 185696   epsilon: 0.9151599700018418    steps: 207     evaluation reward: 1.79\n",
      "episode: 1023   score: 1.0   memory length: 185870   epsilon: 0.9149877100018455    steps: 174     evaluation reward: 1.8\n",
      "episode: 1024   score: 1.0   memory length: 186041   epsilon: 0.9148184200018492    steps: 171     evaluation reward: 1.81\n",
      "episode: 1025   score: 2.0   memory length: 186242   epsilon: 0.9146194300018535    steps: 201     evaluation reward: 1.82\n",
      "episode: 1026   score: 4.0   memory length: 186541   epsilon: 0.91432342000186    steps: 299     evaluation reward: 1.84\n",
      "episode: 1027   score: 1.0   memory length: 186700   epsilon: 0.9141660100018634    steps: 159     evaluation reward: 1.84\n",
      "episode: 1028   score: 1.0   memory length: 186900   epsilon: 0.9139680100018677    steps: 200     evaluation reward: 1.83\n",
      "episode: 1029   score: 6.0   memory length: 187297   epsilon: 0.9135749800018762    steps: 397     evaluation reward: 1.87\n",
      "episode: 1030   score: 1.0   memory length: 187450   epsilon: 0.9134235100018795    steps: 153     evaluation reward: 1.86\n",
      "episode: 1031   score: 1.0   memory length: 187606   epsilon: 0.9132690700018828    steps: 156     evaluation reward: 1.87\n",
      "episode: 1032   score: 3.0   memory length: 187857   epsilon: 0.9130205800018882    steps: 251     evaluation reward: 1.88\n",
      "episode: 1033   score: 0.0   memory length: 187988   epsilon: 0.912890890001891    steps: 131     evaluation reward: 1.83\n",
      "episode: 1034   score: 2.0   memory length: 188174   epsilon: 0.912706750001895    steps: 186     evaluation reward: 1.84\n",
      "episode: 1035   score: 0.0   memory length: 188303   epsilon: 0.9125790400018978    steps: 129     evaluation reward: 1.84\n",
      "episode: 1036   score: 1.0   memory length: 188464   epsilon: 0.9124196500019013    steps: 161     evaluation reward: 1.8\n",
      "episode: 1037   score: 1.0   memory length: 188630   epsilon: 0.9122553100019049    steps: 166     evaluation reward: 1.8\n",
      "episode: 1038   score: 2.0   memory length: 188837   epsilon: 0.9120503800019093    steps: 207     evaluation reward: 1.79\n",
      "episode: 1039   score: 3.0   memory length: 189071   epsilon: 0.9118187200019143    steps: 234     evaluation reward: 1.81\n",
      "episode: 1040   score: 0.0   memory length: 189196   epsilon: 0.911694970001917    steps: 125     evaluation reward: 1.79\n",
      "episode: 1041   score: 2.0   memory length: 189384   epsilon: 0.9115088500019211    steps: 188     evaluation reward: 1.78\n",
      "episode: 1042   score: 1.0   memory length: 189545   epsilon: 0.9113494600019245    steps: 161     evaluation reward: 1.79\n",
      "episode: 1043   score: 1.0   memory length: 189717   epsilon: 0.9111791800019282    steps: 172     evaluation reward: 1.78\n",
      "episode: 1044   score: 1.0   memory length: 189897   epsilon: 0.9110009800019321    steps: 180     evaluation reward: 1.76\n",
      "episode: 1045   score: 0.0   memory length: 190036   epsilon: 0.9108633700019351    steps: 139     evaluation reward: 1.75\n",
      "episode: 1046   score: 4.0   memory length: 190322   epsilon: 0.9105802300019412    steps: 286     evaluation reward: 1.78\n",
      "episode: 1047   score: 3.0   memory length: 190589   epsilon: 0.910315900001947    steps: 267     evaluation reward: 1.78\n",
      "episode: 1048   score: 4.0   memory length: 190899   epsilon: 0.9100090000019536    steps: 310     evaluation reward: 1.82\n",
      "episode: 1049   score: 3.0   memory length: 191136   epsilon: 0.9097743700019587    steps: 237     evaluation reward: 1.83\n",
      "episode: 1050   score: 1.0   memory length: 191317   epsilon: 0.9095951800019626    steps: 181     evaluation reward: 1.81\n",
      "episode: 1051   score: 2.0   memory length: 191515   epsilon: 0.9093991600019669    steps: 198     evaluation reward: 1.77\n",
      "episode: 1052   score: 3.0   memory length: 191763   epsilon: 0.9091536400019722    steps: 248     evaluation reward: 1.76\n",
      "episode: 1053   score: 3.0   memory length: 192013   epsilon: 0.9089061400019776    steps: 250     evaluation reward: 1.78\n",
      "episode: 1054   score: 2.0   memory length: 192226   epsilon: 0.9086952700019821    steps: 213     evaluation reward: 1.76\n",
      "episode: 1055   score: 2.0   memory length: 192429   epsilon: 0.9084943000019865    steps: 203     evaluation reward: 1.78\n",
      "episode: 1056   score: 2.0   memory length: 192617   epsilon: 0.9083081800019905    steps: 188     evaluation reward: 1.77\n",
      "episode: 1057   score: 5.0   memory length: 192925   epsilon: 0.9080032600019972    steps: 308     evaluation reward: 1.8\n",
      "episode: 1058   score: 0.0   memory length: 193056   epsilon: 0.907873570002    steps: 131     evaluation reward: 1.78\n",
      "episode: 1059   score: 2.0   memory length: 193259   epsilon: 0.9076726000020043    steps: 203     evaluation reward: 1.79\n",
      "episode: 1060   score: 0.0   memory length: 193391   epsilon: 0.9075419200020072    steps: 132     evaluation reward: 1.78\n",
      "episode: 1061   score: 0.0   memory length: 193519   epsilon: 0.9074152000020099    steps: 128     evaluation reward: 1.77\n",
      "episode: 1062   score: 2.0   memory length: 193744   epsilon: 0.9071924500020148    steps: 225     evaluation reward: 1.77\n",
      "episode: 1063   score: 2.0   memory length: 193945   epsilon: 0.9069934600020191    steps: 201     evaluation reward: 1.78\n",
      "episode: 1064   score: 2.0   memory length: 194181   epsilon: 0.9067598200020242    steps: 236     evaluation reward: 1.8\n",
      "episode: 1065   score: 3.0   memory length: 194436   epsilon: 0.9065073700020296    steps: 255     evaluation reward: 1.81\n",
      "episode: 1066   score: 2.0   memory length: 194663   epsilon: 0.9062826400020345    steps: 227     evaluation reward: 1.8\n",
      "episode: 1067   score: 1.0   memory length: 194820   epsilon: 0.9061272100020379    steps: 157     evaluation reward: 1.81\n",
      "episode: 1068   score: 3.0   memory length: 195090   epsilon: 0.9058599100020437    steps: 270     evaluation reward: 1.84\n",
      "episode: 1069   score: 2.0   memory length: 195277   epsilon: 0.9056747800020477    steps: 187     evaluation reward: 1.85\n",
      "episode: 1070   score: 1.0   memory length: 195460   epsilon: 0.9054936100020516    steps: 183     evaluation reward: 1.84\n",
      "episode: 1071   score: 1.0   memory length: 195646   epsilon: 0.9053094700020556    steps: 186     evaluation reward: 1.83\n",
      "episode: 1072   score: 3.0   memory length: 195894   epsilon: 0.905063950002061    steps: 248     evaluation reward: 1.85\n",
      "episode: 1073   score: 4.0   memory length: 196141   epsilon: 0.9048194200020663    steps: 247     evaluation reward: 1.87\n",
      "episode: 1074   score: 1.0   memory length: 196296   epsilon: 0.9046659700020696    steps: 155     evaluation reward: 1.87\n",
      "episode: 1075   score: 3.0   memory length: 196563   epsilon: 0.9044016400020753    steps: 267     evaluation reward: 1.89\n",
      "episode: 1076   score: 1.0   memory length: 196720   epsilon: 0.9042462100020787    steps: 157     evaluation reward: 1.87\n",
      "episode: 1077   score: 2.0   memory length: 196922   epsilon: 0.9040462300020831    steps: 202     evaluation reward: 1.86\n",
      "episode: 1078   score: 0.0   memory length: 197050   epsilon: 0.9039195100020858    steps: 128     evaluation reward: 1.84\n",
      "episode: 1079   score: 3.0   memory length: 197285   epsilon: 0.9036868600020909    steps: 235     evaluation reward: 1.84\n",
      "episode: 1080   score: 2.0   memory length: 197476   epsilon: 0.903497770002095    steps: 191     evaluation reward: 1.85\n",
      "episode: 1081   score: 1.0   memory length: 197635   epsilon: 0.9033403600020984    steps: 159     evaluation reward: 1.86\n",
      "episode: 1082   score: 3.0   memory length: 197893   epsilon: 0.9030849400021039    steps: 258     evaluation reward: 1.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1083   score: 2.0   memory length: 198094   epsilon: 0.9028859500021083    steps: 201     evaluation reward: 1.88\n",
      "episode: 1084   score: 2.0   memory length: 198282   epsilon: 0.9026998300021123    steps: 188     evaluation reward: 1.89\n",
      "episode: 1085   score: 1.0   memory length: 198435   epsilon: 0.9025483600021156    steps: 153     evaluation reward: 1.88\n",
      "episode: 1086   score: 1.0   memory length: 198603   epsilon: 0.9023820400021192    steps: 168     evaluation reward: 1.86\n",
      "episode: 1087   score: 2.0   memory length: 198813   epsilon: 0.9021741400021237    steps: 210     evaluation reward: 1.86\n",
      "episode: 1088   score: 0.0   memory length: 198943   epsilon: 0.9020454400021265    steps: 130     evaluation reward: 1.83\n",
      "episode: 1089   score: 0.0   memory length: 199081   epsilon: 0.9019088200021295    steps: 138     evaluation reward: 1.82\n",
      "episode: 1090   score: 4.0   memory length: 199350   epsilon: 0.9016425100021352    steps: 269     evaluation reward: 1.83\n",
      "episode: 1091   score: 1.0   memory length: 199532   epsilon: 0.9014623300021392    steps: 182     evaluation reward: 1.82\n",
      "episode: 1092   score: 2.0   memory length: 199738   epsilon: 0.9012583900021436    steps: 206     evaluation reward: 1.82\n",
      "episode: 1093   score: 2.0   memory length: 199940   epsilon: 0.9010584100021479    steps: 202     evaluation reward: 1.84\n",
      "now time :  2018-12-12 10:49:33.278653\n",
      "episode: 1094   score: 2.0   memory length: 200147   epsilon: 0.9008534800021524    steps: 207     evaluation reward: 1.84\n",
      "episode: 1095   score: 2.0   memory length: 200348   epsilon: 0.9006544900021567    steps: 201     evaluation reward: 1.86\n",
      "episode: 1096   score: 3.0   memory length: 200585   epsilon: 0.9004198600021618    steps: 237     evaluation reward: 1.89\n",
      "episode: 1097   score: 0.0   memory length: 200714   epsilon: 0.9002921500021646    steps: 129     evaluation reward: 1.88\n",
      "episode: 1098   score: 4.0   memory length: 201005   epsilon: 0.9000040600021708    steps: 291     evaluation reward: 1.91\n",
      "episode: 1099   score: 1.0   memory length: 201192   epsilon: 0.8998189300021748    steps: 187     evaluation reward: 1.92\n",
      "episode: 1100   score: 0.0   memory length: 201322   epsilon: 0.8996902300021776    steps: 130     evaluation reward: 1.89\n",
      "episode: 1101   score: 1.0   memory length: 201503   epsilon: 0.8995110400021815    steps: 181     evaluation reward: 1.88\n",
      "episode: 1102   score: 1.0   memory length: 201663   epsilon: 0.899352640002185    steps: 160     evaluation reward: 1.88\n",
      "episode: 1103   score: 1.0   memory length: 201821   epsilon: 0.8991962200021884    steps: 158     evaluation reward: 1.85\n",
      "episode: 1104   score: 3.0   memory length: 202056   epsilon: 0.8989635700021934    steps: 235     evaluation reward: 1.88\n",
      "episode: 1105   score: 3.0   memory length: 202270   epsilon: 0.898751710002198    steps: 214     evaluation reward: 1.87\n",
      "episode: 1106   score: 2.0   memory length: 202484   epsilon: 0.8985398500022026    steps: 214     evaluation reward: 1.85\n",
      "episode: 1107   score: 4.0   memory length: 202749   epsilon: 0.8982775000022083    steps: 265     evaluation reward: 1.87\n",
      "episode: 1108   score: 2.0   memory length: 202954   epsilon: 0.8980745500022127    steps: 205     evaluation reward: 1.86\n",
      "episode: 1109   score: 2.0   memory length: 203156   epsilon: 0.897874570002217    steps: 202     evaluation reward: 1.87\n",
      "episode: 1110   score: 3.0   memory length: 203398   epsilon: 0.8976349900022222    steps: 242     evaluation reward: 1.88\n",
      "episode: 1111   score: 3.0   memory length: 203642   epsilon: 0.8973934300022275    steps: 244     evaluation reward: 1.89\n",
      "episode: 1112   score: 2.0   memory length: 203855   epsilon: 0.8971825600022321    steps: 213     evaluation reward: 1.85\n",
      "episode: 1113   score: 2.0   memory length: 204040   epsilon: 0.896999410002236    steps: 185     evaluation reward: 1.87\n",
      "episode: 1114   score: 2.0   memory length: 204244   epsilon: 0.8967974500022404    steps: 204     evaluation reward: 1.87\n",
      "episode: 1115   score: 3.0   memory length: 204502   epsilon: 0.896542030002246    steps: 258     evaluation reward: 1.9\n",
      "episode: 1116   score: 1.0   memory length: 204677   epsilon: 0.8963687800022497    steps: 175     evaluation reward: 1.9\n",
      "episode: 1117   score: 1.0   memory length: 204834   epsilon: 0.8962133500022531    steps: 157     evaluation reward: 1.88\n",
      "episode: 1118   score: 4.0   memory length: 205123   epsilon: 0.8959272400022593    steps: 289     evaluation reward: 1.89\n",
      "episode: 1119   score: 3.0   memory length: 205347   epsilon: 0.8957054800022641    steps: 224     evaluation reward: 1.92\n",
      "episode: 1120   score: 1.0   memory length: 205501   epsilon: 0.8955530200022674    steps: 154     evaluation reward: 1.89\n",
      "episode: 1121   score: 5.0   memory length: 205799   epsilon: 0.8952580000022738    steps: 298     evaluation reward: 1.93\n",
      "episode: 1122   score: 2.0   memory length: 205993   epsilon: 0.895065940002278    steps: 194     evaluation reward: 1.93\n",
      "episode: 1123   score: 2.0   memory length: 206199   epsilon: 0.8948620000022824    steps: 206     evaluation reward: 1.94\n",
      "episode: 1124   score: 0.0   memory length: 206330   epsilon: 0.8947323100022853    steps: 131     evaluation reward: 1.93\n",
      "episode: 1125   score: 0.0   memory length: 206466   epsilon: 0.8945976700022882    steps: 136     evaluation reward: 1.91\n",
      "episode: 1126   score: 2.0   memory length: 206676   epsilon: 0.8943897700022927    steps: 210     evaluation reward: 1.89\n",
      "episode: 1127   score: 1.0   memory length: 206853   epsilon: 0.8942145400022965    steps: 177     evaluation reward: 1.89\n",
      "episode: 1128   score: 1.0   memory length: 207011   epsilon: 0.8940581200022999    steps: 158     evaluation reward: 1.89\n",
      "episode: 1129   score: 5.0   memory length: 207313   epsilon: 0.8937591400023064    steps: 302     evaluation reward: 1.88\n",
      "episode: 1130   score: 2.0   memory length: 207538   epsilon: 0.8935363900023112    steps: 225     evaluation reward: 1.89\n",
      "episode: 1131   score: 4.0   memory length: 207830   epsilon: 0.8932473100023175    steps: 292     evaluation reward: 1.92\n",
      "episode: 1132   score: 0.0   memory length: 207966   epsilon: 0.8931126700023204    steps: 136     evaluation reward: 1.89\n",
      "episode: 1133   score: 1.0   memory length: 208137   epsilon: 0.8929433800023241    steps: 171     evaluation reward: 1.9\n",
      "episode: 1134   score: 1.0   memory length: 208311   epsilon: 0.8927711200023278    steps: 174     evaluation reward: 1.89\n",
      "episode: 1135   score: 0.0   memory length: 208446   epsilon: 0.8926374700023307    steps: 135     evaluation reward: 1.89\n",
      "episode: 1136   score: 2.0   memory length: 208628   epsilon: 0.8924572900023346    steps: 182     evaluation reward: 1.9\n",
      "episode: 1137   score: 2.0   memory length: 208849   epsilon: 0.8922385000023394    steps: 221     evaluation reward: 1.91\n",
      "episode: 1138   score: 1.0   memory length: 209030   epsilon: 0.8920593100023433    steps: 181     evaluation reward: 1.9\n",
      "episode: 1139   score: 7.0   memory length: 209422   epsilon: 0.8916712300023517    steps: 392     evaluation reward: 1.94\n",
      "episode: 1140   score: 0.0   memory length: 209546   epsilon: 0.8915484700023544    steps: 124     evaluation reward: 1.94\n",
      "episode: 1141   score: 1.0   memory length: 209724   epsilon: 0.8913722500023582    steps: 178     evaluation reward: 1.93\n",
      "episode: 1142   score: 2.0   memory length: 209929   epsilon: 0.8911693000023626    steps: 205     evaluation reward: 1.94\n",
      "episode: 1143   score: 1.0   memory length: 210086   epsilon: 0.891013870002366    steps: 157     evaluation reward: 1.94\n",
      "episode: 1144   score: 0.0   memory length: 210208   epsilon: 0.8908930900023686    steps: 122     evaluation reward: 1.93\n",
      "episode: 1145   score: 1.0   memory length: 210387   epsilon: 0.8907158800023725    steps: 179     evaluation reward: 1.94\n",
      "episode: 1146   score: 2.0   memory length: 210604   epsilon: 0.8905010500023771    steps: 217     evaluation reward: 1.92\n",
      "episode: 1147   score: 0.0   memory length: 210730   epsilon: 0.8903763100023798    steps: 126     evaluation reward: 1.89\n",
      "episode: 1148   score: 4.0   memory length: 211013   epsilon: 0.8900961400023859    steps: 283     evaluation reward: 1.89\n",
      "episode: 1149   score: 3.0   memory length: 211234   epsilon: 0.8898773500023907    steps: 221     evaluation reward: 1.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1150   score: 1.0   memory length: 211402   epsilon: 0.8897110300023943    steps: 168     evaluation reward: 1.89\n",
      "episode: 1151   score: 5.0   memory length: 211739   epsilon: 0.8893774000024015    steps: 337     evaluation reward: 1.92\n",
      "episode: 1152   score: 0.0   memory length: 211874   epsilon: 0.8892437500024044    steps: 135     evaluation reward: 1.89\n",
      "episode: 1153   score: 1.0   memory length: 212030   epsilon: 0.8890893100024078    steps: 156     evaluation reward: 1.87\n",
      "episode: 1154   score: 6.0   memory length: 212345   epsilon: 0.8887774600024145    steps: 315     evaluation reward: 1.91\n",
      "episode: 1155   score: 1.0   memory length: 212504   epsilon: 0.888620050002418    steps: 159     evaluation reward: 1.9\n",
      "episode: 1156   score: 1.0   memory length: 212666   epsilon: 0.8884596700024214    steps: 162     evaluation reward: 1.89\n",
      "episode: 1157   score: 1.0   memory length: 212825   epsilon: 0.8883022600024248    steps: 159     evaluation reward: 1.85\n",
      "episode: 1158   score: 3.0   memory length: 213053   epsilon: 0.8880765400024297    steps: 228     evaluation reward: 1.88\n",
      "episode: 1159   score: 2.0   memory length: 213289   epsilon: 0.8878429000024348    steps: 236     evaluation reward: 1.88\n",
      "episode: 1160   score: 5.0   memory length: 213613   epsilon: 0.8875221400024418    steps: 324     evaluation reward: 1.93\n",
      "episode: 1161   score: 4.0   memory length: 213881   epsilon: 0.8872568200024475    steps: 268     evaluation reward: 1.97\n",
      "episode: 1162   score: 1.0   memory length: 214057   epsilon: 0.8870825800024513    steps: 176     evaluation reward: 1.96\n",
      "episode: 1163   score: 2.0   memory length: 214289   epsilon: 0.8868529000024563    steps: 232     evaluation reward: 1.96\n",
      "episode: 1164   score: 3.0   memory length: 214523   epsilon: 0.8866212400024613    steps: 234     evaluation reward: 1.97\n",
      "episode: 1165   score: 2.0   memory length: 214730   epsilon: 0.8864163100024658    steps: 207     evaluation reward: 1.96\n",
      "episode: 1166   score: 1.0   memory length: 214901   epsilon: 0.8862470200024695    steps: 171     evaluation reward: 1.95\n",
      "episode: 1167   score: 0.0   memory length: 215029   epsilon: 0.8861203000024722    steps: 128     evaluation reward: 1.94\n",
      "episode: 1168   score: 0.0   memory length: 215166   epsilon: 0.8859846700024752    steps: 137     evaluation reward: 1.91\n",
      "episode: 1169   score: 4.0   memory length: 215472   epsilon: 0.8856817300024817    steps: 306     evaluation reward: 1.93\n",
      "episode: 1170   score: 3.0   memory length: 215715   epsilon: 0.885441160002487    steps: 243     evaluation reward: 1.95\n",
      "episode: 1171   score: 2.0   memory length: 215913   epsilon: 0.8852451400024912    steps: 198     evaluation reward: 1.96\n",
      "episode: 1172   score: 3.0   memory length: 216140   epsilon: 0.8850204100024961    steps: 227     evaluation reward: 1.96\n",
      "episode: 1173   score: 3.0   memory length: 216389   epsilon: 0.8847739000025014    steps: 249     evaluation reward: 1.95\n",
      "episode: 1174   score: 3.0   memory length: 216637   epsilon: 0.8845283800025068    steps: 248     evaluation reward: 1.97\n",
      "episode: 1175   score: 4.0   memory length: 216912   epsilon: 0.8842561300025127    steps: 275     evaluation reward: 1.98\n",
      "episode: 1176   score: 1.0   memory length: 217101   epsilon: 0.8840690200025167    steps: 189     evaluation reward: 1.98\n",
      "episode: 1177   score: 3.0   memory length: 217336   epsilon: 0.8838363700025218    steps: 235     evaluation reward: 1.99\n",
      "episode: 1178   score: 2.0   memory length: 217570   epsilon: 0.8836047100025268    steps: 234     evaluation reward: 2.01\n",
      "episode: 1179   score: 4.0   memory length: 217885   epsilon: 0.8832928600025336    steps: 315     evaluation reward: 2.02\n",
      "episode: 1180   score: 3.0   memory length: 218125   epsilon: 0.8830552600025388    steps: 240     evaluation reward: 2.03\n",
      "episode: 1181   score: 0.0   memory length: 218262   epsilon: 0.8829196300025417    steps: 137     evaluation reward: 2.02\n",
      "episode: 1182   score: 1.0   memory length: 218429   epsilon: 0.8827543000025453    steps: 167     evaluation reward: 2.0\n",
      "episode: 1183   score: 5.0   memory length: 218744   epsilon: 0.8824424500025521    steps: 315     evaluation reward: 2.03\n",
      "episode: 1184   score: 3.0   memory length: 218976   epsilon: 0.882212770002557    steps: 232     evaluation reward: 2.04\n",
      "episode: 1185   score: 1.0   memory length: 219144   epsilon: 0.8820464500025607    steps: 168     evaluation reward: 2.04\n",
      "episode: 1186   score: 5.0   memory length: 219514   epsilon: 0.8816801500025686    steps: 370     evaluation reward: 2.08\n",
      "episode: 1187   score: 1.0   memory length: 219672   epsilon: 0.881523730002572    steps: 158     evaluation reward: 2.07\n",
      "episode: 1188   score: 4.0   memory length: 219957   epsilon: 0.8812415800025781    steps: 285     evaluation reward: 2.11\n",
      "episode: 1189   score: 3.0   memory length: 220196   epsilon: 0.8810049700025833    steps: 239     evaluation reward: 2.14\n",
      "episode: 1190   score: 3.0   memory length: 220447   epsilon: 0.8807564800025887    steps: 251     evaluation reward: 2.13\n",
      "episode: 1191   score: 0.0   memory length: 220586   epsilon: 0.8806188700025916    steps: 139     evaluation reward: 2.12\n",
      "episode: 1192   score: 3.0   memory length: 220804   epsilon: 0.8804030500025963    steps: 218     evaluation reward: 2.13\n",
      "episode: 1193   score: 6.0   memory length: 221165   epsilon: 0.8800456600026041    steps: 361     evaluation reward: 2.17\n",
      "episode: 1194   score: 2.0   memory length: 221385   epsilon: 0.8798278600026088    steps: 220     evaluation reward: 2.17\n",
      "episode: 1195   score: 4.0   memory length: 221656   epsilon: 0.8795595700026146    steps: 271     evaluation reward: 2.19\n",
      "episode: 1196   score: 2.0   memory length: 221861   epsilon: 0.879356620002619    steps: 205     evaluation reward: 2.18\n",
      "episode: 1197   score: 1.0   memory length: 222042   epsilon: 0.8791774300026229    steps: 181     evaluation reward: 2.19\n",
      "episode: 1198   score: 2.0   memory length: 222249   epsilon: 0.8789725000026274    steps: 207     evaluation reward: 2.17\n",
      "episode: 1199   score: 4.0   memory length: 222533   epsilon: 0.8786913400026335    steps: 284     evaluation reward: 2.2\n",
      "episode: 1200   score: 3.0   memory length: 222762   epsilon: 0.8784646300026384    steps: 229     evaluation reward: 2.23\n",
      "episode: 1201   score: 2.0   memory length: 222977   epsilon: 0.878251780002643    steps: 215     evaluation reward: 2.24\n",
      "episode: 1202   score: 2.0   memory length: 223217   epsilon: 0.8780141800026482    steps: 240     evaluation reward: 2.25\n",
      "episode: 1203   score: 4.0   memory length: 223514   epsilon: 0.8777201500026546    steps: 297     evaluation reward: 2.28\n",
      "episode: 1204   score: 3.0   memory length: 223756   epsilon: 0.8774805700026598    steps: 242     evaluation reward: 2.28\n",
      "episode: 1205   score: 0.0   memory length: 223884   epsilon: 0.8773538500026625    steps: 128     evaluation reward: 2.25\n",
      "episode: 1206   score: 4.0   memory length: 224164   epsilon: 0.8770766500026685    steps: 280     evaluation reward: 2.27\n",
      "episode: 1207   score: 0.0   memory length: 224286   epsilon: 0.8769558700026712    steps: 122     evaluation reward: 2.23\n",
      "episode: 1208   score: 5.0   memory length: 224659   epsilon: 0.8765866000026792    steps: 373     evaluation reward: 2.26\n",
      "episode: 1209   score: 0.0   memory length: 224793   epsilon: 0.8764539400026821    steps: 134     evaluation reward: 2.24\n",
      "episode: 1210   score: 4.0   memory length: 225057   epsilon: 0.8761925800026877    steps: 264     evaluation reward: 2.25\n",
      "episode: 1211   score: 4.0   memory length: 225328   epsilon: 0.8759242900026936    steps: 271     evaluation reward: 2.26\n",
      "episode: 1212   score: 4.0   memory length: 225615   epsilon: 0.8756401600026997    steps: 287     evaluation reward: 2.28\n",
      "episode: 1213   score: 3.0   memory length: 225864   epsilon: 0.8753936500027051    steps: 249     evaluation reward: 2.29\n",
      "episode: 1214   score: 0.0   memory length: 225999   epsilon: 0.875260000002708    steps: 135     evaluation reward: 2.27\n",
      "episode: 1215   score: 0.0   memory length: 226125   epsilon: 0.8751352600027107    steps: 126     evaluation reward: 2.24\n",
      "episode: 1216   score: 3.0   memory length: 226343   epsilon: 0.8749194400027154    steps: 218     evaluation reward: 2.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1217   score: 2.0   memory length: 226557   epsilon: 0.87470758000272    steps: 214     evaluation reward: 2.27\n",
      "episode: 1218   score: 2.0   memory length: 226768   epsilon: 0.8744986900027245    steps: 211     evaluation reward: 2.25\n",
      "episode: 1219   score: 3.0   memory length: 226984   epsilon: 0.8742848500027292    steps: 216     evaluation reward: 2.25\n",
      "episode: 1220   score: 1.0   memory length: 227144   epsilon: 0.8741264500027326    steps: 160     evaluation reward: 2.25\n",
      "episode: 1221   score: 0.0   memory length: 227272   epsilon: 0.8739997300027353    steps: 128     evaluation reward: 2.2\n",
      "episode: 1222   score: 0.0   memory length: 227402   epsilon: 0.8738710300027381    steps: 130     evaluation reward: 2.18\n",
      "episode: 1223   score: 2.0   memory length: 227596   epsilon: 0.8736789700027423    steps: 194     evaluation reward: 2.18\n",
      "episode: 1224   score: 3.0   memory length: 227838   epsilon: 0.8734393900027475    steps: 242     evaluation reward: 2.21\n",
      "episode: 1225   score: 1.0   memory length: 228016   epsilon: 0.8732631700027513    steps: 178     evaluation reward: 2.22\n",
      "episode: 1226   score: 0.0   memory length: 228147   epsilon: 0.8731334800027541    steps: 131     evaluation reward: 2.2\n",
      "episode: 1227   score: 2.0   memory length: 228349   epsilon: 0.8729335000027585    steps: 202     evaluation reward: 2.21\n",
      "episode: 1228   score: 6.0   memory length: 228709   epsilon: 0.8725771000027662    steps: 360     evaluation reward: 2.26\n",
      "episode: 1229   score: 1.0   memory length: 228869   epsilon: 0.8724187000027697    steps: 160     evaluation reward: 2.22\n",
      "episode: 1230   score: 1.0   memory length: 229026   epsilon: 0.872263270002773    steps: 157     evaluation reward: 2.21\n",
      "episode: 1231   score: 1.0   memory length: 229215   epsilon: 0.8720761600027771    steps: 189     evaluation reward: 2.18\n",
      "episode: 1232   score: 1.0   memory length: 229397   epsilon: 0.871895980002781    steps: 182     evaluation reward: 2.19\n",
      "episode: 1233   score: 4.0   memory length: 229678   epsilon: 0.871617790002787    steps: 281     evaluation reward: 2.22\n",
      "episode: 1234   score: 4.0   memory length: 229947   epsilon: 0.8713514800027928    steps: 269     evaluation reward: 2.25\n",
      "episode: 1235   score: 2.0   memory length: 230152   epsilon: 0.8711485300027972    steps: 205     evaluation reward: 2.27\n",
      "episode: 1236   score: 2.0   memory length: 230385   epsilon: 0.8709178600028022    steps: 233     evaluation reward: 2.27\n",
      "episode: 1237   score: 2.0   memory length: 230582   epsilon: 0.8707228300028065    steps: 197     evaluation reward: 2.27\n",
      "episode: 1238   score: 4.0   memory length: 230862   epsilon: 0.8704456300028125    steps: 280     evaluation reward: 2.3\n",
      "episode: 1239   score: 0.0   memory length: 230990   epsilon: 0.8703189100028152    steps: 128     evaluation reward: 2.23\n",
      "episode: 1240   score: 2.0   memory length: 231174   epsilon: 0.8701367500028192    steps: 184     evaluation reward: 2.25\n",
      "episode: 1241   score: 1.0   memory length: 231350   epsilon: 0.869962510002823    steps: 176     evaluation reward: 2.25\n",
      "episode: 1242   score: 1.0   memory length: 231507   epsilon: 0.8698070800028264    steps: 157     evaluation reward: 2.24\n",
      "episode: 1243   score: 0.0   memory length: 231646   epsilon: 0.8696694700028293    steps: 139     evaluation reward: 2.23\n",
      "episode: 1244   score: 3.0   memory length: 231905   epsilon: 0.8694130600028349    steps: 259     evaluation reward: 2.26\n",
      "episode: 1245   score: 2.0   memory length: 232118   epsilon: 0.8692021900028395    steps: 213     evaluation reward: 2.27\n",
      "episode: 1246   score: 2.0   memory length: 232343   epsilon: 0.8689794400028443    steps: 225     evaluation reward: 2.27\n",
      "episode: 1247   score: 1.0   memory length: 232496   epsilon: 0.8688279700028476    steps: 153     evaluation reward: 2.28\n",
      "episode: 1248   score: 3.0   memory length: 232737   epsilon: 0.8685893800028528    steps: 241     evaluation reward: 2.27\n",
      "episode: 1249   score: 0.0   memory length: 232881   epsilon: 0.8684468200028559    steps: 144     evaluation reward: 2.24\n",
      "episode: 1250   score: 2.0   memory length: 233073   epsilon: 0.86825674000286    steps: 192     evaluation reward: 2.25\n",
      "episode: 1251   score: 7.0   memory length: 233327   epsilon: 0.8680052800028655    steps: 254     evaluation reward: 2.27\n",
      "episode: 1252   score: 2.0   memory length: 233528   epsilon: 0.8678062900028698    steps: 201     evaluation reward: 2.29\n",
      "episode: 1253   score: 2.0   memory length: 233736   epsilon: 0.8676003700028743    steps: 208     evaluation reward: 2.3\n",
      "episode: 1254   score: 3.0   memory length: 233977   epsilon: 0.8673617800028794    steps: 241     evaluation reward: 2.27\n",
      "episode: 1255   score: 2.0   memory length: 234201   epsilon: 0.8671400200028843    steps: 224     evaluation reward: 2.28\n",
      "episode: 1256   score: 1.0   memory length: 234358   epsilon: 0.8669845900028876    steps: 157     evaluation reward: 2.28\n",
      "episode: 1257   score: 1.0   memory length: 234518   epsilon: 0.8668261900028911    steps: 160     evaluation reward: 2.28\n",
      "episode: 1258   score: 1.0   memory length: 234680   epsilon: 0.8666658100028946    steps: 162     evaluation reward: 2.26\n",
      "episode: 1259   score: 6.0   memory length: 235030   epsilon: 0.8663193100029021    steps: 350     evaluation reward: 2.3\n",
      "episode: 1260   score: 2.0   memory length: 235213   epsilon: 0.866138140002906    steps: 183     evaluation reward: 2.27\n",
      "episode: 1261   score: 0.0   memory length: 235347   epsilon: 0.8660054800029089    steps: 134     evaluation reward: 2.23\n",
      "episode: 1262   score: 2.0   memory length: 235539   epsilon: 0.865815400002913    steps: 192     evaluation reward: 2.24\n",
      "episode: 1263   score: 2.0   memory length: 235750   epsilon: 0.8656065100029176    steps: 211     evaluation reward: 2.24\n",
      "episode: 1264   score: 1.0   memory length: 235931   epsilon: 0.8654273200029214    steps: 181     evaluation reward: 2.22\n",
      "episode: 1265   score: 3.0   memory length: 236145   epsilon: 0.865215460002926    steps: 214     evaluation reward: 2.23\n",
      "episode: 1266   score: 1.0   memory length: 236303   epsilon: 0.8650590400029294    steps: 158     evaluation reward: 2.23\n",
      "episode: 1267   score: 2.0   memory length: 236535   epsilon: 0.8648293600029344    steps: 232     evaluation reward: 2.25\n",
      "episode: 1268   score: 6.0   memory length: 236910   epsilon: 0.8644581100029425    steps: 375     evaluation reward: 2.31\n",
      "episode: 1269   score: 2.0   memory length: 237124   epsilon: 0.8642462500029471    steps: 214     evaluation reward: 2.29\n",
      "episode: 1270   score: 3.0   memory length: 237359   epsilon: 0.8640136000029521    steps: 235     evaluation reward: 2.29\n",
      "episode: 1271   score: 3.0   memory length: 237604   epsilon: 0.8637710500029574    steps: 245     evaluation reward: 2.3\n",
      "episode: 1272   score: 2.0   memory length: 237817   epsilon: 0.863560180002962    steps: 213     evaluation reward: 2.29\n",
      "episode: 1273   score: 3.0   memory length: 238069   epsilon: 0.8633107000029674    steps: 252     evaluation reward: 2.29\n",
      "episode: 1274   score: 6.0   memory length: 238463   epsilon: 0.8629206400029759    steps: 394     evaluation reward: 2.32\n",
      "episode: 1275   score: 1.0   memory length: 238638   epsilon: 0.8627473900029796    steps: 175     evaluation reward: 2.29\n",
      "episode: 1276   score: 3.0   memory length: 238892   epsilon: 0.8624959300029851    steps: 254     evaluation reward: 2.31\n",
      "episode: 1277   score: 1.0   memory length: 239047   epsilon: 0.8623424800029884    steps: 155     evaluation reward: 2.29\n",
      "episode: 1278   score: 2.0   memory length: 239264   epsilon: 0.8621276500029931    steps: 217     evaluation reward: 2.29\n",
      "episode: 1279   score: 0.0   memory length: 239398   epsilon: 0.861994990002996    steps: 134     evaluation reward: 2.25\n",
      "episode: 1280   score: 2.0   memory length: 239588   epsilon: 0.861806890003    steps: 190     evaluation reward: 2.24\n",
      "episode: 1281   score: 4.0   memory length: 239884   epsilon: 0.8615138500030064    steps: 296     evaluation reward: 2.28\n",
      "episode: 1282   score: 1.0   memory length: 240038   epsilon: 0.8613613900030097    steps: 154     evaluation reward: 2.28\n",
      "episode: 1283   score: 2.0   memory length: 240249   epsilon: 0.8611525000030142    steps: 211     evaluation reward: 2.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1284   score: 1.0   memory length: 240406   epsilon: 0.8609970700030176    steps: 157     evaluation reward: 2.23\n",
      "episode: 1285   score: 3.0   memory length: 240680   epsilon: 0.8607258100030235    steps: 274     evaluation reward: 2.25\n",
      "episode: 1286   score: 2.0   memory length: 240880   epsilon: 0.8605278100030278    steps: 200     evaluation reward: 2.22\n",
      "episode: 1287   score: 2.0   memory length: 241081   epsilon: 0.8603288200030321    steps: 201     evaluation reward: 2.23\n",
      "episode: 1288   score: 3.0   memory length: 241349   epsilon: 0.8600635000030379    steps: 268     evaluation reward: 2.22\n",
      "episode: 1289   score: 3.0   memory length: 241582   epsilon: 0.8598328300030429    steps: 233     evaluation reward: 2.22\n",
      "episode: 1290   score: 1.0   memory length: 241742   epsilon: 0.8596744300030463    steps: 160     evaluation reward: 2.2\n",
      "episode: 1291   score: 0.0   memory length: 241874   epsilon: 0.8595437500030492    steps: 132     evaluation reward: 2.2\n",
      "episode: 1292   score: 3.0   memory length: 242128   epsilon: 0.8592922900030546    steps: 254     evaluation reward: 2.2\n",
      "episode: 1293   score: 1.0   memory length: 242314   epsilon: 0.8591081500030586    steps: 186     evaluation reward: 2.15\n",
      "episode: 1294   score: 3.0   memory length: 242547   epsilon: 0.8588774800030636    steps: 233     evaluation reward: 2.16\n",
      "episode: 1295   score: 1.0   memory length: 242710   epsilon: 0.8587161100030671    steps: 163     evaluation reward: 2.13\n",
      "episode: 1296   score: 5.0   memory length: 243065   epsilon: 0.8583646600030748    steps: 355     evaluation reward: 2.16\n",
      "episode: 1297   score: 2.0   memory length: 243249   epsilon: 0.8581825000030787    steps: 184     evaluation reward: 2.17\n",
      "episode: 1298   score: 3.0   memory length: 243477   epsilon: 0.8579567800030836    steps: 228     evaluation reward: 2.18\n",
      "episode: 1299   score: 1.0   memory length: 243659   epsilon: 0.8577766000030875    steps: 182     evaluation reward: 2.15\n",
      "episode: 1300   score: 1.0   memory length: 243813   epsilon: 0.8576241400030908    steps: 154     evaluation reward: 2.13\n",
      "episode: 1301   score: 3.0   memory length: 244053   epsilon: 0.857386540003096    steps: 240     evaluation reward: 2.14\n",
      "episode: 1302   score: 5.0   memory length: 244374   epsilon: 0.8570687500031029    steps: 321     evaluation reward: 2.17\n",
      "episode: 1303   score: 3.0   memory length: 244612   epsilon: 0.856833130003108    steps: 238     evaluation reward: 2.16\n",
      "episode: 1304   score: 2.0   memory length: 244814   epsilon: 0.8566331500031124    steps: 202     evaluation reward: 2.15\n",
      "episode: 1305   score: 4.0   memory length: 245136   epsilon: 0.8563143700031193    steps: 322     evaluation reward: 2.19\n",
      "episode: 1306   score: 2.0   memory length: 245333   epsilon: 0.8561193400031235    steps: 197     evaluation reward: 2.17\n",
      "episode: 1307   score: 2.0   memory length: 245553   epsilon: 0.8559015400031282    steps: 220     evaluation reward: 2.19\n",
      "episode: 1308   score: 5.0   memory length: 245861   epsilon: 0.8555966200031349    steps: 308     evaluation reward: 2.19\n",
      "episode: 1309   score: 2.0   memory length: 246064   epsilon: 0.8553956500031392    steps: 203     evaluation reward: 2.21\n",
      "episode: 1310   score: 2.0   memory length: 246264   epsilon: 0.8551976500031435    steps: 200     evaluation reward: 2.19\n",
      "episode: 1311   score: 3.0   memory length: 246502   epsilon: 0.8549620300031486    steps: 238     evaluation reward: 2.18\n",
      "episode: 1312   score: 3.0   memory length: 246751   epsilon: 0.854715520003154    steps: 249     evaluation reward: 2.17\n",
      "episode: 1313   score: 1.0   memory length: 246906   epsilon: 0.8545620700031573    steps: 155     evaluation reward: 2.15\n",
      "episode: 1314   score: 3.0   memory length: 247158   epsilon: 0.8543125900031627    steps: 252     evaluation reward: 2.18\n",
      "episode: 1315   score: 1.0   memory length: 247335   epsilon: 0.8541373600031665    steps: 177     evaluation reward: 2.19\n",
      "episode: 1316   score: 2.0   memory length: 247521   epsilon: 0.8539532200031705    steps: 186     evaluation reward: 2.18\n",
      "episode: 1317   score: 4.0   memory length: 247787   epsilon: 0.8536898800031762    steps: 266     evaluation reward: 2.2\n",
      "episode: 1318   score: 1.0   memory length: 247941   epsilon: 0.8535374200031796    steps: 154     evaluation reward: 2.19\n",
      "episode: 1319   score: 2.0   memory length: 248168   epsilon: 0.8533126900031844    steps: 227     evaluation reward: 2.18\n",
      "episode: 1320   score: 2.0   memory length: 248354   epsilon: 0.8531285500031884    steps: 186     evaluation reward: 2.19\n",
      "episode: 1321   score: 2.0   memory length: 248557   epsilon: 0.8529275800031928    steps: 203     evaluation reward: 2.21\n",
      "episode: 1322   score: 2.0   memory length: 248760   epsilon: 0.8527266100031972    steps: 203     evaluation reward: 2.23\n",
      "episode: 1323   score: 0.0   memory length: 248885   epsilon: 0.8526028600031998    steps: 125     evaluation reward: 2.21\n",
      "episode: 1324   score: 3.0   memory length: 249154   epsilon: 0.8523365500032056    steps: 269     evaluation reward: 2.21\n",
      "episode: 1325   score: 2.0   memory length: 249356   epsilon: 0.85213657000321    steps: 202     evaluation reward: 2.22\n",
      "episode: 1326   score: 3.0   memory length: 249608   epsilon: 0.8518870900032154    steps: 252     evaluation reward: 2.25\n",
      "episode: 1327   score: 3.0   memory length: 249869   epsilon: 0.851628700003221    steps: 261     evaluation reward: 2.26\n",
      "now time :  2018-12-12 11:02:10.039735\n",
      "episode: 1328   score: 4.0   memory length: 250160   epsilon: 0.8513406100032272    steps: 291     evaluation reward: 2.24\n",
      "episode: 1329   score: 2.0   memory length: 250377   epsilon: 0.8511257800032319    steps: 217     evaluation reward: 2.25\n",
      "episode: 1330   score: 2.0   memory length: 250583   epsilon: 0.8509218400032363    steps: 206     evaluation reward: 2.26\n",
      "episode: 1331   score: 1.0   memory length: 250761   epsilon: 0.8507456200032402    steps: 178     evaluation reward: 2.26\n",
      "episode: 1332   score: 2.0   memory length: 250956   epsilon: 0.8505525700032444    steps: 195     evaluation reward: 2.27\n",
      "episode: 1333   score: 5.0   memory length: 251268   epsilon: 0.8502436900032511    steps: 312     evaluation reward: 2.28\n",
      "episode: 1334   score: 3.0   memory length: 251482   epsilon: 0.8500318300032557    steps: 214     evaluation reward: 2.27\n",
      "episode: 1335   score: 1.0   memory length: 251658   epsilon: 0.8498575900032594    steps: 176     evaluation reward: 2.26\n",
      "episode: 1336   score: 0.0   memory length: 251785   epsilon: 0.8497318600032622    steps: 127     evaluation reward: 2.24\n",
      "episode: 1337   score: 3.0   memory length: 252048   epsilon: 0.8494714900032678    steps: 263     evaluation reward: 2.25\n",
      "episode: 1338   score: 0.0   memory length: 252171   epsilon: 0.8493497200032705    steps: 123     evaluation reward: 2.21\n",
      "episode: 1339   score: 1.0   memory length: 252355   epsilon: 0.8491675600032744    steps: 184     evaluation reward: 2.22\n",
      "episode: 1340   score: 1.0   memory length: 252522   epsilon: 0.849002230003278    steps: 167     evaluation reward: 2.21\n",
      "episode: 1341   score: 3.0   memory length: 252751   epsilon: 0.8487755200032829    steps: 229     evaluation reward: 2.23\n",
      "episode: 1342   score: 1.0   memory length: 252910   epsilon: 0.8486181100032864    steps: 159     evaluation reward: 2.23\n",
      "episode: 1343   score: 6.0   memory length: 253313   epsilon: 0.848219140003295    steps: 403     evaluation reward: 2.29\n",
      "episode: 1344   score: 2.0   memory length: 253500   epsilon: 0.848034010003299    steps: 187     evaluation reward: 2.28\n",
      "episode: 1345   score: 2.0   memory length: 253687   epsilon: 0.847848880003303    steps: 187     evaluation reward: 2.28\n",
      "episode: 1346   score: 2.0   memory length: 253887   epsilon: 0.8476508800033073    steps: 200     evaluation reward: 2.28\n",
      "episode: 1347   score: 5.0   memory length: 254200   epsilon: 0.8473410100033141    steps: 313     evaluation reward: 2.32\n",
      "episode: 1348   score: 2.0   memory length: 254389   epsilon: 0.8471539000033181    steps: 189     evaluation reward: 2.31\n",
      "episode: 1349   score: 2.0   memory length: 254611   epsilon: 0.8469341200033229    steps: 222     evaluation reward: 2.33\n",
      "episode: 1350   score: 2.0   memory length: 254847   epsilon: 0.846700480003328    steps: 236     evaluation reward: 2.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1351   score: 4.0   memory length: 255152   epsilon: 0.8463985300033345    steps: 305     evaluation reward: 2.3\n",
      "episode: 1352   score: 5.0   memory length: 255510   epsilon: 0.8460441100033422    steps: 358     evaluation reward: 2.33\n",
      "episode: 1353   score: 3.0   memory length: 255748   epsilon: 0.8458084900033473    steps: 238     evaluation reward: 2.34\n",
      "episode: 1354   score: 0.0   memory length: 255888   epsilon: 0.8456698900033504    steps: 140     evaluation reward: 2.31\n",
      "episode: 1355   score: 4.0   memory length: 256196   epsilon: 0.845364970003357    steps: 308     evaluation reward: 2.33\n",
      "episode: 1356   score: 1.0   memory length: 256367   epsilon: 0.8451956800033606    steps: 171     evaluation reward: 2.33\n",
      "episode: 1357   score: 2.0   memory length: 256577   epsilon: 0.8449877800033652    steps: 210     evaluation reward: 2.34\n",
      "episode: 1358   score: 3.0   memory length: 256824   epsilon: 0.8447432500033705    steps: 247     evaluation reward: 2.36\n",
      "episode: 1359   score: 9.0   memory length: 257211   epsilon: 0.8443601200033788    steps: 387     evaluation reward: 2.39\n",
      "episode: 1360   score: 1.0   memory length: 257388   epsilon: 0.8441848900033826    steps: 177     evaluation reward: 2.38\n",
      "episode: 1361   score: 1.0   memory length: 257545   epsilon: 0.844029460003386    steps: 157     evaluation reward: 2.39\n",
      "episode: 1362   score: 3.0   memory length: 257763   epsilon: 0.8438136400033907    steps: 218     evaluation reward: 2.4\n",
      "episode: 1363   score: 1.0   memory length: 257947   epsilon: 0.8436314800033946    steps: 184     evaluation reward: 2.39\n",
      "episode: 1364   score: 2.0   memory length: 258132   epsilon: 0.8434483300033986    steps: 185     evaluation reward: 2.4\n",
      "episode: 1365   score: 1.0   memory length: 258311   epsilon: 0.8432711200034024    steps: 179     evaluation reward: 2.38\n",
      "episode: 1366   score: 0.0   memory length: 258454   epsilon: 0.8431295500034055    steps: 143     evaluation reward: 2.37\n",
      "episode: 1367   score: 0.0   memory length: 258580   epsilon: 0.8430048100034082    steps: 126     evaluation reward: 2.35\n",
      "episode: 1368   score: 4.0   memory length: 258858   epsilon: 0.8427295900034142    steps: 278     evaluation reward: 2.33\n",
      "episode: 1369   score: 3.0   memory length: 259110   epsilon: 0.8424801100034196    steps: 252     evaluation reward: 2.34\n",
      "episode: 1370   score: 0.0   memory length: 259244   epsilon: 0.8423474500034225    steps: 134     evaluation reward: 2.31\n",
      "episode: 1371   score: 0.0   memory length: 259373   epsilon: 0.8422197400034253    steps: 129     evaluation reward: 2.28\n",
      "episode: 1372   score: 3.0   memory length: 259600   epsilon: 0.8419950100034301    steps: 227     evaluation reward: 2.29\n",
      "episode: 1373   score: 4.0   memory length: 259908   epsilon: 0.8416900900034368    steps: 308     evaluation reward: 2.3\n",
      "episode: 1374   score: 7.0   memory length: 260282   epsilon: 0.8413198300034448    steps: 374     evaluation reward: 2.31\n",
      "episode: 1375   score: 2.0   memory length: 260491   epsilon: 0.8411129200034493    steps: 209     evaluation reward: 2.32\n",
      "episode: 1376   score: 1.0   memory length: 260671   epsilon: 0.8409347200034532    steps: 180     evaluation reward: 2.3\n",
      "episode: 1377   score: 0.0   memory length: 260804   epsilon: 0.840803050003456    steps: 133     evaluation reward: 2.29\n",
      "episode: 1378   score: 2.0   memory length: 261037   epsilon: 0.840572380003461    steps: 233     evaluation reward: 2.29\n",
      "episode: 1379   score: 1.0   memory length: 261209   epsilon: 0.8404021000034647    steps: 172     evaluation reward: 2.3\n",
      "episode: 1380   score: 1.0   memory length: 261394   epsilon: 0.8402189500034687    steps: 185     evaluation reward: 2.29\n",
      "episode: 1381   score: 2.0   memory length: 261597   epsilon: 0.840017980003473    steps: 203     evaluation reward: 2.27\n",
      "episode: 1382   score: 3.0   memory length: 261807   epsilon: 0.8398100800034776    steps: 210     evaluation reward: 2.29\n",
      "episode: 1383   score: 4.0   memory length: 262093   epsilon: 0.8395269400034837    steps: 286     evaluation reward: 2.31\n",
      "episode: 1384   score: 6.0   memory length: 262470   epsilon: 0.8391537100034918    steps: 377     evaluation reward: 2.36\n",
      "episode: 1385   score: 0.0   memory length: 262596   epsilon: 0.8390289700034945    steps: 126     evaluation reward: 2.33\n",
      "episode: 1386   score: 4.0   memory length: 262888   epsilon: 0.8387398900035008    steps: 292     evaluation reward: 2.35\n",
      "episode: 1387   score: 7.0   memory length: 263335   epsilon: 0.8382973600035104    steps: 447     evaluation reward: 2.4\n",
      "episode: 1388   score: 1.0   memory length: 263514   epsilon: 0.8381201500035143    steps: 179     evaluation reward: 2.38\n",
      "episode: 1389   score: 3.0   memory length: 263758   epsilon: 0.8378785900035195    steps: 244     evaluation reward: 2.38\n",
      "episode: 1390   score: 7.0   memory length: 264183   epsilon: 0.8374578400035286    steps: 425     evaluation reward: 2.44\n",
      "episode: 1391   score: 2.0   memory length: 264367   epsilon: 0.8372756800035326    steps: 184     evaluation reward: 2.46\n",
      "episode: 1392   score: 2.0   memory length: 264594   epsilon: 0.8370509500035375    steps: 227     evaluation reward: 2.45\n",
      "episode: 1393   score: 2.0   memory length: 264810   epsilon: 0.8368371100035421    steps: 216     evaluation reward: 2.46\n",
      "episode: 1394   score: 1.0   memory length: 264976   epsilon: 0.8366727700035457    steps: 166     evaluation reward: 2.44\n",
      "episode: 1395   score: 6.0   memory length: 265311   epsilon: 0.8363411200035529    steps: 335     evaluation reward: 2.49\n",
      "episode: 1396   score: 3.0   memory length: 265540   epsilon: 0.8361144100035578    steps: 229     evaluation reward: 2.47\n",
      "episode: 1397   score: 0.0   memory length: 265671   epsilon: 0.8359847200035606    steps: 131     evaluation reward: 2.45\n",
      "episode: 1398   score: 2.0   memory length: 265898   epsilon: 0.8357599900035655    steps: 227     evaluation reward: 2.44\n",
      "episode: 1399   score: 1.0   memory length: 266058   epsilon: 0.8356015900035689    steps: 160     evaluation reward: 2.44\n",
      "episode: 1400   score: 0.0   memory length: 266184   epsilon: 0.8354768500035716    steps: 126     evaluation reward: 2.43\n",
      "episode: 1401   score: 3.0   memory length: 266422   epsilon: 0.8352412300035768    steps: 238     evaluation reward: 2.43\n",
      "episode: 1402   score: 1.0   memory length: 266584   epsilon: 0.8350808500035802    steps: 162     evaluation reward: 2.39\n",
      "episode: 1403   score: 1.0   memory length: 266742   epsilon: 0.8349244300035836    steps: 158     evaluation reward: 2.37\n",
      "episode: 1404   score: 1.0   memory length: 266923   epsilon: 0.8347452400035875    steps: 181     evaluation reward: 2.36\n",
      "episode: 1405   score: 3.0   memory length: 267169   epsilon: 0.8345017000035928    steps: 246     evaluation reward: 2.35\n",
      "episode: 1406   score: 4.0   memory length: 267447   epsilon: 0.8342264800035988    steps: 278     evaluation reward: 2.37\n",
      "episode: 1407   score: 3.0   memory length: 267662   epsilon: 0.8340136300036034    steps: 215     evaluation reward: 2.38\n",
      "episode: 1408   score: 1.0   memory length: 267825   epsilon: 0.8338522600036069    steps: 163     evaluation reward: 2.34\n",
      "episode: 1409   score: 6.0   memory length: 268198   epsilon: 0.8334829900036149    steps: 373     evaluation reward: 2.38\n",
      "episode: 1410   score: 2.0   memory length: 268415   epsilon: 0.8332681600036196    steps: 217     evaluation reward: 2.38\n",
      "episode: 1411   score: 2.0   memory length: 268600   epsilon: 0.8330850100036236    steps: 185     evaluation reward: 2.37\n",
      "episode: 1412   score: 2.0   memory length: 268802   epsilon: 0.8328850300036279    steps: 202     evaluation reward: 2.36\n",
      "episode: 1413   score: 1.0   memory length: 268961   epsilon: 0.8327276200036313    steps: 159     evaluation reward: 2.36\n",
      "episode: 1414   score: 0.0   memory length: 269088   epsilon: 0.832601890003634    steps: 127     evaluation reward: 2.33\n",
      "episode: 1415   score: 3.0   memory length: 269319   epsilon: 0.832373200003639    steps: 231     evaluation reward: 2.35\n",
      "episode: 1416   score: 5.0   memory length: 269691   epsilon: 0.832004920003647    steps: 372     evaluation reward: 2.38\n",
      "episode: 1417   score: 3.0   memory length: 269970   epsilon: 0.831728710003653    steps: 279     evaluation reward: 2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1418   score: 1.0   memory length: 270151   epsilon: 0.8315495200036569    steps: 181     evaluation reward: 2.37\n",
      "episode: 1419   score: 5.0   memory length: 270523   epsilon: 0.8311812400036649    steps: 372     evaluation reward: 2.4\n",
      "episode: 1420   score: 4.0   memory length: 270811   epsilon: 0.8308961200036711    steps: 288     evaluation reward: 2.42\n",
      "episode: 1421   score: 3.0   memory length: 271044   epsilon: 0.8306654500036761    steps: 233     evaluation reward: 2.43\n",
      "episode: 1422   score: 1.0   memory length: 271201   epsilon: 0.8305100200036795    steps: 157     evaluation reward: 2.42\n",
      "episode: 1423   score: 3.0   memory length: 271432   epsilon: 0.8302813300036844    steps: 231     evaluation reward: 2.45\n",
      "episode: 1424   score: 1.0   memory length: 271588   epsilon: 0.8301268900036878    steps: 156     evaluation reward: 2.43\n",
      "episode: 1425   score: 2.0   memory length: 271778   epsilon: 0.8299387900036919    steps: 190     evaluation reward: 2.43\n",
      "episode: 1426   score: 5.0   memory length: 272086   epsilon: 0.8296338700036985    steps: 308     evaluation reward: 2.45\n",
      "episode: 1427   score: 2.0   memory length: 272295   epsilon: 0.829426960003703    steps: 209     evaluation reward: 2.44\n",
      "episode: 1428   score: 1.0   memory length: 272461   epsilon: 0.8292626200037065    steps: 166     evaluation reward: 2.41\n",
      "episode: 1429   score: 1.0   memory length: 272623   epsilon: 0.82910224000371    steps: 162     evaluation reward: 2.4\n",
      "episode: 1430   score: 2.0   memory length: 272843   epsilon: 0.8288844400037148    steps: 220     evaluation reward: 2.4\n",
      "episode: 1431   score: 7.0   memory length: 273237   epsilon: 0.8284943800037232    steps: 394     evaluation reward: 2.46\n",
      "episode: 1432   score: 1.0   memory length: 273402   epsilon: 0.8283310300037268    steps: 165     evaluation reward: 2.45\n",
      "episode: 1433   score: 2.0   memory length: 273608   epsilon: 0.8281270900037312    steps: 206     evaluation reward: 2.42\n",
      "episode: 1434   score: 3.0   memory length: 273853   epsilon: 0.8278845400037365    steps: 245     evaluation reward: 2.42\n",
      "episode: 1435   score: 4.0   memory length: 274150   epsilon: 0.8275905100037428    steps: 297     evaluation reward: 2.45\n",
      "episode: 1436   score: 0.0   memory length: 274274   epsilon: 0.8274677500037455    steps: 124     evaluation reward: 2.45\n",
      "episode: 1437   score: 0.0   memory length: 274405   epsilon: 0.8273380600037483    steps: 131     evaluation reward: 2.42\n",
      "episode: 1438   score: 2.0   memory length: 274617   epsilon: 0.8271281800037529    steps: 212     evaluation reward: 2.44\n",
      "episode: 1439   score: 3.0   memory length: 274886   epsilon: 0.8268618700037587    steps: 269     evaluation reward: 2.46\n",
      "episode: 1440   score: 6.0   memory length: 275231   epsilon: 0.8265203200037661    steps: 345     evaluation reward: 2.51\n",
      "episode: 1441   score: 2.0   memory length: 275432   epsilon: 0.8263213300037704    steps: 201     evaluation reward: 2.5\n",
      "episode: 1442   score: 4.0   memory length: 275734   epsilon: 0.8260223500037769    steps: 302     evaluation reward: 2.53\n",
      "episode: 1443   score: 3.0   memory length: 275971   epsilon: 0.825787720003782    steps: 237     evaluation reward: 2.5\n",
      "episode: 1444   score: 2.0   memory length: 276158   epsilon: 0.825602590003786    steps: 187     evaluation reward: 2.5\n",
      "episode: 1445   score: 1.0   memory length: 276347   epsilon: 0.8254154800037901    steps: 189     evaluation reward: 2.49\n",
      "episode: 1446   score: 1.0   memory length: 276504   epsilon: 0.8252600500037934    steps: 157     evaluation reward: 2.48\n",
      "episode: 1447   score: 1.0   memory length: 276663   epsilon: 0.8251026400037969    steps: 159     evaluation reward: 2.44\n",
      "episode: 1448   score: 1.0   memory length: 276820   epsilon: 0.8249472100038002    steps: 157     evaluation reward: 2.43\n",
      "episode: 1449   score: 1.0   memory length: 276993   epsilon: 0.8247759400038039    steps: 173     evaluation reward: 2.42\n",
      "episode: 1450   score: 3.0   memory length: 277246   epsilon: 0.8245254700038094    steps: 253     evaluation reward: 2.43\n",
      "episode: 1451   score: 4.0   memory length: 277515   epsilon: 0.8242591600038152    steps: 269     evaluation reward: 2.43\n",
      "episode: 1452   score: 2.0   memory length: 277708   epsilon: 0.8240680900038193    steps: 193     evaluation reward: 2.4\n",
      "episode: 1453   score: 1.0   memory length: 277886   epsilon: 0.8238918700038231    steps: 178     evaluation reward: 2.38\n",
      "episode: 1454   score: 3.0   memory length: 278134   epsilon: 0.8236463500038285    steps: 248     evaluation reward: 2.41\n",
      "episode: 1455   score: 3.0   memory length: 278391   epsilon: 0.823391920003834    steps: 257     evaluation reward: 2.4\n",
      "episode: 1456   score: 2.0   memory length: 278592   epsilon: 0.8231929300038383    steps: 201     evaluation reward: 2.41\n",
      "episode: 1457   score: 3.0   memory length: 278830   epsilon: 0.8229573100038434    steps: 238     evaluation reward: 2.42\n",
      "episode: 1458   score: 4.0   memory length: 279126   epsilon: 0.8226642700038498    steps: 296     evaluation reward: 2.43\n",
      "episode: 1459   score: 6.0   memory length: 279501   epsilon: 0.8222930200038578    steps: 375     evaluation reward: 2.4\n",
      "episode: 1460   score: 5.0   memory length: 279829   epsilon: 0.8219683000038649    steps: 328     evaluation reward: 2.44\n",
      "episode: 1461   score: 3.0   memory length: 280057   epsilon: 0.8217425800038698    steps: 228     evaluation reward: 2.46\n",
      "episode: 1462   score: 2.0   memory length: 280264   epsilon: 0.8215376500038742    steps: 207     evaluation reward: 2.45\n",
      "episode: 1463   score: 1.0   memory length: 280431   epsilon: 0.8213723200038778    steps: 167     evaluation reward: 2.45\n",
      "episode: 1464   score: 5.0   memory length: 280768   epsilon: 0.8210386900038851    steps: 337     evaluation reward: 2.48\n",
      "episode: 1465   score: 3.0   memory length: 281006   epsilon: 0.8208030700038902    steps: 238     evaluation reward: 2.5\n",
      "episode: 1466   score: 2.0   memory length: 281214   epsilon: 0.8205971500038947    steps: 208     evaluation reward: 2.52\n",
      "episode: 1467   score: 2.0   memory length: 281399   epsilon: 0.8204140000038986    steps: 185     evaluation reward: 2.54\n",
      "episode: 1468   score: 3.0   memory length: 281653   epsilon: 0.8201625400039041    steps: 254     evaluation reward: 2.53\n",
      "episode: 1469   score: 3.0   memory length: 281885   epsilon: 0.8199328600039091    steps: 232     evaluation reward: 2.53\n",
      "episode: 1470   score: 3.0   memory length: 282103   epsilon: 0.8197170400039138    steps: 218     evaluation reward: 2.56\n",
      "episode: 1471   score: 3.0   memory length: 282335   epsilon: 0.8194873600039188    steps: 232     evaluation reward: 2.59\n",
      "episode: 1472   score: 7.0   memory length: 282747   epsilon: 0.8190794800039276    steps: 412     evaluation reward: 2.63\n",
      "episode: 1473   score: 5.0   memory length: 283108   epsilon: 0.8187220900039354    steps: 361     evaluation reward: 2.64\n",
      "episode: 1474   score: 4.0   memory length: 283389   epsilon: 0.8184439000039414    steps: 281     evaluation reward: 2.61\n",
      "episode: 1475   score: 3.0   memory length: 283642   epsilon: 0.8181934300039468    steps: 253     evaluation reward: 2.62\n",
      "episode: 1476   score: 1.0   memory length: 283822   epsilon: 0.8180152300039507    steps: 180     evaluation reward: 2.62\n",
      "episode: 1477   score: 2.0   memory length: 284023   epsilon: 0.817816240003955    steps: 201     evaluation reward: 2.64\n",
      "episode: 1478   score: 4.0   memory length: 284297   epsilon: 0.8175449800039609    steps: 274     evaluation reward: 2.66\n",
      "episode: 1479   score: 5.0   memory length: 284592   epsilon: 0.8172529300039673    steps: 295     evaluation reward: 2.7\n",
      "episode: 1480   score: 0.0   memory length: 284715   epsilon: 0.8171311600039699    steps: 123     evaluation reward: 2.69\n",
      "episode: 1481   score: 2.0   memory length: 284900   epsilon: 0.8169480100039739    steps: 185     evaluation reward: 2.69\n",
      "episode: 1482   score: 2.0   memory length: 285111   epsilon: 0.8167391200039784    steps: 211     evaluation reward: 2.68\n",
      "episode: 1483   score: 3.0   memory length: 285341   epsilon: 0.8165114200039834    steps: 230     evaluation reward: 2.67\n",
      "episode: 1484   score: 3.0   memory length: 285579   epsilon: 0.8162758000039885    steps: 238     evaluation reward: 2.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1485   score: 3.0   memory length: 285821   epsilon: 0.8160362200039937    steps: 242     evaluation reward: 2.67\n",
      "episode: 1486   score: 7.0   memory length: 286239   epsilon: 0.8156224000040027    steps: 418     evaluation reward: 2.7\n",
      "episode: 1487   score: 4.0   memory length: 286529   epsilon: 0.8153353000040089    steps: 290     evaluation reward: 2.67\n",
      "episode: 1488   score: 4.0   memory length: 286793   epsilon: 0.8150739400040146    steps: 264     evaluation reward: 2.7\n",
      "episode: 1489   score: 1.0   memory length: 286949   epsilon: 0.8149195000040179    steps: 156     evaluation reward: 2.68\n",
      "episode: 1490   score: 7.0   memory length: 287339   epsilon: 0.8145334000040263    steps: 390     evaluation reward: 2.68\n",
      "episode: 1491   score: 1.0   memory length: 287508   epsilon: 0.8143660900040299    steps: 169     evaluation reward: 2.67\n",
      "episode: 1492   score: 2.0   memory length: 287714   epsilon: 0.8141621500040344    steps: 206     evaluation reward: 2.67\n",
      "episode: 1493   score: 3.0   memory length: 287959   epsilon: 0.8139196000040396    steps: 245     evaluation reward: 2.68\n",
      "episode: 1494   score: 3.0   memory length: 288226   epsilon: 0.8136552700040454    steps: 267     evaluation reward: 2.7\n",
      "episode: 1495   score: 3.0   memory length: 288495   epsilon: 0.8133889600040511    steps: 269     evaluation reward: 2.67\n",
      "episode: 1496   score: 3.0   memory length: 288730   epsilon: 0.8131563100040562    steps: 235     evaluation reward: 2.67\n",
      "episode: 1497   score: 6.0   memory length: 289063   epsilon: 0.8128266400040633    steps: 333     evaluation reward: 2.73\n",
      "episode: 1498   score: 3.0   memory length: 289302   epsilon: 0.8125900300040685    steps: 239     evaluation reward: 2.74\n",
      "episode: 1499   score: 6.0   memory length: 289689   epsilon: 0.8122069000040768    steps: 387     evaluation reward: 2.79\n",
      "episode: 1500   score: 1.0   memory length: 289861   epsilon: 0.8120366200040805    steps: 172     evaluation reward: 2.8\n",
      "episode: 1501   score: 6.0   memory length: 290229   epsilon: 0.8116723000040884    steps: 368     evaluation reward: 2.83\n",
      "episode: 1502   score: 5.0   memory length: 290565   epsilon: 0.8113396600040956    steps: 336     evaluation reward: 2.87\n",
      "episode: 1503   score: 4.0   memory length: 290837   epsilon: 0.8110703800041015    steps: 272     evaluation reward: 2.9\n",
      "episode: 1504   score: 2.0   memory length: 291060   epsilon: 0.8108496100041063    steps: 223     evaluation reward: 2.91\n",
      "episode: 1505   score: 3.0   memory length: 291299   epsilon: 0.8106130000041114    steps: 239     evaluation reward: 2.91\n",
      "episode: 1506   score: 4.0   memory length: 291583   epsilon: 0.8103318400041175    steps: 284     evaluation reward: 2.91\n",
      "episode: 1507   score: 3.0   memory length: 291813   epsilon: 0.8101041400041225    steps: 230     evaluation reward: 2.91\n",
      "episode: 1508   score: 2.0   memory length: 292032   epsilon: 0.8098873300041272    steps: 219     evaluation reward: 2.92\n",
      "episode: 1509   score: 1.0   memory length: 292184   epsilon: 0.8097368500041304    steps: 152     evaluation reward: 2.87\n",
      "episode: 1510   score: 2.0   memory length: 292376   epsilon: 0.8095467700041346    steps: 192     evaluation reward: 2.87\n",
      "episode: 1511   score: 0.0   memory length: 292510   epsilon: 0.8094141100041374    steps: 134     evaluation reward: 2.85\n",
      "episode: 1512   score: 4.0   memory length: 292765   epsilon: 0.8091616600041429    steps: 255     evaluation reward: 2.87\n",
      "episode: 1513   score: 8.0   memory length: 293194   epsilon: 0.8087369500041521    steps: 429     evaluation reward: 2.94\n",
      "episode: 1514   score: 7.0   memory length: 293607   epsilon: 0.808328080004161    steps: 413     evaluation reward: 3.01\n",
      "episode: 1515   score: 4.0   memory length: 293869   epsilon: 0.8080687000041666    steps: 262     evaluation reward: 3.02\n",
      "episode: 1516   score: 0.0   memory length: 294000   epsilon: 0.8079390100041695    steps: 131     evaluation reward: 2.97\n",
      "episode: 1517   score: 2.0   memory length: 294208   epsilon: 0.8077330900041739    steps: 208     evaluation reward: 2.96\n",
      "episode: 1518   score: 3.0   memory length: 294436   epsilon: 0.8075073700041788    steps: 228     evaluation reward: 2.98\n",
      "episode: 1519   score: 5.0   memory length: 294747   epsilon: 0.8071994800041855    steps: 311     evaluation reward: 2.98\n",
      "episode: 1520   score: 4.0   memory length: 295038   epsilon: 0.8069113900041918    steps: 291     evaluation reward: 2.98\n",
      "episode: 1521   score: 5.0   memory length: 295372   epsilon: 0.8065807300041989    steps: 334     evaluation reward: 3.0\n",
      "episode: 1522   score: 3.0   memory length: 295605   epsilon: 0.806350060004204    steps: 233     evaluation reward: 3.02\n",
      "episode: 1523   score: 7.0   memory length: 296004   epsilon: 0.8059550500042125    steps: 399     evaluation reward: 3.06\n",
      "episode: 1524   score: 3.0   memory length: 296230   epsilon: 0.8057313100042174    steps: 226     evaluation reward: 3.08\n",
      "episode: 1525   score: 3.0   memory length: 296451   epsilon: 0.8055125200042221    steps: 221     evaluation reward: 3.09\n",
      "episode: 1526   score: 2.0   memory length: 296649   epsilon: 0.8053165000042264    steps: 198     evaluation reward: 3.06\n",
      "episode: 1527   score: 1.0   memory length: 296804   epsilon: 0.8051630500042297    steps: 155     evaluation reward: 3.05\n",
      "episode: 1528   score: 3.0   memory length: 297060   epsilon: 0.8049096100042352    steps: 256     evaluation reward: 3.07\n",
      "episode: 1529   score: 1.0   memory length: 297245   epsilon: 0.8047264600042392    steps: 185     evaluation reward: 3.07\n",
      "episode: 1530   score: 4.0   memory length: 297503   epsilon: 0.8044710400042447    steps: 258     evaluation reward: 3.09\n",
      "episode: 1531   score: 1.0   memory length: 297690   epsilon: 0.8042859100042488    steps: 187     evaluation reward: 3.03\n",
      "episode: 1532   score: 1.0   memory length: 297865   epsilon: 0.8041126600042525    steps: 175     evaluation reward: 3.03\n",
      "episode: 1533   score: 3.0   memory length: 298117   epsilon: 0.8038631800042579    steps: 252     evaluation reward: 3.04\n",
      "episode: 1534   score: 5.0   memory length: 298432   epsilon: 0.8035513300042647    steps: 315     evaluation reward: 3.06\n",
      "episode: 1535   score: 1.0   memory length: 298585   epsilon: 0.803399860004268    steps: 153     evaluation reward: 3.03\n",
      "episode: 1536   score: 0.0   memory length: 298711   epsilon: 0.8032751200042707    steps: 126     evaluation reward: 3.03\n",
      "episode: 1537   score: 1.0   memory length: 298872   epsilon: 0.8031157300042742    steps: 161     evaluation reward: 3.04\n",
      "episode: 1538   score: 7.0   memory length: 299272   epsilon: 0.8027197300042828    steps: 400     evaluation reward: 3.09\n",
      "episode: 1539   score: 1.0   memory length: 299428   epsilon: 0.8025652900042861    steps: 156     evaluation reward: 3.07\n",
      "episode: 1540   score: 3.0   memory length: 299675   epsilon: 0.8023207600042914    steps: 247     evaluation reward: 3.04\n",
      "episode: 1541   score: 4.0   memory length: 299979   epsilon: 0.802019800004298    steps: 304     evaluation reward: 3.06\n",
      "now time :  2018-12-12 11:16:45.132892\n",
      "episode: 1542   score: 3.0   memory length: 300218   epsilon: 0.8017831900043031    steps: 239     evaluation reward: 3.05\n",
      "episode: 1543   score: 2.0   memory length: 300405   epsilon: 0.8015980600043071    steps: 187     evaluation reward: 3.04\n",
      "episode: 1544   score: 6.0   memory length: 300757   epsilon: 0.8012495800043147    steps: 352     evaluation reward: 3.08\n",
      "episode: 1545   score: 3.0   memory length: 301020   epsilon: 0.8009892100043203    steps: 263     evaluation reward: 3.1\n",
      "episode: 1546   score: 2.0   memory length: 301209   epsilon: 0.8008021000043244    steps: 189     evaluation reward: 3.11\n",
      "episode: 1547   score: 2.0   memory length: 301397   epsilon: 0.8006159800043284    steps: 188     evaluation reward: 3.12\n",
      "episode: 1548   score: 2.0   memory length: 301584   epsilon: 0.8004308500043325    steps: 187     evaluation reward: 3.13\n",
      "episode: 1549   score: 5.0   memory length: 301898   epsilon: 0.8001199900043392    steps: 314     evaluation reward: 3.17\n",
      "episode: 1550   score: 5.0   memory length: 302212   epsilon: 0.799809130004346    steps: 314     evaluation reward: 3.19\n",
      "episode: 1551   score: 6.0   memory length: 302569   epsilon: 0.7994557000043536    steps: 357     evaluation reward: 3.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1552   score: 1.0   memory length: 302744   epsilon: 0.7992824500043574    steps: 175     evaluation reward: 3.2\n",
      "episode: 1553   score: 2.0   memory length: 302930   epsilon: 0.7990983100043614    steps: 186     evaluation reward: 3.21\n",
      "episode: 1554   score: 2.0   memory length: 303137   epsilon: 0.7988933800043658    steps: 207     evaluation reward: 3.2\n",
      "episode: 1555   score: 2.0   memory length: 303364   epsilon: 0.7986686500043707    steps: 227     evaluation reward: 3.19\n",
      "episode: 1556   score: 5.0   memory length: 303664   epsilon: 0.7983716500043772    steps: 300     evaluation reward: 3.22\n",
      "episode: 1557   score: 5.0   memory length: 303978   epsilon: 0.7980607900043839    steps: 314     evaluation reward: 3.24\n",
      "episode: 1558   score: 3.0   memory length: 304194   epsilon: 0.7978469500043885    steps: 216     evaluation reward: 3.23\n",
      "episode: 1559   score: 3.0   memory length: 304415   epsilon: 0.7976281600043933    steps: 221     evaluation reward: 3.2\n",
      "episode: 1560   score: 2.0   memory length: 304609   epsilon: 0.7974361000043975    steps: 194     evaluation reward: 3.17\n",
      "episode: 1561   score: 5.0   memory length: 304924   epsilon: 0.7971242500044042    steps: 315     evaluation reward: 3.19\n",
      "episode: 1562   score: 7.0   memory length: 305337   epsilon: 0.7967153800044131    steps: 413     evaluation reward: 3.24\n",
      "episode: 1563   score: 1.0   memory length: 305514   epsilon: 0.7965401500044169    steps: 177     evaluation reward: 3.24\n",
      "episode: 1564   score: 2.0   memory length: 305725   epsilon: 0.7963312600044214    steps: 211     evaluation reward: 3.21\n",
      "episode: 1565   score: 2.0   memory length: 305924   epsilon: 0.7961342500044257    steps: 199     evaluation reward: 3.2\n",
      "episode: 1566   score: 5.0   memory length: 306227   epsilon: 0.7958342800044322    steps: 303     evaluation reward: 3.23\n",
      "episode: 1567   score: 2.0   memory length: 306431   epsilon: 0.7956323200044366    steps: 204     evaluation reward: 3.23\n",
      "episode: 1568   score: 4.0   memory length: 306696   epsilon: 0.7953699700044423    steps: 265     evaluation reward: 3.24\n",
      "episode: 1569   score: 1.0   memory length: 306858   epsilon: 0.7952095900044458    steps: 162     evaluation reward: 3.22\n",
      "episode: 1570   score: 0.0   memory length: 307002   epsilon: 0.7950670300044489    steps: 144     evaluation reward: 3.19\n",
      "episode: 1571   score: 3.0   memory length: 307244   epsilon: 0.7948274500044541    steps: 242     evaluation reward: 3.19\n",
      "episode: 1572   score: 3.0   memory length: 307466   epsilon: 0.7946076700044589    steps: 222     evaluation reward: 3.15\n",
      "episode: 1573   score: 6.0   memory length: 307852   epsilon: 0.7942255300044672    steps: 386     evaluation reward: 3.16\n",
      "episode: 1574   score: 2.0   memory length: 308066   epsilon: 0.7940136700044718    steps: 214     evaluation reward: 3.14\n",
      "episode: 1575   score: 1.0   memory length: 308225   epsilon: 0.7938562600044752    steps: 159     evaluation reward: 3.12\n",
      "episode: 1576   score: 0.0   memory length: 308355   epsilon: 0.793727560004478    steps: 130     evaluation reward: 3.11\n",
      "episode: 1577   score: 3.0   memory length: 308634   epsilon: 0.793451350004484    steps: 279     evaluation reward: 3.12\n",
      "episode: 1578   score: 4.0   memory length: 308933   epsilon: 0.7931553400044904    steps: 299     evaluation reward: 3.12\n",
      "episode: 1579   score: 5.0   memory length: 309263   epsilon: 0.7928286400044975    steps: 330     evaluation reward: 3.12\n",
      "episode: 1580   score: 2.0   memory length: 309471   epsilon: 0.792622720004502    steps: 208     evaluation reward: 3.14\n",
      "episode: 1581   score: 7.0   memory length: 309869   epsilon: 0.7922287000045105    steps: 398     evaluation reward: 3.19\n",
      "episode: 1582   score: 6.0   memory length: 310227   epsilon: 0.7918742800045182    steps: 358     evaluation reward: 3.23\n",
      "episode: 1583   score: 4.0   memory length: 310519   epsilon: 0.7915852000045245    steps: 292     evaluation reward: 3.24\n",
      "episode: 1584   score: 1.0   memory length: 310673   epsilon: 0.7914327400045278    steps: 154     evaluation reward: 3.22\n",
      "episode: 1585   score: 3.0   memory length: 310947   epsilon: 0.7911614800045337    steps: 274     evaluation reward: 3.22\n",
      "episode: 1586   score: 2.0   memory length: 311156   epsilon: 0.7909545700045382    steps: 209     evaluation reward: 3.17\n",
      "episode: 1587   score: 6.0   memory length: 311519   epsilon: 0.790595200004546    steps: 363     evaluation reward: 3.19\n",
      "episode: 1588   score: 5.0   memory length: 311852   epsilon: 0.7902655300045531    steps: 333     evaluation reward: 3.2\n",
      "episode: 1589   score: 4.0   memory length: 312099   epsilon: 0.7900210000045584    steps: 247     evaluation reward: 3.23\n",
      "episode: 1590   score: 9.0   memory length: 312524   epsilon: 0.7896002500045676    steps: 425     evaluation reward: 3.25\n",
      "episode: 1591   score: 3.0   memory length: 312773   epsilon: 0.7893537400045729    steps: 249     evaluation reward: 3.27\n",
      "episode: 1592   score: 5.0   memory length: 313074   epsilon: 0.7890557500045794    steps: 301     evaluation reward: 3.3\n",
      "episode: 1593   score: 3.0   memory length: 313288   epsilon: 0.788843890004584    steps: 214     evaluation reward: 3.3\n",
      "episode: 1594   score: 3.0   memory length: 313523   epsilon: 0.788611240004589    steps: 235     evaluation reward: 3.3\n",
      "episode: 1595   score: 1.0   memory length: 313683   epsilon: 0.7884528400045925    steps: 160     evaluation reward: 3.28\n",
      "episode: 1596   score: 4.0   memory length: 313955   epsilon: 0.7881835600045983    steps: 272     evaluation reward: 3.29\n",
      "episode: 1597   score: 4.0   memory length: 314246   epsilon: 0.7878954700046046    steps: 291     evaluation reward: 3.27\n",
      "episode: 1598   score: 1.0   memory length: 314411   epsilon: 0.7877321200046081    steps: 165     evaluation reward: 3.25\n",
      "episode: 1599   score: 2.0   memory length: 314611   epsilon: 0.7875341200046124    steps: 200     evaluation reward: 3.21\n",
      "episode: 1600   score: 2.0   memory length: 314814   epsilon: 0.7873331500046168    steps: 203     evaluation reward: 3.22\n",
      "episode: 1601   score: 4.0   memory length: 315091   epsilon: 0.7870589200046227    steps: 277     evaluation reward: 3.2\n",
      "episode: 1602   score: 7.0   memory length: 315521   epsilon: 0.786633220004632    steps: 430     evaluation reward: 3.22\n",
      "episode: 1603   score: 3.0   memory length: 315762   epsilon: 0.7863946300046372    steps: 241     evaluation reward: 3.21\n",
      "episode: 1604   score: 4.0   memory length: 316047   epsilon: 0.7861124800046433    steps: 285     evaluation reward: 3.23\n",
      "episode: 1605   score: 2.0   memory length: 316248   epsilon: 0.7859134900046476    steps: 201     evaluation reward: 3.22\n",
      "episode: 1606   score: 2.0   memory length: 316435   epsilon: 0.7857283600046516    steps: 187     evaluation reward: 3.2\n",
      "episode: 1607   score: 4.0   memory length: 316680   epsilon: 0.7854858100046569    steps: 245     evaluation reward: 3.21\n",
      "episode: 1608   score: 4.0   memory length: 316941   epsilon: 0.7852274200046625    steps: 261     evaluation reward: 3.23\n",
      "episode: 1609   score: 4.0   memory length: 317193   epsilon: 0.7849779400046679    steps: 252     evaluation reward: 3.26\n",
      "episode: 1610   score: 4.0   memory length: 317441   epsilon: 0.7847324200046732    steps: 248     evaluation reward: 3.28\n",
      "episode: 1611   score: 3.0   memory length: 317712   epsilon: 0.7844641300046791    steps: 271     evaluation reward: 3.31\n",
      "episode: 1612   score: 4.0   memory length: 317968   epsilon: 0.7842106900046846    steps: 256     evaluation reward: 3.31\n",
      "episode: 1613   score: 0.0   memory length: 318102   epsilon: 0.7840780300046875    steps: 134     evaluation reward: 3.23\n",
      "episode: 1614   score: 4.0   memory length: 318378   epsilon: 0.7838047900046934    steps: 276     evaluation reward: 3.2\n",
      "episode: 1615   score: 5.0   memory length: 318678   epsilon: 0.7835077900046998    steps: 300     evaluation reward: 3.21\n",
      "episode: 1616   score: 4.0   memory length: 318956   epsilon: 0.7832325700047058    steps: 278     evaluation reward: 3.25\n",
      "episode: 1617   score: 4.0   memory length: 319224   epsilon: 0.7829672500047116    steps: 268     evaluation reward: 3.27\n",
      "episode: 1618   score: 2.0   memory length: 319438   epsilon: 0.7827553900047162    steps: 214     evaluation reward: 3.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1619   score: 7.0   memory length: 319858   epsilon: 0.7823395900047252    steps: 420     evaluation reward: 3.28\n",
      "episode: 1620   score: 2.0   memory length: 320061   epsilon: 0.7821386200047296    steps: 203     evaluation reward: 3.26\n",
      "episode: 1621   score: 3.0   memory length: 320287   epsilon: 0.7819148800047344    steps: 226     evaluation reward: 3.24\n",
      "episode: 1622   score: 1.0   memory length: 320440   epsilon: 0.7817634100047377    steps: 153     evaluation reward: 3.22\n",
      "episode: 1623   score: 6.0   memory length: 320748   epsilon: 0.7814584900047443    steps: 308     evaluation reward: 3.21\n",
      "episode: 1624   score: 5.0   memory length: 321039   epsilon: 0.7811704000047506    steps: 291     evaluation reward: 3.23\n",
      "episode: 1625   score: 5.0   memory length: 321351   epsilon: 0.7808615200047573    steps: 312     evaluation reward: 3.25\n",
      "episode: 1626   score: 1.0   memory length: 321525   epsilon: 0.780689260004761    steps: 174     evaluation reward: 3.24\n",
      "episode: 1627   score: 3.0   memory length: 321777   epsilon: 0.7804397800047664    steps: 252     evaluation reward: 3.26\n",
      "episode: 1628   score: 1.0   memory length: 321936   epsilon: 0.7802823700047699    steps: 159     evaluation reward: 3.24\n",
      "episode: 1629   score: 2.0   memory length: 322165   epsilon: 0.7800556600047748    steps: 229     evaluation reward: 3.25\n",
      "episode: 1630   score: 2.0   memory length: 322351   epsilon: 0.7798715200047788    steps: 186     evaluation reward: 3.23\n",
      "episode: 1631   score: 1.0   memory length: 322528   epsilon: 0.7796962900047826    steps: 177     evaluation reward: 3.23\n",
      "episode: 1632   score: 3.0   memory length: 322750   epsilon: 0.7794765100047873    steps: 222     evaluation reward: 3.25\n",
      "episode: 1633   score: 6.0   memory length: 323130   epsilon: 0.7791003100047955    steps: 380     evaluation reward: 3.28\n",
      "episode: 1634   score: 1.0   memory length: 323288   epsilon: 0.7789438900047989    steps: 158     evaluation reward: 3.24\n",
      "episode: 1635   score: 3.0   memory length: 323524   epsilon: 0.778710250004804    steps: 236     evaluation reward: 3.26\n",
      "episode: 1636   score: 3.0   memory length: 323742   epsilon: 0.7784944300048087    steps: 218     evaluation reward: 3.29\n",
      "episode: 1637   score: 5.0   memory length: 324034   epsilon: 0.7782053500048149    steps: 292     evaluation reward: 3.33\n",
      "episode: 1638   score: 4.0   memory length: 324288   epsilon: 0.7779538900048204    steps: 254     evaluation reward: 3.3\n",
      "episode: 1639   score: 3.0   memory length: 324531   epsilon: 0.7777133200048256    steps: 243     evaluation reward: 3.32\n",
      "episode: 1640   score: 3.0   memory length: 324787   epsilon: 0.7774598800048311    steps: 256     evaluation reward: 3.32\n",
      "episode: 1641   score: 2.0   memory length: 324970   epsilon: 0.7772787100048351    steps: 183     evaluation reward: 3.3\n",
      "episode: 1642   score: 5.0   memory length: 325329   epsilon: 0.7769233000048428    steps: 359     evaluation reward: 3.32\n",
      "episode: 1643   score: 3.0   memory length: 325543   epsilon: 0.7767114400048474    steps: 214     evaluation reward: 3.33\n",
      "episode: 1644   score: 0.0   memory length: 325680   epsilon: 0.7765758100048503    steps: 137     evaluation reward: 3.27\n",
      "episode: 1645   score: 2.0   memory length: 325867   epsilon: 0.7763906800048543    steps: 187     evaluation reward: 3.26\n",
      "episode: 1646   score: 2.0   memory length: 326074   epsilon: 0.7761857500048588    steps: 207     evaluation reward: 3.26\n",
      "episode: 1647   score: 4.0   memory length: 326339   epsilon: 0.7759234000048645    steps: 265     evaluation reward: 3.28\n",
      "episode: 1648   score: 4.0   memory length: 326604   epsilon: 0.7756610500048702    steps: 265     evaluation reward: 3.3\n",
      "episode: 1649   score: 1.0   memory length: 326765   epsilon: 0.7755016600048736    steps: 161     evaluation reward: 3.26\n",
      "episode: 1650   score: 5.0   memory length: 327078   epsilon: 0.7751917900048804    steps: 313     evaluation reward: 3.26\n",
      "episode: 1651   score: 3.0   memory length: 327324   epsilon: 0.7749482500048857    steps: 246     evaluation reward: 3.23\n",
      "episode: 1652   score: 3.0   memory length: 327568   epsilon: 0.7747066900048909    steps: 244     evaluation reward: 3.25\n",
      "episode: 1653   score: 2.0   memory length: 327775   epsilon: 0.7745017600048953    steps: 207     evaluation reward: 3.25\n",
      "episode: 1654   score: 0.0   memory length: 327909   epsilon: 0.7743691000048982    steps: 134     evaluation reward: 3.23\n",
      "episode: 1655   score: 2.0   memory length: 328106   epsilon: 0.7741740700049025    steps: 197     evaluation reward: 3.23\n",
      "episode: 1656   score: 2.0   memory length: 328312   epsilon: 0.7739701300049069    steps: 206     evaluation reward: 3.2\n",
      "episode: 1657   score: 1.0   memory length: 328474   epsilon: 0.7738097500049104    steps: 162     evaluation reward: 3.16\n",
      "episode: 1658   score: 5.0   memory length: 328782   epsilon: 0.773504830004917    steps: 308     evaluation reward: 3.18\n",
      "episode: 1659   score: 3.0   memory length: 329010   epsilon: 0.7732791100049219    steps: 228     evaluation reward: 3.18\n",
      "episode: 1660   score: 2.0   memory length: 329220   epsilon: 0.7730712100049264    steps: 210     evaluation reward: 3.18\n",
      "episode: 1661   score: 3.0   memory length: 329460   epsilon: 0.7728336100049316    steps: 240     evaluation reward: 3.16\n",
      "episode: 1662   score: 4.0   memory length: 329763   epsilon: 0.7725336400049381    steps: 303     evaluation reward: 3.13\n",
      "episode: 1663   score: 2.0   memory length: 329963   epsilon: 0.7723356400049424    steps: 200     evaluation reward: 3.14\n",
      "episode: 1664   score: 2.0   memory length: 330163   epsilon: 0.7721376400049467    steps: 200     evaluation reward: 3.14\n",
      "episode: 1665   score: 5.0   memory length: 330512   epsilon: 0.7717921300049542    steps: 349     evaluation reward: 3.17\n",
      "episode: 1666   score: 3.0   memory length: 330775   epsilon: 0.7715317600049598    steps: 263     evaluation reward: 3.15\n",
      "episode: 1667   score: 1.0   memory length: 330944   epsilon: 0.7713644500049635    steps: 169     evaluation reward: 3.14\n",
      "episode: 1668   score: 4.0   memory length: 331255   epsilon: 0.7710565600049701    steps: 311     evaluation reward: 3.14\n",
      "episode: 1669   score: 2.0   memory length: 331441   epsilon: 0.7708724200049741    steps: 186     evaluation reward: 3.15\n",
      "episode: 1670   score: 1.0   memory length: 331604   epsilon: 0.7707110500049776    steps: 163     evaluation reward: 3.16\n",
      "episode: 1671   score: 3.0   memory length: 331835   epsilon: 0.7704823600049826    steps: 231     evaluation reward: 3.16\n",
      "episode: 1672   score: 4.0   memory length: 332114   epsilon: 0.7702061500049886    steps: 279     evaluation reward: 3.17\n",
      "episode: 1673   score: 2.0   memory length: 332327   epsilon: 0.7699952800049932    steps: 213     evaluation reward: 3.13\n",
      "episode: 1674   score: 3.0   memory length: 332576   epsilon: 0.7697487700049985    steps: 249     evaluation reward: 3.14\n",
      "episode: 1675   score: 3.0   memory length: 332804   epsilon: 0.7695230500050034    steps: 228     evaluation reward: 3.16\n",
      "episode: 1676   score: 3.0   memory length: 333047   epsilon: 0.7692824800050087    steps: 243     evaluation reward: 3.19\n",
      "episode: 1677   score: 7.0   memory length: 333410   epsilon: 0.7689231100050165    steps: 363     evaluation reward: 3.23\n",
      "episode: 1678   score: 2.0   memory length: 333604   epsilon: 0.7687310500050206    steps: 194     evaluation reward: 3.21\n",
      "episode: 1679   score: 2.0   memory length: 333836   epsilon: 0.7685013700050256    steps: 232     evaluation reward: 3.18\n",
      "episode: 1680   score: 3.0   memory length: 334087   epsilon: 0.768252880005031    steps: 251     evaluation reward: 3.19\n",
      "episode: 1681   score: 3.0   memory length: 334315   epsilon: 0.7680271600050359    steps: 228     evaluation reward: 3.15\n",
      "episode: 1682   score: 2.0   memory length: 334524   epsilon: 0.7678202500050404    steps: 209     evaluation reward: 3.11\n",
      "episode: 1683   score: 6.0   memory length: 334853   epsilon: 0.7674945400050475    steps: 329     evaluation reward: 3.13\n",
      "episode: 1684   score: 2.0   memory length: 335037   epsilon: 0.7673123800050514    steps: 184     evaluation reward: 3.14\n",
      "episode: 1685   score: 0.0   memory length: 335160   epsilon: 0.7671906100050541    steps: 123     evaluation reward: 3.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1686   score: 4.0   memory length: 335459   epsilon: 0.7668946000050605    steps: 299     evaluation reward: 3.13\n",
      "episode: 1687   score: 1.0   memory length: 335633   epsilon: 0.7667223400050642    steps: 174     evaluation reward: 3.08\n",
      "episode: 1688   score: 3.0   memory length: 335896   epsilon: 0.7664619700050699    steps: 263     evaluation reward: 3.06\n",
      "episode: 1689   score: 3.0   memory length: 336142   epsilon: 0.7662184300050752    steps: 246     evaluation reward: 3.05\n",
      "episode: 1690   score: 0.0   memory length: 336270   epsilon: 0.7660917100050779    steps: 128     evaluation reward: 2.96\n",
      "episode: 1691   score: 4.0   memory length: 336545   epsilon: 0.7658194600050838    steps: 275     evaluation reward: 2.97\n",
      "episode: 1692   score: 6.0   memory length: 336890   epsilon: 0.7654779100050912    steps: 345     evaluation reward: 2.98\n",
      "episode: 1693   score: 5.0   memory length: 337193   epsilon: 0.7651779400050978    steps: 303     evaluation reward: 3.0\n",
      "episode: 1694   score: 5.0   memory length: 337515   epsilon: 0.7648591600051047    steps: 322     evaluation reward: 3.02\n",
      "episode: 1695   score: 4.0   memory length: 337796   epsilon: 0.7645809700051107    steps: 281     evaluation reward: 3.05\n",
      "episode: 1696   score: 7.0   memory length: 338212   epsilon: 0.7641691300051197    steps: 416     evaluation reward: 3.08\n",
      "episode: 1697   score: 2.0   memory length: 338430   epsilon: 0.7639533100051243    steps: 218     evaluation reward: 3.06\n",
      "episode: 1698   score: 1.0   memory length: 338606   epsilon: 0.7637790700051281    steps: 176     evaluation reward: 3.06\n",
      "episode: 1699   score: 1.0   memory length: 338776   epsilon: 0.7636107700051318    steps: 170     evaluation reward: 3.05\n",
      "episode: 1700   score: 6.0   memory length: 339093   epsilon: 0.7632969400051386    steps: 317     evaluation reward: 3.09\n",
      "episode: 1701   score: 5.0   memory length: 339417   epsilon: 0.7629761800051456    steps: 324     evaluation reward: 3.1\n",
      "episode: 1702   score: 3.0   memory length: 339662   epsilon: 0.7627336300051508    steps: 245     evaluation reward: 3.06\n",
      "episode: 1703   score: 1.0   memory length: 339825   epsilon: 0.7625722600051543    steps: 163     evaluation reward: 3.04\n",
      "episode: 1704   score: 3.0   memory length: 340062   epsilon: 0.7623376300051594    steps: 237     evaluation reward: 3.03\n",
      "episode: 1705   score: 5.0   memory length: 340368   epsilon: 0.762034690005166    steps: 306     evaluation reward: 3.06\n",
      "episode: 1706   score: 1.0   memory length: 340534   epsilon: 0.7618703500051696    steps: 166     evaluation reward: 3.05\n",
      "episode: 1707   score: 6.0   memory length: 340890   epsilon: 0.7615179100051772    steps: 356     evaluation reward: 3.07\n",
      "episode: 1708   score: 1.0   memory length: 341050   epsilon: 0.7613595100051807    steps: 160     evaluation reward: 3.04\n",
      "episode: 1709   score: 2.0   memory length: 341234   epsilon: 0.7611773500051846    steps: 184     evaluation reward: 3.02\n",
      "episode: 1710   score: 3.0   memory length: 341463   epsilon: 0.7609506400051895    steps: 229     evaluation reward: 3.01\n",
      "episode: 1711   score: 3.0   memory length: 341676   epsilon: 0.7607397700051941    steps: 213     evaluation reward: 3.01\n",
      "episode: 1712   score: 3.0   memory length: 341908   epsilon: 0.7605100900051991    steps: 232     evaluation reward: 3.0\n",
      "episode: 1713   score: 7.0   memory length: 342320   epsilon: 0.760102210005208    steps: 412     evaluation reward: 3.07\n",
      "episode: 1714   score: 2.0   memory length: 342522   epsilon: 0.7599022300052123    steps: 202     evaluation reward: 3.05\n",
      "episode: 1715   score: 1.0   memory length: 342687   epsilon: 0.7597388800052158    steps: 165     evaluation reward: 3.01\n",
      "episode: 1716   score: 4.0   memory length: 342945   epsilon: 0.7594834600052214    steps: 258     evaluation reward: 3.01\n",
      "episode: 1717   score: 2.0   memory length: 343139   epsilon: 0.7592914000052255    steps: 194     evaluation reward: 2.99\n",
      "episode: 1718   score: 3.0   memory length: 343386   epsilon: 0.7590468700052309    steps: 247     evaluation reward: 3.0\n",
      "episode: 1719   score: 2.0   memory length: 343569   epsilon: 0.7588657000052348    steps: 183     evaluation reward: 2.95\n",
      "episode: 1720   score: 5.0   memory length: 343889   epsilon: 0.7585489000052417    steps: 320     evaluation reward: 2.98\n",
      "episode: 1721   score: 8.0   memory length: 344263   epsilon: 0.7581786400052497    steps: 374     evaluation reward: 3.03\n",
      "episode: 1722   score: 0.0   memory length: 344390   epsilon: 0.7580529100052524    steps: 127     evaluation reward: 3.02\n",
      "episode: 1723   score: 3.0   memory length: 344644   epsilon: 0.7578014500052579    steps: 254     evaluation reward: 2.99\n",
      "episode: 1724   score: 2.0   memory length: 344850   epsilon: 0.7575975100052623    steps: 206     evaluation reward: 2.96\n",
      "episode: 1725   score: 4.0   memory length: 345152   epsilon: 0.7572985300052688    steps: 302     evaluation reward: 2.95\n",
      "episode: 1726   score: 4.0   memory length: 345430   epsilon: 0.7570233100052748    steps: 278     evaluation reward: 2.98\n",
      "episode: 1727   score: 2.0   memory length: 345648   epsilon: 0.7568074900052795    steps: 218     evaluation reward: 2.97\n",
      "episode: 1728   score: 2.0   memory length: 345838   epsilon: 0.7566193900052836    steps: 190     evaluation reward: 2.98\n",
      "episode: 1729   score: 1.0   memory length: 346007   epsilon: 0.7564520800052872    steps: 169     evaluation reward: 2.97\n",
      "episode: 1730   score: 6.0   memory length: 346367   epsilon: 0.7560956800052949    steps: 360     evaluation reward: 3.01\n",
      "episode: 1731   score: 3.0   memory length: 346586   epsilon: 0.7558788700052996    steps: 219     evaluation reward: 3.03\n",
      "episode: 1732   score: 3.0   memory length: 346833   epsilon: 0.7556343400053049    steps: 247     evaluation reward: 3.03\n",
      "episode: 1733   score: 0.0   memory length: 346961   epsilon: 0.7555076200053077    steps: 128     evaluation reward: 2.97\n",
      "episode: 1734   score: 1.0   memory length: 347115   epsilon: 0.755355160005311    steps: 154     evaluation reward: 2.97\n",
      "episode: 1735   score: 3.0   memory length: 347367   epsilon: 0.7551056800053164    steps: 252     evaluation reward: 2.97\n",
      "episode: 1736   score: 2.0   memory length: 347576   epsilon: 0.7548987700053209    steps: 209     evaluation reward: 2.96\n",
      "episode: 1737   score: 5.0   memory length: 347870   epsilon: 0.7546077100053272    steps: 294     evaluation reward: 2.96\n",
      "episode: 1738   score: 1.0   memory length: 348034   epsilon: 0.7544453500053308    steps: 164     evaluation reward: 2.93\n",
      "episode: 1739   score: 6.0   memory length: 348422   epsilon: 0.7540612300053391    steps: 388     evaluation reward: 2.96\n",
      "episode: 1740   score: 3.0   memory length: 348690   epsilon: 0.7537959100053448    steps: 268     evaluation reward: 2.96\n",
      "episode: 1741   score: 4.0   memory length: 348947   epsilon: 0.7535414800053504    steps: 257     evaluation reward: 2.98\n",
      "episode: 1742   score: 6.0   memory length: 349317   epsilon: 0.7531751800053583    steps: 370     evaluation reward: 2.99\n",
      "episode: 1743   score: 5.0   memory length: 349635   epsilon: 0.7528603600053652    steps: 318     evaluation reward: 3.01\n",
      "episode: 1744   score: 5.0   memory length: 349956   epsilon: 0.7525425700053721    steps: 321     evaluation reward: 3.06\n",
      "now time :  2018-12-12 11:41:09.200127\n",
      "episode: 1745   score: 4.0   memory length: 350273   epsilon: 0.7522287400053789    steps: 317     evaluation reward: 3.08\n",
      "episode: 1746   score: 3.0   memory length: 350534   epsilon: 0.7519703500053845    steps: 261     evaluation reward: 3.09\n",
      "episode: 1747   score: 3.0   memory length: 350785   epsilon: 0.7517218600053899    steps: 251     evaluation reward: 3.08\n",
      "episode: 1748   score: 2.0   memory length: 351007   epsilon: 0.7515020800053946    steps: 222     evaluation reward: 3.06\n",
      "episode: 1749   score: 3.0   memory length: 351245   epsilon: 0.7512664600053998    steps: 238     evaluation reward: 3.08\n",
      "episode: 1750   score: 7.0   memory length: 351510   epsilon: 0.7510041100054055    steps: 265     evaluation reward: 3.1\n",
      "episode: 1751   score: 5.0   memory length: 351855   epsilon: 0.7506625600054129    steps: 345     evaluation reward: 3.12\n",
      "episode: 1752   score: 2.0   memory length: 352077   epsilon: 0.7504427800054176    steps: 222     evaluation reward: 3.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1753   score: 3.0   memory length: 352315   epsilon: 0.7502071600054228    steps: 238     evaluation reward: 3.12\n",
      "episode: 1754   score: 2.0   memory length: 352510   epsilon: 0.750014110005427    steps: 195     evaluation reward: 3.14\n",
      "episode: 1755   score: 4.0   memory length: 352799   epsilon: 0.7497280000054332    steps: 289     evaluation reward: 3.16\n",
      "episode: 1756   score: 5.0   memory length: 353122   epsilon: 0.7494082300054401    steps: 323     evaluation reward: 3.19\n",
      "episode: 1757   score: 4.0   memory length: 353391   epsilon: 0.7491419200054459    steps: 269     evaluation reward: 3.22\n",
      "episode: 1758   score: 3.0   memory length: 353643   epsilon: 0.7488924400054513    steps: 252     evaluation reward: 3.2\n",
      "episode: 1759   score: 5.0   memory length: 354010   epsilon: 0.7485291100054592    steps: 367     evaluation reward: 3.22\n",
      "episode: 1760   score: 4.0   memory length: 354275   epsilon: 0.7482667600054649    steps: 265     evaluation reward: 3.24\n",
      "episode: 1761   score: 2.0   memory length: 354461   epsilon: 0.7480826200054689    steps: 186     evaluation reward: 3.23\n",
      "episode: 1762   score: 3.0   memory length: 354676   epsilon: 0.7478697700054735    steps: 215     evaluation reward: 3.22\n",
      "episode: 1763   score: 3.0   memory length: 354894   epsilon: 0.7476539500054782    steps: 218     evaluation reward: 3.23\n",
      "episode: 1764   score: 6.0   memory length: 355304   epsilon: 0.747248050005487    steps: 410     evaluation reward: 3.27\n",
      "episode: 1765   score: 4.0   memory length: 355576   epsilon: 0.7469787700054928    steps: 272     evaluation reward: 3.26\n",
      "episode: 1766   score: 0.0   memory length: 355719   epsilon: 0.7468372000054959    steps: 143     evaluation reward: 3.23\n",
      "episode: 1767   score: 2.0   memory length: 355941   epsilon: 0.7466174200055007    steps: 222     evaluation reward: 3.24\n",
      "episode: 1768   score: 5.0   memory length: 356255   epsilon: 0.7463065600055074    steps: 314     evaluation reward: 3.25\n",
      "episode: 1769   score: 2.0   memory length: 356467   epsilon: 0.746096680005512    steps: 212     evaluation reward: 3.25\n",
      "episode: 1770   score: 2.0   memory length: 356658   epsilon: 0.7459075900055161    steps: 191     evaluation reward: 3.26\n",
      "episode: 1771   score: 3.0   memory length: 356898   epsilon: 0.7456699900055213    steps: 240     evaluation reward: 3.26\n",
      "episode: 1772   score: 3.0   memory length: 357156   epsilon: 0.7454145700055268    steps: 258     evaluation reward: 3.25\n",
      "episode: 1773   score: 8.0   memory length: 357601   epsilon: 0.7449740200055364    steps: 445     evaluation reward: 3.31\n",
      "episode: 1774   score: 4.0   memory length: 357886   epsilon: 0.7446918700055425    steps: 285     evaluation reward: 3.32\n",
      "episode: 1775   score: 6.0   memory length: 358254   epsilon: 0.7443275500055504    steps: 368     evaluation reward: 3.35\n",
      "episode: 1776   score: 0.0   memory length: 358388   epsilon: 0.7441948900055533    steps: 134     evaluation reward: 3.32\n",
      "episode: 1777   score: 2.0   memory length: 358596   epsilon: 0.7439889700055577    steps: 208     evaluation reward: 3.27\n",
      "episode: 1778   score: 1.0   memory length: 358752   epsilon: 0.7438345300055611    steps: 156     evaluation reward: 3.26\n",
      "episode: 1779   score: 3.0   memory length: 359027   epsilon: 0.743562280005567    steps: 275     evaluation reward: 3.27\n",
      "episode: 1780   score: 3.0   memory length: 359275   epsilon: 0.7433167600055723    steps: 248     evaluation reward: 3.27\n",
      "episode: 1781   score: 4.0   memory length: 359564   epsilon: 0.7430306500055786    steps: 289     evaluation reward: 3.28\n",
      "episode: 1782   score: 2.0   memory length: 359774   epsilon: 0.7428227500055831    steps: 210     evaluation reward: 3.28\n",
      "episode: 1783   score: 1.0   memory length: 359936   epsilon: 0.7426623700055865    steps: 162     evaluation reward: 3.23\n",
      "episode: 1784   score: 1.0   memory length: 360113   epsilon: 0.7424871400055904    steps: 177     evaluation reward: 3.22\n",
      "episode: 1785   score: 4.0   memory length: 360374   epsilon: 0.742228750005596    steps: 261     evaluation reward: 3.26\n",
      "episode: 1786   score: 1.0   memory length: 360555   epsilon: 0.7420495600055999    steps: 181     evaluation reward: 3.23\n",
      "episode: 1787   score: 7.0   memory length: 360950   epsilon: 0.7416585100056083    steps: 395     evaluation reward: 3.29\n",
      "episode: 1788   score: 2.0   memory length: 361142   epsilon: 0.7414684300056125    steps: 192     evaluation reward: 3.28\n",
      "episode: 1789   score: 10.0   memory length: 361671   epsilon: 0.7409447200056238    steps: 529     evaluation reward: 3.35\n",
      "episode: 1790   score: 2.0   memory length: 361896   epsilon: 0.7407219700056287    steps: 225     evaluation reward: 3.37\n",
      "episode: 1791   score: 0.0   memory length: 362033   epsilon: 0.7405863400056316    steps: 137     evaluation reward: 3.33\n",
      "episode: 1792   score: 1.0   memory length: 362192   epsilon: 0.740428930005635    steps: 159     evaluation reward: 3.28\n",
      "episode: 1793   score: 7.0   memory length: 362585   epsilon: 0.7400398600056435    steps: 393     evaluation reward: 3.3\n",
      "episode: 1794   score: 5.0   memory length: 362956   epsilon: 0.7396725700056515    steps: 371     evaluation reward: 3.3\n",
      "episode: 1795   score: 3.0   memory length: 363210   epsilon: 0.7394211100056569    steps: 254     evaluation reward: 3.29\n",
      "episode: 1796   score: 3.0   memory length: 363450   epsilon: 0.7391835100056621    steps: 240     evaluation reward: 3.25\n",
      "episode: 1797   score: 9.0   memory length: 363799   epsilon: 0.7388380000056696    steps: 349     evaluation reward: 3.32\n",
      "episode: 1798   score: 4.0   memory length: 364088   epsilon: 0.7385518900056758    steps: 289     evaluation reward: 3.35\n",
      "episode: 1799   score: 3.0   memory length: 364325   epsilon: 0.7383172600056809    steps: 237     evaluation reward: 3.37\n",
      "episode: 1800   score: 1.0   memory length: 364506   epsilon: 0.7381380700056848    steps: 181     evaluation reward: 3.32\n",
      "episode: 1801   score: 6.0   memory length: 364854   epsilon: 0.7377935500056922    steps: 348     evaluation reward: 3.33\n",
      "episode: 1802   score: 2.0   memory length: 365054   epsilon: 0.7375955500056965    steps: 200     evaluation reward: 3.32\n",
      "episode: 1803   score: 3.0   memory length: 365293   epsilon: 0.7373589400057017    steps: 239     evaluation reward: 3.34\n",
      "episode: 1804   score: 3.0   memory length: 365525   epsilon: 0.7371292600057067    steps: 232     evaluation reward: 3.34\n",
      "episode: 1805   score: 5.0   memory length: 365821   epsilon: 0.736836220005713    steps: 296     evaluation reward: 3.34\n",
      "episode: 1806   score: 2.0   memory length: 366019   epsilon: 0.7366402000057173    steps: 198     evaluation reward: 3.35\n",
      "episode: 1807   score: 3.0   memory length: 366249   epsilon: 0.7364125000057222    steps: 230     evaluation reward: 3.32\n",
      "episode: 1808   score: 3.0   memory length: 366481   epsilon: 0.7361828200057272    steps: 232     evaluation reward: 3.34\n",
      "episode: 1809   score: 4.0   memory length: 366758   epsilon: 0.7359085900057332    steps: 277     evaluation reward: 3.36\n",
      "episode: 1810   score: 4.0   memory length: 367068   epsilon: 0.7356016900057398    steps: 310     evaluation reward: 3.37\n",
      "episode: 1811   score: 4.0   memory length: 367354   epsilon: 0.735318550005746    steps: 286     evaluation reward: 3.38\n",
      "episode: 1812   score: 4.0   memory length: 367632   epsilon: 0.735043330005752    steps: 278     evaluation reward: 3.39\n",
      "episode: 1813   score: 5.0   memory length: 367935   epsilon: 0.7347433600057585    steps: 303     evaluation reward: 3.37\n",
      "episode: 1814   score: 4.0   memory length: 368215   epsilon: 0.7344661600057645    steps: 280     evaluation reward: 3.39\n",
      "episode: 1815   score: 3.0   memory length: 368436   epsilon: 0.7342473700057692    steps: 221     evaluation reward: 3.41\n",
      "episode: 1816   score: 3.0   memory length: 368657   epsilon: 0.734028580005774    steps: 221     evaluation reward: 3.4\n",
      "episode: 1817   score: 3.0   memory length: 368909   epsilon: 0.7337791000057794    steps: 252     evaluation reward: 3.41\n",
      "episode: 1818   score: 6.0   memory length: 369256   epsilon: 0.7334355700057869    steps: 347     evaluation reward: 3.44\n",
      "episode: 1819   score: 5.0   memory length: 369566   epsilon: 0.7331286700057935    steps: 310     evaluation reward: 3.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1820   score: 9.0   memory length: 369899   epsilon: 0.7327990000058007    steps: 333     evaluation reward: 3.51\n",
      "episode: 1821   score: 6.0   memory length: 370265   epsilon: 0.7324366600058085    steps: 366     evaluation reward: 3.49\n",
      "episode: 1822   score: 2.0   memory length: 370446   epsilon: 0.7322574700058124    steps: 181     evaluation reward: 3.51\n",
      "episode: 1823   score: 3.0   memory length: 370672   epsilon: 0.7320337300058173    steps: 226     evaluation reward: 3.51\n",
      "episode: 1824   score: 4.0   memory length: 370951   epsilon: 0.7317575200058233    steps: 279     evaluation reward: 3.53\n",
      "episode: 1825   score: 6.0   memory length: 371283   epsilon: 0.7314288400058304    steps: 332     evaluation reward: 3.55\n",
      "episode: 1826   score: 4.0   memory length: 371544   epsilon: 0.731170450005836    steps: 261     evaluation reward: 3.55\n",
      "episode: 1827   score: 5.0   memory length: 371860   epsilon: 0.7308576100058428    steps: 316     evaluation reward: 3.58\n",
      "episode: 1828   score: 3.0   memory length: 372150   epsilon: 0.730570510005849    steps: 290     evaluation reward: 3.59\n",
      "episode: 1829   score: 3.0   memory length: 372382   epsilon: 0.730340830005854    steps: 232     evaluation reward: 3.61\n",
      "episode: 1830   score: 4.0   memory length: 372681   epsilon: 0.7300448200058605    steps: 299     evaluation reward: 3.59\n",
      "episode: 1831   score: 1.0   memory length: 372840   epsilon: 0.7298874100058639    steps: 159     evaluation reward: 3.57\n",
      "episode: 1832   score: 2.0   memory length: 373037   epsilon: 0.7296923800058681    steps: 197     evaluation reward: 3.56\n",
      "episode: 1833   score: 5.0   memory length: 373357   epsilon: 0.729375580005875    steps: 320     evaluation reward: 3.61\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 10:\n",
    "                torch.save(agent.model, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
