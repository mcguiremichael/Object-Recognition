{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from rnn.helpers import time_since\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language recognition with an RNN\n",
    "\n",
    "If you've ever used an online translator you've probably seen a feature that automatically detects the input language. While this might be easy to do if you input unicode characters that are unique to one or a small group of languages (like \"你好\" or \"γεια σας\"), this problem is more challenging if the input only uses the available ASCII characters. In this case, something like \"těší mě\" would beome \"tesi me\" in the ascii form. This is a more challenging problem in which the language must be recognized purely by the pattern of characters rather than unique unicode characters.\n",
    "\n",
    "We will train an RNN to solve this problem for a small set of languages thta can be converted to romanized ASCII form. For training data it would be ideal to have a large and varied dataset in different language styles. However, it is easy to find copies of the Bible which is a large text translated to different languages but in the same easily parsable format, so we will use 20 different copies of the Bible as training data. Using the same book for all of the different languages will hopefully prevent minor overfitting that might arise if we used different books for each language (fitting to common characteristics of the individual books rather than the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesi me\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode as unicodeToAscii\n",
    "\n",
    "all_characters = string.printable\n",
    "n_letters = len(all_characters)\n",
    "\n",
    "print(unicodeToAscii('těší mě'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readFile(filename):\n",
    "    data = open(filename, encoding='utf-8').read().strip()\n",
    "    return unicodeToAscii(data)\n",
    "\n",
    "def get_category_data(data_path):\n",
    "    # Build the category_data dictionary, a list of names per language\n",
    "    category_data = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(data_path):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0].split('_')[0]\n",
    "        all_categories.append(category)\n",
    "        data = readFile(filename)\n",
    "        category_data[category] = data\n",
    "    \n",
    "    return category_data, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text is split into two parts, train and test, so that we can make sure that the model is not simply memorizing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['czech', 'spanish', 'vietnamese', 'esperanto', 'finnish', 'romanian', 'portuguese', 'hungarian', 'german', 'italian', 'maori', 'turkish', 'english', 'xhosa', 'danish', 'french', 'swedish', 'albanian', 'norwegian', 'lithuanian']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'language_data/train/*_train.txt'\n",
    "test_data_path = 'language_data/test/*_test.txt'\n",
    "\n",
    "train_category_data, all_categories = get_category_data(train_data_path)\n",
    "test_category_data, test_all_categories = get_category_data(test_data_path)\n",
    "\n",
    "n_languages = len(all_categories)\n",
    "\n",
    "print(len(all_categories))\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1, dim=1)\n",
    "    category_i = top_i[:, 0]\n",
    "    return category_i\n",
    "\n",
    "# Turn string into long tensor\n",
    "def stringToTensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, 1).long().to(device)\n",
    "    input_text = []\n",
    "    for i in range(batch_size):\n",
    "        category = all_categories[random.randint(0, len(all_categories) - 1)]\n",
    "        line_start = random.randint(0, len(text[category])-chunk_len)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line = text[category][line_start:line_start+chunk_len]\n",
    "        input_text.append(line)\n",
    "        input_data[i] = stringToTensor(line)\n",
    "        target[i] = category_tensor\n",
    "    return input_data, target, input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Model\n",
    "====================\n",
    "\n",
    "For this classification task, we can use the same model we implement for the generation task which is located in `rnn/model.py`. See the `MP4_P2_generation.ipynb` notebook for more instructions. In this case each output vector of our RNN will have the dimension of the number of possible languages (i.e. `n_languages`). We will use this vector to predict a distribution over the languages.\n",
    "\n",
    "In the generation task, we used the output of the RNN at every time step to predict the next letter and our loss included the output from each of these predictions. However, in this task we use the output of the RNN at the end of the sequence to predict the language, so our loss function will use only the predicted output from the last time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 50\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "n_epochs = 3000\n",
    "hidden_size = 150\n",
    "n_layers = 2\n",
    "learning_rate = 0.0002\n",
    "model_type = 'lstm'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(n_letters, hidden_size, n_languages, model_type=model_type, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and the next hidden layer representation. The cross entropy loss should be computed over the last RNN output scores from the end of the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, target_tensor, data_tensor, optimizer, criterion, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - target_target: target character data tensor of shape (batch_size, 1)\n",
    "    - data_tensor: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    - batch_size: data batch size\n",
    "    \n",
    "    Returns:\n",
    "    - output: output from RNN from end of sequence \n",
    "    - loss: computed loss value as python float\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output, loss = None, None\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    batch_size, length = data_tensor.shape\n",
    "    \n",
    "    hidden_start = rnn.init_hidden(batch_size, device)\n",
    "    hidden = hidden_start\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(length):\n",
    "        \n",
    "        curr_input = data_tensor[:,i]\n",
    "        output, hidden = rnn(curr_input, hidden)\n",
    "        \n",
    "    output = output.view(batch_size, -1)\n",
    "    targ_vector = torch.LongTensor(batch_size, output.shape[1]).zero_().to(device)\n",
    "    targ_vector = targ_vector.scatter_(1, target_tensor.data, 1)\n",
    "    \n",
    "        \n",
    "    loss = criterion(output.view(batch_size, -1), target_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.data.cpu().numpy()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return output, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, data_tensor, seq_len=chunk_len, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = data_tensor.to(device)\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = rnn(data_tensor[:,i], hidden)\n",
    "        \n",
    "        return output.squeeze(0)\n",
    "    \n",
    "def eval_test(rnn, category_tensor, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, data_tensor).view(BATCH_SIZE, -1)\n",
    "        loss = criterion(output, category_tensor.squeeze())\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0% (0m 13s) 0.0328 0.0295  sanoi hanelle: \"Jos tahdot olla taydellinen, niin / finnish ✓\n",
      "Train accuracy: 0.9866\n",
      "100 1% (0m 26s) 0.0391 0.0341 e karapotia ana hoki a Rawiri ratou ko ana tangata / maori ✓\n",
      "Train accuracy: 0.98572\n",
      "150 1% (0m 39s) 0.0479 0.0397 myos muille kansoille. Nain sanoo Herra Juudan ja  / finnish ✓\n",
      "Train accuracy: 0.98812\n",
      "200 2% (0m 51s) 0.0266 0.0223 s and their wives together; for I will stretch out / english ✓\n",
      "Train accuracy: 0.98644\n",
      "250 2% (1m 4s) 0.0323 0.0394  Samuel kom till Saul, sade Saul till honom: >>Val / swedish ✓\n",
      "Train accuracy: 0.98724\n",
      "300 3% (1m 16s) 0.0571 0.0470 ux pleins de fine farine petrie a l'huile, pour l' / french ✓\n",
      "Train accuracy: 0.986\n",
      "350 3% (1m 29s) 0.0388 0.0367  vaimoonsa, on kuoleman oma.\" Eraana vuonna Iisak  / finnish ✓\n",
      "Train accuracy: 0.98636\n",
      "400 4% (1m 42s) 0.0427 0.0397  I leve. Hvorfor vil I do, du og ditt folk, ved sv / norwegian ✓\n",
      "Train accuracy: 0.98708\n",
      "450 4% (1m 54s) 0.0559 0.0498 bni et Shimei. Fils de Kehath: Amram, Jitsehar, He / german ✗ (french)\n",
      "Train accuracy: 0.98584\n",
      "500 5% (2m 7s) 0.0312 0.0304 ogxu tie, kaj li instruu al ili la legxojn de la D / esperanto ✓\n",
      "Train accuracy: 0.98728\n",
      "550 5% (2m 19s) 0.0414 0.0359 tou e Mohi ki tawahi o Horano whaka te rawhiti, ar / maori ✓\n",
      "Train accuracy: 0.98764\n",
      "600 6% (2m 32s) 0.0161 0.0157 aryanin, Sadok Ahituvun,<br />Sallum Sadokun, Hilk / turkish ✓\n",
      "Train accuracy: 0.98704\n",
      "650 6% (2m 44s) 0.0305 0.0277 bratri?\" Petr jim odpovedel: \"Obratte se a kazdy z / czech ✓\n",
      "Train accuracy: 0.98728\n",
      "700 7% (2m 57s) 0.0212 0.0180 awa; whakatikaia ta te tangata e tukinotia ana; wh / maori ✓\n",
      "Train accuracy: 0.98668\n",
      "750 7% (3m 9s) 0.0546 0.0460  nem peregrinara nela filho de homem. Eis que como / portuguese ✓\n",
      "Train accuracy: 0.98732\n",
      "800 8% (3m 22s) 0.0460 0.0443 rail Kralina bildiriyor.>> Aram Krali soyle buyurd / turkish ✓\n",
      "Train accuracy: 0.98856\n",
      "850 8% (3m 35s) 0.0276 0.0261 a sina livsdagar framlever han i morker; och mycke / swedish ✓\n",
      "Train accuracy: 0.98716\n",
      "900 9% (3m 47s) 0.0422 0.0355 aevre Port; thi mange, siger jeg eder, skulle soge / danish ✓\n",
      "Train accuracy: 0.98796\n",
      "950 9% (4m 0s) 0.0586 0.0565   -- Nouskaa, lahtekaa liikkeelle sotaan Edomia va / finnish ✓\n",
      "Train accuracy: 0.98772\n",
      "1000 10% (4m 12s) 0.0238 0.0195 rer ved fjellene, og de ryker. Jeg vil lovsynge He / norwegian ✓\n",
      "Train accuracy: 0.98876\n",
      "1050 10% (4m 25s) 0.0365 0.0325 o, e nao me temes? Eu publicarei essa justica tua; / portuguese ✓\n",
      "Train accuracy: 0.98772\n",
      "1100 11% (4m 39s) 0.0423 0.0383  okuya kubahlela abantu bakowenu ekupheleni kwemih / xhosa ✓\n",
      "Train accuracy: 0.98776\n",
      "1150 11% (4m 53s) 0.0298 0.0287 ajn nazotruojn kaj Mian busxbridajxon en vian busx / esperanto ✓\n",
      "Train accuracy: 0.98868\n",
      "1200 12% (5m 7s) 0.0174 0.0158 ova de los ejercitos: En este lugar desierto, sin  / spanish ✓\n",
      "Train accuracy: 0.98788\n",
      "1250 12% (5m 21s) 0.0375 0.0353 cho dem ay bi son se; Tieng vui cuoi cho vang ra t / vietnamese ✓\n",
      "Train accuracy: 0.98816\n",
      "1300 13% (5m 34s) 0.0425 0.0380 s coraitas. Eleazar, filho de Arao, tomou por mulh / portuguese ✓\n",
      "Train accuracy: 0.98824\n",
      "1350 13% (5m 48s) 0.0398 0.0382  da skall denne icke do genom sin faders missgarni / swedish ✓\n",
      "Train accuracy: 0.9874\n",
      "1400 14% (6m 2s) 0.0170 0.0135  die Hugel! Mein Freund gleicht einem Reh oder dem / german ✓\n",
      "Train accuracy: 0.99004\n",
      "1450 14% (6m 16s) 0.0369 0.0340  Israel, care isi poarta idolii in inima, si care  / romanian ✓\n",
      "Train accuracy: 0.9886\n",
      "1500 15% (6m 29s) 0.0211 0.0218 e kemi ndertuar nje altar per te mos e ndjekur me  / albanian ✓\n",
      "Train accuracy: 0.98804\n",
      "1550 15% (6m 43s) 0.0411 0.0359  katoa i mahue o nga Hiti, o nga Amori, o nga Peri / maori ✓\n",
      "Train accuracy: 0.98688\n",
      "1600 16% (6m 57s) 0.0247 0.0239 ora vingador dos seus atos. Exaltai o Senhor nosso / portuguese ✓\n",
      "Train accuracy: 0.9888\n",
      "1650 16% (7m 11s) 0.0235 0.0172 pruhy jeden s druhym, takze pribytek byl spojen v  / czech ✓\n",
      "Train accuracy: 0.98904\n",
      "1700 17% (7m 25s) 0.0251 0.0214 Rodder forneden og baerer sin Frugt foroven; thi f / danish ✓\n",
      "Train accuracy: 0.98804\n",
      "1750 17% (7m 38s) 0.0426 0.0418 nert valo aldozatat, ezt mondja az Ur Isten. Es le / hungarian ✓\n",
      "Train accuracy: 0.9894\n",
      "1800 18% (7m 52s) 0.0317 0.0313 Zoti rron, birit tend, nuk  do t'i bjere ne toke a / albanian ✓\n",
      "Train accuracy: 0.98872\n",
      "1850 18% (8m 6s) 0.0465 0.0412 lisi puuttunut, etta joku meikalaisista olisi maan / finnish ✓\n",
      "Train accuracy: 0.98872\n",
      "1900 19% (8m 20s) 0.0291 0.0247 r priesaika, tyletu, ji turi laikytis izado ir pri / lithuanian ✓\n",
      "Train accuracy: 0.98748\n",
      "1950 19% (8m 33s) 0.0324 0.0284 eis, terra que Moises, servo do Senhor, vos deu al / portuguese ✓\n",
      "Train accuracy: 0.98948\n",
      "2000 20% (8m 47s) 0.0278 0.0222 ad alta voce, dicendo: \"Benedetto sia l'Eterno, ch / italian ✓\n",
      "Train accuracy: 0.98932\n",
      "2050 20% (9m 1s) 0.0435 0.0403  e de andar sempre nos seus caminhos), entao acres / portuguese ✓\n",
      "Train accuracy: 0.98864\n",
      "2100 21% (9m 15s) 0.0329 0.0293 ter til Kvinden, medens hun sad ude pa Marken, men / danish ✓\n",
      "Train accuracy: 0.988\n",
      "2150 21% (9m 28s) 0.0305 0.0267  wakubafundi bakhe kuye, Nkosi, sifundise ukuthand / xhosa ✓\n",
      "Train accuracy: 0.98944\n",
      "2200 22% (9m 42s) 0.0347 0.0300 pravuje o rozum? Proc blyskas ocima? Svym duchem s / czech ✓\n",
      "Train accuracy: 0.9884\n",
      "2250 22% (9m 56s) 0.0141 0.0124 agt og det Edsforbund, HERREN din Gud.i Dag slutte / danish ✓\n",
      "Train accuracy: 0.98924\n",
      "2300 23% (10m 10s) 0.0480 0.0476 a le nto kakumkani; ngenxa yokuba akukho kumkani m / xhosa ✓\n",
      "Train accuracy: 0.9886\n",
      "2350 23% (10m 23s) 0.0156 0.0177 el af Aram; og Jorams Son, Kong Ahazja af Juda, dr / danish ✓\n",
      "Train accuracy: 0.98904\n",
      "2400 24% (10m 37s) 0.0470 0.0408 ithe te huajt dhe u paraqiten per te rrefyer mekat / albanian ✓\n",
      "Train accuracy: 0.98924\n",
      "2450 24% (10m 51s) 0.0284 0.0267  for Lejren, uden at bringe dem hen til Abenbaring / danish ✓\n",
      "Train accuracy: 0.98924\n",
      "2500 25% (11m 5s) 0.0302 0.0263  Ambos eram justos diante de Deus, andando irrepre / portuguese ✓\n",
      "Train accuracy: 0.98916\n",
      "2550 25% (11m 18s) 0.0426 0.0340 ikepen allasz ellene egyetlen helytartonak is, a k / hungarian ✓\n",
      "Train accuracy: 0.98896\n",
      "2600 26% (11m 32s) 0.0206 0.0175 remos nosotros a todo aquel que echo Jehova nuestr / spanish ✓\n",
      "Train accuracy: 0.98796\n",
      "2650 26% (11m 46s) 0.0247 0.0226 ua cac nguoi. Cac nguoi cho an vat chi co huyet, c / vietnamese ✓\n",
      "Train accuracy: 0.9894\n",
      "2700 27% (12m 0s) 0.0252 0.0241 e ali; e voi non avete voluto! Ecco, la vostra cas / italian ✓\n",
      "Train accuracy: 0.99012\n",
      "2750 27% (12m 13s) 0.0258 0.0250 . Kiam la dekreto eliris, oni komencis la mortigad / esperanto ✓\n",
      "Train accuracy: 0.98908\n",
      "2800 28% (12m 27s) 0.0298 0.0249 luna hozana!>> diye bagiran cocuklari gorunce ofke / turkish ✓\n",
      "Train accuracy: 0.98872\n",
      "2850 28% (12m 41s) 0.0395 0.0362 i Haradahut dhe fushuan ne Makeloth. U nisen nga M / albanian ✓\n",
      "Train accuracy: 0.98888\n",
      "2900 28% (12m 55s) 0.0361 0.0338 r gjengjeld! Men nar du gjor gjestebud, da be fatt / norwegian ✓\n",
      "Train accuracy: 0.98932\n",
      "2950 29% (13m 8s) 0.0364 0.0330 alula ubuso bezulu, ke yona imiqondiso yamaxesha a / xhosa ✓\n",
      "Train accuracy: 0.98856\n",
      "3000 30% (13m 22s) 0.0370 0.0371 m k domu a slucuji pole s polem, takze nezbyva zad / czech ✓\n",
      "Train accuracy: 0.99028\n",
      "3050 30% (13m 36s) 0.0412 0.0382 e kaha rawa koe i ahau, a kua taea tau: kua waiho  / maori ✓\n",
      "Train accuracy: 0.99008\n",
      "3100 31% (13m 49s) 0.0390 0.0360 ua ziduri de linga gradina imparatului, pe cind in / romanian ✓\n",
      "Train accuracy: 0.98968\n",
      "3150 31% (14m 3s) 0.0213 0.0211 que vosotros no sabeis de donde sea, y a mi me abr / spanish ✓\n",
      "Train accuracy: 0.99036\n",
      "3200 32% (14m 17s) 0.0193 0.0161 vereis al Hijo del hombre sentado a la diestra de  / spanish ✓\n",
      "Train accuracy: 0.99052\n",
      "3250 32% (14m 31s) 0.0245 0.0210 o, co nguoi Pha-ri-si va may thay day luat tu cac  / vietnamese ✓\n",
      "Train accuracy: 0.98884\n",
      "3300 33% (14m 44s) 0.0349 0.0302 d ramme dig. Jeg giver dig i deres Hand, og de ska / danish ✓\n",
      "Train accuracy: 0.98912\n",
      "3350 33% (14m 58s) 0.0374 0.0361 \"Mina ylistan sinua, Isa, taivaan ja maan Herra, s / finnish ✓\n",
      "Train accuracy: 0.98892\n",
      "3400 34% (15m 12s) 0.0365 0.0324  are mila de tine.`` ,,Nenorocito, batuto de furtu / romanian ✓\n",
      "Train accuracy: 0.98948\n",
      "3450 34% (15m 25s) 0.0161 0.0163 kwamehlo akho; kodwa yazi ukuba ngenxa yezo zinto  / xhosa ✓\n",
      "Train accuracy: 0.99044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 35% (15m 39s) 0.0293 0.0285  Dronet af mange Vande. Men truer han ad dem, flyg / danish ✓\n",
      "Train accuracy: 0.98948\n",
      "3550 35% (15m 53s) 0.0274 0.0226  to koutou tokomaha: kohikohia e tera, e tera, ma  / maori ✓\n",
      "Train accuracy: 0.98992\n",
      "3600 36% (16m 7s) 0.0376 0.0331 ia e whakahe ana i ta te tangata tika, he rite tah / maori ✓\n",
      "Train accuracy: 0.99032\n",
      "3650 36% (16m 21s) 0.0287 0.0279 hetheli-makhulu, abaveleli bempi leyo, wathi kubo, / xhosa ✓\n",
      "Train accuracy: 0.99008\n",
      "3700 37% (16m 36s) 0.0385 0.0339 vados e ovelhas em abundancia, e convidou a todos  / portuguese ✓\n",
      "Train accuracy: 0.9884\n",
      "3750 37% (16m 51s) 0.0289 0.0327 a ja sanoi miehelle: \"Ojenna katesi.\" Mies teki ni / finnish ✓\n",
      "Train accuracy: 0.98956\n",
      "3800 38% (17m 5s) 0.0262 0.0217 or condotta, le loro trasgressioni, giacche si son / italian ✓\n",
      "Train accuracy: 0.98972\n",
      "3850 38% (17m 19s) 0.0286 0.0230  afraid here in Judah: how much more then if we go / english ✓\n",
      "Train accuracy: 0.99072\n",
      "3900 39% (17m 34s) 0.0196 0.0175 indleko, ukuba unazo na izinto zokuyigqiba? Hleze, / xhosa ✓\n",
      "Train accuracy: 0.98928\n",
      "3950 39% (17m 47s) 0.0274 0.0231 spodinovych bylo zle, co David spachal. Hospodin p / czech ✓\n",
      "Train accuracy: 0.99028\n"
     ]
    }
   ],
   "source": [
    "n_iters = 10000 ######2000 #100000\n",
    "print_every = 50\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "current_test_loss = 0\n",
    "all_losses = []\n",
    "all_test_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate/2)\n",
    "\n",
    "\n",
    "number_correct = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_data, target_category, text_data = load_random_batch(train_category_data, chunk_len, BATCH_SIZE)\n",
    "    output, loss = train(rnn, target_category, input_data, optimizer, criterion)\n",
    "    current_loss += loss\n",
    "    \n",
    "    _, test_loss = eval_test(rnn, target_category, input_data)\n",
    "    current_test_loss += test_loss\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    number_correct += (target_category.squeeze()==guess_i.squeeze()).long().sum()\n",
    "    \n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        sample_idx = 0\n",
    "        guess = all_categories[guess_i[sample_idx]]\n",
    "        \n",
    "        category = all_categories[int(target_category[sample_idx])]\n",
    "        \n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, test_loss, text_data[sample_idx], guess, correct))\n",
    "        print('Train accuracy: {}'.format(float(number_correct)/float(print_every*BATCH_SIZE)))\n",
    "        number_correct = 0\n",
    "    \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_test_losses.append(current_test_loss / plot_every)\n",
    "        current_test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses, color='b')\n",
    "plt.plot(all_test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results\n",
    "-------------------\n",
    "\n",
    "We now vizualize the performance of our model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.\n",
    "\n",
    "In this evaluation we consider sequences of variable sizes rather than the fixed length sequences we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_languages, n_languages)\n",
    "n_confusion = 1000\n",
    "num_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "    input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "    for j in range(eval_batch_size):\n",
    "        category = all_categories[category_i[j]] \n",
    "        confusion[category_i[j]][guess_i[j]] += 1\n",
    "        num_correct += int(guess_i[j]==category_i[j])\n",
    "        total += 1\n",
    "\n",
    "print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_languages):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on User Input\n",
    "---------------------\n",
    "\n",
    "Now you can test your model on your own input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> This is a phrase to test the model on user input\n",
      "(7.58) english\n",
      "(2.91) spanish\n",
      "(2.29) albanian\n",
      "(2.15) vietnamese\n",
      "(1.47) french\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=5):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        input_data = stringToTensor(input_line).long().unsqueeze(0).to(device)\n",
    "        output = evaluate(rnn, input_data, seq_len=len(input_line), batch_size=1)\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.topk(n_predictions, dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        topv.shape\n",
    "        topi.shape\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('This is a phrase to test the model on user input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kaggle submission file\n",
    "\n",
    "Once you have found a good set of hyperparameters submit the output of your model on the Kaggle test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE KAGGLE SUBMISSION CODE ####\n",
    "import csv\n",
    "\n",
    "kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "with open(kaggle_test_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output_rows = []\n",
    "for i, line in enumerate(lines):\n",
    "    sample = line.rstrip()\n",
    "    sample_chunk_len = len(sample)\n",
    "    input_data = stringToTensor(sample).unsqueeze(0)\n",
    "    output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "    guess_i = categoryFromOutput(output)\n",
    "    output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "submission_file_path = 'kaggle_rnn_submission.txt'\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    output_rows = [('id', 'category')] + output_rows\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
